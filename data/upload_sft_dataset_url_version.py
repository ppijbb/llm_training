from datasets import load_dataset, concatenate_datasets, Dataset, Features, Value, Sequence, load_from_disk, DatasetDict
import json
from typing import List, Dict, Any, Optional, cast
from tqdm.auto import tqdm
import os
import requests
from urllib.parse import urlparse
import hashlib
import gc
import datetime
import argparse
import sys
import pandas as pd
import tempfile
import shutil

# ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ ëª©ë¡
dataset_configs = [
    ("HuggingFaceTB/smoltalk", "all"),
    ("R0k1e/UltraLink", None),
    ("PrincetonPLI/Instruct-SkillMix-SDD", None),
    ("allenai/WildChat-1M", None),
    ("nvidia/OpenCodeInstruct", None),
    ("microsoft/orca-agentinstruct-1M-v1", "default"),
    ("MaziyarPanahi/Llama-Nemotron-Post-Training-Dataset-v1-ShareGPT", "default"),
    ("nvidia/Llama-Nemotron-Post-Training-Dataset", "SFT"),
    ("open-r1/Mixture-of-Thoughts", "all"),
    ("Salesforce/blip3-kale", "core"),
    ("liuhaotian/LLaVA-Instruct-150K", None),
    ("Lin-Chen/ShareGPT4V", "ShareGPT4V")
]

def construct_image_url(image_path, dataset_name):
    """ë°ì´í„°ì…‹ë³„ë¡œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì‹¤ì œ URLë¡œ ë³€í™˜"""
    if dataset_name == "Lin-Chen/ShareGPT4V":
        if image_path.startswith('coco/'):
            filename = os.path.basename(image_path)
            return f"http://images.cocodataset.org/train2017/{filename}"
    elif dataset_name == "liuhaotian/LLaVA-Instruct-150K":
        if not image_path.startswith('http'):
            return f"http://images.cocodataset.org/train2017/{image_path}"
    
    return None

def validate_image_url(image_url):
    """ì´ë¯¸ì§€ URLì´ ìœ íš¨í•œì§€ ê°„ë‹¨íˆ í™•ì¸"""
    try:
        parsed = urlparse(image_url)
        return bool(parsed.scheme) and bool(parsed.netloc)
    except:
        return False

def convert_to_target_format_url(sample: Dict[str, Any], dataset_name: str) -> Optional[Dict[str, Any]]:
    """
    ê° ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œì„ ëª©í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (URL ë°©ì‹)
    ì´ë¯¸ì§€ëŠ” ì‹¤ì œ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šê³  URLë§Œ ì €ì¥í•©ë‹ˆë‹¤.
    
    ëª©í‘œ í˜•ì‹:
    {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "ì§ˆë¬¸"},
                    {"type": "image_url", "image_url": {"url": "https://..."}}
                ]
            },
            {
                "role": "assistant", 
                "content": [{"type": "text", "text": "ë‹µë³€"}]
            }
        ],
        "source_dataset": "dataset_name",
        "original_data": {...}
    }
    """
    
    result: Dict[str, Any] = {
        "messages": [],
        "source_dataset": dataset_name,
        "original_data": sample.copy()
    }
    
    try:
        # í…ìŠ¤íŠ¸ ì „ìš© ë°ì´í„°ì…‹ë“¤ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
        if dataset_name == "HuggingFaceTB/smoltalk":
            if "messages" in sample and isinstance(sample["messages"], list):
                for msg in sample["messages"]:
                    if isinstance(msg, dict) and "role" in msg and "content" in msg:
                        result["messages"].append({
                            "role": msg["role"],
                            "content": [{"type": "text", "text": str(msg["content"])}]
                        })
        
        elif dataset_name == "R0k1e/UltraLink":
            if "data" in sample and isinstance(sample["data"], list) and len(sample["data"]) >= 2:
                data = sample["data"]
                for i in range(0, len(data), 2):
                    if i + 1 < len(data):
                        result["messages"].extend([
                            {"role": "user", "content": [{"type": "text", "text": str(data[i])}]},
                            {"role": "assistant", "content": [{"type": "text", "text": str(data[i + 1])}]}
                        ])
        
        # ... ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ë“¤ë„ ë™ì¼ ...
        
        # ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ë“¤ ì²˜ë¦¬ (URL ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
        elif dataset_name in ["Lin-Chen/ShareGPT4V", "liuhaotian/LLaVA-Instruct-150K"]:
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_url = None
            if "image" in sample and sample["image"] is not None:
                if isinstance(sample["image"], str):
                    if sample["image"].startswith('http'):
                        image_url = sample["image"]
                    else:
                        image_url = construct_image_url(sample["image"], dataset_name)
            elif "images" in sample and sample["images"] is not None:
                if isinstance(sample["images"], list) and len(sample["images"]) > 0:
                    img_path = sample["images"][0]
                    if isinstance(img_path, str):
                        if img_path.startswith('http'):
                            image_url = img_path
                        else:
                            image_url = construct_image_url(img_path, dataset_name)
                elif isinstance(sample["images"], str):
                    if sample["images"].startswith('http'):
                        image_url = sample["images"]
                    else:
                        image_url = construct_image_url(sample["images"], dataset_name)
            
            # URL ìœ íš¨ì„± ê²€ì‚¬
            if image_url and not validate_image_url(image_url):
                image_url = None
            
            # conversations ì²˜ë¦¬
            if "conversations" in sample and isinstance(sample["conversations"], list):
                for i, conv in enumerate(sample["conversations"]):
                    if not isinstance(conv, dict):
                        continue
                        
                    # role ê²°ì •
                    role = "assistant"
                    if "from" in conv:
                        if conv["from"] in ["human", "user"]:
                            role = "user"
                        elif conv["from"] in ["gpt", "assistant"]:
                            role = "assistant"
                    
                    # content ìƒì„±
                    content_list = []
                    text_content = str(conv.get("value", ""))
                    
                    if text_content:
                        # <image> íƒœê·¸ ì œê±°
                        text_content = text_content.replace("<image>", "").strip()
                        
                        if text_content:
                            content_list.append({
                                "type": "text",
                                "text": text_content
                            })
                    
                    # ì²« ë²ˆì§¸ user ë©”ì‹œì§€ì— ì´ë¯¸ì§€ URL ì¶”ê°€
                    if role == "user" and i == 0 and image_url:
                        content_list.append({
                            "type": "image_url",
                            "image_url": {"url": image_url}
                        })
                    
                    if content_list:
                        result["messages"].append({
                            "role": role,
                            "content": content_list
                        })
        
        elif dataset_name == "Salesforce/blip3-kale":
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_url = None
            if "url" in sample and sample["url"]:
                if validate_image_url(sample["url"]):
                    image_url = sample["url"]
            elif "image" in sample and sample["image"]:
                if isinstance(sample["image"], str) and validate_image_url(sample["image"]):
                    image_url = sample["image"]
            
            # captionì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            caption = str(sample.get("caption", "")).strip()
            if not caption:
                caption = str(sample.get("cogvlm_caption", "")).strip()
            
            if caption:
                user_content: List[Dict[str, Any]] = [{"type": "text", "text": "Describe this image."}]
                if image_url:
                    user_content.append({
                        "type": "image_url", 
                        "image_url": {"url": image_url}
                    })
                
                result["messages"] = [
                    {"role": "user", "content": user_content},
                    {"role": "assistant", "content": [{"type": "text", "text": caption}]}
                ]
        
        # ë¹ˆ messagesì¸ ê²½ìš° None ë°˜í™˜
        if not result["messages"]:
            return None
            
        return result
        
    except Exception as e:
        print(f"ìƒ˜í”Œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆë›°ê¸°): {dataset_name} - {str(e)}")
        return None

def process_dataset_url(dataset_name: str, config_name: Optional[str] = None, max_samples: Optional[int] = None):
    """ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ì—¬ ëª©í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (URL ë°©ì‹ - ë¹ ë¥¸ ì²˜ë¦¬)"""
    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ ë¡œì§ì€ ë™ì¼
        if dataset_name == "microsoft/orca-agentinstruct-1M-v1":
            split = "creative_content"
        elif dataset_name == "MaziyarPanahi/Llama-Nemotron-Post-Training-Dataset-v1-ShareGPT":
            split = "chat"
        elif dataset_name == "nvidia/Llama-Nemotron-Post-Training-Dataset":
            split = "chat"
        else:
            split = "train"
        
        try:
            if config_name:
                full_dataset = load_dataset(dataset_name, config_name, split=split, streaming=True)
            else:
                full_dataset = load_dataset(dataset_name, split=split, streaming=True)
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return

        success_count = 0
        total_count = 0
        
        desc = f"{dataset_name.split('/')[-1]}"
        if config_name:
            desc += f"({config_name})"
        
        progress_bar = tqdm(desc=desc, unit="samples", leave=False)
        
        batch = []
        batch_size = 1000  # URL ë°©ì‹ì€ ë¹ ë¥´ë¯€ë¡œ ë” í° ë°°ì¹˜ ì‚¬ìš©
        
        for sample in full_dataset:
            if max_samples and total_count >= max_samples:
                break
            
            # URL ë°©ì‹ì€ ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œë„ ì¶©ë¶„íˆ ë¹ ë¦„
            converted = convert_to_target_format_url(sample, dataset_name)
            if converted:
                batch.append(converted)
                success_count += 1
            
            total_count += 1
            progress_bar.update(1)
            progress_bar.set_postfix({
                "processed": f"{success_count}/{total_count}",
                "success_rate": f"{success_count/total_count*100:.1f}%"
            })
            
            # ë°°ì¹˜ê°€ ì°¼ìœ¼ë©´ yield
            if len(batch) >= batch_size:
                yield batch
                batch = []
        
        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch:
            yield batch
        
        progress_bar.close()
        yield f"âœ… {dataset_name}: {success_count}/{total_count} ìƒ˜í”Œ ë³€í™˜ ì™„ë£Œ (ì„±ê³µë¥ : {success_count/total_count*100:.1f}%)" if total_count > 0 else f"â„¹ï¸ {dataset_name}: ì²˜ë¦¬í•  ìƒ˜í”Œ ì—†ìŒ"

    except Exception as e:
        yield f"âŒ {dataset_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def merge_and_create_dataset_url(
    output_name: str = "unified-multimodal-sft-url", 
    max_samples_per_dataset: Optional[int] = None,
    local_path: str = "./"
):
    """URL ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ë³‘í•© (í›¨ì”¬ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    print(f"ğŸš€ URL ë°©ì‹ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ ë³‘í•© ì‹œì‘...")
    
    staging_dir = f"{local_path}/{output_name}_staging".replace("//", "/")
    os.makedirs(staging_dir, exist_ok=True)
    jsonl_path = os.path.join(staging_dir, "data.jsonl")
    
    total_samples = 0
    completion_messages = []

    # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ datetime í•¸ë“¤ëŸ¬
    def datetime_handler(x):
        if isinstance(x, datetime.datetime):
            return x.isoformat()
        raise TypeError(f"Object of type {type(x).__name__} is not JSON serializable")

    # JSONL íŒŒì¼ì— ì§ì ‘ ì €ì¥
    with open(jsonl_path, "w", encoding="utf-8") as f:
        dataset_progress = tqdm(dataset_configs, desc="ë°ì´í„°ì…‹ ì²˜ë¦¬", unit="dataset")

        for dataset_name, config_name in dataset_progress:
            dataset_progress.set_description(f"ì²˜ë¦¬ì¤‘: {dataset_name.split('/')[-1]}")
            try:
                for result in process_dataset_url(dataset_name, config_name, max_samples_per_dataset):
                    if isinstance(result, list):  # ë°°ì¹˜ ê²°ê³¼
                        for sample in result:
                            # original_dataë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                            try:
                                sample["original_data"] = json.dumps(sample["original_data"], ensure_ascii=False, default=str)
                            except (TypeError, OverflowError):
                                sample["original_data"] = "{}"

                            f.write(json.dumps(
                                sample, 
                                ensure_ascii=False, 
                                default=datetime_handler
                            ) + "\n")
                            total_samples += 1
                        
                        dataset_progress.set_postfix({"ì´ ìƒ˜í”Œ": total_samples})

                    elif isinstance(result, str):  # ì™„ë£Œ ë©”ì‹œì§€
                        completion_messages.append(result)
            except Exception as e:
                tqdm.write(f"âŒ {dataset_name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue

    dataset_progress.close()

    # ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥
    tqdm.write("\n" + "="*20 + " ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ " + "="*20)
    for msg in completion_messages:
        tqdm.write(msg)
    tqdm.write("="*55)

    if total_samples == 0:
        print("âŒ ë³€í™˜ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    tqdm.write(f"\nğŸ¯ ì´ {total_samples}ê°œ ìƒ˜í”Œ ë³€í™˜ ì™„ë£Œ")
    
    # Dataset ê°ì²´ ìƒì„±
    tqdm.write("ğŸ“¦ Dataset ê°ì²´ ìƒì„± ì¤‘...")

    # URL ë°©ì‹ì˜ ìŠ¤í‚¤ë§ˆ
    features = Features({
        'messages': Sequence(
            Features({
                'role': Value('string'),
                'content': Sequence(
                    Features({
                        'type': Value('string'),
                        'text': Value('string'),
                        'image_url': Features({
                            'url': Value('string')
                        })
                    })
                )
            })
        ),
        'source_dataset': Value('string'),
        'original_data': Value('string')
    })
    
    def data_generator():
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    record = json.loads(line.strip())
                    # content í•„ë“œ ì •ê·œí™”
                    if 'messages' in record:
                        for message in record['messages']:
                            if 'content' in message:
                                for content_item in message['content']:
                                    # text í•„ë“œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ì¶”ê°€
                                    if 'text' not in content_item:
                                        content_item['text'] = ""
                                    # image_urlì´ ì—†ìœ¼ë©´ ë¹ˆ êµ¬ì¡° ì¶”ê°€
                                    if 'image_url' not in content_item:
                                        content_item['image_url'] = {'url': ''}
                    yield record
                except json.JSONDecodeError:
                    continue
    
    # Dataset ìƒì„±
    dataset = Dataset.from_generator(data_generator, features=features)
    
    # ë¡œì»¬ ì €ì¥
    final_save_path = f"{local_path}/{output_name}".replace("//", "/")
    dataset.save_to_disk(final_save_path)
    
    tqdm.write(f"âœ… ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {final_save_path}")
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    shutil.rmtree(staging_dir, ignore_errors=True)
    
    return final_save_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="URL ë°©ì‹ ë©€í‹°ëª¨ë‹¬ í†µí•© ë°ì´í„°ì…‹ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--output_name", type=str, default="unified-multimodal-sft-url", help="ìƒì„±í•  ë°ì´í„°ì…‹ì˜ ë¡œì»¬ í´ë” ì´ë¦„")
    parser.add_argument("--max_samples", type=int, default=None, help="ë°ì´í„°ì…‹ë³„ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--local_path", type=str, default="./", help="ë°ì´í„°ì…‹ì„ ì €ì¥í•  ë¡œì»¬ ê²½ë¡œ")

    args = parser.parse_args()

    print(f"ğŸ¯ URL ë°©ì‹ ì²˜ë¦¬ ì‹œì‘")
    print(f"ğŸ¯ íƒ€ê²Ÿ ë¡œì»¬ ê²½ë¡œ: {os.path.join(args.local_path, args.output_name)}")
    
    final_path = merge_and_create_dataset_url(
        output_name=args.output_name,
        max_samples_per_dataset=args.max_samples,
        local_path=args.local_path
    )
    
    if final_path:
        print("\nğŸ‰ URL ë°©ì‹ ë³‘í•© ì™„ë£Œ!")
        print(f"âœ… ë°ì´í„°ì…‹ì´ '{final_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ì´ë¯¸ì§€ëŠ” URLë¡œ ì €ì¥ë˜ì–´ ì‹¤ì œ í•™ìŠµ ì‹œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 