from datasets import load_dataset, concatenate_datasets, Dataset, Features, Value, Sequence, Image as ImageFeature, load_from_disk, DatasetDict
import json
from typing import List, Dict, Any, Optional, cast
from tqdm.auto import tqdm
import os
import requests
from PIL import Image
from io import BytesIO
from huggingface_hub.utils.tqdm import disable_progress_bars
import concurrent.futures
from functools import partial
import threading
import time
from urllib.parse import urlparse
import hashlib
import gc
import datetime
import argparse
import sys
import pandas as pd
import tempfile
import shutil

disable_progress_bars()  # ì§„í–‰ í‘œì‹œì¤„ ë¹„í™œì„±í™”

# ì´ë¯¸ì§€ ìºì‹œ ë° ì„¸ì…˜ ì„¤ì •
image_cache = {}
cache_lock = threading.Lock()

# ì„¸ì…˜ í’€ ìƒì„± (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì—°ê²°)
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
})

# ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ ëª©ë¡
dataset_configs = [
    ("HuggingFaceTB/smoltalk", "all"),
    ("R0k1e/UltraLink", None),
    ("PrincetonPLI/Instruct-SkillMix-SDD", None),
    ("allenai/WildChat-1M", None),
    ("nvidia/OpenCodeInstruct", None),
    ("microsoft/orca-agentinstruct-1M-v1", "default"),  # default config ì‚¬ìš©
    ("MaziyarPanahi/Llama-Nemotron-Post-Training-Dataset-v1-ShareGPT", "default"),  # default config ì‚¬ìš©
    ("nvidia/Llama-Nemotron-Post-Training-Dataset", "SFT"),  # SFT config ì‚¬ìš©
    ("open-r1/Mixture-of-Thoughts", "all"),
    ("Salesforce/blip3-kale", "core"),
    ("liuhaotian/LLaVA-Instruct-150K", None),
    ("Lin-Chen/ShareGPT4V", "ShareGPT4V")
]

def construct_image_url(image_path, dataset_name):
    """ë°ì´í„°ì…‹ë³„ë¡œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì‹¤ì œ URLë¡œ ë³€í™˜"""
    if dataset_name == "Lin-Chen/ShareGPT4V":
        # ShareGPT4VëŠ” COCO ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©
        if image_path.startswith('coco/'):
            # coco/train2017/000000000009.jpg -> COCO ì´ë¯¸ì§€ URL
            filename = os.path.basename(image_path)
            return f"http://images.cocodataset.org/train2017/{filename}"
    elif dataset_name == "liuhaotian/LLaVA-Instruct-150K":
        # LLaVAë„ COCO ì´ë¯¸ì§€ë¥¼ ì£¼ë¡œ ì‚¬ìš©
        if not image_path.startswith('http'):
            # íŒŒì¼ëª…ë§Œ ìˆëŠ” ê²½ìš° COCO URL êµ¬ì„± ì‹œë„
            return f"http://images.cocodataset.org/train2017/{image_path}"
    
    return None

def get_image_cache_key(image_url):
    """ì´ë¯¸ì§€ URLì—ì„œ ìºì‹œ í‚¤ ìƒì„±"""
    return hashlib.md5(image_url.encode()).hexdigest()

def download_image_with_retry(image_url, max_retries=3, timeout=10):
    """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    for attempt in range(max_retries):
        try:
            response = session.get(image_url, timeout=timeout)
            response.raise_for_status()
            return response.content
        except Exception as e:
            if attempt == max_retries - 1:
                return None
            time.sleep(0.5 * (2 ** attempt))  # ì§€ìˆ˜ ë°±ì˜¤í”„
    return None

def load_image_from_url_or_path(image_source, dataset_name=None):
    """
    URLì´ë‚˜ ê²½ë¡œì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ìºì‹œ ë° ìµœì í™” í¬í•¨)
    """
    try:
        # ì´ë¯¸ PIL Image ê°ì²´ì¸ ê²½ìš°
        if hasattr(image_source, 'size') and hasattr(image_source, 'convert'):
            return image_source.convert('RGB')
        
        # ë¬¸ìì—´ì¸ ê²½ìš° (URL ë˜ëŠ” íŒŒì¼ëª…)
        if isinstance(image_source, str):
            # HTTP/HTTPS URLì¸ ê²½ìš°
            if image_source.startswith('http://') or image_source.startswith('https://'):
                # ìºì‹œ í™•ì¸
                cache_key = get_image_cache_key(image_source)
                with cache_lock:
                    if cache_key in image_cache:
                        return image_cache[cache_key]
                
                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                image_data = download_image_with_retry(image_source)
                if image_data:
                    image = Image.open(BytesIO(image_data))
                    image = image.convert('RGB')
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œì„ ìœ„í•œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
                    max_size = (1024, 1024)
                    if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                        image.thumbnail(max_size, Image.Resampling.LANCZOS)
                    
                    # ìºì‹œì— ì €ì¥ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
                    with cache_lock:
                        if len(image_cache) >= 100:
                            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                            oldest_key = next(iter(image_cache))
                            del image_cache[oldest_key]
                        image_cache[cache_key] = image
                    
                    return image
                return None
            
            # ë¡œì»¬ íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
            elif os.path.exists(image_source):
                image = Image.open(image_source)
                image = image.convert('RGB')
                # í¬ê¸° ì¡°ì •
                max_size = (1024, 1024)
                if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                    image.thumbnail(max_size, Image.Resampling.LANCZOS)
                return image
            
            # íŒŒì¼ëª…ë§Œ ìˆëŠ” ê²½ìš° - URL êµ¬ì„± ì‹œë„
            else:
                if dataset_name:
                    constructed_url = construct_image_url(image_source, dataset_name)
                    if constructed_url:
                        return load_image_from_url_or_path(constructed_url, dataset_name)
                return None
        
        # bytes ë°ì´í„°ì¸ ê²½ìš°
        elif isinstance(image_source, bytes):
            image = Image.open(BytesIO(image_source))
            image = image.convert('RGB')
            # í¬ê¸° ì¡°ì •
            max_size = (1024, 1024)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)
            return image
            
        else:
            return None
        
    except Exception as e:
        return None

def process_image_batch(image_sources_with_info, max_workers=8):
    """ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    results = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ì´ë¯¸ì§€ ë¡œë“œ ì‘ì—…ì„ ë™ì‹œì— ì‹œì‘
        future_to_info = {
            executor.submit(load_image_from_url_or_path, img_source, dataset_name): (img_source, dataset_name, idx)
            for idx, (img_source, dataset_name) in enumerate(image_sources_with_info)
        }
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for future in concurrent.futures.as_completed(future_to_info):
            img_source, dataset_name, idx = future_to_info[future]
            try:
                result = future.result()
                results.append((idx, result))
            except Exception as e:
                results.append((idx, None))
    
    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[0])
    return [result for _, result in results]

def convert_to_target_format(sample: Dict[str, Any], dataset_name: str) -> Optional[Dict[str, Any]]:
    """
    ê° ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œì„ ëª©í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    í…ìŠ¤íŠ¸ ì „ìš© ë°ì´í„°ì…‹ê³¼ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì„ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ëª©í‘œ í˜•ì‹ (index í•„ë“œ ì™„ì „ ì œê±°):
    {
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "ì§ˆë¬¸"},
                    {"type": "image", "text": null}  # ë©€í‹°ëª¨ë‹¬ì¸ ê²½ìš°ë§Œ
                ]
            },
            {
                "role": "assistant", 
                "content": [
                    {"type": "text", "text": "ë‹µë³€"}
                ]
            }
        ],
        "images": [actual_image_object],  # ë©€í‹°ëª¨ë‹¬ì¸ ê²½ìš°ë§Œ
        "source_dataset": "dataset_name",
        "original_data": {...}
    }
    """
    
    result: Dict[str, Any] = {
        "messages": [],
        "images": [],
        "source_dataset": dataset_name,
        "original_data": sample.copy()
    }
    
    try:
        # í…ìŠ¤íŠ¸ ì „ìš© ë°ì´í„°ì…‹ë“¤ ì²˜ë¦¬
        if dataset_name == "HuggingFaceTB/smoltalk":
            if "messages" in sample and isinstance(sample["messages"], list):
                for msg in sample["messages"]:
                    if isinstance(msg, dict) and "role" in msg and "content" in msg:
                        result["messages"].append({
                            "role": msg["role"],
                            "content": [{"type": "text", "text": str(msg["content"])}]
                        })
        
        elif dataset_name == "R0k1e/UltraLink":
            if "data" in sample and isinstance(sample["data"], list) and len(sample["data"]) >= 2:
                data = sample["data"]
                for i in range(0, len(data), 2):
                    if i + 1 < len(data):
                        result["messages"].extend([
                            {"role": "user", "content": [{"type": "text", "text": str(data[i])}]},
                            {"role": "assistant", "content": [{"type": "text", "text": str(data[i + 1])}]}
                        ])
        
        elif dataset_name == "PrincetonPLI/Instruct-SkillMix-SDD":
            if "instruction" in sample and "output" in sample:
                user_content_str = str(sample["instruction"])
                if "input" in sample and sample["input"] and str(sample["input"]).strip():
                    user_content_str += f"\n\nInput: {sample['input']}"
                
                result["messages"] = [
                    {"role": "user", "content": [{"type": "text", "text": user_content_str}]},
                    {"role": "assistant", "content": [{"type": "text", "text": str(sample["output"])}]}
                ]
        
        elif dataset_name == "allenai/WildChat-1M":
            if "conversation" in sample and isinstance(sample["conversation"], list):
                for conv in sample["conversation"]:
                    if isinstance(conv, dict) and "role" in conv and "content" in conv:
                        result["messages"].append({
                            "role": conv["role"],
                            "content": [{"type": "text", "text": str(conv["content"])}]
                        })
        
        elif dataset_name == "nvidia/OpenCodeInstruct":
            if "input" in sample and "output" in sample:
                result["messages"] = [
                    {"role": "user", "content": [{"type": "text", "text": str(sample["input"])}]},
                    {"role": "assistant", "content": [{"type": "text", "text": str(sample["output"])}]}
                ]
        
        elif dataset_name == "microsoft/orca-agentinstruct-1M-v1":
            if "messages" in sample and isinstance(sample["messages"], list):
                for msg in sample["messages"]:
                    if isinstance(msg, dict) and "role" in msg and "content" in msg:
                        result["messages"].append({
                            "role": msg["role"],
                            "content": [{"type": "text", "text": str(msg["content"])}]
                        })
        
        elif "Nemotron" in dataset_name:
            if "conversations" in sample and isinstance(sample["conversations"], list):
                for conv in sample["conversations"]:
                    if isinstance(conv, dict) and "from" in conv and "value" in conv:
                        role = "user" if conv["from"] in ["human", "user"] else "assistant"
                        result["messages"].append({
                            "role": role,
                            "content": [{"type": "text", "text": str(conv["value"])}]
                        })
        
        elif dataset_name == "open-r1/Mixture-of-Thoughts":
            if "messages" in sample and isinstance(sample["messages"], list):
                for msg in sample["messages"]:
                    if isinstance(msg, dict) and "role" in msg and "content" in msg:
                        result["messages"].append({
                            "role": msg["role"],
                            "content": [{"type": "text", "text": str(msg["content"])}]
                        })
        
        # ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ë“¤ ì²˜ë¦¬
        elif dataset_name in ["Lin-Chen/ShareGPT4V", "liuhaotian/LLaVA-Instruct-150K"]:
            # ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë¡œë“œ
            image_obj = None
            if "image" in sample and sample["image"] is not None:
                image_obj = load_image_from_url_or_path(sample["image"], dataset_name)
            elif "images" in sample and sample["images"] is not None:
                if isinstance(sample["images"], list) and len(sample["images"]) > 0:
                    image_obj = load_image_from_url_or_path(sample["images"][0], dataset_name)
                else:
                    image_obj = load_image_from_url_or_path(sample["images"], dataset_name)
            
            if image_obj is not None:
                result["images"].append(image_obj)
            
            # conversations ì²˜ë¦¬
            if "conversations" in sample and isinstance(sample["conversations"], list):
                for i, conv in enumerate(sample["conversations"]):
                    if not isinstance(conv, dict):
                        continue
                        
                    # role ê²°ì •
                    role = "assistant"
                    if "from" in conv:
                        if conv["from"] in ["human", "user"]:
                            role = "user"
                        elif conv["from"] in ["gpt", "assistant"]:
                            role = "assistant"
                    
                    # content ìƒì„±
                    content_list = []
                    text_content = str(conv.get("value", ""))
                    
                    if text_content:
                        # <image> íƒœê·¸ ì œê±° (ì´ë¯¸ì§€ëŠ” ë³„ë„ ì²˜ë¦¬)
                        text_content = text_content.replace("<image>", "").strip()
                        
                        if text_content:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                            content_list.append({
                                "type": "text",
                                "text": text_content
                            })
                    
                    # ì²« ë²ˆì§¸ user ë©”ì‹œì§€ì— ì´ë¯¸ì§€ ì¶”ê°€
                    if role == "user" and i == 0 and result["images"]:
                        content_list.append({
                            "type": "image", 
                            "text": None
                        })
                    
                    if content_list:  # contentê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                        result["messages"].append({
                            "role": role,
                            "content": content_list
                        })
        
        elif dataset_name == "Salesforce/blip3-kale":
            # ì´ë¯¸ì§€ ë¡œë“œ
            image_obj = None
            if "url" in sample:
                image_obj = load_image_from_url_or_path(sample["url"], dataset_name)
            elif "image" in sample:
                image_obj = load_image_from_url_or_path(sample["image"], dataset_name)
            
            if image_obj is not None:
                result["images"].append(image_obj)
            
            # captionì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            caption = str(sample.get("caption", "")).strip()
            if not caption:
                caption = str(sample.get("cogvlm_caption", "")).strip()
            
            if caption:
                # ì²« ë²ˆì§¸ user ë©”ì‹œì§€ì— ì´ë¯¸ì§€ í¬í•¨
                user_content: List[Dict[str, Any]] = [{"type": "text", "text": "Describe this image."}]
                if result["images"]:
                    user_content.append({"type": "image", "text": None})
                
                result["messages"] = [
                    {"role": "user", "content": user_content},
                    {"role": "assistant", "content": [{"type": "text", "text": caption}]}
                ]
        
        # ë¹ˆ messagesì¸ ê²½ìš° None ë°˜í™˜
        if not result["messages"]:
            return None
            
        return result
        
    except Exception as e:
        # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ None ë°˜í™˜í•˜ì—¬ ê±´ë„ˆë›°ê¸°
        print(f"ìƒ˜í”Œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ (ê±´ë„ˆë›°ê¸°): {dataset_name} - {str(e)}")
        return None

def process_samples_batch(samples_batch, dataset_name, max_workers=8):
    """ìƒ˜í”Œ ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
    # ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒ˜í”Œë“¤ì„ ë¨¼ì € ì‹ë³„
    image_samples = []
    non_image_samples = []
    
    for i, sample in enumerate(samples_batch):
        has_image = False
        
        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ë°ì´í„°ì…‹ì¸ì§€ í™•ì¸
        if dataset_name in ["Lin-Chen/ShareGPT4V", "liuhaotian/LLaVA-Instruct-150K", "Salesforce/blip3-kale"]:
            if ("image" in sample and sample["image"]) or ("images" in sample and sample["images"]) or ("url" in sample and sample["url"]):
                has_image = True
        
        if has_image:
            image_samples.append((i, sample))
        else:
            non_image_samples.append((i, sample))
    
    # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ìƒ˜í”Œë“¤ì„ ë¨¼ì € ë¹ ë¥´ê²Œ ì²˜ë¦¬
    results: List[Optional[Dict[str, Any]]] = [None] * len(samples_batch)
    
    for i, sample in non_image_samples:
        converted = convert_to_target_format(sample, dataset_name)
        results[i] = converted
    
    # ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒ˜í”Œë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    if image_samples:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_idx = {
                executor.submit(convert_to_target_format, sample, dataset_name): i
                for i, sample in image_samples
            }
            
            for future in concurrent.futures.as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    result = future.result()
                    results[idx] = result
                except Exception as e:
                    results[idx] = None
    
    return [r for r in results if r is not None]

def process_dataset(dataset_name: str, config_name: Optional[str] = None, max_samples: Optional[int] = None, num_workers: int = 8):
    """ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ì—¬ ëª©í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (ë³‘ë ¬ ì²˜ë¦¬ ì¶”ê°€)"""
    try:
        # íŠ¹ì • ë°ì´í„°ì…‹ë“¤ì˜ split ì„¤ì •
        if dataset_name == "microsoft/orca-agentinstruct-1M-v1":
            split = "creative_content"
        elif dataset_name == "MaziyarPanahi/Llama-Nemotron-Post-Training-Dataset-v1-ShareGPT":
            split = "chat"
        elif dataset_name == "nvidia/Llama-Nemotron-Post-Training-Dataset":
            split = "chat"
        else:
            split = "train"
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        try:
            if config_name:
                full_dataset = load_dataset(dataset_name, config_name, split=split, streaming=True)
            else:
                full_dataset = load_dataset(dataset_name, split=split, streaming=True)
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨ ({split} split): {e}")
            # train splitìœ¼ë¡œ ì¬ì‹œë„
            if split != "train":
                try:
                    print(f"ğŸ”„ train splitìœ¼ë¡œ ì¬ì‹œë„...")
                    if config_name:
                        full_dataset = load_dataset(dataset_name, config_name, split="train", streaming=True)
                    else:
                        full_dataset = load_dataset(dataset_name, split="train", streaming=True)
                except Exception as e2:
                    print(f"âŒ train splitìœ¼ë¡œë„ ì‹¤íŒ¨: {e2}")
                    return
            else:
                return

        success_count = 0
        total_count = 0
        batch_size = max(8, num_workers)  # ë°°ì¹˜ í¬ê¸°ë¥¼ ì›Œì»¤ ìˆ˜ì— ë§ì¶¤
        current_batch = []
        
        # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ tqdm ì„¤ì •
        desc = f"{dataset_name.split('/')[-1]}"
        if config_name:
            desc += f"({config_name})"
        
        # leave=Falseë¥¼ ì¶”ê°€í•˜ì—¬ ì™„ë£Œ í›„ ì§„í–‰ ë§‰ëŒ€ê°€ ì‚¬ë¼ì§€ë„ë¡ í•¨
        progress_bar = tqdm(desc=desc, unit="samples", leave=False)
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
        for sample in full_dataset:
            if max_samples and total_count >= max_samples:
                break
            
            current_batch.append(sample)
            total_count += 1
            
            # ë°°ì¹˜ê°€ ì°¼ê±°ë‚˜ ë§ˆì§€ë§‰ ìƒ˜í”Œì¸ ê²½ìš° ì²˜ë¦¬
            if len(current_batch) >= batch_size or (max_samples and total_count >= max_samples):
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_results = process_samples_batch(current_batch, dataset_name, num_workers)
                
                if batch_results:
                    success_count += len(batch_results)
                    yield batch_results
                
                progress_bar.update(len(current_batch))
                progress_bar.set_postfix({
                    "processed": f"{success_count}/{total_count}",
                    "success_rate": f"{success_count/total_count*100:.1f}%"
                })
                
                current_batch = []
        
        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            batch_results = process_samples_batch(current_batch, dataset_name, num_workers)
            if batch_results:
                success_count += len(batch_results)
                yield batch_results
            progress_bar.update(len(current_batch))
        
        progress_bar.close()
        
        # ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ë°˜í™˜ê°’ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë‚˜ì¤‘ì— í•œ ë²ˆì— ì¶œë ¥í•˜ë„ë¡ í•¨
        yield f"âœ… {dataset_name}: {success_count}/{total_count} ìƒ˜í”Œ ë³€í™˜ ì™„ë£Œ (ì„±ê³µë¥ : {success_count/total_count*100:.1f}%)" if total_count > 0 else f"â„¹ï¸ {dataset_name}: ì²˜ë¦¬í•  ìƒ˜í”Œ ì—†ìŒ"

    except Exception as e:
        yield f"âŒ {dataset_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

def generate_cleaned_records(file_path: str):
    """
    Reads a JSONL file line-by-line, cleans the data, and yields records.
    This generator approach is highly memory-efficient.
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        # tqdm will show processing speed (it/s) without a total count,
        # which avoids reading the file twice.
        for line in tqdm(f, desc="Streaming and cleaning records"):
            try:
                record = json.loads(line)
                
                # Clean the 'messages' field in-place for efficiency
                if 'messages' in record and isinstance(record['messages'], list):
                    for message in record['messages']:
                        if 'content' in message and isinstance(message['content'], list):
                            for content_item in message['content']:
                                # Fix 1: Ensure 'index' is always an integer (None -> -1)
                                if content_item.get('index') is None:
                                    content_item['index'] = -1
                                
                                # Fix 2: Ensure 'text' is always a string (None -> "")
                                if content_item.get('text') is None:
                                    content_item['text'] = ""

                yield record

            except (json.JSONDecodeError, TypeError):
                print(f"Skipping malformed line: {line.strip()}")

def merge_and_create_dataset(
    output_name: str = "unified-multimodal-sft", 
    max_samples_per_dataset: Optional[int] = None, 
    num_workers: int = 8,  # ë” ì ì€ ì›Œì»¤ ìˆ˜ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    local_path: str = "./",
):
    """
    ëª¨ë“  ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì„ ë³‘í•©í•˜ê³  ëª©í‘œ í˜•ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¤‘ê°„ ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print(f"ğŸš€ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ ë³‘í•© ì‹œì‘... (ì›Œì»¤ ìˆ˜: {num_workers})")
    
    # 1. ì„ì‹œ ì €ì¥ ê³µê°„ ì„¤ì •
    staging_dir = f"{local_path}/{output_name}_staging".replace("//", "/")
    images_dir = os.path.join(staging_dir, "images")
    os.makedirs(images_dir, exist_ok=True)
    jsonl_path = os.path.join(staging_dir, "data.jsonl")
    
    tqdm.write(f"ğŸ“‚ ì„ì‹œ ì €ì¥ ê²½ë¡œ: {staging_dir}")

    # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ datetime í•¸ë“¤ëŸ¬
    def datetime_handler(x):
        if isinstance(x, datetime.datetime):
            return x.isoformat()
        raise TypeError(f"Object of type {type(x).__name__} is not JSON serializable")

    total_samples = 0
    image_counter = 0
    completion_messages = []

    # 2. ë°ì´í„°ë¥¼ JSONLê³¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥
    with open(jsonl_path, "w", encoding="utf-8") as f:
        dataset_progress = tqdm(dataset_configs, desc="ë°ì´í„°ì…‹ ì²˜ë¦¬", unit="dataset")

        for dataset_name, config_name in dataset_progress:
            dataset_progress.set_description(f"ì²˜ë¦¬ì¤‘: {dataset_name.split('/')[-1]}")
            try:
                for result in process_dataset(dataset_name, config_name, max_samples_per_dataset, num_workers):
                    if isinstance(result, list): # ë°°ì¹˜ ê²°ê³¼
                        for sample in result:
                            # ì´ë¯¸ì§€ ì²˜ë¦¬: PIL ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¡œ ëŒ€ì²´
                            image_paths = []
                            if sample.get("images"):
                                for img in sample["images"]:
                                    if hasattr(img, 'save'):
                                        image_filename = f"{image_counter:08d}.png"
                                        # ìƒëŒ€ ê²½ë¡œë¡œ ì €ì¥
                                        img_save_path = os.path.join(images_dir, image_filename)
                                        img.save(img_save_path, "PNG")
                                        image_paths.append(os.path.join("images", image_filename))
                                        image_counter += 1
                            
                            sample["images"] = image_paths
                            
                            # original_dataë¥¼ ì•ˆì „í•˜ê²Œ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                            try:
                                sample["original_data"] = json.dumps(sample["original_data"], ensure_ascii=False, default=str)
                            except (TypeError, OverflowError):
                                sample["original_data"] = "{}" # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°ì²´ë¡œ

                            f.write(json.dumps(
                                sample, 
                                ensure_ascii=False, 
                                default=datetime_handler
                            ) + "\n")
                            total_samples += 1
                        
                        dataset_progress.set_postfix({"ì´ ìƒ˜í”Œ": total_samples})

                    elif isinstance(result, str): # ì™„ë£Œ ë©”ì‹œì§€
                        completion_messages.append(result)
            except Exception as e:
                tqdm.write(f"âŒ {dataset_name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue

            # ë°ì´í„°ì…‹ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ìµœì í™”
            with cache_lock:
                image_cache.clear()
            gc.collect()
            tqdm.write(f"ğŸ§  {dataset_name.split('/')[-1]} ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ.")

    dataset_progress.close()

    tqdm.write("\n" + "="*20 + " ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ " + "="*20)
    for msg in completion_messages:
        tqdm.write(msg)
    tqdm.write("="*55)

    if total_samples == 0:
        print("âŒ ë³€í™˜ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    tqdm.write(f"\nğŸ¯ ì´ {total_samples}ê°œ ìƒ˜í”Œ ë³€í™˜ ì™„ë£Œ ë° ì„ì‹œ ì €ì¥ ì™„ë£Œ")
    
    # 3. ë””ìŠ¤í¬ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œ
    tqdm.write("ğŸ“¦ ì„ì‹œ íŒŒì¼ë¡œë¶€í„° Dataset ê°ì²´ ìƒì„± ì¤‘...")

    # ë°ì´í„°ì…‹ì˜ ìµœì¢… ìŠ¤í‚¤ë§ˆ(êµ¬ì¡°) ì •ì˜
    features = Features({
        'messages': Sequence(
            Features({
                'role': Value('string'),
                'content': Sequence(
                    Features({
                        'type': Value('string'),
                        'text': Value('string'),
                        'index': Value('int64')
                    })
                )
            })
        ),
        'images': Sequence(Value('string')), # ë¨¼ì € ë¬¸ìì—´ë¡œ ë¡œë“œ
        'source_dataset': Value('string'),
        'original_data': Value('string')
    })
    
    # ë¡œì»¬ ì €ì¥
    tqdm.write("ğŸ’¾ ë¡œì»¬ ì €ì¥ ì¤‘ (ìµœì¢… Arrow í¬ë§·)...")
    final_save_path = f"{local_path}/{output_name}"

    # 1. ì œë„ˆë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ ë° ì •ì œ
    tqdm.write("   - ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ ë° ì •ì œ ì¤‘...")
    iterable_dataset = Dataset.from_generator(
        generate_cleaned_records,
        features=features,
        gen_kwargs={"file_path": jsonl_path},
    )
    # IterableDatasetì„ ì¼ë°˜ Datasetìœ¼ë¡œ ë³€í™˜í•˜ì—¬ .map()ê³¼ .filter() ì‚¬ìš©
    dataset = Dataset.from_list(list(tqdm(iterable_dataset, desc="Converting to standard dataset")))

    # 2. ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì‹¤ì œ ì´ë¯¸ì§€ ê°ì²´ë¡œ ë³€í™˜ (ìƒëŒ€ ê²½ë¡œ ê¸°ì¤€ ì„¤ì •)
    staging_dir = os.path.dirname(jsonl_path)
    def resolve_and_load_images(example):
        if example['images']:
            # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            absolute_paths = [os.path.join(staging_dir, p) for p in example['images']]
            # ì´ë¯¸ì§€ ë¡œë“œ (ì˜¤ë¥˜ ë°œìƒ ì‹œ None)
            example['images'] = [path if os.path.exists(path) else None for path in absolute_paths]
        return example

    # ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë³€í™˜í•˜ê³ , Noneì¸ ì´ë¯¸ì§€ë¥¼ í•„í„°ë§ (ë‹¤ì¤‘ ì²˜ë¦¬ë¡œ ê°€ì†)
    tqdm.write(f"   - ì´ë¯¸ì§€ ê²½ë¡œ ë³€í™˜ ì¤‘ (ì›Œì»¤: {num_workers})...")
    dataset = dataset.map(resolve_and_load_images, num_proc=num_workers)
    dataset = dataset.filter(lambda example: not (example.get('images') and None in example['images']), num_proc=num_workers)

    # ìµœì¢…ì ìœ¼ë¡œ Image Featureë¡œ ìºìŠ¤íŒ…
    tqdm.write("   - ì´ë¯¸ì§€ ë°ì´í„° íƒ€ì… ë³€í™˜ ì¤‘...")
    unified_dataset = dataset.cast_column("images", Sequence(ImageFeature()))

    # ìºì‹œ ì •ë¦¬
    with cache_lock:
        image_cache.clear()
    
    unified_dataset.save_to_disk(final_save_path)
    tqdm.write(f"   - ìµœì¢… ë°ì´í„°ì…‹ ê²½ë¡œ: {final_save_path}")
    
    return final_save_path



def upload_dataset_to_hub(dataset_path: str, repo_id: str, private: bool = False, num_workers: Optional[int] = None, chunk_size: Optional[int] = None):
    """
    ë¡œì»¬ì— ì €ì¥ëœ ë°ì´í„°ì…‹ì„ í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
    """
    if num_workers is None:
        num_workers = min(4, os.cpu_count() or 4)  # ì›Œì»¤ ìˆ˜ ì œí•œ
    
    if chunk_size is None:
        chunk_size = min(200, num_workers * 25)  # ë” ì‘ì€ ì²­í¬ í¬ê¸°
        
    jsonl_path = os.path.join(dataset_path, "data.jsonl")
    
    if not os.path.exists(jsonl_path):
        print(f"âŒ JSONL íŒŒì¼ ì—†ìŒ: {jsonl_path}")
        return False

    print(f"ğŸš€ ë°ì´í„°ì…‹ ì—…ë¡œë“œ: {repo_id}")
    print(f"ğŸ“Š ì²­í¬ í¬ê¸°: {chunk_size}, ì›Œì»¤ ìˆ˜: {num_workers}")
    
    try:
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±
        def data_generator():
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    try:
                        record = json.loads(line.strip())
                        
                        # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ JSON ë¬¸ìì—´ë¡œ í†µí•©
                        unified_record = {
                            'data': json.dumps(record, ensure_ascii=False, default=str)
                        }
                        
                        yield unified_record
                        
                        # ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ì£¼ê¸°ì ìœ¼ë¡œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                        if line_num % 500 == 0:
                            gc.collect()
                            
                    except Exception as e:
                        print(f"   ë¼ì¸ {line_num} ê±´ë„ˆë›°ê¸°: {e}")
                        continue
        
        # ê·¹ë„ë¡œ ë‹¨ìˆœí•œ ìŠ¤í‚¤ë§ˆ - ëª¨ë“  í•„ë“œë¥¼ ë¬¸ìì—´ë¡œ
        from datasets import Features, Value
        features = Features({
            'data': Value('string')  # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ JSON ë¬¸ìì—´ë¡œ
        })
        
        print("ğŸ“¦ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        iterable_dataset = Dataset.from_generator(
            data_generator,
            features=features
        )
        
        # IterableDatasetì„ ì¼ë°˜ Datasetìœ¼ë¡œ ë³€í™˜
        print("ğŸ“¦ IterableDatasetì„ Datasetìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        dataset = Dataset.from_list(list(tqdm(iterable_dataset, desc="Converting to Dataset")))
        
        print(f"âœ… ë°ì´í„°ì…‹ ìƒì„±: {len(dataset)} ìƒ˜í”Œ")
        
        # ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
        small_batch_size = min(50, chunk_size // 4)
        
        # ì´ì œ ëª¨ë“  ë°ì´í„°ê°€ ë‹¨ìˆœí•œ JSON ë¬¸ìì—´ë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ 
        # ë³µì¡í•œ ë³€í™˜ ê³¼ì •ì´ í•„ìš” ì—†ìŒ
        print("âœ… ë‹¨ìˆœ ìŠ¤í‚¤ë§ˆë¡œ ë°ì´í„° ë³€í™˜ ê³¼ì • ìƒëµ")
        
        # ìµœì¢… ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # 6. ìµœì¢… ì—…ë¡œë“œ (ì‘ì€ ìƒ¤ë“œ í¬ê¸°ë¡œ)
        max_shard_size = "100MB"  # ê³ ì •ëœ ì‘ì€ ìƒ¤ë“œ í¬ê¸°
        print(f"ğŸš€ '{repo_id}'ë¡œ ì—…ë¡œë“œ (ìƒ¤ë“œ í¬ê¸°: {max_shard_size})...")
        
        dataset.push_to_hub(
            repo_id,
            private=private,
            max_shard_size=max_shard_size,
            commit_message=f"Upload dataset: {len(dataset)} samples"
        )
        
        print(f"âœ… ì—…ë¡œë“œ ì„±ê³µ!")
        print(f"ğŸ“Š ìƒ˜í”Œ ìˆ˜: {len(dataset):,}")
        print(f"ğŸ”— https://huggingface.co/datasets/{repo_id}")
        return True
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    finally:
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()

def inspect_dataset(dataset_path: str = "./unified-multimodal-sft"):
    """ìƒì„±ëœ ë°ì´í„°ì…‹ ê²€ì‚¬"""
    try:
        print(f"ğŸ” ë°ì´í„°ì…‹ ê²€ì‚¬: {dataset_path}")
        
        # DatasetDictì¸ì§€ Datasetì¸ì§€ í™•ì¸
        if os.path.exists(os.path.join(dataset_path, "dataset_dict.json")):
            # DatasetDict í˜•íƒœë¡œ ì €ì¥ëœ ê²½ìš°
            from datasets import DatasetDict
            dataset_dict = DatasetDict.load_from_disk(dataset_path)
            if "train" in dataset_dict:
                dataset = cast(Dataset, dataset_dict["train"])
                print(f"ğŸ“Š DatasetDictì—ì„œ train split ë¡œë“œ - ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
            else:
                # ì²« ë²ˆì§¸ split ì‚¬ìš©
                split_name = list(dataset_dict.keys())[0]
                dataset = cast(Dataset, dataset_dict[split_name])
                print(f"ğŸ“Š DatasetDictì—ì„œ '{split_name}' split ë¡œë“œ - ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
        else:
            # ì¼ë°˜ Dataset í˜•íƒœë¡œ ì €ì¥ëœ ê²½ìš°
            dataset = cast(Dataset, Dataset.load_from_disk(dataset_path))
            print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
        
        # êµ¬ì¡° ê²€ì‚¬
        sample_with_image = None
        sample_without_image = None
        
        for sample_any in dataset:
            sample = cast(Dict[str, Any], sample_any)
            if sample.get("images") and not sample_with_image:
                sample_with_image = sample
            elif not sample.get("images") and not sample_without_image:
                sample_without_image = sample
            
            if sample_with_image and sample_without_image:
                break
        
        # ì´ë¯¸ì§€ í¬í•¨ ìƒ˜í”Œ ì˜ˆì‹œ
        if sample_with_image:
            print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ í¬í•¨ ìƒ˜í”Œ ì˜ˆì‹œ:")
            print(f"   ì´ë¯¸ì§€ ìˆ˜: {len(sample_with_image['images'])}")
            print(f"   ë©”ì‹œì§€ ìˆ˜: {len(sample_with_image['messages'])}")
            
            # ì²« ë²ˆì§¸ ë©”ì‹œì§€ êµ¬ì¡° í™•ì¸
            if sample_with_image['messages']:
                first_msg = sample_with_image['messages'][0]
                print(f"   ì²« ë²ˆì§¸ ë©”ì‹œì§€ role: {first_msg.get('role')}")
                print(f"   ì²« ë²ˆì§¸ ë©”ì‹œì§€ content ìˆ˜: {len(first_msg.get('content', []))}")
                
                for j, content in enumerate(first_msg.get('content', [])[:3]):
                    content_type = content.get('type', 'unknown')
                    if content_type == 'text':
                        text_preview = content.get('text', '')[:50] + "..." if len(content.get('text', '')) > 50 else content.get('text', '')
                        print(f"     Content {j+1}: {content_type} - '{text_preview}'")
                    else:
                        print(f"     Content {j+1}: {content_type} - index: {content.get('index')}")
        
        # í†µê³„
        image_count = sum(1 for s in dataset if cast(Dict[str, Any], s).get("images"))
        print(f"\nğŸ“ˆ ì´ë¯¸ì§€ í¬í•¨ ìƒ˜í”Œ: {image_count}/{len(dataset)} ({image_count/len(dataset)*100:.1f}%)")
        
        # ì›ë³¸ ë°ì´í„°ì…‹ë³„ í†µê³„
        source_stats: Dict[str, int] = {}
        for s in dataset:
            sample = cast(Dict[str, Any], s)
            source = sample.get("source_dataset", "unknown")
            source_stats[source] = source_stats.get(source, 0) + 1
        
        print(f"\nğŸ“Š ì›ë³¸ ë°ì´í„°ì…‹ë³„ ë¶„í¬:")
        for source, count in sorted(source_stats.items()):
            print(f"   {source}: {count}ê°œ ({count/len(dataset)*100:.1f}%)")
        
        # ì›ë³¸ ë°ì´í„° ë³´ì¡´ í™•ì¸
        original_data_count = sum(1 for s in dataset if cast(Dict[str, Any], s).get("original_data"))
        print(f"\nğŸ’¾ ì›ë³¸ ë°ì´í„° ë³´ì¡´: {original_data_count}/{len(dataset)} ({original_data_count/len(dataset)*100:.1f}%)")
        
        # ì›ë³¸ ë°ì´í„° ì˜ˆì‹œ (ì²« ë²ˆì§¸ ìƒ˜í”Œ)
        if len(dataset) > 0:
            first_sample = cast(Dict[str, Any], dataset[0])
            if first_sample.get("original_data"):
                print(f"\nğŸ” ì›ë³¸ ë°ì´í„° ì˜ˆì‹œ (ì²« ë²ˆì§¸ ìƒ˜í”Œ):")
                try:
                    original_str = first_sample["original_data"]
                    original = json.loads(original_str)
                    print(f"   ì›ë³¸ ë°ì´í„° í‚¤: {list(original.keys())}")
                    for key, value in list(original.items())[:3]:  # ì²˜ìŒ 3ê°œ í‚¤ë§Œ í‘œì‹œ
                        if isinstance(value, str) and len(value) > 50:
                            print(f"   {key}: {value[:50]}...")
                        else:
                            print(f"   {key}: {value}")
                except (json.JSONDecodeError, TypeError):
                     print(f"   ì›ë³¸ ë°ì´í„° (raw): {first_sample['original_data'][:100]}...")

        return dataset
        
    except Exception as e:
        print(f"âŒ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None



def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í…ìŠ¤íŠ¸ + ë©€í‹°ëª¨ë‹¬ í†µí•© ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # merge ëª…ë ¹ì–´
    parser_merge = subparsers.add_parser("merge", help="ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ ë³‘í•©í•˜ì—¬ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.")
    parser_merge.add_argument("--output_name", type=str, help="ìƒì„±í•  ë°ì´í„°ì…‹ì˜ ë¡œì»¬ í´ë” ì´ë¦„")
    parser_merge.add_argument("--max_samples", type=int, default=None, help="ë°ì´í„°ì…‹ë³„ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜")
    parser_merge.add_argument("--num_workers", type=int, default=16, help="ë°ì´í„° ì²˜ë¦¬ ì›Œì»¤ ìˆ˜")
    parser_merge.add_argument("--local_path", type=str, default="./", help="ë°ì´í„°ì…‹ì„ ì €ì¥í•  ë¡œì»¬ ê²½ë¡œ")

    # upload ëª…ë ¹ì–´
    parser_upload = subparsers.add_parser("upload", help="ë¡œì»¬ì— ì €ì¥ëœ ë°ì´í„°ì…‹ì„ í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.")
    parser_upload.add_argument("--dataset_path", type=str, help="ì—…ë¡œë“œí•  ë¡œì»¬ ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser_upload.add_argument("--repo_id", type=str, help="í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œ ë¦¬í¬ì§€í† ë¦¬ ID (ì˜ˆ: username/repo-name)")
    parser_upload.add_argument("--private", action="store_true", help="ë¦¬í¬ì§€í† ë¦¬ë¥¼ ë¹„ê³µê°œë¡œ ì„¤ì •")
    parser_upload.add_argument("--num_workers", type=int, default=None, help="ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: CPU ì½”ì–´ ìˆ˜)")
    parser_upload.add_argument("--chunk_size", type=int, default=None, help="ë©”ëª¨ë¦¬ ì²˜ë¦¬ ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: ë™ì  ê³„ì‚°)")

    # inspect ëª…ë ¹ì–´
    parser_inspect = subparsers.add_parser("inspect", help="ë¡œì»¬ ë°ì´í„°ì…‹ì˜ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
    parser_inspect.add_argument("dataset_path", nargs="?", default="./unified-multimodal-sft", help="ê²€ì‚¬í•  ë°ì´í„°ì…‹ ê²½ë¡œ")

    args = parser.parse_args()

    if args.command == "merge":
        print(f"ğŸ¯ íƒ€ê²Ÿ ë¡œì»¬ ê²½ë¡œ: {os.path.join(args.local_path, args.output_name)}")
        print(f"ğŸ”§ ì›Œì»¤ ìˆ˜: {args.num_workers}")
        final_path = merge_and_create_dataset(
            output_name=args.output_name,
            max_samples_per_dataset=args.max_samples,
            num_workers=args.num_workers,
            local_path=args.local_path
        )
        if final_path:
            print("\nğŸ‰ ë³‘í•© ì™„ë£Œ!")
            print(f"âœ… ë°ì´í„°ì…‹ì´ '{final_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"\nğŸ‘‰ ì´ì œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í—ˆë¸Œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print(f"   python {sys.argv[0]} upload {final_path} <your_hf_username>/{args.output_name}")

    elif args.command == "upload":
        upload_dataset_to_hub(
            dataset_path=args.dataset_path,
            repo_id=args.repo_id,
            private=args.private,
            num_workers=args.num_workers,
            chunk_size=args.chunk_size
        )

    elif args.command == "inspect":
        inspect_dataset(args.dataset_path)

if __name__ == "__main__":
    main()
    