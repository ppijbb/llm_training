import logging
from tqdm import tqdm
from datasets import load_dataset, get_dataset_config_names, concatenate_datasets
from transformers import AutoProcessor
import torch
from typing import Dict, Any, List, Optional
import traceback

# Configure logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

def get_simple_sft_dataset(
    dataset_name: str = "HuggingFaceTB/smoltalk", 
    config_name: str = "default",
    tokenizer=None,
    max_length: int = 2048,
    max_samples: int = 1000,
    test_size: float = 0.1
):
    """
    ê°„ë‹¨í•œ SFT ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©ì…ë‹ˆë‹¤.
    """
    if tokenizer is None:
        raise ValueError("Tokenizer must be provided")
    
    logger.info(f"ğŸ“¦ ë¡œë”© ì¤‘: {dataset_name}")
    logger.info(f"   - config_name: {config_name}")
    logger.info(f"   - max_samples: {max_samples}")
    
    # ì‘ì€ ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = None
    try:
        try:
            config_names = get_dataset_config_names(dataset_name)
            sample_for_each_config = max_samples // len(config_names)
            logger.info(f"   - ì‹œë„: load_dataset({dataset_name}, {config_name}, split='train', streaming=False)")
            loading_dataset = tqdm(config_names, desc=f"Loading {dataset_name} configs")
            for config in loading_dataset:
                loading_dataset.set_description(f"Loading {dataset_name} config: {config}")
                config_data = load_dataset(path=dataset_name, name=config, split="train", streaming=False)
                config_data = config_data.shuffle(seed=42) 
                config_data = config_data.select(range(sample_for_each_config))if len(config_data) > sample_for_each_config else config_data
                dataset = config_data if dataset is None else concatenate_datasets([dataset, config_data])

        except Exception as e:
            traceback.print_exc()
            logger.error(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info(f"   - ì‹œë„: load_dataset({dataset_name}, split='train', streaming=True)")
            dataset = load_dataset(path=dataset_name, name="all", split="train", streaming=True)
            dataset = dataset.shuffle(seed=42)
            dataset = dataset.select(range(max_samples)) if len(dataset) > max_samples else dataset
        
        logger.info("   âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise Exception(f"ğŸ˜¢ ë°ì´í„°ì…‹ ë¡œë”© ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    if dataset is None:
        raise RuntimeError("ë°ì´í„°ì…‹ ë¡œë”© ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    # ì œí•œëœ ìƒ˜í”Œë§Œ ê°€ì ¸ì˜¤ê¸°
    samples = []
    converted_count = 0
    skipped_count = 0
    
    logger.info(f"   ğŸ“Š ìƒ˜í”Œ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {max_samples}ê°œ)")
    for i, sample in enumerate(dataset):
        if i >= max_samples:
            break
        
        if i % 100 == 0 and i > 0:
            logger.debug(f"      - ì§„í–‰ë¥ : {i}/{max_samples}, ë³€í™˜ë¨: {converted_count}, ê±´ë„ˆëœ€: {skipped_count}")
        
        # ë°ì´í„°ì…‹ë³„ ë©”ì‹œì§€ í˜•ì‹ ë³€í™˜
        converted = convert_sample_to_messages(sample, dataset_name)
        if converted:
            samples.append(converted)
            converted_count += 1
        else:
            skipped_count += 1
            if skipped_count <= 5:  # ì²˜ìŒ 5ê°œ ì‹¤íŒ¨í•œ ìƒ˜í”Œë§Œ ì¶œë ¥
                logger.warning(f"      âš ï¸ ìƒ˜í”Œ {i} ë³€í™˜ ì‹¤íŒ¨: {sample}")
    
    logger.info(f"âœ… {len(samples)}ê°œ ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ (ë³€í™˜: {converted_count}, ê±´ë„ˆëœ€: {skipped_count})")
    
    if len(samples) == 0:
        logger.error("âŒ ë³€í™˜ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
        logger.info("   ì²« ë²ˆì§¸ ì›ë³¸ ìƒ˜í”Œ ì˜ˆì‹œ:")
        for i, sample in enumerate(dataset):
            if i >= 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                break
            logger.debug(f"   ìƒ˜í”Œ {i}: {sample}")
        raise ValueError("ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    split_idx = int(len(samples) * (1 - test_size))
    train_samples = samples[:split_idx]
    test_samples = samples[split_idx:]
    
    logger.info(f"   ğŸ“Š ë¶„í• : í›ˆë ¨ {len(train_samples)}ê°œ, í…ŒìŠ¤íŠ¸ {len(test_samples)}ê°œ")
    
    # í† í¬ë‚˜ì´ì¦ˆ ì²˜ë¦¬
    logger.info("   ğŸ”„ í† í¬ë‚˜ì´ì§• ì‹œì‘...")
    train_dataset = []
    test_dataset = []
    
    tokenize_failed = 0
    for i, sample in enumerate(train_samples):
        processed = process_sample(sample, tokenizer, max_length)
        if processed is not None:
            train_dataset.append(processed)
        else:
            tokenize_failed += 1
            if tokenize_failed <= 3:  # ì²˜ìŒ 3ê°œ ì‹¤íŒ¨ë§Œ ì¶œë ¥
                logger.warning(f"      âš ï¸ í›ˆë ¨ ìƒ˜í”Œ {i} í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨")
    
    for i, sample in enumerate(test_samples):
        processed = process_sample(sample, tokenizer, max_length)
        if processed is not None:
            test_dataset.append(processed)
        else:
            tokenize_failed += 1
    
    logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼ - í›ˆë ¨: {len(train_dataset)}ê°œ, í…ŒìŠ¤íŠ¸: {len(test_dataset)}ê°œ (í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {tokenize_failed}ê°œ)")
    
    if len(train_dataset) == 0:
        raise ValueError("í† í¬ë‚˜ì´ì§• í›„ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì € ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    from datasets import Dataset, DatasetDict
    return DatasetDict({
        "train": Dataset.from_list(train_dataset),
        "test": Dataset.from_list(test_dataset)
    })

def convert_sample_to_messages(sample: Dict[str, Any], dataset_name: str) -> Optional[Dict[str, Any]]:
    """ìƒ˜í”Œì„ messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    if dataset_name == "HuggingFaceTB/smoltalk" or "smoltalk" in dataset_name.lower():
        if "messages" in sample and isinstance(sample["messages"], list):
            return {"messages": sample["messages"]}
    
    elif "orca-agentinstruct" in dataset_name:
        if "messages" in sample and isinstance(sample["messages"], list):
            return {"messages": sample["messages"]}
    
    # ê¸°ë³¸ instruction-output í˜•ì‹ ì²˜ë¦¬
    if "instruction" in sample and "output" in sample:
        messages = [
            {"role": "user", "content": sample["instruction"]},
            {"role": "assistant", "content": sample["output"]}
        ]
        return {"messages": messages}
    
    # conversations í˜•ì‹ ì²˜ë¦¬
    if "conversations" in sample and isinstance(sample["conversations"], list):
        messages = []
        for conv in sample["conversations"]:
            if isinstance(conv, dict) and "from" in conv and "value" in conv:
                role = "user" if conv["from"] in ["human", "user"] else "assistant"
                messages.append({"role": role, "content": conv["value"]})
        if messages:
            return {"messages": messages}
    
    # text í•„ë“œë§Œ ìˆëŠ” ê²½ìš° (ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸)
    if "text" in sample and isinstance(sample["text"], str):
        # ê°„ë‹¨í•œ ëŒ€í™”ë¡œ ë³€í™˜
        messages = [
            {"role": "user", "content": "Continue the following text:"},
            {"role": "assistant", "content": sample["text"]}
        ]
        return {"messages": messages}
    
    return None

def process_sample(sample: Dict[str, Any], tokenizer, max_length: int):
    """ìƒ˜í”Œì„ í† í¬ë‚˜ì´ì¦ˆí•˜ì—¬ í›ˆë ¨ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        if "messages" not in sample:
            return None
        
        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        messages = sample["messages"]
        if not isinstance(messages, list) or len(messages) == 0:
            return None
        
        # ê° ë©”ì‹œì§€ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸
        for msg in messages:
            if not isinstance(msg, dict) or "role" not in msg or "content" not in msg:
                return None
            if not isinstance(msg["content"], str) or len(msg["content"].strip()) == 0:
                return None
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš© - AutoProcessor í˜¸í™˜ì„± ê°œì„ 
        try:
            # AutoProcessorì¸ ê²½ìš° tokenizer ì†ì„±ì„ í†µí•´ ì ‘ê·¼
            if hasattr(tokenizer, 'tokenizer'):
                actual_tokenizer = tokenizer.tokenizer
            else:
                actual_tokenizer = tokenizer
            
            tokenized = actual_tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=False,
                # max_length=max_length,
                # truncation=True,
                return_dict=True,
                return_tensors="pt"
            )
        except Exception as e1:
            # ëŒ€ì•ˆ ë°©ë²•: ì±„íŒ… í…œí”Œë¦¿ ì—†ì´ ì§ì ‘ í…ìŠ¤íŠ¸ ë³€í™˜
            logger.warning(f"   âš ï¸ apply_chat_template ì‹¤íŒ¨, ëŒ€ì•ˆ ë°©ë²• ì‹œë„: {e1}")
            
            # ë©”ì‹œì§€ë¥¼ ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            text = ""
            for msg in messages:
                if msg["role"] == "user":
                    text += f"<start_of_turn>user\n{msg['content']}<end_of_turn>\n"
                elif msg["role"] == "assistant":
                    text += f"<start_of_turn>model\n{msg['content']}<end_of_turn>\n"
            
            # í† í¬ë‚˜ì´ì§•
            if hasattr(tokenizer, 'tokenizer'):
                actual_tokenizer = tokenizer.tokenizer
            else:
                actual_tokenizer = tokenizer
            
            tokenized = actual_tokenizer(
                text,
                max_length=max_length,
                # truncation=True,
                # padding=False,
                return_tensors="pt"
            )
        
        if tokenized is None:
            return None
        
        # í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ Datasetì— ì €ì¥ ê°€ëŠ¥í•˜ê²Œ í•¨
        result = {}
        for key, value in tokenized.items():
            if isinstance(value, torch.Tensor):
                result[key] = value.squeeze().tolist()
            else:
                result[key] = value
        
        # ìµœì†Œ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ì€ ì‹œí€€ìŠ¤ ì œì™¸)
        if "input_ids" in result and len(result["input_ids"]) < 10:
            return None
        
        return result
        
    except Exception as e:
        # ë””ë²„ê¹…ì„ ìœ„í•´ ì˜ˆì™¸ ì •ë³´ ì¶œë ¥ (ì²˜ìŒ ëª‡ ê°œë§Œ)
        logger.error(f"âŒ í† í¬ë‚˜ì´ì§• ì˜ˆì™¸: {str(e)}")
        logger.error(f"   ìƒ˜í”Œ: {sample}")
        logger.error(f"   í† í¬ë‚˜ì´ì € íƒ€ì…: {type(tokenizer)}")
        logger.error(f"   í† í¬ë‚˜ì´ì €ì— chat_templateì´ ìˆëŠ”ê°€: {hasattr(tokenizer, 'chat_template')}")
        if hasattr(tokenizer, 'chat_template'):
            logger.error(f"   chat_template ê¸¸ì´: {len(str(tokenizer.chat_template)) if tokenizer.chat_template else 0}")
        import traceback
        traceback.print_exc()
        return None

def create_simple_collate_fn(tokenizer):
    """ê°„ë‹¨í•œ collate function"""
    tokenizer = tokenizer.tokenizer
    def collate_fn(examples):
        # input_idsì™€ attention_mask ì¶”ì¶œ
        input_ids = [torch.tensor(ex["input_ids"]) for ex in examples if "input_ids" in ex]
        attention_mask = [torch.tensor(ex["attention_mask"]) for ex in examples if "attention_mask" in ex]
        
        if not input_ids:
            return None
        
        # íŒ¨ë”© ì²˜ë¦¬
        from torch.nn.utils.rnn import pad_sequence
        
        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id or tokenizer.eos_token_id)
        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
        
        # labelsëŠ” input_idsì™€ ë™ì¼ (causal LM)
        labels = input_ids.clone()
        
        # íŒ¨ë”© í† í°ì€ loss ê³„ì‚°ì—ì„œ ì œì™¸
        pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id
        labels[labels == pad_token_id] = -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    return collate_fn

# í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ë°ì´í„°ì…‹ ë¹Œë” í•¨ìˆ˜ë“¤
def smoltalk_dataset(tokenizer, max_samples: int = 500):
    """SmolTalk ë°ì´í„°ì…‹ ë¹Œë”"""
    return get_simple_sft_dataset(
        dataset_name="HuggingFaceTB/smoltalk",
        config_name="all", 
        tokenizer=tokenizer,
        max_samples=max_samples
    )

def orca_mini_dataset(tokenizer, max_samples: int = 500):
    """Orca ë¯¸ë‹ˆ ë°ì´í„°ì…‹ ë¹Œë”"""
    return get_simple_sft_dataset(
        dataset_name="microsoft/orca-agentinstruct-1M-v1",
        config_name="creative_content",
        tokenizer=tokenizer, 
        max_samples=max_samples
    )

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    dataset = smoltalk_dataset(tokenizer, max_samples=100)
    logger.info(f"ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset}")
    logger.info(f"ìƒ˜í”Œ ì˜ˆì‹œ: {dataset['train'][0]}") 