import logging
from tqdm import tqdm
from datasets import load_dataset, get_dataset_config_names, concatenate_datasets
from transformers import AutoProcessor
import torch
from typing import Dict, Any, List, Optional
import traceback
import gc
import os
import random

# Configure logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

def get_simple_sft_dataset(
    dataset_name: str = "HuggingFaceTB/smoltalk", 
    tokenizer=None,
    max_length: int = 2048,
    max_samples: int = 1000,
    test_size: float = 0.1,
    use_streaming: bool = True,
    chunk_size: int = 100
):
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ SFT ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ëª¨ë“  configë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
    """
    if tokenizer is None:
        raise ValueError("Tokenizer must be provided")
    
    logger.info(f"ğŸ“¦ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©: {dataset_name}")
    logger.info(f"   - max_samples: {max_samples}")
    logger.info(f"   - streaming: {use_streaming}")
    logger.info(f"   - chunk_size: {chunk_size}")
    
    log_memory_usage("ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘")
    
    # ëª¨ë“  configë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    try:
        available_configs = get_dataset_config_names(dataset_name)
        selected_configs = []
        for i in range(25):
            random.shuffle(available_configs)
            selected_configs += [random.choice(available_configs)]
        available_configs = selected_configs
        logger.info(f"   ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ configs: {len(available_configs)}ê°œ")
        logger.info(f"   ğŸ¯ ëª¨ë“  config ì‚¬ìš©: {len(available_configs)}ê°œ")
        
        # configë‹¹ ìƒ˜í”Œ ìˆ˜ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°
        samples_per_config = max(1, max_samples // len(available_configs))
        logger.info(f"   ğŸ“Š Configë‹¹ ìƒ˜í”Œ ìˆ˜: {samples_per_config}ê°œ")
        
        train_samples = []
        test_samples = []
        total_processed = 0
        
        # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
        config_pbar = tqdm(available_configs, desc="Config ì²˜ë¦¬", unit="config")
        
        # ê° configë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
        for i, config in enumerate(config_pbar):
            if total_processed >= max_samples:
                break
                
            try:
                config_pbar.set_description(f"Config {i+1}/{len(available_configs)}: {config[:30]}...")
                
                # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°ì´í„°ì…‹ ë¡œë“œ
                config_dataset = load_dataset(
                    path=dataset_name, 
                    name=config,
                    split="train", 
                    streaming=True
                )
                
                # configì—ì„œ ìƒ˜í”Œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                config_processed = 0
                sample_pbar = tqdm(total=samples_per_config, desc=f"ìƒ˜í”Œ ì²˜ë¦¬", unit="sample", leave=False)
                
                for sample in config_dataset:
                    if config_processed >= samples_per_config or total_processed >= max_samples:
                        break
                    
                    # ìƒ˜í”Œ ë³€í™˜
                    converted = convert_sample_to_messages(sample, dataset_name)
                    if "images" in converted:
                        for img in converted["images"]:
                            if img is None:
                                converted = None
                    if "messages" in converted:
                        user_msg_len = len([msg for msg in converted["messages"] if msg["role"] == "user"])
                        images_len = len(["images"])
                        if user_msg_len > 1 and images_len > 1:
                            print(converted)
                            converted = None

                    if converted:
                        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                        is_train = (total_processed % int(1/test_size)) != 0
                        
                        if is_train :
                            train_samples.append(converted)
                        else:
                            test_samples.append(converted)

                        total_processed += 1
                        config_processed += 1

                        # tqdm ì—…ë°ì´íŠ¸
                        sample_pbar.update(1)
                        memory_gb = get_memory_usage()
                        sample_pbar.set_postfix({
                            "ì´ ì²˜ë¦¬": f"{total_processed}/{max_samples}",
                            "Train": len(train_samples),
                            "Test": len(test_samples),
                            "ë©”ëª¨ë¦¬": f"{memory_gb:.1f}GB"
                        })
                
                sample_pbar.close()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del config_dataset
                gc.collect()
                
            except Exception as e:
                tqdm.write(f"âš ï¸ Config {config} ì‹¤íŒ¨: {e}")
                continue
        
        config_pbar.close()
        
        logger.info(f"âœ… ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ: Train {len(train_samples)}ê°œ, Test {len(test_samples)}ê°œ")
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise Exception(f"ğŸ˜¢ ë°ì´í„°ì…‹ ë¡œë”© ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    if len(train_samples) == 0:
        raise ValueError("ë³€í™˜ëœ í›ˆë ¨ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    # Datasetìœ¼ë¡œ ë³€í™˜ (ìµœì†Œí•œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©)
    print("ğŸ“Š Dataset ë³€í™˜ ì‹œì‘...")
    
    from datasets import Dataset, DatasetDict
    
    # tqdmìœ¼ë¡œ Dataset ë³€í™˜ ì§„í–‰ ìƒí™© í‘œì‹œ
    with tqdm(total=2, desc="Dataset ë³€í™˜", unit="dataset") as pbar:
        pbar.set_description("Train Dataset ìƒì„±")
        train_dataset = Dataset.from_list(train_samples)
        pbar.update(1)
        
        pbar.set_description("Test Dataset ìƒì„±")
        test_dataset = Dataset.from_list(test_samples)
        pbar.update(1)
    
    # ì›ë³¸ ë¦¬ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ í•´ì œ
    del train_samples, test_samples
    gc.collect()
    
    print("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
    
    return DatasetDict({
        "train": train_dataset,
        "test": test_dataset
    })


def convert_sample_to_messages(sample: Dict[str, Any], dataset_name: str) -> Optional[Dict[str, Any]]:
    """ìƒ˜í”Œì„ messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    if dataset_name == "HuggingFaceTB/smoltalk" or "smoltalk" in dataset_name.lower():
        if "messages" in sample and isinstance(sample["messages"], list):
            img = sample.get("image", [])
            if not isinstance(img, list):
                img = [img]
            return {"messages": sample["messages"], "image": img}
    
    elif "orca-agentinstruct" in dataset_name:
        if "messages" in sample and isinstance(sample["messages"], list):
            img = sample.get("image", [])
            if not isinstance(img, list):
                img = [img]
            
            sample.update({"messages": sample["messages"], "images": img})
            return sample
    
    # ê¸°ë³¸ instruction-output í˜•ì‹ ì²˜ë¦¬
    if "instruction" in sample and "output" in sample:
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": sample["instruction"]}]},
            {"role": "assistant", "content": sample["output"]}
        ]
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img]
        return{"messages": messages, "images": img}
    
    # conversations í˜•ì‹ ì²˜ë¦¬
    if "conversations" in sample and isinstance(sample["conversations"], list):
        messages = []
        first_user_message_found = False
        for conv in sample["conversations"]:
            if isinstance(conv, dict) and "from" in conv and "value" in conv:
                role = "user" if conv["from"] in ["human", "user"] else "assistant"
                content = []
                if role == "user" and not first_user_message_found:
                    content.append({"type": "image"})
                    first_user_message_found = True
                content.append({"type": "text", "text": conv["value"]})
                messages.append({"role": role, "content": content})
        if messages:
            img = sample.get("image", [])
            if not isinstance(img, list):
                img = [img]
            return {"messages": messages, "images": img}
    
    # text í•„ë“œë§Œ ìˆëŠ” ê²½ìš° (ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸)
    if "text" in sample and isinstance(sample["text"], str):
        # ê°„ë‹¨í•œ ëŒ€í™”ë¡œ ë³€í™˜
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": "Continue the following text:"}]},
            {"role": "assistant", "content": sample["text"]}
        ]
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img]
        return {"messages": messages, "images": img}
    
    return None


def process_sample(sample: Dict[str, Any], tokenizer, max_length: int):
    """ìƒ˜í”Œì„ í† í¬ë‚˜ì´ì¦ˆí•˜ì—¬ í›ˆë ¨ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        if "messages" not in sample:
            return None
        
        # ë©”ì‹œì§€ ìœ íš¨ì„± ê²€ì‚¬
        messages = sample["messages"]
        if not isinstance(messages, list) or len(messages) == 0:
            return None
        
        # ê° ë©”ì‹œì§€ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸
        for msg in messages:
            if not isinstance(msg, dict) or "role" not in msg or "content" not in msg:
                return None
            if not isinstance(msg["content"], str) or len(msg["content"].strip()) == 0:
                return None
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš© - AutoProcessor í˜¸í™˜ì„± ê°œì„ 
        try:
            # AutoProcessorì¸ ê²½ìš° tokenizer ì†ì„±ì„ í†µí•´ ì ‘ê·¼
            if hasattr(tokenizer, 'tokenizer'):
                actual_tokenizer = tokenizer.tokenizer
            else:
                actual_tokenizer = tokenizer
            
            tokenized = actual_tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=False,
                # max_length=max_length,
                # truncation=True,
                return_dict=True,
                return_tensors="pt"
            )
        except Exception as e1:
            # ëŒ€ì•ˆ ë°©ë²•: ì±„íŒ… í…œí”Œë¦¿ ì—†ì´ ì§ì ‘ í…ìŠ¤íŠ¸ ë³€í™˜
            logger.warning(f"   âš ï¸ apply_chat_template ì‹¤íŒ¨, ëŒ€ì•ˆ ë°©ë²• ì‹œë„: {e1}")
            
            # ë©”ì‹œì§€ë¥¼ ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            text = ""
            for msg in messages:
                if msg["role"] == "user":
                    text += f"<start_of_turn>user\n{msg['content']}<end_of_turn>\n"
                elif msg["role"] == "assistant":
                    text += f"<start_of_turn>model\n{msg['content']}<end_of_turn>\n"
            
            # í† í¬ë‚˜ì´ì§•
            if hasattr(tokenizer, 'tokenizer'):
                actual_tokenizer = tokenizer.tokenizer
            else:
                actual_tokenizer = tokenizer
            
            tokenized = actual_tokenizer(
                text,
                max_length=max_length,
                # truncation=True,
                # padding=False,
                return_tensors="pt"
            )
        
        if tokenized is None:
            return None
        
        # í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ Datasetì— ì €ì¥ ê°€ëŠ¥í•˜ê²Œ í•¨
        result = {}
        for key, value in tokenized.items():
            if isinstance(value, torch.Tensor):
                result[key] = value.squeeze().tolist()
            else:
                result[key] = value
        
        # ìµœì†Œ ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ì€ ì‹œí€€ìŠ¤ ì œì™¸)
        if "input_ids" in result and len(result["input_ids"]) < 10:
            return None
        
        return result
        
    except Exception as e:
        # ë””ë²„ê¹…ì„ ìœ„í•´ ì˜ˆì™¸ ì •ë³´ ì¶œë ¥ (ì²˜ìŒ ëª‡ ê°œë§Œ)
        logger.error(f"âŒ í† í¬ë‚˜ì´ì§• ì˜ˆì™¸: {str(e)}")
        logger.error(f"   ìƒ˜í”Œ: {sample}")
        logger.error(f"   í† í¬ë‚˜ì´ì € íƒ€ì…: {type(tokenizer)}")
        logger.error(f"   í† í¬ë‚˜ì´ì €ì— chat_templateì´ ìˆëŠ”ê°€: {hasattr(tokenizer, 'chat_template')}")
        if hasattr(tokenizer, 'chat_template'):
            logger.error(f"   chat_template ê¸¸ì´: {len(str(tokenizer.chat_template)) if tokenizer.chat_template else 0}")
        import traceback
        traceback.print_exc()
        return None

def create_memory_efficient_collate_fn(tokenizer, max_length: int = 2048):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ collate function"""
    if hasattr(tokenizer, 'tokenizer'):
        actual_tokenizer = tokenizer.tokenizer
    else:
        actual_tokenizer = tokenizer
    
    def collate_fn(examples):
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
        batch_input_ids = []
        batch_attention_mask = []
        
        for ex in examples:
            if "messages" in ex:
                # ì‹¤ì‹œê°„ í† í¬ë‚˜ì´ì§• (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                try:
                    tokenized = actual_tokenizer.apply_chat_template(
                        ex["messages"],
                        tokenize=True,
                        add_generation_prompt=False,
                        max_length=max_length,
                        truncation=True,
                        return_tensors="pt"
                    )
                    
                    if tokenized is not None and "input_ids" in tokenized:
                        batch_input_ids.append(tokenized["input_ids"].squeeze())
                        batch_attention_mask.append(tokenized["attention_mask"].squeeze())
                except Exception as e:
                    logger.warning(f"í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {e}")
                    continue
        
        if not batch_input_ids:
            return None
        
        # íŒ¨ë”© ì²˜ë¦¬
        from torch.nn.utils.rnn import pad_sequence
        
        input_ids = pad_sequence(
            batch_input_ids, 
            batch_first=True, 
            padding_value=actual_tokenizer.pad_token_id or actual_tokenizer.eos_token_id
        )
        attention_mask = pad_sequence(
            batch_attention_mask, 
            batch_first=True, 
            padding_value=0
        )
        
        # labelsëŠ” input_idsì™€ ë™ì¼ (causal LM)
        labels = input_ids.clone()
        
        # íŒ¨ë”© í† í°ì€ loss ê³„ì‚°ì—ì„œ ì œì™¸
        pad_token_id = actual_tokenizer.pad_token_id or actual_tokenizer.eos_token_id
        labels[labels == pad_token_id] = -100
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del batch_input_ids, batch_attention_mask
        gc.collect()
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    return collate_fn

def create_simple_collate_fn(tokenizer):
    """ê°„ë‹¨í•œ collate function (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    tokenizer = tokenizer.tokenizer
    def collate_fn(examples):
        # input_idsì™€ attention_mask ì¶”ì¶œ
        input_ids = [torch.tensor(ex["input_ids"]) for ex in examples if "input_ids" in ex]
        attention_mask = [torch.tensor(ex["attention_mask"]) for ex in examples if "attention_mask" in ex]
        
        if not input_ids:
            return None
        
        # íŒ¨ë”© ì²˜ë¦¬
        from torch.nn.utils.rnn import pad_sequence
        
        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id or tokenizer.eos_token_id)
        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
        
        # labelsëŠ” input_idsì™€ ë™ì¼ (causal LM)
        labels = input_ids.clone()
        
        # íŒ¨ë”© í† í°ì€ loss ê³„ì‚°ì—ì„œ ì œì™¸
        pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id
        labels[labels == pad_token_id] = -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    return collate_fn

def get_memory_usage():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    import psutil
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return memory_info.rss / (1024 * 1024 * 1024)  # GB ë‹¨ìœ„

def log_memory_usage(stage: str):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¡œê¹…í•©ë‹ˆë‹¤."""
    memory_gb = get_memory_usage()
    logger.info(f"   ğŸ’¾ {stage} - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_gb:.2f} GB")

# í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ë°ì´í„°ì…‹ ë¹Œë” í•¨ìˆ˜ë“¤
def smoltalk_dataset(tokenizer, max_samples: int = 500, use_streaming: bool = True):
    """SmolTalk ë°ì´í„°ì…‹ ë¹Œë” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    log_memory_usage("SmolTalk ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_simple_sft_dataset(
        dataset_name="HuggingFaceTB/smoltalk",
        tokenizer=tokenizer,
        max_samples=max_samples,
        use_streaming=use_streaming,
        chunk_size=50
    )
    log_memory_usage("SmolTalk ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def orca_mini_dataset(tokenizer, max_samples: int = 500, use_streaming: bool = True):
    """Orca ë¯¸ë‹ˆ ë°ì´í„°ì…‹ ë¹Œë” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    log_memory_usage("Orca ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_simple_sft_dataset(
        dataset_name="microsoft/orca-agentinstruct-1M-v1",
        tokenizer=tokenizer, 
        max_samples=max_samples,
        use_streaming=use_streaming,
        chunk_size=50
    )
    log_memory_usage("Orca ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def create_memory_efficient_dataset(
    dataset_name: str,
    tokenizer,
    max_samples: int = 1000,
    chunk_size: int = 50,
    use_streaming: bool = True
):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ìƒì„±ê¸°"""
    log_memory_usage(f"{dataset_name} ë°ì´í„°ì…‹ ì‹œì‘")
    
    dataset = get_simple_sft_dataset(
        dataset_name=dataset_name,
        tokenizer=tokenizer,
        max_samples=max_samples,
        use_streaming=use_streaming,
        chunk_size=chunk_size
    )
    
    log_memory_usage(f"{dataset_name} ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def get_available_configs(dataset_name: str) -> List[str]:
    """ë°ì´í„°ì…‹ì˜ ì‚¬ìš© ê°€ëŠ¥í•œ config ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        configs = get_dataset_config_names(dataset_name)
        logger.info(f"ğŸ“‹ {dataset_name} ì‚¬ìš© ê°€ëŠ¥í•œ configs ({len(configs)}ê°œ):")
        for i, config in enumerate(configs[:10]):  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            logger.info(f"   {i+1}. {config}")
        if len(configs) > 10:
            logger.info(f"   ... ë° {len(configs) - 10}ê°œ ë”")
        return configs
    except Exception as e:
        logger.error(f"âŒ config ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []


if __name__ == "__main__":
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í…ŒìŠ¤íŠ¸
    from transformers import AutoTokenizer
    
    logger.info("ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    log_memory_usage("í”„ë¡œê·¸ë¨ ì‹œì‘")
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    log_memory_usage("í† í¬ë‚˜ì´ì € ë¡œë“œ í›„")
    
    # Config ì²˜ë¦¬ í™•ì¸ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸
    try:
        logger.info("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ config í™•ì¸")
        configs = get_available_configs("HuggingFaceTB/smoltalk")
        logger.info(f"ì´ {len(configs)}ê°œ config ë°œê²¬")
    except Exception as e:
        logger.error(f"Config í™•ì¸ ì‹¤íŒ¨: {e}")
    
    # SmolTalk ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ (ìŠ¤íŠ¸ë¦¬ë°) - ëª¨ë“  config ì²˜ë¦¬
    try:
        logger.info("ğŸ“¦ SmolTalk ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ (ìŠ¤íŠ¸ë¦¬ë° - ëª¨ë“  config)")
        dataset = smoltalk_dataset(tokenizer, max_samples=100, use_streaming=True)
        log_memory_usage("SmolTalk ë°ì´í„°ì…‹ ìƒì„± í›„")
        
        logger.info(f"ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset}")
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸
        try:
            first_sample = dataset['train'][0]
            logger.info(f"ìƒ˜í”Œ ì˜ˆì‹œ: {first_sample}")
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            
    except Exception as e:
        logger.error(f"SmolTalk ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ì¼ë°˜ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
    try:
        logger.info("ğŸ“¦ SmolTalk ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ (ì¼ë°˜)")
        dataset2 = smoltalk_dataset(tokenizer, max_samples=100, use_streaming=False)
        log_memory_usage("SmolTalk ì¼ë°˜ ë°ì´í„°ì…‹ ìƒì„± í›„")
        
        logger.info(f"ì¼ë°˜ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset2}")
            
    except Exception as e:
        logger.error(f"SmolTalk ì¼ë°˜ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    log_memory_usage("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    logger.info("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
