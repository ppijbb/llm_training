import logging
from tqdm import tqdm
from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names, concatenate_datasets
from transformers import AutoProcessor
import torch
from typing import Dict, Any, List, Optional
import traceback
import gc
import os
import random
import tempfile
import pathlib
import shutil
import json
from PIL import Image
from datasets import Dataset, DatasetDict, load_dataset, Image as DatasetImage, Sequence, Features
from collections import defaultdict

# simple_sft_datasetì˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ import
from data.simple_sft_dataset import (
    validate_image_data,
    validate_messages,
    safe_flatten_images,
    get_memory_usage,
    log_memory_usage
)

def dataset_exists(dataset_name: str) -> bool:
    """
    ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì´ Hugging Face Hubì— ì¡´ì¬í•˜ëŠ”ì§€ ê°„ë‹¨íˆ í™•ì¸í•©ë‹ˆë‹¤.
    ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€í•˜ë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        _ = get_dataset_config_names(dataset_name)
        return True
    except Exception:
        logger.warning(f"âš ï¸ ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_name} (ê±´ë„ˆëœ€)")
        return False

def convert_sample_to_messages(sample: Dict[str, Any], dataset_name: str) -> Optional[Dict[str, Any]]:
    """
    ìƒ˜í”Œì„ messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ ì§€ì› í™•ì¥)
    """
    # ScienceQA í˜•ì‹ ì²˜ë¦¬
    if "ScienceQA" in dataset_name or "scienceqa" in dataset_name.lower():
        question = sample.get("question", "")
        choices = sample.get("choices", [])
        answer = sample.get("answer", "")
        explanation = sample.get("explanation", "")
        
        # ì§ˆë¬¸ê³¼ ì„ íƒì§€ êµ¬ì„±
        question_text = question
        if choices:
            choices_text = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(choices)])
            question_text = f"{question}\n\n{choices_text}"
        
        # ë‹µë³€ êµ¬ì„±
        answer_text = answer
        if explanation:
            answer_text = f"{answer}\n\nExplanation: {explanation}"
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img] if img is not None else []
        img = validate_image_data(img)
        
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬, ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì „ìš©
        if img:
            messages = [
                {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": question_text}]},
                {"role": "assistant", "content": [{"type": "text", "text": answer_text}]}
            ]
        else:
            messages = [
                {"role": "user", "content": [{"type": "text", "text": question_text}]},
                {"role": "assistant", "content": [{"type": "text", "text": answer_text}]}
            ]
        
        return {"messages": messages, "images": img if img else []}
    
    # LLaVA-OneVision-Data í˜•ì‹ ì²˜ë¦¬
    if "llava-onevision" in dataset_name.lower() or "onevision" in dataset_name.lower():
        # LLaVA í˜•ì‹: conversations ë˜ëŠ” messages í•„ë“œ ì‚¬ìš©
        if "conversations" in sample:
            messages = []
            img = sample.get("image", [])
            if not isinstance(img, list):
                img = [img] if img is not None else []
            img = validate_image_data(img)
            
            first_user = True
            for conv in sample["conversations"]:
                if isinstance(conv, dict):
                    role = conv.get("from", "").lower()
                    value = conv.get("value", "")
                    
                    if role in ["human", "user"]:
                        content = []
                        if first_user and img:
                            content.append({"type": "image"})
                            first_user = False
                        if value:
                            content.append({"type": "text", "text": str(value)})
                        if content:
                            messages.append({"role": "user", "content": content})
                    elif role in ["gpt", "assistant"]:
                        if value:
                            messages.append({"role": "assistant", "content": [{"type": "text", "text": str(value)}]})
            
            if messages and img:
                return {"messages": messages, "images": img}
            elif messages:
                # ì´ë¯¸ì§€ê°€ ì—†ì–´ë„ ì²˜ë¦¬
                return {"messages": messages, "images": []}
        
        # messages í˜•ì‹ ì§ì ‘ ì§€ì›
        if "messages" in sample and isinstance(sample["messages"], list):
            img = sample.get("image", [])
            if not isinstance(img, list):
                img = [img] if img is not None else []
            img = validate_image_data(img)
            
            messages = validate_messages(sample["messages"])
            return {"messages": messages, "images": img if img else []}
        
        # instruction-output í˜•ì‹
        if "instruction" in sample and "output" in sample:
            img = sample.get("image", [])
            if not isinstance(img, list):
                img = [img] if img is not None else []
            img = validate_image_data(img)
            
            if img:
                messages = [
                    {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": sample["instruction"]}]},
                    {"role": "assistant", "content": [{"type": "text", "text": sample["output"]}]}
                ]
            else:
                messages = [
                    {"role": "user", "content": [{"type": "text", "text": sample["instruction"]}]},
                    {"role": "assistant", "content": [{"type": "text", "text": sample["output"]}]}
                ]
            
            return {"messages": messages, "images": img if img else []}
    
    # VQA í˜•ì‹ ì²˜ë¦¬ (VQAv2) - í•˜ìœ„ í˜¸í™˜ì„±
    if "VQA" in dataset_name or "vqa" in dataset_name.lower():
        question = sample.get("question", "")
        answers = sample.get("answers", [])
        if isinstance(answers, list) and len(answers) > 0:
            if isinstance(answers[0], dict):
                answer = answers[0].get("answer", "")
            else:
                answer = str(answers[0])
        else:
            answer = sample.get("answer", "")
        
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": question}]},
            {"role": "assistant", "content": [{"type": "text", "text": answer}]}
        ]
        
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img] if img is not None else []
        img = validate_image_data(img)
        if not img:
            return None
        
        return {"messages": messages, "images": img}
    
    # Flickr30k í˜•ì‹ ì²˜ë¦¬ - í•˜ìœ„ í˜¸í™˜ì„±
    if "flickr30k" in dataset_name.lower():
        captions = sample.get("caption", [])
        if not isinstance(captions, list):
            captions = [captions] if captions else []
        
        if not captions:
            return None
        
        caption = str(captions[0])
        
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": "Describe this image."}]},
            {"role": "assistant", "content": [{"type": "text", "text": caption}]}
        ]
        
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img] if img is not None else []
        img = validate_image_data(img)
        if not img:
            return None
        
        return {"messages": messages, "images": img}
    
    # CORD (OCR) í˜•ì‹ ì²˜ë¦¬
    if "cord" in dataset_name.lower():
        # CORDëŠ” ë¬¸ì„œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨
        text = sample.get("text", "")
        if not text:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": "Extract and read the text from this document."}]},
            {"role": "assistant", "content": [{"type": "text", "text": text}]}
        ]
        
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img] if img is not None else []
        img = validate_image_data(img)
        if not img:
            return None
        
        return {"messages": messages, "images": img}
    
    # FUNSD (OCR) í˜•ì‹ ì²˜ë¦¬
    if "funsd" in dataset_name.lower() or "layoutlmv3" in dataset_name.lower():
        words = sample.get("words", [])
        bboxes = sample.get("bboxes", [])
        
        # ë‹¨ì–´ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        text = " ".join([str(word) for word in words]) if words else ""
        if not text:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": "Extract and read the text from this document."}]},
            {"role": "assistant", "content": [{"type": "text", "text": text}]}
        ]
        
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img] if img is not None else []
        img = validate_image_data(img)
        if not img:
            return None
        
        return {"messages": messages, "images": img}
    
    # SciAlpaca / Camel-AI Science í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©)
    if "scialpaca" in dataset_name.lower() or "camel-ai/science" in dataset_name.lower():
        # ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ instruction-output í˜•ì‹ì„ ë”°ë¦„
        instruction = sample.get("instruction", "")
        output = sample.get("output", "")
        
        # Camel-AI ScienceëŠ” message_1, message_2 í˜•ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
        if not instruction and "message_1" in sample and "message_2" in sample:
            instruction = sample["message_1"]
            output = sample["message_2"]

        if not instruction or not output:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": instruction}]},
            {"role": "assistant", "content": [{"type": "text", "text": output}]}
        ]
        return {"messages": messages, "images": []}

    # SciTLDR í˜•ì‹ ì²˜ë¦¬
    if "scitldr" in dataset_name.lower():
        # source (abstract) -> target (summary)
        source_text = " ".join(sample.get("source", []))
        target_text = " ".join(sample.get("target", []))
        
        if not source_text or not target_text:
            return None

        instruction = f"Summarize the following scientific text in one or two sentences:\n\n{source_text}"
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": instruction}]},
            {"role": "assistant", "content": [{"type": "text", "text": target_text}]}
        ]
        return {"messages": messages, "images": []}

    # SROIE (OCR) í˜•ì‹ ì²˜ë¦¬
    if "sroie" in dataset_name.lower():
        text = sample.get("text", "")
        if not text:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": "Extract and read the text from this document."}]},
            {"role": "assistant", "content": [{"type": "text", "text": text}]}
        ]
        
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img] if img is not None else []
        img = validate_image_data(img)
        if not img:
            return None
        
        return {"messages": messages, "images": img}
    
    # Evol-CodeAlpaca í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©)
    if "evol-codealpaca" in dataset_name.lower():
        instruction = sample.get("instruction", "")
        output = sample.get("output", "")
        _input = sample.get("input", "")
        if not instruction or not output:
            return None
        user_text = instruction if not _input else f"{instruction}\n\nInput:\n{_input}"
        messages = [
            {"role": "user", "content": [{"type": "text", "text": user_text}]},
            {"role": "assistant", "content": [{"type": "text", "text": output}]}
        ]
        return {"messages": messages, "images": []}
    
    # OCR-VQA ê³„ì—´ (ì¼ë°˜ VQA ìŠ¤í‚¤ë§ˆ ì¬ì‚¬ìš©)
    if "ocr-vqa" in dataset_name.lower() or "ocrvqa" in dataset_name.lower():
        question = sample.get("question", "")
        answers = sample.get("answers", [])
        answer = ""
        if isinstance(answers, list) and answers:
            if isinstance(answers[0], dict):
                answer = answers[0].get("answer", "")
            else:
                answer = str(answers[0])
        else:
            answer = sample.get("answer", "")
        if not question:
            return None
        messages = [
            {"role": "user", "content": [{"type": "image"}, {"type": "text", "text": question}]},
            {"role": "assistant", "content": [{"type": "text", "text": answer}]}
        ]
        img = sample.get("image", [])
        if not isinstance(img, list):
            img = [img] if img is not None else []
        img = validate_image_data(img)
        if not img:
            return None
        return {"messages": messages, "images": img}
    
    # MetaMathQA í˜•ì‹ ì²˜ë¦¬ (í•™ìŠµìš© ìˆ˜í•™ instruction)
    if "metamathqa" in dataset_name.lower() or "meta-math" in dataset_name.lower():
        query = sample.get("query", "")
        response = sample.get("response", "")
        if not query or not response:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": query}]},
            {"role": "assistant", "content": [{"type": "text", "text": response}]}
        ]
        return {"messages": messages, "images": []}
    
    # Math-Python-Reasoning í˜•ì‹ ì²˜ë¦¬ (í•™ìŠµìš© ìˆ˜í•™ Python ì¶”ë¡ )
    if "math-python-reasoning" in dataset_name.lower():
        instruction = sample.get("instruction", "")
        output = sample.get("output", "")
        if not instruction or not output:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": instruction}]},
            {"role": "assistant", "content": [{"type": "text", "text": output}]}
        ]
        return {"messages": messages, "images": []}
    
    # UltraInteract í˜•ì‹ ì²˜ë¦¬ (í•™ìŠµìš© ë…¼ë¦¬ ì¶”ë¡  instruction)
    if "ultrainteract" in dataset_name.lower() or "ultra-interact" in dataset_name.lower():
        # UltraInteractëŠ” ë‹¤ì–‘í•œ í˜•ì‹ì´ ìˆì„ ìˆ˜ ìˆìŒ
        if "messages" in sample:
            messages = validate_messages(sample["messages"])
            return {"messages": messages, "images": []}
        elif "instruction" in sample and "output" in sample:
            messages = [
                {"role": "user", "content": [{"type": "text", "text": sample["instruction"]}]},
                {"role": "assistant", "content": [{"type": "text", "text": sample["output"]}]}
            ]
            return {"messages": messages, "images": []}
        elif "question" in sample and "answer" in sample:
            messages = [
                {"role": "user", "content": [{"type": "text", "text": sample["question"]}]},
                {"role": "assistant", "content": [{"type": "text", "text": sample["answer"]}]}
            ]
            return {"messages": messages, "images": []}
    
    # UltraFeedback í˜•ì‹ ì²˜ë¦¬ (í•™ìŠµìš© ì¶”ë¡  instruction)
    if "ultrafeedback" in dataset_name.lower():
        # UltraFeedbackì€ ë‹¤ì–‘í•œ í˜•ì‹ì´ ìˆì„ ìˆ˜ ìˆìŒ
        if "messages" in sample:
            messages = validate_messages(sample["messages"])
            return {"messages": messages, "images": []}
        elif "instruction" in sample and "output" in sample:
            messages = [
                {"role": "user", "content": [{"type": "text", "text": sample["instruction"]}]},
                {"role": "assistant", "content": [{"type": "text", "text": sample["output"]}]}
            ]
            return {"messages": messages, "images": []}
        elif "prompt" in sample and "response" in sample:
            messages = [
                {"role": "user", "content": [{"type": "text", "text": sample["prompt"]}]},
                {"role": "assistant", "content": [{"type": "text", "text": sample["response"]}]}
            ]
            return {"messages": messages, "images": []}
    
    # GSM8K í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©) - ë²¤ì¹˜ë§ˆí¬ìš©, í•˜ìœ„ í˜¸í™˜ì„±
    if "gsm8k" in dataset_name.lower():
        question = sample.get("question", "")
        answer = sample.get("answer", "")
        if not question or not answer:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": question}]},
            {"role": "assistant", "content": [{"type": "text", "text": answer}]}
        ]
        return {"messages": messages, "images": []}
    
    # MATH í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©) - ë²¤ì¹˜ë§ˆí¬ìš©, í•˜ìœ„ í˜¸í™˜ì„±
    if "competition_math" in dataset_name.lower() or "hendrycks/math" in dataset_name.lower():
        problem = sample.get("problem", "")
        solution = sample.get("solution", "")
        if not problem or not solution:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": problem}]},
            {"role": "assistant", "content": [{"type": "text", "text": solution}]}
        ]
        return {"messages": messages, "images": []}
    
    # PubMedQA í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©) - ì œê±°ë¨, í•˜ìœ„ í˜¸í™˜ì„±
    if "pubmed_qa" in dataset_name.lower():
        question = sample.get("question", "")
        long_answer = sample.get("long_answer", "")
        final_decision = sample.get("final_decision", "")
        
        if not question:
            return None
        
        answer_text = long_answer if long_answer else final_decision
        if not answer_text:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": question}]},
            {"role": "assistant", "content": [{"type": "text", "text": answer_text}]}
        ]
        return {"messages": messages, "images": []}
    
    # CodeSearchNet í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©)
    if "code_search_net" in dataset_name.lower() or "codesearchnet" in dataset_name.lower():
        code = sample.get("code", "")
        docstring = sample.get("docstring", "")
        func_name = sample.get("func_name", "")
        
        if not code:
            return None
        
        # ì½”ë“œì™€ ì„¤ëª…ì„ instruction-output í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        instruction = f"Write code for: {docstring}" if docstring else f"Write code for function: {func_name}" if func_name else "Write the following code:"
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": instruction}]},
            {"role": "assistant", "content": [{"type": "text", "text": code}]}
        ]
        return {"messages": messages, "images": []}
    
    # CoNaLa í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©)
    if "conala" in dataset_name.lower():
        intent = sample.get("intent", "")
        snippet = sample.get("snippet", "")
        
        if not intent or not snippet:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": intent}]},
            {"role": "assistant", "content": [{"type": "text", "text": snippet}]}
        ]
        return {"messages": messages, "images": []}
    
    # The Stack / StarCoderData í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©) - í•˜ìœ„ í˜¸í™˜ì„±
    if "the-stack" in dataset_name.lower() or "starcoderdata" in dataset_name.lower():
        content = sample.get("content", "")
        if not content:
            return None
        
        # ì½”ë“œ ë°ì´í„°ì…‹ì€ instruction-output í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        messages = [
            {"role": "user", "content": [{"type": "text", "text": "Write the following code:"}]},
            {"role": "assistant", "content": [{"type": "text", "text": content}]}
        ]
        return {"messages": messages, "images": []}
    
    # LogiQA í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©) - ë²¤ì¹˜ë§ˆí¬ìš©, í•˜ìœ„ í˜¸í™˜ì„±
    if "logiqa" in dataset_name.lower():
        question = sample.get("question", "")
        options = sample.get("options", [])
        answer = sample.get("answer", "")
        
        if not question or not answer:
            return None
        
        question_text = question
        if options:
            options_text = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options)])
            question_text = f"{question}\n\n{options_text}"
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": question_text}]},
            {"role": "assistant", "content": [{"type": "text", "text": answer}]}
        ]
        return {"messages": messages, "images": []}
    
    # ReClor í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©) - ë²¤ì¹˜ë§ˆí¬ìš©, í•˜ìœ„ í˜¸í™˜ì„±
    if "reclor" in dataset_name.lower():
        question = sample.get("question", "")
        answers = sample.get("answers", [])
        label = sample.get("label", -1)
        
        if not question:
            return None
        
        question_text = question
        if answers and isinstance(answers, list):
            options_text = "\n".join([f"{chr(65+i)}. {ans}" for i, ans in enumerate(answers)])
            question_text = f"{question}\n\n{options_text}"
        
        answer_text = answers[label] if label >= 0 and label < len(answers) else (answers[0] if answers else "")
        if not answer_text:
            return None
        
        messages = [
            {"role": "user", "content": [{"type": "text", "text": question_text}]},
            {"role": "assistant", "content": [{"type": "text", "text": answer_text}]}
        ]
        return {"messages": messages, "images": []}
    
    # OpenOrca í˜•ì‹ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ ì „ìš©)
    if "openorca" in dataset_name.lower() or "open-orca" in dataset_name.lower():
        # OpenOrcaëŠ” conversations í˜•ì‹ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        if "conversations" in sample:
            messages = []
            for conv in sample["conversations"]:
                if isinstance(conv, dict):
                    role = conv.get("from", "user")
                    value = conv.get("value", "")
                    if value:
                        role_mapped = "user" if role in ["human", "user"] else "assistant"
                        messages.append({
                            "role": role_mapped,
                            "content": [{"type": "text", "text": value}]
                        })
            if messages:
                return {"messages": messages, "images": []}
        
        # instruction-output í˜•ì‹
        if "instruction" in sample and "response" in sample:
            messages = [
                {"role": "user", "content": [{"type": "text", "text": sample["instruction"]}]},
                {"role": "assistant", "content": [{"type": "text", "text": sample["response"]}]}
            ]
            return {"messages": messages, "images": []}
    
    # simple_sft_datasetì˜ ê¸°ë³¸ ë³€í™˜ ë¡œì§ ì‚¬ìš©
    from data.simple_sft_dataset import convert_sample_to_messages as base_convert
    result = base_convert(sample, dataset_name)
    
    # base_convertê°€ Noneì„ ë°˜í™˜í•˜ê±°ë‚˜ ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°, í…ìŠ¤íŠ¸ ì „ìš©ìœ¼ë¡œ ì²˜ë¦¬ ì‹œë„
    if result is None:
        # instruction-output í˜•ì‹ ì¬ì‹œë„
        if "instruction" in sample and "output" in sample:
            messages = [
                {"role": "user", "content": [{"type": "text", "text": sample["instruction"]}]},
                {"role": "assistant", "content": [{"type": "text", "text": sample["output"]}]}
            ]
            return {"messages": messages, "images": []}
    
    # base_convert ê²°ê³¼ì— ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
    if result and "images" in result:
        if not result["images"]:
            result["images"] = []
    
    return result

# Configure logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹ ì„¤ì •
# ê° ë„ë©”ì¸ë³„ë¡œ í…ìŠ¤íŠ¸ ì „ìš© ë˜ëŠ” ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì„ ì§€ì •í•©ë‹ˆë‹¤.
# í…ìŠ¤íŠ¸ ì „ìš© ë°ì´í„°ì…‹ë„ í—ˆìš©í•˜ë©°, ìµœì¢…ì ìœ¼ë¡œ messages í˜•ì‹ìœ¼ë¡œ í†µí•©ë©ë‹ˆë‹¤.
DOMAIN_DATASETS = {
    "math": [
        "meta-math/MetaMathQA",  # MetaMathQA: ìˆ˜í•™ instruction ë°ì´í„°ì…‹ (í•™ìŠµìš©)
        "sdiazlor/math-python-reasoning-dataset",  # Math-Python-Reasoning: ìˆ˜í•™ Python ì¶”ë¡  (í•™ìŠµìš©)
    ],
    "science": [
        "derek-thomas/ScienceQA",  # SciTLDR: ê³¼í•™ ë…¼ë¬¸ ìš”ì•½ (í•™ìŠµìš©)
        "armanc/ScienceQA"
    ],
    "code": [
        "theblackcat102/evol-codealpaca-v1", # Evol-CodeAlpaca: ì½”ë“œ instruction (í•™ìŠµìš©)
        "microsoft/rStar-Coder",  # rStar-Coder: ì½”ë“œ ë°ì´í„°ì…‹
    ],
    "puzzle": [
        "openbmb/UltraInteract_sft",  # UltraInteract_sft: ë…¼ë¦¬ ì¶”ë¡  instruction ë°ì´í„°ì…‹ (í•™ìŠµìš©)
        "HuggingFaceH4/ultrafeedback_binarized",  # UltraFeedback: ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ëŒ€ì²´ í•„ìš”
    ],
    "vision": [
        "lmms-lab/LLaVA-OneVision-Data",  # LLaVA-OneVision-Data: ë‹¤ì–‘í•œ ë¹„ì „ íƒœìŠ¤í¬ (ë©€í‹°ëª¨ë‹¬)
        # "textvqa",  # TextVQA: ì¡´ì¬í•˜ì§€ ì•ŠìŒ, ëŒ€ì²´ í•„ìš”
    ],
    "ocr": [
        "howard-hou/OCR-VQA",  # OCR-VQA: OCR ì§ˆì˜ì‘ë‹µ ë°ì´í„°ì…‹
        "allenai/olmOCR-mix-1025",  # olmOCR-mix: PDF OCR ë°ì´í„°ì…‹
    ],
    "chat": [
        "HuggingFaceTB/smoltalk",  # SmolTalk: ì¼ë°˜ ì±„íŒ… (ë©€í‹°ëª¨ë‹¬ ê°€ëŠ¥)
        "Open-Orca/OpenOrca",  # OpenOrca: ì¼ë°˜ ëŒ€í™” (í…ìŠ¤íŠ¸ ì „ìš©)
    ]
}

def get_domain_from_config(config_name: str, dataset_name: str) -> Optional[str]:
    """
    Config ì´ë¦„ê³¼ ë°ì´í„°ì…‹ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ë„ë©”ì¸ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.
    
    Args:
        config_name: ë°ì´í„°ì…‹ config ì´ë¦„
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
    
    Returns:
        ì¶”ë¡ ëœ ë„ë©”ì¸ ì´ë¦„ ë˜ëŠ” None
    """
    config_lower = config_name.lower()
    dataset_lower = dataset_name.lower()
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ë„ë©”ì¸ ë§¤ì¹­ (ìš°ì„ ìˆœìœ„ ìˆœ)
    math_keywords = ["math", "mathematical", "algebra", "geometry", "calculus", "arithmetic", "equation"]
    science_keywords = ["science", "physics", "chemistry", "biology", "scientific", "astronomy", "geology"]
    code_keywords = ["code", "programming", "python", "javascript", "coding", "software", "algorithm", "function"]
    puzzle_keywords = ["puzzle", "logic", "reasoning", "riddle", "brain", "challenge", "problem"]
    vision_keywords = ["vision", "visual", "image", "photo", "picture", "camera", "see", "look"]
    ocr_keywords = ["ocr", "text", "document", "scan", "recognition", "read", "extract", "textual"]
    
    if any(keyword in config_lower for keyword in math_keywords):
        return "math"
    elif any(keyword in config_lower for keyword in science_keywords):
        return "science"
    elif any(keyword in config_lower for keyword in code_keywords):
        return "code"
    elif any(keyword in config_lower for keyword in puzzle_keywords):
        return "puzzle"
    elif any(keyword in config_lower for keyword in vision_keywords):
        return "vision"
    elif any(keyword in config_lower for keyword in ocr_keywords):
        return "ocr"
    
    # ë°ì´í„°ì…‹ ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­
    if any(keyword in dataset_lower for keyword in math_keywords):
        return "math"
    elif any(keyword in dataset_lower for keyword in science_keywords):
        return "science"
    elif any(keyword in dataset_lower for keyword in code_keywords):
        return "code"
    elif any(keyword in dataset_lower for keyword in puzzle_keywords):
        return "puzzle"
    elif any(keyword in dataset_lower for keyword in vision_keywords):
        return "vision"
    elif any(keyword in dataset_lower for keyword in ocr_keywords):
        return "ocr"
    
    return None

def get_multi_domain_sft_dataset(
    domain_configs: Optional[Dict[str, List[str]]] = None,
    tokenizer=None,
    max_length: int = 2048,
    max_samples_per_domain: int = 200,
    test_size: float = 0.1,
    use_streaming: bool = True,
    chunk_size: int = 1000
):
    """
    ë©€í‹° ë„ë©”ì¸ SFT ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        domain_configs: ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹ ì„¤ì • ë”•ì…”ë„ˆë¦¬
            ì˜ˆ: {"math": ["dataset1", "dataset2"], "science": ["dataset3"]}
        tokenizer: í† í¬ë‚˜ì´ì €
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        max_samples_per_domain: ë„ë©”ì¸ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        test_size: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨
        use_streaming: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
        chunk_size: ì²­í¬ í¬ê¸°
    
    Returns:
        DatasetDict with train/test splits, ê° ìƒ˜í”Œì— 'domain' í•„ë“œ í¬í•¨
    """
    if tokenizer is None:
        raise ValueError("Tokenizer must be provided")
    
    if domain_configs is None:
        domain_configs = DOMAIN_DATASETS
    
    logger.info(f"ğŸ“¦ ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘")
    logger.info(f"   - ë„ë©”ì¸ ìˆ˜: {len(domain_configs)}ê°œ")
    logger.info(f"   - ë„ë©”ì¸ë‹¹ ìµœëŒ€ ìƒ˜í”Œ: {max_samples_per_domain}ê°œ")
    logger.info(f"   - ì´ ìµœëŒ€ ìƒ˜í”Œ: {max_samples_per_domain * len(domain_configs)}ê°œ")
    logger.info(f"   - streaming: {use_streaming}")
    
    log_memory_usage("ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘")
    
    base_temp_dir = "/mls/conan/tmp"
    os.makedirs(base_temp_dir, exist_ok=True)
    temp_dir = tempfile.mkdtemp(dir=base_temp_dir)
    logger.info(f"ğŸ“‚ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {temp_dir}")
    images_dir = os.path.join(temp_dir, "images")
    os.makedirs(images_dir, exist_ok=True)

    try:
        # ë„ë©”ì¸ë³„ ìƒ˜í”Œ ì¹´ìš´í„°
        domain_counts = defaultdict(lambda: {"train": 0, "test": 0})
        total_processed = 0
        image_counter = 0
        
        train_jsonl_path = os.path.join(temp_dir, "train.jsonl")
        test_jsonl_path = os.path.join(temp_dir, "test.jsonl")

        with open(train_jsonl_path, "w", encoding="utf-8") as train_f, \
             open(test_jsonl_path, "w", encoding="utf-8") as test_f:
            
            # ê° ë„ë©”ì¸ë³„ë¡œ ì²˜ë¦¬
            domain_pbar = tqdm(domain_configs.items(), desc="ë„ë©”ì¸ ì²˜ë¦¬", unit="domain")
            
            for domain, dataset_names in domain_pbar:
                domain_pbar.set_description(f"ë„ë©”ì¸: {domain}")
                
                # ë¹ˆ ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                if not dataset_names:
                    logger.warning(f"   âš ï¸ {domain} ë„ë©”ì¸ì— ë°ì´í„°ì…‹ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                domain_processed = 0
                
                # ScienceQA ë¯¸ëŸ¬ ì¤‘ë³µ ë°©ì§€ í”Œë˜ê·¸
                scienceqa_taken = False
                
                for dataset_name in dataset_names:
                    if domain_processed >= max_samples_per_domain:
                        break
                    
                    try:
                        logger.info(f"   ğŸ“‹ {domain} ë„ë©”ì¸ - ë°ì´í„°ì…‹: {dataset_name}")
                        
                        # ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸
                        if not dataset_exists(dataset_name):
                            continue
                        
                        # ë°ì´í„°ì…‹ì˜ config ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  ì„œë¸Œì…‹ í™•ì¸)
                        try:
                            available_configs = get_dataset_config_names(dataset_name)
                            if not available_configs:
                                logger.warning(f"   âš ï¸ {dataset_name}ì— configê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ split ì‚¬ìš©")
                                available_configs = ["default"]
                            else:
                                logger.info(f"   ğŸ“‹ {dataset_name} - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  Config/Subset ({len(available_configs)}ê°œ):")
                                # ëª¨ë“  configë¥¼ ì¶œë ¥ (ì œí•œ ì—†ì´)
                                # for idx, c in enumerate(available_configs, 1):
                                #     logger.info(f"      {idx}. {c}")
                                logger.info(f"   âœ… ì´ {len(available_configs)}ê°œ ì„œë¸Œì…‹ í™•ì¸ ì™„ë£Œ")
                        except Exception as e:
                            logger.warning(f"   âš ï¸ Config ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}, ê¸°ë³¸ split ì‚¬ìš©")
                            available_configs = ["default"]
                        
                        # LLaVA-OneVision-DataëŠ” onevision ì„œë¸Œì…‹ë§Œ ì‚¬ìš© (ì—†ìœ¼ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ config ì‚¬ìš©)
                        if "llava-onevision" in dataset_name.lower() or "llava-onevision-data" in dataset_name.lower():
                            filtered = [c for c in available_configs if "onevision" in str(c).lower()]
                            if filtered:
                                available_configs = filtered
                            else:
                                # onevisionì´ ì—†ìœ¼ë©´ ì²˜ìŒ ëª‡ ê°œ configë§Œ ì‚¬ìš© (ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ)
                                logger.info(f"   â„¹ï¸ 'onevision' configê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ config ì¤‘ ì¼ë¶€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                available_configs = available_configs[:5]  # ì²˜ìŒ 5ê°œë§Œ ì‚¬ìš©
                        
                        # ScienceQA ë¯¸ëŸ¬ê°€ ë‹¤ìˆ˜ì¸ ê²½ìš° í•œìª½ë§Œ ì‚¬ìš©
                        if domain == "science" and ("scienceqa" in dataset_name.lower()):
                            if scienceqa_taken:
                                logger.info(f"   ğŸ” ScienceQA ë¯¸ëŸ¬ ì¤‘ë³µ ë°©ì§€ë¡œ ê±´ë„ˆëœ€: {dataset_name}")
                                continue
                            scienceqa_taken = True
                        
                        # Configë³„ë¡œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
                        samples_per_config = max(1, max_samples_per_domain // max(len(available_configs), 1))
                        
                        config_pbar = tqdm(available_configs, desc=f"  {domain} config", unit="config", leave=False)
                        
                        for config in config_pbar:
                            if domain_processed >= max_samples_per_domain:
                                break
                            
                            try:
                                # Config ì´ë¦„ìœ¼ë¡œ ë„ë©”ì¸ ì¬í™•ì¸
                                inferred_domain = get_domain_from_config(config, dataset_name)
                                if inferred_domain and inferred_domain != domain:
                                    logger.debug(f"   ğŸ”„ Config {config}ì˜ ë„ë©”ì¸ì´ {inferred_domain}ìœ¼ë¡œ ì¶”ë¡ ë¨ (ìš”ì²­: {domain})")
                                    # ì¶”ë¡ ëœ ë„ë©”ì¸ì´ ë‹¤ë¥´ë©´ ê±´ë„ˆë›°ê¸° (ì„ íƒì )
                                    # continue
                                
                                config_pbar.set_description(f"  {domain} config: {config[:30]}...")
                                
                                # ì‚¬ìš© ê°€ëŠ¥í•œ split í™•ì¸
                                try:
                                    if config == "default":
                                        available_splits = get_dataset_split_names(dataset_name)
                                    else:
                                        available_splits = get_dataset_split_names(dataset_name, config_name=config)
                                    
                                    logger.info(f"   ğŸ“‹ Config {config} - ì‚¬ìš© ê°€ëŠ¥í•œ split: {available_splits}")
                                    
                                    # Train split ì„ íƒ: train_sft > train
                                    train_split = None
                                    if "train_sft" in available_splits:
                                        train_split = "train_sft"
                                        logger.info(f"   âœ… Train split ì„ íƒ: train_sft")
                                    elif "train" in available_splits:
                                        train_split = "train"
                                        logger.info(f"   âœ… Train split ì„ íƒ: train")
                                    else:
                                        logger.warning(f"   âš ï¸ Config {config}ì— train ë˜ëŠ” train_sft splitì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                                        continue
                                    
                                    # Test split ì„ íƒ: test_sft > test (ì—†ì–´ë„ ê³„ì† ì§„í–‰)
                                    test_split = None
                                    if "test_sft" in available_splits:
                                        test_split = "test_sft"
                                        logger.info(f"   âœ… Test split ì„ íƒ: test_sft")
                                    elif "test" in available_splits:
                                        test_split = "test"
                                        logger.info(f"   âœ… Test split ì„ íƒ: test")
                                    else:
                                        logger.info(f"   â„¹ï¸ Config {config}ì— test ë˜ëŠ” test_sft splitì´ ì—†ìŠµë‹ˆë‹¤. trainë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                                    
                                except Exception as e:
                                    logger.warning(f"   âš ï¸ Split ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}, ê¸°ë³¸ train ì‚¬ìš©")
                                    train_split = "train"
                                    test_split = None
                                
                                # Train split ì²˜ë¦¬
                                try:
                                    if config == "default":
                                        train_dataset = load_dataset(
                                            path=dataset_name,
                                            split=train_split,
                                            streaming=use_streaming
                                        )
                                    else:
                                        train_dataset = load_dataset(
                                            path=dataset_name,
                                            name=config,
                                            split=train_split,
                                            streaming=use_streaming
                                        )
                                    
                                    train_samples_per_config = samples_per_config
                                    if test_split:
                                        # test splitì´ ìˆìœ¼ë©´ train ìƒ˜í”Œ ìˆ˜ë¥¼ ì¡°ì •
                                        train_samples_per_config = int(samples_per_config * (1 - test_size))
                                    
                                    sample_pbar = tqdm(
                                        total=min(train_samples_per_config, max_samples_per_domain - domain_processed),
                                        desc=f"    Train ìƒ˜í”Œ ì²˜ë¦¬",
                                        unit="sample",
                                        leave=False
                                    )
                                    
                                    train_processed = 0
                                    for sample in train_dataset:
                                        if domain_processed >= max_samples_per_domain or train_processed >= train_samples_per_config:
                                            break
                                        
                                        # ìƒ˜í”Œ ë³€í™˜
                                        converted = convert_sample_to_messages(sample, dataset_name)
                                        if not converted:
                                            continue

                                        # ì´ë¯¸ì§€ ì²˜ë¦¬
                                        image_paths = []
                                        if "images" in converted and converted["images"]:
                                            flattened_images = validate_image_data(converted["images"])
                                            
                                            if flattened_images:
                                                valid_sample = True
                                                
                                                for img_obj in flattened_images:
                                                    if isinstance(img_obj, Image.Image):
                                                        try:
                                                            img_path = os.path.join(images_dir, f"{image_counter}.png")
                                                            img_obj.save(img_path, "PNG")
                                                            image_paths.append(img_path)
                                                            image_counter += 1
                                                        except Exception as img_e:
                                                            logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {img_e}")
                                                            valid_sample = False
                                                            break
                                                    elif img_obj is not None:
                                                        logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(img_obj)}")
                                                        valid_sample = False
                                                        break
                                                
                                                if not valid_sample:
                                                    continue
                                        
                                        converted["images"] = image_paths
                                        converted["domain"] = domain
                                        
                                        train_f.write(json.dumps(converted) + "\n")
                                        domain_counts[domain]["train"] += 1
                                        domain_processed += 1
                                        total_processed += 1
                                        train_processed += 1
                                        
                                        sample_pbar.update(1)
                                        memory_gb = get_memory_usage()
                                        sample_pbar.set_postfix({
                                            "ë„ë©”ì¸": f"{domain_processed}/{max_samples_per_domain}",
                                            "ì´ ì²˜ë¦¬": f"{total_processed}",
                                            "ë©”ëª¨ë¦¬": f"{memory_gb:.1f}GB"
                                        })
                                    
                                    sample_pbar.close()
                                    del train_dataset
                                    gc.collect()
                                    
                                except Exception as e:
                                    logger.warning(f"   âš ï¸ Train split {train_split} ë¡œë“œ ì‹¤íŒ¨: {e}")
                                    continue
                                
                                # Test split ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
                                if test_split:
                                    try:
                                        if config == "default":
                                            test_dataset = load_dataset(
                                                path=dataset_name,
                                                split=test_split,
                                                streaming=use_streaming
                                            )
                                        else:
                                            test_dataset = load_dataset(
                                                path=dataset_name,
                                                name=config,
                                                split=test_split,
                                                streaming=use_streaming
                                            )
                                        
                                        test_samples_per_config = int(samples_per_config * test_size)
                                        
                                        sample_pbar = tqdm(
                                            total=min(test_samples_per_config, int(max_samples_per_domain * test_size)),
                                            desc=f"    Test ìƒ˜í”Œ ì²˜ë¦¬",
                                            unit="sample",
                                            leave=False
                                        )
                                        
                                        test_processed = 0
                                        for sample in test_dataset:
                                            if test_processed >= test_samples_per_config:
                                                break
                                            
                                            # ìƒ˜í”Œ ë³€í™˜
                                            converted = convert_sample_to_messages(sample, dataset_name)
                                            if not converted:
                                                continue

                                            # ì´ë¯¸ì§€ ì²˜ë¦¬
                                            image_paths = []
                                            if "images" in converted and converted["images"]:
                                                flattened_images = validate_image_data(converted["images"])
                                                
                                                if flattened_images:
                                                    valid_sample = True
                                                    
                                                    for img_obj in flattened_images:
                                                        if isinstance(img_obj, Image.Image):
                                                            try:
                                                                img_path = os.path.join(images_dir, f"{image_counter}.png")
                                                                img_obj.save(img_path, "PNG")
                                                                image_paths.append(img_path)
                                                                image_counter += 1
                                                            except Exception as img_e:
                                                                logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {img_e}")
                                                                valid_sample = False
                                                                break
                                                        elif img_obj is not None:
                                                            logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(img_obj)}")
                                                            valid_sample = False
                                                            break
                                                    
                                                    if not valid_sample:
                                                        continue
                                            
                                            converted["images"] = image_paths
                                            converted["domain"] = domain
                                            
                                            test_f.write(json.dumps(converted) + "\n")
                                            domain_counts[domain]["test"] += 1
                                            total_processed += 1
                                            test_processed += 1
                                            
                                            sample_pbar.update(1)
                                            memory_gb = get_memory_usage()
                                            sample_pbar.set_postfix({
                                                "ì´ ì²˜ë¦¬": f"{total_processed}",
                                                "ë©”ëª¨ë¦¬": f"{memory_gb:.1f}GB"
                                            })
                                        
                                        sample_pbar.close()
                                        del test_dataset
                                        gc.collect()
                                        
                                    except Exception as e:
                                        logger.warning(f"   âš ï¸ Test split {test_split} ë¡œë“œ ì‹¤íŒ¨: {e}")
                                        # Test split ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                                
                            except Exception as e:
                                logger.warning(f"   âš ï¸ Config {config} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                                continue
                        
                        config_pbar.close()
                        
                    except Exception as e:
                        logger.warning(f"   âš ï¸ ë°ì´í„°ì…‹ {dataset_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                logger.info(f"   âœ… {domain} ë„ë©”ì¸ ì™„ë£Œ: Train {domain_counts[domain]['train']}ê°œ, Test {domain_counts[domain]['test']}ê°œ")
            
            domain_pbar.close()

        # ë„ë©”ì¸ë³„ í†µê³„ ì¶œë ¥
        logger.info("ğŸ“Š ë„ë©”ì¸ë³„ ìƒ˜í”Œ í†µê³„ (ê· ë“±í™” ì „):")
        for domain, counts in domain_counts.items():
            logger.info(f"   - {domain}: Train {counts['train']}ê°œ, Test {counts['test']}ê°œ")
        
        # ë„ë©”ì¸ë³„ ìƒ˜í”Œ ìˆ˜ ê· ë“±í™”
        # ê° ë„ë©”ì¸ì—ì„œ ë™ì¼í•œ ìˆ˜ì˜ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ë„ë¡ ì¡°ì •
        balanced_train_count = 0
        balanced_test_count = 0
        
        if domain_counts:
            min_train = min([c["train"] for c in domain_counts.values()] + [max_samples_per_domain])
            min_test = min([c["test"] for c in domain_counts.values()] + [int(max_samples_per_domain * test_size)])
            
            logger.info(f"âš–ï¸ ë„ë©”ì¸ë³„ ìƒ˜í”Œ ìˆ˜ ê· ë“±í™”:")
            logger.info(f"   - ìµœì†Œ Train ìƒ˜í”Œ ìˆ˜: {min_train}ê°œ")
            logger.info(f"   - ìµœì†Œ Test ìƒ˜í”Œ ìˆ˜: {min_test}ê°œ")
            
            # JSONL íŒŒì¼ì„ ë‹¤ì‹œ ì½ì–´ì„œ ê· ë“±í™”
            if min_train > 0 or min_test > 0:
                logger.info("ğŸ”„ ìƒ˜í”Œ ìˆ˜ ê· ë“±í™”ë¥¼ ìœ„í•´ JSONL íŒŒì¼ ì¬ì²˜ë¦¬...")
                
                # ì„ì‹œ íŒŒì¼ë¡œ ì¬ì‘ì„±
                balanced_train_path = os.path.join(temp_dir, "train_balanced.jsonl")
                balanced_test_path = os.path.join(temp_dir, "test_balanced.jsonl")
            
                domain_train_samples = defaultdict(list)
                domain_test_samples = defaultdict(list)
                
                # ê¸°ì¡´ JSONL íŒŒì¼ ì½ê¸°
                with open(train_jsonl_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            sample = json.loads(line)
                            domain = sample.get("domain", "unknown")
                            domain_train_samples[domain].append(sample)
                
                with open(test_jsonl_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            sample = json.loads(line)
                            domain = sample.get("domain", "unknown")
                            domain_test_samples[domain].append(sample)
                
                # ê° ë„ë©”ì¸ë³„ë¡œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
                balanced_domain_counts = defaultdict(lambda: {"train": 0, "test": 0})
                
                with open(balanced_train_path, "w", encoding="utf-8") as train_f, \
                     open(balanced_test_path, "w", encoding="utf-8") as test_f:
                    
                    for domain in domain_configs.keys():
                        # Train ìƒ˜í”Œ ê· ë“±í™”
                        train_samples = domain_train_samples[domain]
                        if len(train_samples) > min_train:
                            random.shuffle(train_samples)
                            train_samples = train_samples[:min_train]
                        
                        for sample in train_samples:
                            train_f.write(json.dumps(sample) + "\n")
                            balanced_domain_counts[domain]["train"] += 1
                            balanced_train_count += 1
                        
                        # Test ìƒ˜í”Œ ê· ë“±í™”
                        test_samples = domain_test_samples[domain]
                        if len(test_samples) > min_test:
                            random.shuffle(test_samples)
                            test_samples = test_samples[:min_test]
                        
                        for sample in test_samples:
                            test_f.write(json.dumps(sample) + "\n")
                            balanced_domain_counts[domain]["test"] += 1
                            balanced_test_count += 1
                
                # ê· ë“±í™”ëœ íŒŒì¼ë¡œ êµì²´
                train_jsonl_path = balanced_train_path
                test_jsonl_path = balanced_test_path
                
                logger.info("ğŸ“Š ë„ë©”ì¸ë³„ ìƒ˜í”Œ í†µê³„ (ê· ë“±í™” í›„):")
                for domain, counts in balanced_domain_counts.items():
                    logger.info(f"   - {domain}: Train {counts['train']}ê°œ, Test {counts['test']}ê°œ")
                
                logger.info(f"âœ… ê· ë“±í™” ì™„ë£Œ: Train {balanced_train_count}ê°œ, Test {balanced_test_count}ê°œ")
            else:
                total_train = sum(c["train"] for c in domain_counts.values())
                total_test = sum(c["test"] for c in domain_counts.values())
                balanced_train_count = total_train
                balanced_test_count = total_test
                logger.info(f"âœ… ì´ ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ: Train {total_train}ê°œ, Test {total_test}ê°œ")
        else:
            balanced_train_count = 0
            balanced_test_count = 0
        
        # JSONL íŒŒì¼ë¡œë¶€í„° ë°ì´í„°ì…‹ ë¡œë“œ
        data_files = {}
        final_train_count = balanced_train_count
        final_test_count = balanced_test_count
        
        if final_train_count > 0:
            data_files["train"] = train_jsonl_path
        if final_test_count > 0:
            data_files["test"] = test_jsonl_path

        if not data_files:
            raise ValueError("ë³€í™˜ëœ í›ˆë ¨ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        logger.info("ğŸ§  JSONL íŒŒì¼ë¡œë¶€í„° ë°ì´í„°ì…‹ ë¡œë”©...")
        dataset_dict = load_dataset("json", data_files=data_files)
        
        logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì´ë¯¸ì§€ ê°ì²´ë¡œ ìºìŠ¤íŒ… (lazy loading)...")
        for split in dataset_dict:
            current_features = dataset_dict[split].features
            new_features = current_features.copy()
            if 'images' in new_features:
                def preprocess_images(example):
                    """ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ - ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ í‰ë©´í™”"""
                    if 'images' in example and example['images']:
                        example['images'] = validate_image_data(example['images'])
                    # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€
                    elif 'images' not in example:
                        example['images'] = []
                    return example
                
                dataset_dict[split] = dataset_dict[split].map(preprocess_images)
                # ì´ë¯¸ì§€ê°€ ìˆëŠ” ìƒ˜í”Œë§Œ Sequence(DatasetImage)ë¡œ ìºìŠ¤íŒ…
                # ë¹ˆ ë¦¬ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                if isinstance(new_features['images'], Sequence):
                    new_features['images'] = Sequence(DatasetImage(decode=True))
                    dataset_dict[split] = dataset_dict[split].cast(new_features)

        logger.info("âœ… ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
        
        return dataset_dict

    except Exception as e:
        logger.error(f"âŒ ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        shutil.rmtree(temp_dir, ignore_errors=True)
        raise Exception(f"ğŸ˜¢ ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë”© ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from e


def create_simple_collate_fn(processor, max_length: int = 2048):
    """SFTTrainerìš© ì»¤ìŠ¤í…€ data collator - ë©€í‹° ë„ë©”ì¸ ì§€ì›"""
    from trl.trainer.sft_trainer import DataCollatorForVisionLanguageModeling
    
    class CustomSFTDataCollator(DataCollatorForVisionLanguageModeling):
        def __init__(self, processor, max_length: int = 2048):
            super().__init__(processor=processor, max_length=max_length)
            self.processor = processor
            self.max_length = max_length
            
        def __call__(self, features):
            assert features is not None, "features is None"

            for i, feature in enumerate(features):
                if "messages" in feature:
                    feature["messages"] = validate_messages(feature["messages"])
                if 'images' not in feature or not feature['images']:
                    raise ValueError(f"ìƒ˜í”Œ {i}ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! ëª¨ë“  ìƒ˜í”Œì€ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
                
                feature['images'] = validate_image_data(feature['images'])
                if not feature['images']:
                    raise ValueError(f"ìƒ˜í”Œ {i}ì˜ ì´ë¯¸ì§€ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            
            try:
                return self.torch_call(examples=features)
            except Exception as e:
                import traceback
                traceback.print_exc()
                logger.error(f"âš ï¸ Processor ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                raise
    
    return CustomSFTDataCollator(processor, max_length)


# ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹ ë¹Œë” í•¨ìˆ˜ë“¤
def math_domain_dataset(tokenizer, max_samples: int = 200, use_streaming: bool = True):
    """ìˆ˜í•™ ë„ë©”ì¸ ë°ì´í„°ì…‹"""
    log_memory_usage("ìˆ˜í•™ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_multi_domain_sft_dataset(
        domain_configs={"math": DOMAIN_DATASETS["math"]},
        tokenizer=tokenizer,
        max_samples_per_domain=max_samples,
        use_streaming=use_streaming
    )
    log_memory_usage("ìˆ˜í•™ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def science_domain_dataset(tokenizer, max_samples: int = 200, use_streaming: bool = True):
    """ê³¼í•™ ë„ë©”ì¸ ë°ì´í„°ì…‹"""
    log_memory_usage("ê³¼í•™ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_multi_domain_sft_dataset(
        domain_configs={"science": DOMAIN_DATASETS["science"]},
        tokenizer=tokenizer,
        max_samples_per_domain=max_samples,
        use_streaming=use_streaming
    )
    log_memory_usage("ê³¼í•™ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def code_domain_dataset(tokenizer, max_samples: int = 200, use_streaming: bool = True):
    """ì½”ë“œ ë„ë©”ì¸ ë°ì´í„°ì…‹"""
    log_memory_usage("ì½”ë“œ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_multi_domain_sft_dataset(
        domain_configs={"code": DOMAIN_DATASETS["code"]},
        tokenizer=tokenizer,
        max_samples_per_domain=max_samples,
        use_streaming=use_streaming
    )
    log_memory_usage("ì½”ë“œ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def puzzle_domain_dataset(tokenizer, max_samples: int = 200, use_streaming: bool = True):
    """í¼ì¦ ë„ë©”ì¸ ë°ì´í„°ì…‹"""
    log_memory_usage("í¼ì¦ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_multi_domain_sft_dataset(
        domain_configs={"puzzle": DOMAIN_DATASETS["puzzle"]},
        tokenizer=tokenizer,
        max_samples_per_domain=max_samples,
        use_streaming=use_streaming
    )
    log_memory_usage("í¼ì¦ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def vision_domain_dataset(tokenizer, max_samples: int = 200, use_streaming: bool = True):
    """ë¹„ì „ ë„ë©”ì¸ ë°ì´í„°ì…‹"""
    log_memory_usage("ë¹„ì „ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_multi_domain_sft_dataset(
        domain_configs={"vision": DOMAIN_DATASETS["vision"]},
        tokenizer=tokenizer,
        max_samples_per_domain=max_samples,
        use_streaming=use_streaming
    )
    log_memory_usage("ë¹„ì „ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def ocr_domain_dataset(tokenizer, max_samples: int = 200, use_streaming: bool = True):
    """OCR ë„ë©”ì¸ ë°ì´í„°ì…‹"""
    log_memory_usage("OCR ë„ë©”ì¸ ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_multi_domain_sft_dataset(
        domain_configs={"ocr": DOMAIN_DATASETS["ocr"]},
        tokenizer=tokenizer,
        max_samples_per_domain=max_samples,
        use_streaming=use_streaming
    )
    log_memory_usage("OCR ë„ë©”ì¸ ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset

def all_domains_dataset(tokenizer, max_samples_per_domain: int = 200, use_streaming: bool = True):
    """ëª¨ë“  ë„ë©”ì¸ í†µí•© ë°ì´í„°ì…‹"""
    log_memory_usage("ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì‹œì‘")
    dataset = get_multi_domain_sft_dataset(
        domain_configs=DOMAIN_DATASETS,
        tokenizer=tokenizer,
        max_samples_per_domain=max_samples_per_domain,
        use_streaming=use_streaming
    )
    log_memory_usage("ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ ì™„ë£Œ")
    return dataset


if __name__ == "__main__":
    from transformers import AutoTokenizer
    
    logger.info("ğŸš€ ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    log_memory_usage("í”„ë¡œê·¸ë¨ ì‹œì‘")
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2b-it")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    log_memory_usage("í† í¬ë‚˜ì´ì € ë¡œë“œ í›„")
    
    # ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
    try:
        logger.info("ğŸ“¦ ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸")
        dataset = all_domains_dataset(tokenizer, max_samples_per_domain=50, use_streaming=True)
        log_memory_usage("ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ ìƒì„± í›„")
        
        logger.info(f"ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset}")
        
        # ë„ë©”ì¸ë³„ ìƒ˜í”Œ í™•ì¸
        if 'train' in dataset:
            train_domains = {}
            for i in range(min(100, len(dataset['train']))):
                sample = dataset['train'][i]
                domain = sample.get('domain', 'unknown')
                train_domains[domain] = train_domains.get(domain, 0) + 1
            
            logger.info(f"Train ì„¸íŠ¸ ë„ë©”ì¸ ë¶„í¬: {train_domains}")
        
    except Exception as e:
        logger.error(f"ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
    
    log_memory_usage("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    logger.info("âœ… ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

