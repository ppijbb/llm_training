import torch
import torch.nn.functional as F
import numpy as np
from collections import defaultdict, deque
from typing import Dict, Any, Optional, Callable
import json
import time
import os
from transformers.image_utils import load_image

# GramSpec ë¶„ì„ ë„êµ¬ import
try:
    from eval.gramspec_moe_analysis import GramSpecAnalyzer
    GRAMSPEC_ANALYSIS_AVAILABLE = True
except ImportError:
    GRAMSPEC_ANALYSIS_AVAILABLE = False

# GramSpec ì‹¤ì œ ê²€ì¦ ë„êµ¬ import
try:
    from eval.gramspec_semantic_validation import GramSpecSemanticValidator
    GRAMSPEC_VALIDATION_AVAILABLE = True
except ImportError:
    GRAMSPEC_VALIDATION_AVAILABLE = False

# ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ import
try:
    from eval.analyze_expert_specialization import (
        collect_expert_activations,
        compute_expert_similarity,
        analyze_expert_task_correlation,
    )
    EXPERT_SPECIALIZATION_AVAILABLE = True
except ImportError:
    EXPERT_SPECIALIZATION_AVAILABLE = False

try:
    from eval.run_gramspec_validation import (
        evaluate_model_perplexity,
        run_expression_ablation_study,
        run_information_processing_comparison,
    )
    GRAMSPEC_VALIDATION_SCRIPT_AVAILABLE = True
except ImportError:
    GRAMSPEC_VALIDATION_SCRIPT_AVAILABLE = False

try:
    from eval.measure_efficiency import (
        measure_forward_throughput,
        measure_generation_latency,
        estimate_flops,
    )
    EFFICIENCY_MEASUREMENT_AVAILABLE = True
except ImportError:
    EFFICIENCY_MEASUREMENT_AVAILABLE = False

def _is_main_process() -> bool:
    """Best-effort check for rank-0 to gate logging/plotting on distributed runs."""
    try:
        import torch.distributed as dist
        if dist.is_available() and dist.is_initialized():
            return dist.get_rank() == 0
    except Exception:
        pass
    
    # í™˜ê²½ë³€ìˆ˜ë¡œë„ ì²´í¬ (DeepSpeed ë“±ì—ì„œ ì‚¬ìš©)
    try:
        rank = int(os.getenv("RANK", "0"))
        return rank == 0
    except (ValueError, TypeError):
        pass
    
    return True

def _get_process_info() -> dict:
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
    info = {
        'rank': None,
        'world_size': None,
        'local_rank': None,
        'RANK': os.getenv("RANK", "N/A"),
        'LOCAL_RANK': os.getenv("LOCAL_RANK", "N/A"),
        'WORLD_SIZE': os.getenv("WORLD_SIZE", "N/A"),
    }
    try:
        import torch.distributed as dist
        if dist.is_available() and dist.is_initialized():
            info['rank'] = dist.get_rank()
            info['world_size'] = dist.get_world_size()
            try:
                info['local_rank'] = dist.get_rank() % torch.cuda.device_count()
            except:
                pass
    except Exception:
        pass
    return info

class TorchMoECallback:
    """Pure PyTorch MoE monitoring callback with generation logging"""

    def __init__(
        self,
        num_experts: int,
        log_every_n_steps: int = 1,  # ê¸°ë³¸ê°’ì„ 1ë¡œ ë³€ê²½í•˜ì—¬ ë§¤ stepë§ˆë‹¤ ë¡œê¹…
        log_heatmap_every: int = 1000,
        log_tsne_every: int = 5000,  # t-SNE ì‹œê°í™” ì£¼ê¸° (ê³„ì‚° ë¹„ìš©ì´ ë†’ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ì„ í¬ê²Œ ì„¤ì •)
        tsne_sample_size: int = 2000,  # t-SNE ê³„ì‚°ìš© ìƒ˜í”Œ í¬ê¸°
        alert_threshold_imbalance: float = 5.0,
        unused_expert_threshold: float = 0.3,
        entropy_threshold: float = 0.1,
        window_size: int = 1000,
        logger: Optional[Any] = None,
        log_to_console: bool = False,
        save_detailed_logs: bool = False,
        log_dir: str = "./moe_logs",
        debug_logging: bool = False,
        enable_generation_logging: bool = True,
        generation_log_dir: str = "./moe_generation_logs",
        max_generation_samples: int = 3,
        generation_log_every: int = 20,
        force_all_ranks: bool = True
    ):
        self.log_every_n_steps = log_every_n_steps
        self.log_heatmap_every = log_heatmap_every
        self.log_tsne_every = log_tsne_every
        self.tsne_sample_size = tsne_sample_size
        self.alert_threshold_imbalance = alert_threshold_imbalance
        self.unused_expert_threshold = unused_expert_threshold
        self.num_experts = num_experts
        self.entropy_threshold = entropy_threshold
        self.window_size = window_size
        self.logger = logger
        self.log_to_console = log_to_console
        self.save_detailed_logs = save_detailed_logs
        self.log_dir = log_dir
        self.debug_logging = debug_logging
        self.force_all_ranks = bool(force_all_ranks)
        self.is_main_process = True  # í•­ìƒ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰
        self.last_logged_step = -1

        # Generation logging ì„¤ì •
        self.enable_generation_logging = enable_generation_logging
        self.generation_log_dir = generation_log_dir
        self.max_generation_samples = max_generation_samples
        self.generation_log_every = generation_log_every
        self.generation_step_count = 0

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € (ë‚˜ì¤‘ì— ì„¤ì •)
        self.model = None
        self.tokenizer = None

        # ë‚´ë¶€ ìƒíƒœ (step ì œê±°)
        self.expert_usage_history = defaultdict(lambda: deque(maxlen=window_size))
        self.routing_stats = defaultdict(list)
        self.alerts_history = []
        self.detailed_logs = []

        # Pending ë¡œê¹… ì •ë³´ (on_logì—ì„œ ì‚¬ìš©)
        self.pending_metrics = {}   # step -> log_data
        self.pending_heatmaps = {}  # step -> heatmap_data
        self.pending_alerts = {}    # step -> alert_data

        # hooks ì €ì¥ì†Œ
        self.hooks = []
        self.layer_outputs = {}
        
        # Layerë³„ expert usage tracking (ì‹¤ì œ ê²€ì¦ìš©)
        self.layer_expert_usage_counts = {}  # layer_name -> torch.Tensor [num_experts]
        
        # t-SNE ì‹œê°í™”ìš© ë°ì´í„° ë²„í¼ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìµœê·¼ Nê°œ stepë§Œ ìœ ì§€)
        self.tsne_data_buffer = defaultdict(lambda: {
            'hidden_states': deque(maxlen=50),  # ìµœê·¼ 50ê°œ stepì˜ hidden states
            'expert_assignments': deque(maxlen=50),
            'routing_weights': deque(maxlen=50)
        })
        
        
        # Vision ëª¨ë“ˆ ëª¨ë‹ˆí„°ë§ (vision_tower, multi_modal_projector)
        self.vision_hooks = []
        self.vision_tower_outputs = []  # vision_tower ì¶œë ¥ íˆìŠ¤í† ë¦¬
        self.projector_outputs = []  # projector ì¶œë ¥ íˆìŠ¤í† ë¦¬
        self.vision_usage_stats = {
            'vision_tower_calls': 0,
            'projector_calls': 0,
            'pixel_values_received': 0,
            'image_features_generated': 0,
        }
        self.vision_modules_info = {
            'vision_tower': {'module': None, 'name': None},
            'projector': {'module': None, 'name': None}
        }
        
        # GramSpec ë¶„ì„ê¸° (ì˜µì…˜)
        self.gramspec_analyzer = None
        if GRAMSPEC_ANALYSIS_AVAILABLE:
            try:
                self.gramspec_analyzer = GramSpecAnalyzer(num_experts=num_experts, router_dim=128)
            except Exception as e:
                self._log_debug(f"Warning: Could not initialize GramSpecAnalyzer: {e}")
        
        # GramSpec ì‹¤ì œ ê²€ì¦ê¸° (ì˜µì…˜)
        self.gramspec_validator = None
        if GRAMSPEC_VALIDATION_AVAILABLE:
            try:
                # num_layersëŠ” register_modelì—ì„œ ì„¤ì •
                self.gramspec_validator = None  # ë‚˜ì¤‘ì— ì´ˆê¸°í™”
            except Exception as e:
                self._log_debug(f"Warning: Could not initialize GramSpecSemanticValidator: {e}")

        if save_detailed_logs:
            import os
            os.makedirs(log_dir, exist_ok=True)

        if enable_generation_logging:
            import os
            os.makedirs(generation_log_dir, exist_ok=True)
    
    def _log_debug(self, message: str):
        """ë‚´ë¶€ ë””ë²„ê·¸ ë©”ì‹œì§€ ë¡œê¹…"""
        # log_to_consoleì´ Trueì¼ ë•Œë§Œ ì¶œë ¥ (debug_loggingì€ wandbì—ë§Œ ê¸°ë¡)
        if self.log_to_console:
            prefix = "[MoE Debug]" if self.debug_logging else "[MoE]"
            print(f"{prefix} {message}")
    
    def register_model(self, model: torch.nn.Module, tokenizer=None):
        """ëª¨ë¸ì— hooks ë“±ë¡í•˜ê³  í† í¬ë‚˜ì´ì € ì„¤ì • (ì¹˜ëª…ì  ë²„ê·¸ ìˆ˜ì •: DeepSpeed ë˜í•‘ ëŒ€ì‘)"""
        # DeepSpeed ë˜í•‘ ì²˜ë¦¬ (model.moduleì´ ì‹¤ì œ ëª¨ë¸)
        actual_model = model.module if hasattr(model, 'module') else model
        self.model = actual_model  # â† ì´ê±° ì•ˆ í•˜ë©´ hookì´ wrapperì— ê±¸ë¦¼
        self.tokenizer = tokenizer
        self._register_hooks()
        
        # Layer ê°œìˆ˜ ì¶”ì¶œ ë° validator ì´ˆê¸°í™”
        if GRAMSPEC_VALIDATION_AVAILABLE:
            try:
                num_layers = self._count_moe_layers(model)
                if num_layers > 0:
                    self.gramspec_validator = GramSpecSemanticValidator(
                        num_layers=num_layers,
                        num_experts=self.num_experts
                    )
                    self._log_debug(f"GramSpecSemanticValidator initialized with {num_layers} layers")
            except Exception as e:
                self._log_debug(f"Warning: Could not initialize validator: {e}")

        if self.enable_generation_logging and tokenizer is None:
            self._log_debug("Warning: Generation logging enabled but no tokenizer provided")

        return self
    
    def _count_moe_layers(self, model: torch.nn.Module) -> int:
        """ëª¨ë¸ì—ì„œ MoE layer ê°œìˆ˜ ì„¸ê¸°"""
        count = 0
        for name, module in model.named_modules():
            if self._is_moe_layer(module):
                count += 1
        return count

    def set_tokenizer(self, tokenizer):
        """í† í¬ë‚˜ì´ì € ì„¤ì •"""
        self.tokenizer = tokenizer
        return self
    
    def _register_hooks(self):
        """MoE ë ˆì´ì–´ì— forward hooks ë“±ë¡"""
        moe_count = 0
        for name, module in self.model.named_modules():
            if self._is_moe_layer(module):
                hook = module.register_forward_hook(
                    self._create_hook_fn(name)
                )
                self.hooks.append(hook)
                moe_count += 1
        
        if moe_count == 0:
            self._log_debug("âŒ WARNING: No MoE layers found! Model structure:")
            # ëª¨ë¸ êµ¬ì¡° ì¼ë¶€ ì¶œë ¥
            for name, module in list(self.model.named_modules())[:20]:
                self._log_debug(f"    - {name}: {type(module).__name__}")
        
        self._log_debug(f"ğŸ“Š Total MoE hooks registered: {len(self.hooks)}")
        
        # Vision ëª¨ë“ˆ hooks ë“±ë¡
        self._register_vision_hooks()
    
    def _is_moe_layer(self, module):
        """MoE ë ˆì´ì–´ ê°ì§€"""
        # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ MoE ë ˆì´ì–´ í´ë˜ìŠ¤ë“¤ (ì¹˜ëª…ì  ë²„ê·¸ ìˆ˜ì •: GramSpecMoEGRINMoE ì¶”ê°€)
        moe_class_names = [
            'GramSpecMoEGRINMoE',      # â† ì´ê±° ì—†ìœ¼ë©´ hook 0ê°œ (ê°€ì¥ ì¤‘ìš”!)
            'G3MoESharedExpertsLayer', 
            'G3MoESparseGRINBlock', 
            'G3MoEGRINMoE',
            'GRINMoESparseMoeBlock', 
            'G2MoEGRINMoeLayer', 
            'GramSpecMoEBlock',
            'MixtralSparseMoeBlock',   # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ë„ ìœ ì§€
            'SparseMLP', 
            'SwitchTransformerMLP'
        ]
        
        module_name = module.__class__.__name__
        
        # í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì²´í¬
        is_moe_by_name = any(cls_name in module_name for cls_name in moe_class_names)
        
        # ì†ì„±ìœ¼ë¡œ ì²´í¬ (router, experts ë“±)
        has_router = hasattr(module, 'router')
        has_experts = hasattr(module, 'experts')
        has_gate = hasattr(module, 'gate')
        
        # G3MoE íŠ¹ì • ì²´í¬: routerê°€ G3MoERouterì¸ì§€ í™•ì¸
        is_g3moe_router = False
        if has_router:
            router = getattr(module, 'router', None)
            if router is not None:
                router_class_name = router.__class__.__name__
                is_g3moe_router = ('G3MoERouter' in router_class_name or 
                                  'GramSpecMoERouter' in router_class_name or
                                  'GramSpecRouter' in router_class_name or
                                  getattr(router, '_is_gramspec_moe_router', False) or
                                  getattr(router, '_is_g3moe_router', False))
        
        is_moe = (is_moe_by_name or 
                  (has_router and has_experts) or  # routerì™€ experts ë‘˜ ë‹¤ ìˆìœ¼ë©´ MoE
                  (is_g3moe_router and has_experts) or  # G3MoE router + experts
                  has_gate)
        
        return is_moe
    
    def _register_vision_hooks(self):
        """Vision towerì™€ projectorì— forward hooks ë“±ë¡"""
        if self.model is None:
            return
        
        # Vision tower ì°¾ê¸°
        vision_tower = None
        projector = None
        vision_tower_name = None
        projector_name = None
        
        # G3MoE ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶° vision_towerì™€ multi_modal_projector ì°¾ê¸°
        if hasattr(self.model, 'vision_tower'):
            vision_tower = self.model.vision_tower
            vision_tower_name = 'vision_tower'
        elif hasattr(self.model, 'model') and hasattr(self.model.model, 'vision_tower'):
            vision_tower = self.model.model.vision_tower
            vision_tower_name = 'model.vision_tower'
        
        if hasattr(self.model, 'multi_modal_projector'):
            projector = self.model.multi_modal_projector
            projector_name = 'multi_modal_projector'
        elif hasattr(self.model, 'model') and hasattr(self.model.model, 'multi_modal_projector'):
            projector = self.model.model.multi_modal_projector
            projector_name = 'model.multi_modal_projector'
        
        # Vision ëª¨ë“ˆ ì •ë³´ ì €ì¥ (requires_grad ì²´í¬ìš©)
        self.vision_modules_info = {
            'vision_tower': {'module': vision_tower, 'name': vision_tower_name},
            'projector': {'module': projector, 'name': projector_name}
        }
        
        # Vision tower hook ë“±ë¡
        if vision_tower is not None:
            def vision_tower_hook(module, input, output):
                try:
                    self.vision_usage_stats['vision_tower_calls'] += 1
                    
                    # Inputì—ì„œ pixel_values ì¶”ì¶œ
                    pixel_values = None
                    if isinstance(input, tuple):
                        # ì²« ë²ˆì§¸ ì¸ìê°€ pixel_valuesì¼ ìˆ˜ ìˆìŒ
                        if len(input) > 0 and torch.is_tensor(input[0]):
                            # shape í™•ì¸: (batch, channels, height, width)
                            if len(input[0].shape) == 4:
                                pixel_values = input[0]
                    elif isinstance(input, dict):
                        pixel_values = input.get('pixel_values')
                    
                    if pixel_values is not None and torch.is_tensor(pixel_values):
                        batch_size = pixel_values.shape[0] if pixel_values.dim() >= 1 else 1
                        self.vision_usage_stats['pixel_values_received'] += batch_size
                    
                    # Output í†µê³„ ìˆ˜ì§‘
                    hidden_state = None
                    if hasattr(output, 'last_hidden_state'):
                        hidden_state = output.last_hidden_state
                    elif isinstance(output, torch.Tensor):
                        hidden_state = output
                    elif isinstance(output, tuple) and len(output) > 0:
                        # BaseModelOutputWithPast í˜•íƒœì¼ ìˆ˜ ìˆìŒ
                        if torch.is_tensor(output[0]):
                            hidden_state = output[0]
                    
                    if hidden_state is not None and torch.is_tensor(hidden_state):
                        # í†µê³„ ì •ë³´ë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                        with torch.no_grad():
                            stats = {
                                'shape': list(hidden_state.shape),
                                'mean': hidden_state.float().mean().item() if hidden_state.numel() > 0 else 0.0,
                                'std': hidden_state.float().std().item() if hidden_state.numel() > 0 else 0.0,
                                'min': hidden_state.float().min().item() if hidden_state.numel() > 0 else 0.0,
                                'max': hidden_state.float().max().item() if hidden_state.numel() > 0 else 0.0,
                            }
                            self.vision_tower_outputs.append(stats)
                            # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
                            if len(self.vision_tower_outputs) > 100:
                                self.vision_tower_outputs.pop(0)
                except Exception as e:
                    self._log_debug(f"Error in vision_tower hook: {e}")
            
            hook = vision_tower.register_forward_hook(vision_tower_hook)
            self.vision_hooks.append(hook)
            self._log_debug("Registered vision_tower hook")
        
        # Projector hook ë“±ë¡
        if projector is not None:
            def projector_hook(module, input, output):
                try:
                    self.vision_usage_stats['projector_calls'] += 1
                    if isinstance(output, torch.Tensor):
                        batch_size = output.shape[0] if output.dim() >= 1 else 1
                        self.vision_usage_stats['image_features_generated'] += batch_size
                        
                        # í†µê³„ ì •ë³´ë§Œ ì €ì¥
                        with torch.no_grad():
                            stats = {
                                'shape': list(output.shape),
                                'mean': output.float().mean().item() if output.numel() > 0 else 0.0,
                                'std': output.float().std().item() if output.numel() > 0 else 0.0,
                                'min': output.float().min().item() if output.numel() > 0 else 0.0,
                                'max': output.float().max().item() if output.numel() > 0 else 0.0,
                            }
                            self.projector_outputs.append(stats)
                            # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
                            if len(self.projector_outputs) > 100:
                                self.projector_outputs.pop(0)
                except Exception as e:
                    self._log_debug(f"Error in projector hook: {e}")
            
            hook = projector.register_forward_hook(projector_hook)
            self.vision_hooks.append(hook)
            self._log_debug("Registered multi_modal_projector hook")
    
    def _create_hook_fn(self, layer_name):
        """íŠ¹ì • ë ˆì´ì–´ìš© hook í•¨ìˆ˜ ìƒì„±"""
        def hook_fn(module, input, output):
            try:
                # ë””ë²„ê·¸: hookì´ í˜¸ì¶œë˜ëŠ”ì§€ í™•ì¸ (ì²˜ìŒ ëª‡ ë²ˆë§Œ)
                if not hasattr(self, '_hook_call_count'):
                    self._hook_call_count = {}
                if layer_name not in self._hook_call_count:
                    self._hook_call_count[layer_name] = 0
                self._hook_call_count[layer_name] += 1
                # if self._hook_call_count[layer_name] <= 3 and self.log_to_console:
                #     self._log_debug(f"ğŸ” Hook called for {layer_name} (call #{self._hook_call_count[layer_name]})")
                
                # t-SNEìš© ë°ì´í„° ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìµœê·¼ stepë§Œ)
                # input[0]ì€ hidden states (MoE layer ì…ë ¥)
                if isinstance(input, tuple) and len(input) > 0:
                    hidden_states = input[0]
                    if torch.is_tensor(hidden_states) and hidden_states.numel() > 0:
                        # CPUë¡œ ì´ë™í•˜ê³  flatten (ë©”ëª¨ë¦¬ ì ˆì•½)
                        hidden_states_cpu = hidden_states.detach().to('cpu', non_blocking=True)
                        # [batch, seq, hidden_dim] -> [batch*seq, hidden_dim]
                        if hidden_states_cpu.dim() == 3:
                            hidden_states_flat = hidden_states_cpu.reshape(-1, hidden_states_cpu.size(-1))
                            # ìƒ˜í”Œë§í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½ (ìµœëŒ€ 1000ê°œ í† í°ë§Œ)
                            if hidden_states_flat.size(0) > 1000:
                                indices = torch.randperm(hidden_states_flat.size(0))[:1000]
                                hidden_states_flat = hidden_states_flat[indices]
                            self.tsne_data_buffer[layer_name]['hidden_states'].append(hidden_states_flat)

                routing_info = self._extract_routing_info(module, input, output)
                if routing_info:
                    # if self._hook_call_count[layer_name] <= 3 and self.log_to_console:
                    #     self._log_debug(f"  âœ… Extracted routing info: {list(routing_info.keys())}")
                    # Store only lightweight, CPU-detached summaries to avoid GPU memory growth
                    lightweight_entry = {}
                    if 'expert_assignments' in routing_info and routing_info['expert_assignments'] is not None:
                        ea = routing_info['expert_assignments']
                        if torch.is_tensor(ea):
                            ea = ea.detach().to('cpu', non_blocking=True)
                            # 1ì°¨ì›ìœ¼ë¡œ í™•ì‹¤íˆ ë³€í™˜ (bincount ìš”êµ¬ì‚¬í•­)
                            if ea.dim() > 1:
                                ea = ea.flatten()
                            elif ea.dim() == 0:
                                ea = ea.unsqueeze(0)
                            if ea.dim() != 1:
                                ea = ea.view(-1)
                        lightweight_entry['expert_assignments'] = ea
                    # Keep num_experts metadata if present
                    if 'num_experts' in routing_info:
                        lightweight_entry['num_experts'] = routing_info['num_experts']
                    # Optionally carry an already-aggregated avg entropy (scalar)
                    if 'avg_routing_entropy' in routing_info and routing_info['avg_routing_entropy'] is not None:
                        val = routing_info['avg_routing_entropy']
                        if torch.is_tensor(val):
                            val = val.detach().to('cpu')
                        lightweight_entry['avg_routing_entropy'] = val
                    if 'ortho_loss' in routing_info and routing_info['ortho_loss'] is not None:
                        val = routing_info['ortho_loss']
                        if torch.is_tensor(val):
                            val = val.detach().to('cpu')
                        lightweight_entry['ortho_loss'] = val
                    if 'aux_loss' in routing_info and routing_info['aux_loss'] is not None:
                        val = routing_info['aux_loss']
                        if torch.is_tensor(val):
                            val = val.detach().to('cpu')
                        lightweight_entry['aux_loss'] = val
                    # G3MoE specific metrics
                    if 'speciality_loss' in routing_info and routing_info['speciality_loss'] is not None:
                        val = routing_info['speciality_loss']
                        if torch.is_tensor(val):
                            val = val.detach().to('cpu')
                        lightweight_entry['speciality_loss'] = val
                    if 'cosine_similarities' in routing_info and routing_info['cosine_similarities'] is not None:
                        val = routing_info['cosine_similarities']
                        if torch.is_tensor(val):
                            val = val.detach().to('cpu')
                        lightweight_entry['cosine_similarities'] = val
                    if 'expression_loss' in routing_info and routing_info['expression_loss'] is not None:
                        val = routing_info['expression_loss']
                        if torch.is_tensor(val):
                            val = val.detach().to('cpu')
                        lightweight_entry['expression_loss'] = val
                    self.layer_outputs[layer_name] = lightweight_entry
                    
                    # t-SNEìš© expert assignments ì €ì¥
                    if 'expert_assignments' in lightweight_entry:
                        ea = lightweight_entry['expert_assignments']
                        if torch.is_tensor(ea) and ea.numel() > 0:
                            # ìƒ˜í”Œë§í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                            if ea.size(0) > 1000:
                                indices = torch.randperm(ea.size(0))[:1000]
                                ea = ea[indices]
                            self.tsne_data_buffer[layer_name]['expert_assignments'].append(ea)
                    
                    # ë””ë²„ê¹… ë¡œê·¸ëŠ” í•­ìƒ ì¶œë ¥ (step ì •ë³´ ì œê±°)
                    # self._log_debug(f"{layer_name}: extracted {list(routing_info.keys())}")
                else:
                    if self._hook_call_count[layer_name] <= 3 and self.log_to_console:
                        # ë””ë²„ê·¸: ì™œ routing_infoê°€ Noneì¸ì§€ í™•ì¸
                        has_last_selected = hasattr(module, 'last_selected_experts')
                        output_is_tuple = isinstance(output, tuple)
                        output_len = len(output) if output_is_tuple else 0
                        self._log_debug(f"  âŒ No routing info extracted for {layer_name}")
                        self._log_debug(f"     - has last_selected_experts: {has_last_selected}")
                        if has_last_selected:
                            se = module.last_selected_experts
                            self._log_debug(f"     - last_selected_experts shape: {se.shape if torch.is_tensor(se) else type(se)}")
                        self._log_debug(f"     - output is tuple: {output_is_tuple}, len: {output_len}")
                        if output_is_tuple and len(output) >= 2:
                            router_info = output[-1]
                            self._log_debug(f"     - router_info type: {type(router_info)}, is tuple: {isinstance(router_info, tuple)}")
                            if isinstance(router_info, tuple):
                                self._log_debug(f"     - router_info len: {len(router_info)}")
            except Exception as e:
                self._log_debug(f"Warning: Failed to extract routing info from {layer_name}: {e}")
        return hook_fn
    
    @torch.no_grad()
    def _extract_routing_info(self, module, input, output):
        """ëª¨ë“ˆì—ì„œ ë¼ìš°íŒ… ì •ë³´ ì¶”ì¶œ (ì¹˜ëª…ì  ë²„ê·¸ ìˆ˜ì •: routerì˜ last_xxx ì†ì„± ì§ì ‘ ì½ê¸°)"""
        routing_info = {}
        # Lightweight mode: avoid retaining large tensors by default
        lightweight = True

        # ===== ìš°ì„ ìˆœìœ„ 1: Routerì—ì„œ ì§ì ‘ ì¶”ì¶œ (ê°€ì¥ ì•ˆì •ì ) =====
        router = getattr(module, 'router', None)
        if router is not None:
            # Routerì˜ last_xxx ì†ì„±ë“¤ì´ ì‹¤ì œë¡œ ì¡´ì¬í•¨ (ì½”ë“œ í™•ì¸ ì™„ë£Œ)
            if hasattr(router, 'last_selected_experts') and router.last_selected_experts is not None:
                selected_experts = router.last_selected_experts
                # selected_experts: [batch*seq, top_k] í˜•íƒœ
                if selected_experts.dim() == 2:
                    selected_experts_flat = selected_experts.flatten()
                    routing_info['expert_assignments'] = selected_experts_flat
                else:
                    routing_info['expert_assignments'] = selected_experts.flatten() if selected_experts.dim() > 0 else selected_experts
            
            if hasattr(router, 'last_routing_weights') and router.last_routing_weights is not None:
                routing_weights = router.last_routing_weights
                if routing_weights.dim() == 2:
                    routing_info['routing_probs'] = routing_weights.flatten()
                else:
                    routing_info['routing_probs'] = routing_weights.flatten() if routing_weights.dim() > 0 else routing_weights
            
            if hasattr(router, 'num_experts'):
                routing_info['num_experts'] = router.num_experts
            elif hasattr(router, 'last_num_experts'):
                routing_info['num_experts'] = router.last_num_experts

        # ===== ìš°ì„ ìˆœìœ„ 2: ëª¨ë“ˆì—ì„œ ì§ì ‘ ì €ì¥ëœ ì •ë³´ (fallback) =====
        if 'expert_assignments' not in routing_info and hasattr(module, 'last_selected_experts'):
            selected_experts = module.last_selected_experts
            # selected_experts: [batch*seq, top_k] í˜•íƒœ
            if selected_experts.dim() == 2:
                # top_k expertsë¥¼ flattení•˜ì—¬ ë‹¨ì¼ ì°¨ì›ìœ¼ë¡œ ë³€í™˜
                selected_experts_flat = selected_experts.flatten()
                routing_info['expert_assignments'] = selected_experts_flat
                
                # routing_weightsë„ í•¨ê»˜ ì €ì¥ (entropy ê³„ì‚°ìš©)
                if hasattr(module, 'last_routing_weights'):
                    routing_weights = module.last_routing_weights
                    if routing_weights.dim() == 2:
                        routing_info['routing_probs'] = routing_weights.flatten()
                
                # num_experts ì €ì¥
                if hasattr(module, 'last_num_experts'):
                    routing_info['num_experts'] = module.last_num_experts
            else:
                routing_info['expert_assignments'] = selected_experts
        
        # ===== Routerì—ì„œ Loss ë©”íŠ¸ë¦­ ì§ì ‘ ì¶”ì¶œ (ì¹˜ëª…ì  ë²„ê·¸ ìˆ˜ì •) =====
        if router is not None:
            # Routerì˜ last_xxx ì†ì„±ì—ì„œ loss ë©”íŠ¸ë¦­ ì¶”ì¶œ
            if hasattr(router, 'last_speciality_loss') and router.last_speciality_loss is not None:
                val = router.last_speciality_loss
                if torch.is_tensor(val):
                    val = val.detach().to('cpu')
                routing_info['speciality_loss'] = val
            elif hasattr(router, 'last_ortho_loss') and router.last_ortho_loss is not None:
                # ortho_lossë¥¼ speciality_lossë¡œë„ ì‚¬ìš©
                val = router.last_ortho_loss
                if torch.is_tensor(val):
                    val = val.detach().to('cpu')
                routing_info['speciality_loss'] = val
            
            if hasattr(router, 'last_cosine_similarities') and router.last_cosine_similarities is not None:
                val = router.last_cosine_similarities
                if torch.is_tensor(val):
                    val = val.detach().to('cpu')
                routing_info['cosine_similarities'] = val
            
            if hasattr(router, 'last_expression_reg_loss') and router.last_expression_reg_loss is not None:
                val = router.last_expression_reg_loss
                if torch.is_tensor(val):
                    val = val.detach().to('cpu')
                routing_info['expression_loss'] = val
        
        # ì‹¤ì œ G3MoE/GRIN ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶˜ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: outputì—ì„œ ì§ì ‘ ì¶”ì¶œ)
        # G3MoEGRINMoE output: (hidden_states, (routing_probs_full, hn, speciality_loss, cosine_similarities, expression_loss))
        # G3MoEDecoderLayer output: (hidden_states, (self_attn_weights?), (router_logits, hn, speciality_loss, cosine_similarities, expression_loss))
        if isinstance(output, tuple) and len(output) >= 2:
            hidden_states = output[0]
            router_info_tuple = output[-1]  # ë§ˆì§€ë§‰ ìš”ì†Œê°€ routing info íŠœí”Œ
            
            # G3MoE nested tuple êµ¬ì¡° íŒŒì‹±
            if isinstance(router_info_tuple, tuple) and len(router_info_tuple) >= 5:
                routing_probs_full = router_info_tuple[0]  # ì‹¤ì œë¡œëŠ” routing_probs_full (softmaxëœ í™•ë¥ )
                # hn = router_info_tuple[1]  # ì‚¬ìš© ì•ˆ í•¨
                speciality_loss = router_info_tuple[2]
                cosine_similarities = router_info_tuple[3]
                expression_loss = router_info_tuple[4]
                
                # âœ… routing_probs_fullì—ì„œ expert assignments ì¶”ì¶œ (last_selected_expertsë³´ë‹¤ ìš°ì„ )
                # hookì´ forward ì¤‘ì— í˜¸ì¶œë˜ë¯€ë¡œ outputì—ì„œ ì§ì ‘ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ë” ì•ˆì •ì 
                if routing_probs_full is not None and torch.is_tensor(routing_probs_full):
                    # routing_probs_full: [batch*seq, num_experts] í˜•íƒœì˜ softmax í™•ë¥ 
                    if routing_probs_full.dim() >= 2:
                        # top-kë¥¼ ê³ ë ¤í•˜ì—¬ top-1 expert ì„ íƒ (argmax)
                        expert_assignments = routing_probs_full.argmax(dim=-1)  # [batch*seq]
                        # last_selected_expertsê°€ ì—†ê±°ë‚˜ ì´ë¯¸ ì¶”ì¶œí•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©
                        if 'expert_assignments' not in routing_info:
                            routing_info['expert_assignments'] = expert_assignments.flatten()
                        # routing_probsëŠ” í•­ìƒ ì—…ë°ì´íŠ¸ (entropy ê³„ì‚°ìš©)
                        routing_info['routing_probs'] = routing_probs_full.flatten()
                        # num_experts ì •ë³´ë„ ì¶”ì¶œ
                        if 'num_experts' not in routing_info:
                            routing_info['num_experts'] = routing_probs_full.size(-1)
                    else:
                        expert_assignments = routing_probs_full.argmax(dim=-1)
                        if 'expert_assignments' not in routing_info:
                            routing_info['expert_assignments'] = expert_assignments
                        routing_info['routing_probs'] = routing_probs_full
                        if 'num_experts' not in routing_info and hasattr(module, 'num_experts'):
                            routing_info['num_experts'] = module.num_experts
                
                # Loss ë©”íŠ¸ë¦­ ì €ì¥ (CPUë¡œ ì´ë™)
                if speciality_loss is not None:
                    val = speciality_loss.detach().to('cpu') if torch.is_tensor(speciality_loss) else speciality_loss
                    routing_info['speciality_loss'] = val
                if cosine_similarities is not None:
                    val = cosine_similarities.detach().to('cpu') if torch.is_tensor(cosine_similarities) else cosine_similarities
                    routing_info['cosine_similarities'] = val
                if expression_loss is not None:
                    val = expression_loss.detach().to('cpu') if torch.is_tensor(expression_loss) else expression_loss
                    routing_info['expression_loss'] = val
            elif isinstance(router_info_tuple, tuple) and len(router_info_tuple) > 0:
                # ë‹¤ë¥¸ í˜•íƒœì˜ íŠœí”Œ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
                router_logits = router_info_tuple[0]
                if router_logits is not None and torch.is_tensor(router_logits):
                    # Compute expert assignments cheaply; skip storing full probs/logits
                    if 'expert_assignments' not in routing_info:
                        expert_assignments = router_logits.argmax(dim=-1)
                        routing_info['expert_assignments'] = expert_assignments.flatten() if expert_assignments.dim() > 1 else expert_assignments
                    if not lightweight:
                        routing_probs = torch.nn.functional.softmax(router_logits, dim=-1)
                        routing_info['routing_probs'] = routing_probs.flatten() if routing_probs.dim() > 1 else routing_probs
                        routing_info['gate_logits'] = router_logits
            else:
                # ë‹¨ì¼ í…ì„œì¸ ê²½ìš°
                router_logits = router_info_tuple
                if router_logits is not None and torch.is_tensor(router_logits):
                    if 'expert_assignments' not in routing_info:
                        expert_assignments = router_logits.argmax(dim=-1)
                        routing_info['expert_assignments'] = expert_assignments.flatten() if expert_assignments.dim() > 1 else expert_assignments
                    if not lightweight:
                        routing_probs = torch.nn.functional.softmax(router_logits, dim=-1)
                        routing_info['routing_probs'] = routing_probs.flatten() if routing_probs.dim() > 1 else routing_probs
                        routing_info['gate_logits'] = router_logits
        
        # ë‹¤ì–‘í•œ MoE êµ¬í˜„ì—ì„œ ë¼ìš°íŒ… ì •ë³´ ì¶”ì¶œ
        # ì†ì„±ìœ¼ë¡œ ì €ì¥ëœ ê²½ìš° (fallback)
        for attr in ['last_expert_assignments', 'expert_assignments', 'selected_experts']:
            if hasattr(module, attr) and 'expert_assignments' not in routing_info:
                routing_info['expert_assignments'] = getattr(module, attr)
                break
        
        for attr in ['last_routing_probs', 'routing_probs', 'gate_probs']:
            if hasattr(module, attr):
                if not lightweight:
                    routing_info['routing_probs'] = getattr(module, attr)
                break
                
        for attr in ['last_gate_logits', 'gate_logits', 'router_logits']:
            if hasattr(module, attr):
                if not lightweight:
                    routing_info['gate_logits'] = getattr(module, attr)
                break
        
        # 2. ê¸°ì¡´ outputì—ì„œ ì¶”ì¶œ (ë‹¤ë¥¸ MoE êµ¬í˜„ìš©)
        if isinstance(output, tuple) and len(output) >= 3:
            # (hidden_states, routing_weights, selected_experts) í˜•íƒœ
            if output[2] is not None:
                routing_info['expert_assignments'] = output[2]
            if not lightweight and output[1] is not None:
                routing_info['routing_probs'] = output[1]
        
        # 3. gate/router ì„œë¸Œëª¨ë“ˆì—ì„œ ì¶”ì¶œ
        if hasattr(module, 'gate'):
            gate = module.gate
            for attr in ['last_routing_probs', 'routing_probs']:
                if hasattr(gate, attr):
                    if not lightweight:
                        routing_info['routing_probs'] = getattr(gate, attr)
                    break
        if hasattr(module, 'router'):
            router = module.router
            for attr in ['last_routing_probs', 'routing_probs']:
                if hasattr(router, attr):
                    if not lightweight:
                        routing_info['routing_probs'] = getattr(router, attr)
                    break
        # 4. combine_weights í˜•íƒœë¡œë§Œ ì œê³µë˜ëŠ” ê²½ìš°
        cw = getattr(module, 'combine_weights', None)
        if cw is None and isinstance(output, tuple) and len(output) >= 3:
            cw = output[2]
        if cw is not None:
            routing_info['expert_assignments'] = cw.argmax(dim=-1)
            if not lightweight:
                routing_info['routing_probs'] = cw
        
        # num_experts ì •ë³´ ì¶”ì¶œ
        if hasattr(module, 'num_experts'):
            routing_info['num_experts'] = module.num_experts
        elif hasattr(module, 'gate') and hasattr(module.gate, 'num_experts'):
            routing_info['num_experts'] = module.gate.num_experts
        elif hasattr(module, 'config') and hasattr(module.config, 'n_routed_experts'):
            routing_info['num_experts'] = module.config.n_routed_experts
        elif hasattr(module, 'config') and hasattr(module.config, 'num_local_experts'):
            routing_info['num_experts'] = module.config.num_local_experts
        elif len(getattr(module, 'experts', [])) > 0:
            routing_info['num_experts'] = len(module.experts)
        
        # Optionally pre-aggregate avg entropy without keeping full probs
        if not lightweight and 'routing_probs' in routing_info and routing_info['routing_probs'] is not None:
            probs = routing_info['routing_probs']
            if probs.dim() > 2:
                probs = probs.view(-1, probs.size(-1))
            token_entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)
            routing_info['avg_routing_entropy'] = token_entropy.mean()
        
        if hasattr(output, 'ortho_loss'):
            routing_info['ortho_loss'] = output.ortho_loss
        if hasattr(output, 'aux_loss'):
            routing_info['aux_loss'] = output.aux_loss
        
        # GramSpec ê´€ë ¨ ë©”íŠ¸ë¦­ (routerì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ê²½ìš°)
        if hasattr(module, 'router'):
            router = module.router
            # Expression lossëŠ” ê³„ì‚° ì‹œì ì—ë§Œ ì¡´ì¬í•˜ë¯€ë¡œ ì§ì ‘ ì¶”ì¶œ ë¶ˆê°€
            # ëŒ€ì‹  routerì˜ expression_projector ìƒíƒœë¥¼ í™•ì¸
            if hasattr(router, 'expression_projector'):
                # Orthogonal lossëŠ” forward ì¤‘ì— ê³„ì‚°ë˜ë¯€ë¡œ ë³„ë„ ì €ì¥ í•„ìš”
                # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥
                pass

        return routing_info if routing_info else None
    
    def on_step_begin(self):
        """Step ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        self.layer_outputs.clear()

        # Vision í†µê³„ëŠ” ëˆ„ì ë˜ë¯€ë¡œ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
        # ëŒ€ì‹  stepë³„ ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•˜ê¸° ìœ„í•´ ì´ì „ ê°’ ì €ì¥
        self.prev_vision_stats = self.vision_usage_stats.copy()
    
    def on_step_end(self, current_step: int, **kwargs):
        """Step ì¢…ë£Œ ì‹œ í˜¸ì¶œ - current_stepì€ í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜"""
        
        # G3MoERouter EMA ìˆ˜ë™ ì—…ë°ì´íŠ¸ (Gradient Checkpointing í˜¸í™˜)
        if self.model is not None:
            for module in self.model.modules():
                if hasattr(module, 'update_expert_load_ema') and hasattr(module, 'last_current_load') and module.last_current_load is not None:
                    module.update_expert_load_ema(module.last_current_load)
                    module.last_current_load = None

        # í˜„ì¬ step ì¶”ì  (ë””ë²„ê·¸ ë©”ì‹œì§€ ë¡œê¹…ìš©)
        self._current_step = current_step

        # ê°•ì œ ëª¨ë“  ë­í¬ ë¡œê¹… í—ˆìš© ë° hook ì‹¤íŒ¨ ì‹œ ëª¨ë¸ ìƒíƒœì—ì„œ ìˆ˜ì§‘í•˜ì—¬ í•­ìƒ ì§€í‘œ ì‚°ì¶œ
        if not self.layer_outputs:
            if self.log_to_console:
                self._log_debug(f"âš ï¸ Step {current_step}: No layer_outputs from hooks. Attempting to collect from model state...")
                # Hook í˜¸ì¶œ íšŸìˆ˜ í™•ì¸
                if hasattr(self, '_hook_call_count'):
                    total_calls = sum(self._hook_call_count.values())
                    self._log_debug(f"   - Total hook calls so far: {total_calls}")
                    if total_calls == 0:
                        self._log_debug(f"   - âš ï¸ WARNING: Hooks were never called! Check if model forward is being executed.")
                    else:
                        self._log_debug(f"   - Hook calls per layer: {dict(list(self._hook_call_count.items())[:5])}")
            collected = self._collect_from_model_state()
            if not collected:
                if self.log_to_console:
                    self._log_debug(f"âŒ Step {current_step}: No routing info captured via hooks or model state.")
                    self._log_debug(f"   - Hooks count: {len(self.hooks)}")
                    self._log_debug(f"   - Model is None: {self.model is None}")
                    if self.model is not None:
                        # ëª¨ë¸ì—ì„œ MoE ë ˆì´ì–´ ì°¾ê¸° ì‹œë„
                        moe_layers_found = []
                        for name, module in self.model.named_modules():
                            if self._is_moe_layer(module):
                                moe_layers_found.append(name)
                                # ì²« ë²ˆì§¸ MoE ë ˆì´ì–´ì—ì„œ last_selected_experts í™•ì¸
                                if len(moe_layers_found) == 1 and hasattr(module, 'last_selected_experts'):
                                    se = module.last_selected_experts
                                    self._log_debug(f"   - Sample MoE layer '{name}' has last_selected_experts: {se.shape if torch.is_tensor(se) else type(se)}")
                        self._log_debug(f"   - MoE layers in model: {len(moe_layers_found)}")
                        if moe_layers_found:
                            self._log_debug(f"   - Sample MoE layers: {moe_layers_found[:3]}")
            else:
                self.layer_outputs.update(collected)
                if self.log_to_console:
                    self._log_debug(f"âœ… Step {current_step}: Collected {len(collected)} routing info from model state")
                    self._log_debug(f"   - Collected layer names: {list(collected.keys())[:3]}")
        else:
            if self.log_to_console and current_step % 10 == 0:  # 10 stepë§ˆë‹¤ë§Œ ë¡œê·¸
                self._log_debug(f"âœ… Step {current_step}: layer_outputs has {len(self.layer_outputs)} entries")
                # ì‹¤ì œ MoE ë ˆì´ì–´ ìˆ˜ì™€ ë¹„êµ
                if self.model is not None:
                    moe_layers_in_model = []
                    for name, module in self.model.named_modules():
                        if self._is_moe_layer(module):
                            moe_layers_in_model.append(name)
                    if len(moe_layers_in_model) != len(self.layer_outputs):
                        self._log_debug(f"   âš ï¸ Mismatch: {len(moe_layers_in_model)} MoE layers in model, but {len(self.layer_outputs)} in layer_outputs")
                        missing_layers = [l for l in moe_layers_in_model if l not in self.layer_outputs]
                        if missing_layers:
                            self._log_debug(f"   âš ï¸ Missing layers in layer_outputs: {missing_layers[:5]}")
                        # ëˆ„ë½ëœ ë ˆì´ì–´ëŠ” ëª¨ë¸ ìƒíƒœì—ì„œ ìˆ˜ì§‘ ì‹œë„
                        if missing_layers:
                            collected = self._collect_from_model_state()
                            for layer_name in missing_layers:
                                if layer_name in collected:
                                    self.layer_outputs[layer_name] = collected[layer_name]
                                    self._log_debug(f"   âœ… Collected missing layer '{layer_name}' from model state")

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        step_metrics = self._calculate_step_metrics()
        # wrapperìš© last_metrics ì €ì¥
        self.last_metrics = step_metrics

        # ë””ë²„ê·¸: ë©”íŠ¸ë¦­ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if not step_metrics:
            if self.log_to_console:
                self._log_debug(f"Step {current_step}: step_metrics is empty! layer_outputs: {len(self.layer_outputs)}")
                if self.layer_outputs:
                    # layer_outputsëŠ” ìˆëŠ”ë° ë©”íŠ¸ë¦­ì´ ì—†ëŠ” ê²½ìš° - ì²« ë²ˆì§¸ ë ˆì´ì–´ í™•ì¸
                    first_layer = list(self.layer_outputs.keys())[0]
                    first_data = self.layer_outputs[first_layer]
                    self._log_debug(f"   - First layer '{first_layer}' data keys: {list(first_data.keys())}")
                    if 'expert_assignments' in first_data:
                        ea = first_data['expert_assignments']
                        self._log_debug(f"   - expert_assignments: shape={ea.shape if torch.is_tensor(ea) else 'N/A'}, numel={ea.numel() if torch.is_tensor(ea) else 'N/A'}")

        # _log_metricsë¥¼ ë§¤ stepë§ˆë‹¤ í˜¸ì¶œí•˜ì—¬ log_data ìƒì„±
        self._log_metrics(step_metrics, current_step)

        # âœ… on_step_endì—ì„œëŠ” pendingì—ë§Œ ì €ì¥, wandb ë¡œê¹…ì€ on_logì—ì„œ Trainerì™€ í•¨ê»˜ ì²˜ë¦¬
        # (step ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ wandb ë¡œê¹…í•˜ì§€ ì•ŠìŒ)
        if hasattr(self, 'last_log_data') and self.last_log_data:
            # pendingì— ì €ì¥ (on_logì—ì„œ Trainerì˜ WandbCallbackê³¼ í•¨ê»˜ ë¡œê¹…)
            self.pending_metrics[current_step] = self.last_log_data.copy()
            
            if self.log_to_console and (current_step % 10 == 0 or current_step <= 5):
                moe_metrics = {
                    k: v for k, v in self.last_log_data.items() 
                    if (k.startswith('moe/') or 
                        k.startswith('multi_modality/') or 
                        k.startswith('train/router/'))
                }
                self._log_debug(f"âœ… Step {current_step}: stored {len(moe_metrics)} MoE metrics in pending (will be logged in on_log)")
        else:
            if self.log_to_console and (current_step % 10 == 0 or current_step <= 5):
                self._log_debug(f"âš ï¸ Step {current_step}: no last_log_data to store")
                self._log_debug(f"   - has last_log_data attr: {hasattr(self, 'last_log_data')}")
                if hasattr(self, 'last_log_data'):
                    self._log_debug(f"   - last_log_data is None/empty: {self.last_log_data is None or not self.last_log_data}")
                # step_metrics í™•ì¸
                if hasattr(self, 'last_metrics'):
                    self._log_debug(f"   - last_metrics: {self.last_metrics}")

        # íˆíŠ¸ë§µ ìƒì„±
        if current_step % self.log_heatmap_every == 0:
            self._generate_heatmaps(current_step)
        
        # t-SNE ì‹œê°í™” ìƒì„±
        if current_step % self.log_tsne_every == 0:
            self._generate_tsne_visualizations(current_step)

        # ê²½ê³  ì²´í¬
        alerts = self._check_alerts(step_metrics)
        if alerts:
            self._handle_alerts(alerts, current_step)

        # ìƒì„¸ ë¡œê·¸ ì €ì¥
        if self.save_detailed_logs:
            self._save_detailed_log(step_metrics, current_step)

        # ìƒì„± ë¡œê¹… (ì„¤ì •ëœ ì£¼ê¸°ë§ˆë‹¤)
        if (self.enable_generation_logging and
            current_step % self.generation_log_every == 0 and
            self.model is not None and
            self.tokenizer is not None):
            self._log_debug(f"[MoE Generation] Logging generations at step {current_step}")
            self._log_generations(current_step)

    def _get_tokenizer_ids(self, tokenizer):
        """Processor ë˜ëŠ” Tokenizerì—ì„œ pad_token_idì™€ eos_token_idë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
        # Processor ê°ì²´ì¸ ê²½ìš° ì‹¤ì œ í† í¬ë‚˜ì´ì €ì— ì ‘ê·¼
        actual_tokenizer = tokenizer
        if hasattr(tokenizer, 'tokenizer'):
            actual_tokenizer = tokenizer.tokenizer
        
        # pad_token_id ê°€ì ¸ì˜¤ê¸°
        pad_token_id = None
        if hasattr(actual_tokenizer, 'pad_token_id') and actual_tokenizer.pad_token_id is not None:
            pad_token_id = actual_tokenizer.pad_token_id
        elif hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is not None:
            # pad_tokenì´ ìˆìœ¼ë©´ tokenizerë¥¼ í†µí•´ idë¡œ ë³€í™˜
            if hasattr(tokenizer, 'convert_tokens_to_ids'):
                pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
            elif hasattr(actual_tokenizer, 'convert_tokens_to_ids'):
                pad_token_id = actual_tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
        
        # eos_token_id ê°€ì ¸ì˜¤ê¸°
        eos_token_id = None
        if hasattr(actual_tokenizer, 'eos_token_id') and actual_tokenizer.eos_token_id is not None:
            eos_token_id = actual_tokenizer.eos_token_id
        elif hasattr(tokenizer, 'eos_token') and tokenizer.eos_token is not None:
            # eos_tokenì´ ìˆìœ¼ë©´ tokenizerë¥¼ í†µí•´ idë¡œ ë³€í™˜
            if hasattr(tokenizer, 'convert_tokens_to_ids'):
                eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
            elif hasattr(actual_tokenizer, 'convert_tokens_to_ids'):
                eos_token_id = actual_tokenizer.convert_tokens_to_ids(tokenizer.eos_token)
        
        # fallback: pad_token_idê°€ ì—†ìœ¼ë©´ eos_token_id ì‚¬ìš©
        if pad_token_id is None:
            pad_token_id = eos_token_id
        
        return pad_token_id, eos_token_id
    
    @torch.no_grad()
    def _test_vlm_capabilities(self, model, tokenizer):
        """VLM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸: ë©€í‹°ëª¨ë‹¬ê³¼ í…ìŠ¤íŠ¸ ì „ìš© ì¼€ì´ìŠ¤ ëª¨ë‘ í…ŒìŠ¤íŠ¸"""
        # tokenizerê°€ Noneì´ë©´ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
        if tokenizer is None:
            self._log_debug("âš ï¸ VLM test skipped: tokenizer is None")
            return
        
        # modelì´ Noneì´ë©´ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ
        if model is None:
            self._log_debug("âš ï¸ VLM test skipped: model is None")
            return
        
        self._log_debug("="*80)
        self._log_debug("ğŸ” VLM Capabilities Test (Training Start)")
        self._log_debug("="*80)
        
        test_results = {
            "multimodal_tests": [],
            "text_only_tests": [],
            "chat_template_tests": []
        }
        
        original_mode = model.training
        model.eval()
        try:
            # í…ŒìŠ¤íŠ¸ 1: ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸) í…ŒìŠ¤íŠ¸
            self._log_debug("\nğŸ“¸ Test 1: Multimodal (Image + Text) Generation")
            try:
                sample_image_url = "https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg"
                image = load_image(sample_image_url)
                
                # Chat template ì ìš© í…ŒìŠ¤íŠ¸
                multimodal_messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Describe this image in Korean."},
                            {"type": "image"}
                        ]
                    }
                ]
                
                # Chat template ì ìš©
                try:
                    chat_template_result = tokenizer.apply_chat_template(
                        multimodal_messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    test_results["chat_template_tests"].append({
                        "type": "multimodal",
                        "status": "success",
                        "template_length": len(chat_template_result)
                    })
                    self._log_debug(f"  âœ… Chat template applied successfully (length: {len(chat_template_result)})")
                except Exception as e:
                    test_results["chat_template_tests"].append({
                        "type": "multimodal",
                        "status": "failed",
                        "error": str(e)
                    })
                    self._log_debug(f"  âŒ Chat template failed: {e}")
                    raise
                
                # í† í¬ë‚˜ì´ì§• ë° ìƒì„± í…ŒìŠ¤íŠ¸
                test_input_text = chat_template_result.replace("<bos>", "")[:-1] if "<bos>" in chat_template_result else chat_template_result
                inputs = tokenizer(
                    text=test_input_text,
                    images=image,
                    return_tensors="pt"
                )
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
                
                pad_token_id, eos_token_id = self._get_tokenizer_ids(tokenizer)
                start_time = time.time()
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=30,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=pad_token_id,
                    eos_token_id=eos_token_id,
                    use_cache=True,
                )
                
                input_length = inputs['input_ids'].shape[1]
                generated_text = tokenizer.decode(
                    outputs[0][input_length:],
                    skip_special_tokens=True
                )
                end_time = time.time()
                test_results["multimodal_tests"].append({
                    "status": "success",
                    "generated_length": len(generated_text),
                    "generated_preview": generated_text.strip()[:100]
                })

                self._log_debug(f"  âœ… Multimodal generation successful (time: {end_time - start_time} seconds)")
                self._log_debug(f"     Generated: {generated_text.strip()[:100]}...")
                
            except Exception as e:
                test_results["multimodal_tests"].append({
                    "status": "failed",
                    "error": str(e)
                })
                self._log_debug(f"  âŒ Multimodal test failed: {e}")
                import traceback
                self._log_debug(f"     Traceback: {traceback.format_exc()}")
            
            # í…ŒìŠ¤íŠ¸ 2: í…ìŠ¤íŠ¸ ì „ìš© í…ŒìŠ¤íŠ¸
            self._log_debug("\nğŸ“ Test 2: Text-Only Generation")
            try:
                text_only_messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "What is the capital of France?"}
                        ]
                    }
                ]
                
                # Chat template ì ìš© í…ŒìŠ¤íŠ¸
                try:
                    chat_template_result = tokenizer.apply_chat_template(
                        text_only_messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    test_results["chat_template_tests"].append({
                        "type": "text_only",
                        "status": "success",
                        "template_length": len(chat_template_result)
                    })
                    self._log_debug(f"  âœ… Chat template applied successfully (length: {len(chat_template_result)})")
                except Exception as e:
                    test_results["chat_template_tests"].append({
                        "type": "text_only",
                        "status": "failed",
                        "error": str(e)
                    })
                    self._log_debug(f"  âŒ Chat template failed: {e}")
                    raise
                
                # í† í¬ë‚˜ì´ì§• ë° ìƒì„± í…ŒìŠ¤íŠ¸
                test_input_text = chat_template_result.replace("<bos>", "")[:-1] if "<bos>" in chat_template_result else chat_template_result
                
                # í…ìŠ¤íŠ¸ ì „ìš©ì´ë¯€ë¡œ images íŒŒë¼ë¯¸í„° ì—†ì´ ì²˜ë¦¬
                if hasattr(tokenizer, 'tokenizer'):
                    # AutoProcessorì¸ ê²½ìš°
                    inputs = tokenizer(
                        text=test_input_text,
                        return_tensors="pt"
                    )
                else:
                    # AutoTokenizerì¸ ê²½ìš°
                    inputs = tokenizer(
                        test_input_text,
                        return_tensors="pt"
                    )
                
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
                
                pad_token_id, eos_token_id = self._get_tokenizer_ids(tokenizer)
                
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=30,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=pad_token_id,
                    eos_token_id=eos_token_id,
                    use_cache=True,
                )
                
                input_length = inputs['input_ids'].shape[1]
                generated_text = tokenizer.decode(
                    outputs[0][input_length:],
                    skip_special_tokens=True
                )
                
                test_results["text_only_tests"].append({
                    "status": "success",
                    "generated_length": len(generated_text),
                    "generated_preview": generated_text.strip()[:100]
                })
                self._log_debug(f"  âœ… Text-only generation successful")
                self._log_debug(f"     Generated: {generated_text.strip()[:100]}...")
                
            except Exception as e:
                test_results["text_only_tests"].append({
                    "status": "failed",
                    "error": str(e)
                })
                self._log_debug(f"  âŒ Text-only test failed: {e}")
                import traceback
                self._log_debug(f"     Traceback: {traceback.format_exc()}")
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
            self._log_debug("\n" + "="*80)
            self._log_debug("ğŸ“Š VLM Test Summary")
            self._log_debug("="*80)
            
            multimodal_success = any(t.get("status") == "success" for t in test_results["multimodal_tests"])
            text_only_success = any(t.get("status") == "success" for t in test_results["text_only_tests"])
            chat_template_success = all(t.get("status") == "success" for t in test_results["chat_template_tests"])
            
            self._log_debug(f"  Multimodal Test: {'âœ… PASS' if multimodal_success else 'âŒ FAIL'}")
            self._log_debug(f"  Text-Only Test: {'âœ… PASS' if text_only_success else 'âŒ FAIL'}")
            self._log_debug(f"  Chat Template Test: {'âœ… PASS' if chat_template_success else 'âŒ FAIL'}")
            
            # wandbì— ë¡œê¹…
            if self.logger and hasattr(self.logger, 'log'):
                try:
                    wandb_log_data = {
                        'vlm_test/multimodal_success': 1.0 if multimodal_success else 0.0,
                        'vlm_test/text_only_success': 1.0 if text_only_success else 0.0,
                        'vlm_test/chat_template_success': 1.0 if chat_template_success else 0.0,
                    }
                    
                    # ìƒì„¸ ê²°ê³¼ë„ ì¶”ê°€
                    if test_results["multimodal_tests"]:
                        mm_result = test_results["multimodal_tests"][0]
                        if mm_result.get("status") == "success":
                            wandb_log_data['vlm_test/multimodal_generated_length'] = mm_result.get("generated_length", 0)
                    
                    if test_results["text_only_tests"]:
                        to_result = test_results["text_only_tests"][0]
                        if to_result.get("status") == "success":
                            wandb_log_data['vlm_test/text_only_generated_length'] = to_result.get("generated_length", 0)
                    
                    self.logger.log(wandb_log_data, step=0, commit=True)
                    self._log_debug(f"  âœ… Test results logged to wandb")
                except Exception as e:
                    self._log_debug(f"  âš ï¸ Failed to log test results to wandb: {e}")
            
            # ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
            all_tests_passed = multimodal_success and text_only_success and chat_template_success
            if all_tests_passed:
                self._log_debug("\nâœ… All VLM tests passed!")
            else:
                self._log_debug("\nâš ï¸ Some VLM tests failed. Check the logs above for details.")
            
            self._log_debug("="*80 + "\n")
            
        except Exception as e:
            self._log_debug(f"âŒ VLM test error: {e}")
            import traceback
            self._log_debug(traceback.format_exc())
        finally:
            model.train(original_mode)
    
    @torch.no_grad()
    def _log_generations(self, current_step: int):
        """ëª¨ë¸ ìƒì„± ê²°ê³¼ ë¡œê¹…"""
        # if not self.is_main_process:
        #     return

        try:
            self.generation_step_count += 1

            sample_image_urls = [
                "https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg",
                "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg",
                "https://ocr.space/Content/Images/table-ocr-original.webp",
            ]

            test_input = self.tokenizer.apply_chat_template(
                [
                    {
                        "role": "system",
                        "content": [
                            {"type": "text", "text": "You are a helpful assistant."}
                        ]
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Describe this image in Korean."},
                            {"type": "image"}
                        ]
                    }
                ],
                # tokenize=True,
                add_generation_prompt=True,
                # return_tensors="pt",
                # return_dict=True,
            )

            generation_logs = []
            sample_count = 0

            # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì „í™˜
            original_mode = self.model.training
            self.model.eval()

            # ì²˜ë¦¬í•  ì´ë¯¸ì§€ URL ì„ íƒ
            images_to_process = sample_image_urls[:self.max_generation_samples]
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ CPU ì‚¬ìš©ë¥  ê°ì†Œ ë° ì†ë„ í–¥ìƒ
            try:
                # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ë¡œë“œ
                images = [load_image(url) for url in images_to_process]
                
                # ë°°ì¹˜ í† í¬ë‚˜ì´ì§• (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ê°œë³„ ì²˜ë¦¬ í•„ìš”)
                # Vision ëª¨ë¸ì˜ ê²½ìš° ë°°ì¹˜ ì²˜ë¦¬ê°€ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°œë³„ ì²˜ë¦¬ ìœ ì§€í•˜ë˜ ìµœì í™”
                pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id
                test_input_text = test_input.replace("<bos>", "")[:-1]
                
                for idx, image in enumerate(images):
                    if sample_count >= self.max_generation_samples:
                        break
                    
                    try:
                        # ì…ë ¥ í† í°í™”
                        inputs = self.tokenizer(
                            text=test_input_text,
                            images=image,
                            return_tensors="pt"
                        )
                        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

                        # ìƒì„± ì‹¤í–‰ (ìµœì í™”ëœ ì„¤ì •)
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=50,  # 100 -> 50ìœ¼ë¡œ ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ
                            num_return_sequences=1,
                            do_sample=False,  # greedy decodingìœ¼ë¡œ ì†ë„ í–¥ìƒ ë° CPU ë¶€í•˜ ê°ì†Œ
                            pad_token_id=pad_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            use_cache=True,  # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
                        )

                        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
                        input_length = inputs['input_ids'].shape[1]
                        generated_text = self.tokenizer.decode(
                            outputs[0][input_length:],
                            skip_special_tokens=True
                        )

                        # ë¡œê·¸ ë°ì´í„° êµ¬ì„±
                        log_entry = {
                            "step": current_step,
                            "generation_step": self.generation_step_count,
                            "sample_index": sample_count,
                            "prompt": "Describe this image in Korean.",
                            "generated": generated_text.strip(),
                            "full_response": self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                        }

                        generation_logs.append(log_entry)
                        sample_count += 1

                        # ì½˜ì†” ë¡œê·¸ (ê°„ì†Œí™”)
                        if self.log_to_console:
                            self._log_debug(f"Generation sample {sample_count}: {generated_text.strip()[:60]}...")

                    except Exception as e:
                        self._log_debug(f"Error generating for sample {sample_count}: {e}")
                        sample_count += 1
                        continue
                        
            except Exception as batch_error:
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê°œë³„ ì²˜ë¦¬ë¡œ fallback
                self._log_debug(f"âš ï¸ Batch processing failed, falling back to individual: {batch_error}")
                
                for sample_image_url in images_to_process:
                    if sample_count >= self.max_generation_samples:
                        break

                    try:
                        # ì…ë ¥ í† í°í™”
                        image = load_image(sample_image_url)

                        inputs = self.tokenizer(
                            text=test_input_text,
                            images=image,
                            return_tensors="pt")
                        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

                        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id

                        # ìƒì„± ì‹¤í–‰ (ìµœì í™”ëœ ì„¤ì •)
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=50,
                            num_return_sequences=1,
                            do_sample=False,
                            pad_token_id=pad_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            use_cache=True,
                        )

                        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
                        input_length = inputs['input_ids'].shape[1]
                        generated_text = self.tokenizer.decode(
                            outputs[0][input_length:],
                            skip_special_tokens=True
                        )

                        # ë¡œê·¸ ë°ì´í„° êµ¬ì„±
                        log_entry = {
                            "step": current_step,
                            "generation_step": self.generation_step_count,
                            "sample_index": sample_count,
                            "prompt": "Describe this image in Korean.",
                            "generated": generated_text.strip(),
                            "full_response": self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                        }

                        generation_logs.append(log_entry)
                        sample_count += 1

                        # ì½˜ì†” ë¡œê·¸ (ê°„ì†Œí™”)
                        if self.log_to_console:
                            self._log_debug(f"Generation sample {sample_count}: {generated_text.strip()[:60]}...")

                    except Exception as e:
                        self._log_debug(f"Error generating for sample {sample_count}: {e}")
                        sample_count += 1
                        continue

            # ìƒì„± ë¡œê·¸ íŒŒì¼ ì €ì¥
            if generation_logs:
                log_file = os.path.join(
                    self.generation_log_dir,
                    f"generation_log_step_{current_step}_gen_{self.generation_step_count}.json"
                )

                with open(log_file, 'w', encoding='utf-8') as f:
                    json.dump(generation_logs, f, ensure_ascii=False, indent=2)

                self._log_debug(f"Generation logs saved to {log_file}")

                # ë¡œê±°ì— ìƒì„± ê²°ê³¼ ë¡œê¹… (Wandb ë“±)
                if self.logger and hasattr(self.logger, 'log'):
                    try:
                        gen_log_data = {}
                        for i, log_entry in enumerate(generation_logs):
                            gen_log_data[f'generation/step_{current_step}/sample_{i}/prompt'] = log_entry['prompt']
                            gen_log_data[f'generation/step_{current_step}/sample_{i}/generated'] = log_entry['generated'][:200] + "..."
                        self.logger.log(gen_log_data, step=current_step, commit=True)
                    except Exception as e:
                        self._log_debug(f"Warning: Failed to log generation to wandb at step {current_step}: {e}")

            # ëª¨ë¸ì„ ì›ë˜ ëª¨ë“œë¡œ ë³µì›
            self.model.train(original_mode)
            self._log_debug(f"âœ… Completed generation logging at step {current_step} ({sample_count} samples)")

        except Exception as e:
            self._log_debug(f"Error during generation logging: {e}")
            # ëª¨ë¸ì„ ë‹¤ì‹œ training ëª¨ë“œë¡œ ì „í™˜
            if self.model is not None:
                self.model.train(original_mode)


    @torch.no_grad()
    def _calculate_step_metrics(self):
        """í˜„ì¬ stepì˜ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        # ë””ë²„ê·¸: layer_outputsì— í¬í•¨ëœ ë ˆì´ì–´ í™•ì¸
        if self.log_to_console and len(self.layer_outputs) > 0:
            layer_names = list(self.layer_outputs.keys())
            if len(layer_names) <= 10:
                self._log_debug(f"ğŸ“Š _calculate_step_metrics: processing {len(layer_names)} layers: {layer_names}")
            else:
                self._log_debug(f"ğŸ“Š _calculate_step_metrics: processing {len(layer_names)} layers (first 10: {layer_names[:10]})")
        
        for layer_name, routing_info in self.layer_outputs.items():
            layer_metrics = {}
            
            expert_assignments = routing_info.get('expert_assignments')
            routing_probs = routing_info.get('routing_probs')
            
            # âœ… num_expertsë¥¼ ë°ì´í„°ì—ì„œ ìœ ë„ (ê°€ëŠ¥í•œ í•œ ë°ì´í„° ê¸°ë°˜, ìµœí›„ì—ë§Œ ì´ˆê¸°í™” ì¸ì ì‚¬ìš©)
            num_experts = routing_info.get('num_experts')
            if num_experts is None and routing_probs is not None and torch.is_tensor(routing_probs) and routing_probs.numel() > 0:
                num_experts = int(routing_probs.size(-1))
            if num_experts is None and expert_assignments is not None and torch.is_tensor(expert_assignments) and expert_assignments.numel() > 0:
                try:
                    num_experts = int(expert_assignments.max().item() + 1)
                except Exception:
                    pass
            if num_experts is None:
                num_experts = int(self.num_experts)
            
            # GramSpec ë¶„ì„ (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.gramspec_analyzer is not None and hasattr(routing_info, 'gram_matrix'):
                # GramSpec ë¶„ì„ê¸°ëŠ” forward hookì—ì„œ ì§ì ‘ í˜¸ì¶œí•´ì•¼ í•¨
                # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ ë©”íŠ¸ë¦­ë§Œ ê³„ì‚°
                pass
            
            if expert_assignments is not None:
                # CPUë¡œ ì´ë™ ë° clamp
                if torch.is_tensor(expert_assignments):
                    if expert_assignments.is_cuda:
                        expert_assignments = expert_assignments.cpu()
                    
                    # ì°¨ì› í™•ì¸ ë° 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (bincount ìš”êµ¬ì‚¬í•­)
                    if expert_assignments.dim() > 1:
                        expert_assignments = expert_assignments.flatten()
                    elif expert_assignments.dim() == 0:
                        # ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
                        expert_assignments = expert_assignments.unsqueeze(0)
                    
                    # ìµœì¢…ì ìœ¼ë¡œ 1ì°¨ì›ì¸ì§€ í™•ì¸
                    if expert_assignments.dim() != 1:
                        expert_assignments = expert_assignments.view(-1)
                    
                    # ìŒìˆ˜ ì œê±° ë° long íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                    expert_assignments = expert_assignments.clamp(min=0).long()
                
                # Expert ì‚¬ìš© ë¶„í¬ ê³„ì‚°
                if expert_assignments.numel() == 0:
                    # ë¹ˆ í…ì„œì¸ ê²½ìš° ê¸°ë³¸ê°’
                    usage_counts = torch.zeros(num_experts, dtype=torch.long)
                else:
                    # âœ… ì˜¬ë°”ë¥¸ minlengthë¡œ bincount ê³„ì‚°
                    inferred_len = int(expert_assignments.max().item() + 1) if expert_assignments.numel() > 0 else num_experts
                    minlength = max(num_experts, inferred_len)
                    usage_counts = torch.bincount(expert_assignments, minlength=minlength)[:num_experts]
                self.expert_usage_history[layer_name].append(usage_counts)
                
                # Layerë³„ expert usage tracking (ì‹¤ì œ ê²€ì¦ìš©)
                if layer_name not in self.layer_expert_usage_counts:
                    self.layer_expert_usage_counts[layer_name] = torch.zeros(num_experts, dtype=torch.long)
                to_add = usage_counts
                if to_add.size(0) != num_experts:
                    to_add = F.pad(to_add, (0, max(0, num_experts - to_add.size(0))))[:num_experts]
                self.layer_expert_usage_counts[layer_name] += to_add
                
                usage_distribution = usage_counts.float() / (usage_counts.sum().clamp_min(1.0))
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                layer_metrics.update({
                    'usage_counts': usage_counts,
                    'usage_distribution': usage_distribution,
                    'expert_cv': torch.std(usage_distribution) / (torch.mean(usage_distribution) + 1e-8),
                    'max_usage_ratio': usage_distribution.max() / (usage_distribution.mean() + 1e-8),
                    'unused_experts': (usage_counts == 0).sum().item(),
                    'active_experts': (usage_counts > 0).sum().item(),
                    # utilization_mean: usage_distributionì˜ í‰ê·  (ê° expertì˜ í‰ê·  ì‚¬ìš©ë¥ )
                    'utilization_mean': usage_distribution.mean().item(),
                })
                
                # MaxVio (Global Load Imbalance) calculation
                try:
                    maxvio = self._calculate_maxvio(usage_counts, num_experts)
                    # ìœ íš¨í•œ ê°’ì¸ ê²½ìš°ì—ë§Œ ì¶”ê°€ (Noneì´ ì•„ë‹Œ ê²½ìš°)
                    if maxvio is not None:
                        layer_metrics['maxvio'] = maxvio
                except Exception as e:
                    if self.log_to_console:
                        self._log_debug(f"Warning: Failed to calculate MaxVio for {layer_name}: {e}")
                    # 0ìœ¼ë¡œ fallbackí•˜ì§€ ì•ŠìŒ - ë©”íŠ¸ë¦­ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            
            if routing_probs is not None:
                # ë¼ìš°íŒ… ì—”íŠ¸ë¡œí”¼
                if routing_probs.dim() > 2:
                    routing_probs = routing_probs.view(-1, routing_probs.size(-1))
                
                # ê° í† í°ì˜ ë¼ìš°íŒ… ì—”íŠ¸ë¡œí”¼
                safe_probs = routing_probs.clamp_min(1e-12)
                token_entropy = -torch.sum(safe_probs * torch.log(safe_probs), dim=-1)
                avg_entropy = token_entropy.mean()
                
                layer_metrics.update({
                    'routing_entropy': avg_entropy,
                    'min_entropy': token_entropy.min(),
                    'max_entropy': token_entropy.max(),
                })
                
                # Routing variance calculation
                try:
                    routing_variance = self._calculate_routing_variance(routing_probs)
                    # ìœ íš¨í•œ ê°’ì¸ ê²½ìš°ì—ë§Œ ì¶”ê°€ (Noneì´ ì•„ë‹Œ ê²½ìš°)
                    if routing_variance is not None:
                        layer_metrics['routing_variance'] = routing_variance
                except Exception as e:
                    if self.log_to_console:
                        self._log_debug(f"Warning: Failed to calculate routing variance for {layer_name}: {e}")
                    # 0ìœ¼ë¡œ fallbackí•˜ì§€ ì•ŠìŒ - ë©”íŠ¸ë¦­ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                
                # Top-k score gap calculation
                try:
                    topk_gap = self._calculate_topk_score_gap(routing_probs)
                    # ìœ íš¨í•œ ê°’ì¸ ê²½ìš°ì—ë§Œ ì¶”ê°€ (Noneì´ ì•„ë‹Œ ê²½ìš°)
                    if topk_gap is not None:
                        layer_metrics['topk_score_gap'] = topk_gap
                except Exception as e:
                    if self.log_to_console:
                        self._log_debug(f"Warning: Failed to calculate top-k gap for {layer_name}: {e}")
                    # 0ìœ¼ë¡œ fallbackí•˜ì§€ ì•ŠìŒ - ë©”íŠ¸ë¦­ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            
            # G3MoE specific metrics
            if 'speciality_loss' in routing_info and routing_info['speciality_loss'] is not None:
                val = routing_info['speciality_loss']
                if torch.is_tensor(val):
                    layer_metrics['speciality_loss'] = val.item() if val.numel() == 1 else val.mean().item()
                else:
                    layer_metrics['speciality_loss'] = float(val)
            
            if 'cosine_similarities' in routing_info and routing_info['cosine_similarities'] is not None:
                val = routing_info['cosine_similarities']
                if torch.is_tensor(val):
                    layer_metrics['cosine_similarities'] = val.item() if val.numel() == 1 else val.mean().item()
                else:
                    layer_metrics['cosine_similarities'] = float(val)
            
            if 'expression_loss' in routing_info and routing_info['expression_loss'] is not None:
                val = routing_info['expression_loss']
                if torch.is_tensor(val):
                    layer_metrics['expression_loss'] = val.item() if val.numel() == 1 else val.mean().item()
                else:
                    layer_metrics['expression_loss'] = float(val)
            
            # Gram matrix orthogonality calculation (if routing_logits available)
            # Check multiple possible keys for routing logits
            routing_logits = (routing_info.get('gate_logits') or 
                            routing_info.get('routing_logits') or
                            routing_info.get('router_logits'))
            if routing_logits is not None and torch.is_tensor(routing_logits) and routing_logits.numel() > 0:
                try:
                    # Ensure routing_logits is on CPU for calculation
                    if routing_logits.is_cuda:
                        routing_logits_cpu = routing_logits.detach().cpu()
                    else:
                        routing_logits_cpu = routing_logits.detach()
                    gram_ortho = self._calculate_gram_orthogonality(routing_logits_cpu)
                    # ìœ íš¨í•œ ê°’ì¸ ê²½ìš°ì—ë§Œ ì¶”ê°€ (Noneì´ ì•„ë‹Œ ê²½ìš°)
                    if gram_ortho is not None:
                        layer_metrics['gram_orthogonality'] = gram_ortho
                except Exception as e:
                    if self.log_to_console and hasattr(self, '_current_step') and self._current_step % 100 == 0:
                        self._log_debug(f"Warning: Failed to calculate Gram orthogonality for {layer_name}: {e}")
                    # Don't add gram_orthogonality if calculation fails - gracefully skip
            
            metrics[layer_name] = layer_metrics
        
        # Layer-wise balance ë¶„ì„ (ì‹¤ì œ ê²€ì¦ ì§€í‘œ)
        if self.gramspec_validator is not None and self.layer_expert_usage_counts:
            # Layer index ì¶”ì¶œ (layer_nameì—ì„œ)
            layer_idx_map = {}
            for layer_name in self.layer_expert_usage_counts.keys():
                # layer_nameì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: "model.layers.5.moe" -> 5)
                import re
                match = re.search(r'\.(\d+)\.', layer_name)
                if match:
                    layer_idx = int(match.group(1))
                    layer_idx_map[layer_idx] = self.layer_expert_usage_counts[layer_name]
            
            if layer_idx_map:
                layer_balance_metrics = self.gramspec_validator.analyze_layer_wise_balance(layer_idx_map)
                metrics['_layer_wise_balance'] = layer_balance_metrics
        
        return metrics

    @torch.no_grad()
    def _calculate_maxvio(self, usage_counts, num_experts):
        """Calculate MaxVio (Global Load Imbalance) metric.
        
        MaxVio = max(|load_i - target|) / target
        where target = total_tokens / num_experts
        
        Args:
            usage_counts: Tensor of shape [num_experts] with token counts per expert
            num_experts: Number of experts
            
        Returns:
            MaxVio value as float, or None if calculation is not possible
        """
        if usage_counts is None or usage_counts.numel() == 0:
            return None
        
        total_tokens = usage_counts.sum().float()
        if total_tokens == 0:
            return None
        
        target_per_expert = total_tokens / num_experts
        deviations = torch.abs(usage_counts.float() - target_per_expert)
        maxvio = deviations.max() / (target_per_expert + 1e-8)
        return maxvio.item()
    
    @torch.no_grad()
    def _calculate_routing_variance(self, routing_probs):
        """Calculate routing variance metric.
        
        Measures the variance of routing probability distributions across experts.
        Higher variance indicates more discriminative routing decisions.
        
        Args:
            routing_probs: Tensor of shape [N, num_experts] with routing probabilities
            
        Returns:
            Average variance across tokens as float, or None if calculation is not possible
        """
        if routing_probs is None or routing_probs.numel() == 0:
            return None
        
        # Ensure 2D: [N, num_experts]
        if routing_probs.dim() > 2:
            routing_probs = routing_probs.view(-1, routing_probs.size(-1))
        elif routing_probs.dim() == 1:
            # Single token case, reshape to [1, num_experts]
            routing_probs = routing_probs.unsqueeze(0)
        
        # Need at least 2 experts for variance calculation
        if routing_probs.size(-1) < 2:
            return None
        
        # Calculate variance across experts for each token, then average
        token_variances = torch.var(routing_probs, dim=-1)
        return token_variances.mean().item()
    
    @torch.no_grad()
    def _calculate_topk_score_gap(self, routing_probs):
        """Calculate top-k score gap metric.
        
        Measures the difference between top-1 and top-2 routing scores.
        Larger gap indicates more confident routing decisions.
        
        Args:
            routing_probs: Tensor of shape [N, num_experts] with routing probabilities
            
        Returns:
            Average gap between top-1 and top-2 scores as float, or None if calculation is not possible
        """
        if routing_probs is None or routing_probs.numel() == 0:
            return None
        
        # Ensure 2D: [N, num_experts]
        if routing_probs.dim() > 2:
            routing_probs = routing_probs.view(-1, routing_probs.size(-1))
        elif routing_probs.dim() == 1:
            routing_probs = routing_probs.unsqueeze(0)
        
        # Need at least 2 experts for top-k gap
        if routing_probs.size(-1) < 2:
            return None
        
        # Get top-2 values for each token
        top_k_values, _ = torch.topk(routing_probs, k=min(2, routing_probs.size(-1)), dim=-1)
        
        if top_k_values.size(-1) < 2:
            return None
        
        # Calculate gap: top-1 - top-2
        gap = (top_k_values[:, 0] - top_k_values[:, 1]).mean()
        return gap.item()
    
    @torch.no_grad()
    def _calculate_gram_orthogonality(self, routing_logits):
        """Calculate Gram matrix orthogonality metric.
        
        Measures how orthogonal the expert representations are by computing
        the Frobenius norm of (Gram - I) where Gram is the Gram matrix of
        normalized routing logits.
        
        Args:
            routing_logits: Tensor of shape [N, num_experts, router_dim] or [N, router_dim]
            
        Returns:
            Orthogonality residual (Frobenius norm) as float, or None if calculation is not possible
        """
        if routing_logits is None or routing_logits.numel() == 0:
            return None
        
        # Handle different input shapes
        if routing_logits.dim() == 2:
            # [N, router_dim] - single expert representation per token
            # Normalize
            normalized = F.normalize(routing_logits, p=2, dim=-1)
            # Compute Gram matrix: [N, router_dim] @ [router_dim, N] = [N, N]
            gram = torch.matmul(normalized, normalized.t())
            # Identity matrix
            identity = torch.eye(gram.size(0), device=gram.device, dtype=gram.dtype)
            # Frobenius norm of difference
            diff = gram - identity
            ortho_residual = torch.norm(diff, p='fro').item()
            return ortho_residual
        elif routing_logits.dim() == 3:
            # [N, num_experts, router_dim] - multiple expert representations per token
            # Normalize each expert representation
            normalized = F.normalize(routing_logits, p=2, dim=-1)
            # Compute Gram matrix: [N, num_experts, router_dim] @ [N, router_dim, num_experts] = [N, num_experts, num_experts]
            gram = torch.matmul(normalized, normalized.transpose(-2, -1))
            # Identity matrix for each token
            num_experts = gram.size(-1)
            identity = torch.eye(num_experts, device=gram.device, dtype=gram.dtype)
            identity = identity.unsqueeze(0).expand(gram.size(0), -1, -1)
            # Frobenius norm of difference for each token, then average
            diff = gram - identity
            # Compute Frobenius norm per token: [N, num_experts, num_experts] -> [N]
            token_norms = torch.norm(diff.view(diff.size(0), -1), p='fro', dim=-1)
            ortho_residual = token_norms.mean().item()
            return ortho_residual
        else:
            return None

    @torch.no_grad()
    def _collect_from_model_state(self):
        """
        forward hookì´ ì‹¤í–‰ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬, ëª¨ë¸ ëª¨ë“ˆì˜ ìƒíƒœ ë³€ìˆ˜ì—ì„œ
        ìµœê·¼ ë¼ìš°íŒ… ì •ë³´ë¥¼ ì§ì ‘ ìˆ˜ì§‘í•œë‹¤.
        - ëŒ€ìƒ: last_selected_experts, last_routing_weights, last_num_experts
        """
        if self.model is None:
            return {}
        collected = {}
        try:
            for name, module in self.model.named_modules():
                has_any = any(
                    hasattr(module, attr) for attr in (
                        'last_selected_experts', 'last_routing_weights', 'last_num_experts'
                    )
                )
                if not has_any:
                    continue
                entry = {}
                if hasattr(module, 'last_selected_experts'):
                    lse = module.last_selected_experts
                    if torch.is_tensor(lse) and lse.numel() > 0:
                        entry['expert_assignments'] = lse.detach().to('cpu', non_blocking=True)
                if hasattr(module, 'last_routing_weights'):
                    lrw = module.last_routing_weights
                    if torch.is_tensor(lrw) and lrw.numel() > 0:
                        entry['routing_probs'] = lrw.detach().to('cpu', non_blocking=True)
                if hasattr(module, 'last_num_experts'):
                    try:
                        entry['num_experts'] = int(module.last_num_experts)
                    except Exception:
                        pass
                if entry:
                    collected[name] = entry
        except Exception as e:
            if self.log_to_console:
                self._log_debug(f"_collect_from_model_state error: {e}")
        return collected
    
    def _log_metrics(self, metrics, current_step: int):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        # ë””ë²„ê¹…: ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘ (wandbì—ë§Œ ê¸°ë¡, console ì¶œë ¥ ì•ˆ í•¨)
        
        log_data = {}
        
        # ë ˆì´ì–´ë³„ ë©”íŠ¸ë¦­ (moe ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬)
        logged_layers = []
        for layer_name, layer_metrics in metrics.items():
            if layer_name.startswith('_'):
                continue  # ë‚´ë¶€ ë©”íŠ¸ë¦­ì€ ê±´ë„ˆë›°ê¸°
            logged_layers.append(layer_name)
            for metric_name, value in layer_metrics.items():
                if torch.is_tensor(value) and value.numel() == 1:
                    log_data[f'moe/{layer_name}/{metric_name}'] = value.item()
                elif isinstance(value, (int, float)):
                    log_data[f'moe/{layer_name}/{metric_name}'] = value
        
        # ë””ë²„ê·¸: ë¡œê¹…ëœ ë ˆì´ì–´ í™•ì¸
        if self.log_to_console and current_step % 10 == 0:
            self._log_debug(f"ğŸ“Š _log_metrics at step {current_step}: logged {len(logged_layers)} layers")
            if len(logged_layers) <= 10:
                self._log_debug(f"   Logged layers: {logged_layers}")
            else:
                self._log_debug(f"   Logged layers (first 10): {logged_layers[:10]}")
            # layer_outputsì™€ ë¹„êµ
            if hasattr(self, 'layer_outputs'):
                layer_outputs_keys = list(self.layer_outputs.keys())
                missing_layers = [l for l in layer_outputs_keys if l not in logged_layers]
                if missing_layers:
                    self._log_debug(f"   âš ï¸ Missing layers in metrics: {missing_layers[:10]}")
        
        # ì „ì²´ í‰ê·  ë©”íŠ¸ë¦­ (moe ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬)
        if metrics:
            # ì‹¤ì œë¡œ ê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ ê³„ì‚° (0ìœ¼ë¡œ fallbackí•˜ì§€ ì•ŠìŒ)
            cv_values = [m['expert_cv'].item() if torch.is_tensor(m['expert_cv']) else m['expert_cv'] 
                         for m in metrics.values() 
                         if isinstance(m, dict) and not isinstance(m, str) and 'expert_cv' in m]
            entropy_values = [m['routing_entropy'].item() if torch.is_tensor(m['routing_entropy']) else m['routing_entropy']
                              for m in metrics.values() 
                              if isinstance(m, dict) and not isinstance(m, str) and 'routing_entropy' in m]
            unused_values = [m['unused_experts'] 
                            for m in metrics.values() 
                            if isinstance(m, dict) and not isinstance(m, str) and 'unused_experts' in m]
            
            if cv_values:
                log_data['moe/avg_expert_cv'] = np.mean(cv_values)
            if entropy_values:
                log_data['moe/avg_routing_entropy'] = np.mean(entropy_values)
            if unused_values:
                log_data['moe/total_unused_experts'] = sum(unused_values)
            
            # Utilization mean aggregation
            utilization_mean_values = [m['utilization_mean'] 
                                      for m in metrics.values() 
                                      if isinstance(m, dict) and not isinstance(m, str) and 'utilization_mean' in m]
            if utilization_mean_values:
                log_data['moe/avg_utilization_mean'] = np.mean(utilization_mean_values)
            
            # MaxVio aggregation (Noneì´ ì•„ë‹Œ ê°’ë§Œ í¬í•¨)
            maxvio_values = [m['maxvio'] 
                            for m in metrics.values() 
                            if isinstance(m, dict) and not isinstance(m, str) and 'maxvio' in m 
                            and m['maxvio'] is not None]
            if maxvio_values:
                log_data['moe/avg_maxvio'] = np.mean(maxvio_values)
            
            # Routing variance aggregation (Noneì´ ì•„ë‹Œ ê°’ë§Œ í¬í•¨)
            routing_variance_values = [m['routing_variance'] 
                                      for m in metrics.values() 
                                      if isinstance(m, dict) and not isinstance(m, str) and 'routing_variance' in m
                                      and m['routing_variance'] is not None]
            if routing_variance_values:
                log_data['moe/avg_routing_variance'] = np.mean(routing_variance_values)
            
            # Top-k score gap aggregation (Noneì´ ì•„ë‹Œ ê°’ë§Œ í¬í•¨)
            topk_gap_values = [m['topk_score_gap'] 
                              for m in metrics.values() 
                              if isinstance(m, dict) and not isinstance(m, str) and 'topk_score_gap' in m
                              and m['topk_score_gap'] is not None]
            if topk_gap_values:
                log_data['moe/avg_topk_score_gap'] = np.mean(topk_gap_values)
            
            # G3MoE specific metrics (í‰ê·  ê³„ì‚°)
            speciality_loss_values = [m['speciality_loss'] 
                                     for m in metrics.values() 
                                     if isinstance(m, dict) and not isinstance(m, str) and 'speciality_loss' in m]
            cosine_similarities_values = [m['cosine_similarities'] 
                                         for m in metrics.values() 
                                         if isinstance(m, dict) and not isinstance(m, str) and 'cosine_similarities' in m]
            expression_loss_values = [m['expression_loss'] 
                                     for m in metrics.values() 
                                     if isinstance(m, dict) and not isinstance(m, str) and 'expression_loss' in m]
            
            if speciality_loss_values:
                log_data['moe/avg_speciality_loss'] = np.mean(speciality_loss_values)
            if cosine_similarities_values:
                log_data['moe/avg_cosine_similarities'] = np.mean(cosine_similarities_values)
            if expression_loss_values:
                log_data['moe/avg_expression_loss'] = np.mean(expression_loss_values)
            
            # Gram orthogonality aggregation (Noneì´ ì•„ë‹Œ ê°’ë§Œ í¬í•¨)
            gram_ortho_values = [m['gram_orthogonality'] 
                                for m in metrics.values() 
                                if isinstance(m, dict) and not isinstance(m, str) and 'gram_orthogonality' in m
                                and m['gram_orthogonality'] is not None]
            if gram_ortho_values:
                log_data['moe/avg_gram_orthogonality'] = np.mean(gram_ortho_values)
            
            # Layer-wise balance ë©”íŠ¸ë¦­ (ì‹¤ì œ ê²€ì¦ ì§€í‘œ) - moe ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
            if '_layer_wise_balance' in metrics:
                balance_metrics = metrics['_layer_wise_balance']
                # ì‹¤ì œë¡œ ê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ ë¡œê¹… (0ìœ¼ë¡œ fallbackí•˜ì§€ ì•ŠìŒ)
                if 'layer_utilization_cv' in balance_metrics:
                    log_data['moe/validation/layer_utilization_cv'] = balance_metrics['layer_utilization_cv']
                if 'layer_utilization_mean' in balance_metrics:
                    log_data['moe/validation/layer_utilization_mean'] = balance_metrics['layer_utilization_mean']
                if 'layer_entropy_mean' in balance_metrics:
                    log_data['moe/validation/layer_entropy_mean'] = balance_metrics['layer_entropy_mean']
                if 'early_late_utilization_ratio' in balance_metrics:
                    log_data['moe/validation/early_late_ratio'] = balance_metrics['early_late_utilization_ratio']
        
        # Paper ë²¤ì¹˜ë§ˆí¬ ë©”íŠ¸ë¦­ ì¶”ê°€ (GramSpecAnalyzerê°€ ìˆëŠ” ê²½ìš°) - moe ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
        if self.gramspec_analyzer is not None:
            try:
                paper_metrics = self.gramspec_analyzer.get_paper_metrics_summary()
                if paper_metrics:
                    # Load balancing metrics
                    if 'load_balancing' in paper_metrics:
                        lb = paper_metrics['load_balancing']
                        if 'coefficient_of_variation' in lb and lb['coefficient_of_variation'] is not None:
                            log_data['moe/paper/load_balancing/cv'] = lb['coefficient_of_variation']
                        if 'load_imbalance_ratio' in lb and lb['load_imbalance_ratio'] is not None:
                            log_data['moe/paper/load_balancing/imbalance_ratio'] = lb['load_imbalance_ratio']
                        if 'expert_utilization_rate' in lb and lb['expert_utilization_rate'] is not None:
                            log_data['moe/paper/load_balancing/utilization_rate'] = lb['expert_utilization_rate']
                    
                    # Expert specialization metrics
                    if 'expert_specialization' in paper_metrics:
                        es = paper_metrics['expert_specialization']
                        if 'expert_diversity_score' in es and es['expert_diversity_score'] is not None:
                            log_data['moe/paper/expert_specialization/diversity_score'] = es['expert_diversity_score']
                        if 'expert_similarity_mean' in es and es['expert_similarity_mean'] is not None:
                            log_data['moe/paper/expert_specialization/similarity_mean'] = es['expert_similarity_mean']
                        if 'expert_specialization_strength' in es and es['expert_specialization_strength'] is not None:
                            log_data['moe/paper/expert_specialization/specialization_strength'] = es['expert_specialization_strength']
                    
                    # Gram matrix quality
                    if 'gram_matrix_quality' in paper_metrics:
                        gm = paper_metrics['gram_matrix_quality']
                        if 'orthogonality' in gm and gm['orthogonality'] is not None:
                            log_data['moe/paper/gram_matrix/orthogonality'] = gm['orthogonality']
                        if 'orthogonality_std' in gm and gm['orthogonality_std'] is not None:
                            log_data['moe/paper/gram_matrix/orthogonality_std'] = gm['orthogonality_std']
                    
                    # Routing quality
                    if 'routing_quality' in paper_metrics:
                        rq = paper_metrics['routing_quality']
                        if 'routing_confidence' in rq and rq['routing_confidence'] is not None:
                            log_data['moe/paper/routing/confidence'] = rq['routing_confidence']
                        if 'cosine_similarity_mean' in rq and rq['cosine_similarity_mean'] is not None:
                            log_data['moe/paper/routing/cosine_similarity_mean'] = rq['cosine_similarity_mean']
            except Exception as e:
                self._log_debug(f"Warning: Failed to get paper metrics: {e}")
        
        # Vision ëª¨ë“ˆ ì‚¬ìš© í†µê³„ ì¶”ê°€ (stepë³„ ì¦ê°€ëŸ‰ ê³„ì‚°) - vision ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
        if hasattr(self, 'prev_vision_stats'):
            step_vision_tower_calls = self.vision_usage_stats['vision_tower_calls'] - self.prev_vision_stats.get('vision_tower_calls', 0)
            step_projector_calls = self.vision_usage_stats['projector_calls'] - self.prev_vision_stats.get('projector_calls', 0)
            step_pixel_values = self.vision_usage_stats['pixel_values_received'] - self.prev_vision_stats.get('pixel_values_received', 0)
            step_image_features = self.vision_usage_stats['image_features_generated'] - self.prev_vision_stats.get('image_features_generated', 0)
            
            # Vision ì‚¬ìš© í†µê³„ëŠ” í•­ìƒ ë¡œê¹… (0ì´ì–´ë„) - vision ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
            log_data['multi_modality/vision_tower_calls_per_step'] = step_vision_tower_calls
            log_data['multi_modality/projector_calls_per_step'] = step_projector_calls
            log_data['multi_modality/pixel_values_per_step'] = step_pixel_values
            log_data['multi_modality/image_features_per_step'] = step_image_features
            
            # ëˆ„ì  í†µê³„ë„ í•¨ê»˜ ë¡œê¹… - vision ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
            log_data['multi_modality/vision_tower_calls_total'] = self.vision_usage_stats['vision_tower_calls']
            log_data['multi_modality/projector_calls_total'] = self.vision_usage_stats['projector_calls']
            log_data['multi_modality/pixel_values_total'] = self.vision_usage_stats['pixel_values_received']
            log_data['multi_modality/image_features_total'] = self.vision_usage_stats['image_features_generated']
            
            # Vision ì‚¬ìš©ë¥  (ì´ë¯¸ì§€ê°€ ìˆëŠ” ë°°ì¹˜ ë¹„ìœ¨) - vision ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
            log_data['multi_modality/vision_usage_rate'] = 1.0 if step_vision_tower_calls > 0 else 0.0
            
            # Vision tower ì¶œë ¥ í†µê³„ - vision ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
            if self.vision_tower_outputs:
                recent_outputs = self.vision_tower_outputs[-10:]  # ìµœê·¼ 10ê°œ
                log_data['multi_modality/tower_output_mean'] = np.mean([o['mean'] for o in recent_outputs])
                log_data['multi_modality/tower_output_std'] = np.mean([o['std'] for o in recent_outputs])
                log_data['multi_modality/tower_output_min'] = np.min([o['min'] for o in recent_outputs])
                log_data['multi_modality/tower_output_max'] = np.max([o['max'] for o in recent_outputs])
            
            # Projector ì¶œë ¥ í†µê³„ - vision ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
            if self.projector_outputs:
                recent_outputs = self.projector_outputs[-10:]  # ìµœê·¼ 10ê°œ
                log_data['multi_modality/projector_output_mean'] = np.mean([o['mean'] for o in recent_outputs])
                log_data['multi_modality/projector_output_std'] = np.mean([o['std'] for o in recent_outputs])
                log_data['multi_modality/projector_output_min'] = np.min([o['min'] for o in recent_outputs])
                log_data['multi_modality/projector_output_max'] = np.max([o['max'] for o in recent_outputs])
            
            # Routerì˜ requires_grad ìƒíƒœ ì²´í¬ (MoE upcyclingì˜ í•µì‹¬) - router ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
            if self.model is not None:
                router_count = 0
                router_trainable_count = 0
                router_total_params = 0
                router_trainable_params = 0
                
                try:
                    # G3MoERouter ì°¾ê¸°
                    from models.g3moe_model import G3MoERouter
                    for name, module in self.model.named_modules():
                        if (getattr(module, "_is_g3moe_router", False) or 
                            isinstance(module, G3MoERouter)):
                            router_count += 1
                            router_params = list(module.parameters(recurse=True))
                            if router_params:
                                router_total_params += len(router_params)
                                router_trainable = sum(1 for p in router_params if p.requires_grad)
                                router_trainable_params += router_trainable
                                if router_trainable > 0:
                                    router_trainable_count += 1
                    
                    # GramSpecRouter ì°¾ê¸°
                    try:
                        from models.gramspec_moe import GramSpecRouter
                        for name, module in self.model.named_modules():
                            if isinstance(module, GramSpecRouter):
                                router_count += 1
                                router_params = list(module.parameters(recurse=True))
                                if router_params:
                                    router_total_params += len(router_params)
                                    router_trainable = sum(1 for p in router_params if p.requires_grad)
                                    router_trainable_params += router_trainable
                                    if router_trainable > 0:
                                        router_trainable_count += 1
                    except ImportError:
                        pass
                    
                    # Router í†µê³„ ë¡œê¹… - router ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
                    if router_count > 0:
                        log_data['train/router/total_routers'] = router_count
                        log_data['train/router/trainable_routers'] = router_trainable_count
                        log_data['train/router/requires_grad'] = 1.0 if router_trainable_count > 0 else 0.0
                        log_data['train/router/trainable_params'] = router_trainable_params
                        log_data['train/router/total_params'] = router_total_params
                        log_data['train/router/trainable_ratio'] = router_trainable_params / max(router_total_params, 1)
                        log_data['train/router/trainable_router_ratio'] = router_trainable_count / max(router_count, 1)
                except Exception as e:
                    process_info = _get_process_info()
                    import traceback
                    error_msg = (
                        f"[MoE Callback] âŒ ERROR in _log_metrics (router requires_grad check):\n"
                        f"  Process: rank={process_info['rank']}, RANK={process_info['RANK']}\n"
                        f"  Step: {current_step}\n"
                        f"  Method: _log_metrics\n"
                        f"  Error: {type(e).__name__}: {str(e)}\n"
                        f"  Traceback:\n{traceback.format_exc()}"
                    )
                    self._log_debug(error_msg)
        
        # log_dataê°€ ë¹„ì–´ìˆì–´ë„ ìµœì†Œí•œì˜ ë””ë²„ê·¸ ì •ë³´ëŠ” ë¡œê¹… - moe ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¦¬
        if not log_data:
            log_data['moe/no_metrics'] = 1.0
            log_data['moe/layer_outputs_count'] = len(self.layer_outputs)
            log_data['moe/hooks_count'] = len(self.hooks)
            log_data['moe/vision_hooks_count'] = len(self.vision_hooks)
            log_data['moe/metrics_empty'] = 1.0 if not metrics else 0.0
        
        # log_dataë¥¼ ì €ì¥í•˜ì—¬ Trainerì˜ logsì— ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ í•¨
        self.last_log_data = log_data
        
        # ë””ë²„ê·¸: log_data ìƒì„± í™•ì¸ (ì´ˆê¸° stepì—ì„œë§Œ)
        if self.log_to_console and current_step <= 5:
            self._log_debug(f"âœ… _log_metrics at step {current_step}: created {len(log_data)} metrics")
            if log_data:
                sample_keys = list(log_data.keys())[:5]
                self._log_debug(f"   Sample keys: {sample_keys}")
            else:
                self._log_debug(f"   âš ï¸ log_data is empty! metrics dict: {list(metrics.keys()) if metrics else 'empty'}")

        # ì½˜ì†” ì¶œë ¥ (log_to_console=Trueì¼ ë•Œë§Œ)
        if self.log_to_console:
            self._log_debug(f"Step {current_step} MoE Metrics ({len(log_data)} metrics):")
            for key, value in log_data.items():
                # train/ prefix ì œê±°í•˜ì—¬ ì½˜ì†”ì— ì¶œë ¥
                display_key = key.replace('train/', '') if key.startswith('train/') else key
                if 'avg_' in display_key or 'total_' in display_key or 'paper/' in display_key or 'router/' in display_key or 'vision/' in display_key or 'moe/' in display_key:
                    if value is not None and isinstance(value, (int, float)):
                        self._log_debug(f"  {display_key}: {value:.4f}")
                    else:
                        self._log_debug(f"  {display_key}: {value}")
    
    def _generate_heatmaps(self, current_step: int):
        """Expert ì‚¬ìš©ë¥  íˆíŠ¸ë§µ ë°ì´í„° ìƒì„±"""
        # ì´ë¯¸ on_step_endì—ì„œ rank ì²´í¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ
        
        if not self.expert_usage_history:
            if self.debug_logging:
                self._log_debug(f"No expert usage history available for heatmap at step {current_step}")
            return
        
        try:
            import matplotlib
            matplotlib.use('Agg')  # Non-interactive backend
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            heatmap_created = False
            
            for layer_name in self.expert_usage_history:
                history = self.expert_usage_history[layer_name]
                # ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„° í•„ìš” (10ê°œì—ì„œ ì™„í™”)
                if len(history) < 2:
                    if self.log_to_console:
                        self._log_debug(f"Insufficient data for {layer_name} heatmap (need at least 2 steps, got {len(history)})")
                    continue
                
                try:
                    # ëª¨ë“  í…ì„œë¥¼ ë™ì¼í•œ í¬ê¸°ë¡œ ë§ì¶”ê¸° ìœ„í•´ ìµœëŒ€ í¬ê¸°ë¡œ íŒ¨ë”©
                    usage_tensors = list(history)
                    if not usage_tensors:
                        continue
                    
                    # ë¹ˆ í…ì„œ í•„í„°ë§
                    valid_tensors = [t for t in usage_tensors if t.numel() > 0]
                    if not valid_tensors:
                        continue
                    
                    max_size = max(tensor.size(0) for tensor in valid_tensors)
                    if max_size == 0:
                        continue
                    
                    # ê° í…ì„œë¥¼ ìµœëŒ€ í¬ê¸°ë¡œ íŒ¨ë”©
                    padded_tensors = []
                    for tensor in valid_tensors:
                        if tensor.size(0) < max_size:
                            padding = torch.zeros(max_size - tensor.size(0), dtype=tensor.dtype)
                            padded_tensor = torch.cat([tensor, padding])
                        else:
                            padded_tensor = tensor
                        padded_tensors.append(padded_tensor)
                    
                    if not padded_tensors:
                        continue
                    
                    usage_matrix = torch.stack(padded_tensors)
                    usage_matrix = usage_matrix.float()
                    
                    # ì •ê·œí™”
                    row_sums = usage_matrix.sum(dim=1, keepdim=True)
                    usage_matrix = usage_matrix / (row_sums + 1e-8)
                    
                    # íˆíŠ¸ë§µ ìƒì„±
                    plt.figure(figsize=(12, 6))
                    sns.heatmap(usage_matrix.T.numpy(), 
                                cmap='YlOrRd', 
                                xticklabels=False,
                                yticklabels=True,
                                cbar_kws={'label': 'Usage Ratio'})
                    plt.title(f'{layer_name} Expert Usage Distribution (Step {current_step})')
                    plt.xlabel('Time Steps')
                    plt.ylabel('Expert Index')
                    plt.tight_layout()
                    
                    # Heatmap ë°ì´í„°ë¥¼ pendingì— ì €ì¥ (on_logì—ì„œ ë¡œê¹…)
                    try:
                        import wandb
                        if current_step not in self.pending_heatmaps:
                            self.pending_heatmaps[current_step] = {}
                        self.pending_heatmaps[current_step][layer_name] = wandb.Image(plt)
                        if self.log_to_console:
                            self._log_debug(f"âœ… Generated heatmap for {layer_name} at step {current_step}")
                    except ImportError:
                        if self.log_to_console:
                            self._log_debug(f"âš ï¸ wandb not available for heatmap generation")
                    except Exception as e:
                        process_info = _get_process_info()
                        import traceback
                        error_msg = (
                            f"[MoE Callback] âŒ ERROR in _generate_heatmaps:\n"
                            f"  Process: rank={process_info['rank']}, RANK={process_info['RANK']}\n"
                            f"  Step: {current_step}\n"
                            f"  Layer: {layer_name}\n"
                            f"  Method: _generate_heatmaps\n"
                            f"  Error: {type(e).__name__}: {str(e)}\n"
                            f"  Traceback:\n{traceback.format_exc()}"
                        )
                        # ì—ëŸ¬ëŠ” í•­ìƒ ì¶œë ¥
                        print(error_msg)

                    
                    # íŒŒì¼ë¡œ ì €ì¥
                    if self.save_detailed_logs:
                        try:
                            safe_layer_name = layer_name.replace(".", "_").replace("/", "_")
                            heatmap_path = os.path.join(self.log_dir, f'{safe_layer_name}_heatmap_step_{current_step}.png')
                            plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')
                            if self.log_to_console:
                                self._log_debug(f"Heatmap saved to {heatmap_path}")
                        except Exception as e:
                            if self.log_to_console:
                                self._log_debug(f"Warning: Failed to save heatmap: {e}")
                    
                    plt.close()
                    heatmap_created = True
                    
                except Exception as e:
                    import traceback
                    if self.log_to_console:
                        self._log_debug(f"Error creating heatmap for {layer_name}: {e}\n{traceback.format_exc()}")
                    continue
            
            if not heatmap_created and self.log_to_console:
                self._log_debug(f"No heatmaps were created at step {current_step}")
                
        except ImportError as e:
            if self.log_to_console:
                self._log_debug(f"Warning: matplotlib/seaborn not available for heatmap logging: {e}")
        except Exception as e:
            import traceback
            if self.log_to_console:
                self._log_debug(f"Error during heatmap logging: {e}\n{traceback.format_exc()}")
    
    def _generate_tsne_visualizations(self, current_step: int):
        """Layerë³„ t-SNE ì‹œê°í™” ìƒì„± (expert clustering ì‹œê°í™”)"""
        if not self.tsne_data_buffer:
            if self.log_to_console:
                self._log_debug(f"No t-SNE data available at step {current_step}")
            return
        
        try:
            from sklearn.manifold import TSNE
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            
            tsne_created = False
            
            for layer_name, buffer in self.tsne_data_buffer.items():
                hidden_states_list = buffer['hidden_states']
                expert_assignments_list = buffer['expert_assignments']
                
                if not hidden_states_list or not expert_assignments_list:
                    continue
                
                # ìµœê·¼ ë°ì´í„°ë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
                recent_hidden = list(hidden_states_list)[-10:]  # ìµœê·¼ 10ê°œ step
                recent_experts = list(expert_assignments_list)[-10:]
                
                if not recent_hidden or not recent_experts:
                    continue
                
                try:
                    # ë°ì´í„° ê²°í•©
                    all_hidden = torch.cat(recent_hidden, dim=0).numpy()  # [num_tokens, hidden_dim]
                    all_experts = torch.cat(recent_experts, dim=0).numpy()  # [num_tokens]
                    
                    # ìƒ˜í”Œë§ (t-SNE ê³„ì‚° ë¹„ìš© ì ˆê°)
                    if len(all_hidden) > self.tsne_sample_size:
                        indices = np.random.choice(len(all_hidden), self.tsne_sample_size, replace=False)
                        sampled_hidden = all_hidden[indices]
                        sampled_experts = all_experts[indices]
                    else:
                        sampled_hidden = all_hidden
                        sampled_experts = all_experts
                    
                    if len(sampled_hidden) < 10:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
                        if self.log_to_console:
                            self._log_debug(f"Insufficient samples for {layer_name} t-SNE: {len(sampled_hidden)}")
                        continue
                    
                    # t-SNE ê³„ì‚°
                    if self.log_to_console:
                        self._log_debug(f"Computing t-SNE for {layer_name} with {len(sampled_hidden)} samples...")
                    
                    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(sampled_hidden) - 1))
                    embeddings_2d = tsne.fit_transform(sampled_hidden)
                    
                    # ì‹œê°í™”
                    num_experts = int(sampled_experts.max() + 1) if len(sampled_experts) > 0 else self.num_experts
                    fig, ax = plt.subplots(1, 1, figsize=(12, 10))
                    
                    # Expertë³„ë¡œ ìƒ‰ìƒ êµ¬ë¶„
                    colors = plt.cm.tab20(np.linspace(0, 1, num_experts))
                    for expert_id in range(num_experts):
                        mask = sampled_experts == expert_id
                        if mask.sum() > 0:
                            ax.scatter(
                                embeddings_2d[mask, 0],
                                embeddings_2d[mask, 1],
                                label=f'Expert {expert_id}',
                                alpha=0.6,
                                s=20,
                                c=[colors[expert_id % len(colors)]]
                            )
                    
                    ax.set_title(f'{layer_name} Token Clustering by Expert (t-SNE)\nStep {current_step}', fontsize=14)
                    ax.set_xlabel('t-SNE Dimension 1', fontsize=12)
                    ax.set_ylabel('t-SNE Dimension 2', fontsize=12)
                    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
                    ax.grid(alpha=0.3)
                    plt.tight_layout()
                    
                    # wandbì— ë¡œê¹…
                    try:
                        import wandb
                        if wandb.run is not None:
                            if current_step not in self.pending_heatmaps:
                                self.pending_heatmaps[current_step] = {}
                            self.pending_heatmaps[current_step][f'{layer_name}_tsne'] = wandb.Image(plt)
                            if self.log_to_console:
                                self._log_debug(f"âœ… Generated t-SNE visualization for {layer_name} at step {current_step}")
                    except ImportError:
                        if self.log_to_console:
                            self._log_debug(f"âš ï¸ wandb not available for t-SNE visualization")
                    except Exception as e:
                        if self.log_to_console:
                            self._log_debug(f"Warning: Failed to log t-SNE to wandb: {e}")
                    
                    # íŒŒì¼ë¡œ ì €ì¥
                    if self.save_detailed_logs:
                        try:
                            safe_layer_name = layer_name.replace(".", "_").replace("/", "_")
                            tsne_path = os.path.join(self.log_dir, f'{safe_layer_name}_tsne_step_{current_step}.png')
                            plt.savefig(tsne_path, dpi=150, bbox_inches='tight')
                            if self.log_to_console:
                                self._log_debug(f"t-SNE visualization saved to {tsne_path}")
                        except Exception as e:
                            if self.log_to_console:
                                self._log_debug(f"Warning: Failed to save t-SNE visualization: {e}")
                    
                    plt.close()
                    tsne_created = True
                    
                except Exception as e:
                    import traceback
                    if self.log_to_console:
                        self._log_debug(f"Error creating t-SNE for {layer_name}: {e}\n{traceback.format_exc()}")
                    continue
            
            if not tsne_created and self.log_to_console:
                self._log_debug(f"No t-SNE visualizations were created at step {current_step}")
                
        except ImportError as e:
            if self.log_to_console:
                self._log_debug(f"Warning: sklearn/matplotlib not available for t-SNE visualization: {e}")
        except Exception as e:
            import traceback
            if self.log_to_console:
                self._log_debug(f"Error during t-SNE visualization: {e}\n{traceback.format_exc()}")
    
    def _check_alerts(self, metrics):
        """ê²½ê³  ìƒí™© ì²´í¬"""
        alerts = []
        
        for layer_name, layer_metrics in metrics.items():
            # ì‹¬ê°í•œ ë¶ˆê· í˜•
            if 'max_usage_ratio' in layer_metrics:
                ratio = layer_metrics['max_usage_ratio']
                if isinstance(ratio, torch.Tensor):
                    ratio = ratio.item()
                if ratio > self.alert_threshold_imbalance:
                    alerts.append({
                        'type': 'severe_imbalance',
                        'layer': layer_name,
                        'severity': ratio,
                        'message': f'{layer_name}: Severe expert imbalance (ratio: {ratio:.2f})'
                    })
            
            # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” experts
            if 'unused_experts' in layer_metrics and 'usage_counts' in layer_metrics:
                unused = layer_metrics['unused_experts']
                usage_counts = layer_metrics['usage_counts']
                if torch.is_tensor(usage_counts):
                    total_experts = usage_counts.numel()
                else:
                    total_experts = len(usage_counts) if hasattr(usage_counts, '__len__') else self.num_experts
                
                if total_experts > 0 and unused / total_experts > self.unused_expert_threshold:
                    alerts.append({
                        'type': 'unused_experts',
                        'layer': layer_name,
                        'unused_count': unused,
                        'total_experts': total_experts,
                        'message': f'{layer_name}: {unused}/{total_experts} experts unused'
                    })
            
            # ë‚®ì€ ë¼ìš°íŒ… ì—”íŠ¸ë¡œí”¼
            if 'routing_entropy' in layer_metrics:
                entropy = layer_metrics['routing_entropy']
                if isinstance(entropy, torch.Tensor):
                    entropy = entropy.item()
                if entropy < self.entropy_threshold:
                    alerts.append({
                        'type': 'low_entropy',
                        'layer': layer_name,
                        'entropy': entropy,
                        'message': f'{layer_name}: Low routing entropy ({entropy:.4f})'
                    })
        
        return alerts
    
    def _handle_alerts(self, alerts, current_step: int):
        """ê²½ê³  ì²˜ë¦¬"""
        # ì´ë¯¸ on_step_endì—ì„œ rank ì²´í¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ
            
        for alert in alerts:
            self.alerts_history.append({
                'step': current_step,
                'timestamp': time.time(),
                **alert
            })
            
            # ê²½ê³  ë©”ì‹œì§€ëŠ” log_to_consoleì¼ ë•Œë§Œ ì¶œë ¥
            if self.log_to_console:
                self._log_debug(f"âš ï¸  MoE Alert at step {current_step}: {alert['message']}")
            
            # Alert ë°ì´í„°ë¥¼ pendingì— ì €ì¥ (on_logì—ì„œ ë¡œê¹…)
            if current_step not in self.pending_alerts:
                self.pending_alerts[current_step] = []
            self.pending_alerts[current_step].append({
                'type': alert["type"],
                'layer': alert["layer"],
                'severity': alert.get('severity', 1)
            })
    
    def _save_detailed_log(self, metrics, current_step: int):
        """ìƒì„¸ ë¡œê·¸ ì €ì¥"""
        log_entry = {
            'step': current_step,
            'timestamp': time.time(),
            'metrics': {
                layer: {k: v.tolist() if torch.is_tensor(v) else v 
                       for k, v in layer_metrics.items()}
                for layer, layer_metrics in metrics.items()
            }
        }
        
        with open(f'{self.log_dir}/detailed_log_step_{current_step}.json', 'w') as f:
            json.dump(log_entry, f, indent=2)
    
    def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        
        # Vision hooks ì •ë¦¬
        for hook in self.vision_hooks:
            hook.remove()
        self.vision_hooks.clear()
    
    def get_summary(self):
        """ì „ì²´ í›ˆë ¨ì— ëŒ€í•œ ìš”ì•½ í†µê³„"""
        summary = {
            'total_alerts': len(self.alerts_history),
            'alert_types': {}
        }
        
        # ê²½ê³  ìœ í˜•ë³„ ì§‘ê³„
        for alert in self.alerts_history:
            alert_type = alert['type']
            if alert_type not in summary['alert_types']:
                summary['alert_types'][alert_type] = 0
            summary['alert_types'][alert_type] += 1
        
        return summary

from transformers.trainer_callback import TrainerCallback, TrainerControl, TrainerState
from transformers.training_args import TrainingArguments
from typing import Dict, Any

class TransformersMoECallbackWrapper(TrainerCallback):
    """Transformers TrainerCallback wrapper for TorchMoECallback"""
    
    def __init__(self, torch_callback: TorchMoECallback):
        self.torch_callback = torch_callback
        self._model_registered = False
    
    def on_train_begin(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        tokenizer=None,
        **kwargs
    ):
        """í›ˆë ¨ ì‹œì‘ ì‹œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë“±ë¡ ë° VLM í…ŒìŠ¤íŠ¸"""
        # âœ… tokenizerê°€ Noneì´ë©´ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
        if tokenizer is None:
            # ë°©ë²• 1: kwargsì—ì„œ trainer ê°€ì ¸ì˜¤ê¸°
            trainer = kwargs.get('trainer')
            if trainer is not None:
                # SFTTrainerëŠ” processing_classë¥¼ ì‚¬ìš©
                if hasattr(trainer, 'processing_class') and trainer.processing_class is not None:
                    tokenizer = trainer.processing_class
                    self.torch_callback._log_debug("âœ… Retrieved tokenizer from trainer.processing_class")
                # ì¼ë°˜ TrainerëŠ” tokenizer ì†ì„± ì‚¬ìš©
                elif hasattr(trainer, 'tokenizer') and trainer.tokenizer is not None:
                    tokenizer = trainer.tokenizer
                    self.torch_callback._log_debug("âœ… Retrieved tokenizer from trainer.tokenizer")
            
            # ë°©ë²• 2: kwargsì—ì„œ ì§ì ‘ tokenizer ì°¾ê¸°
            if tokenizer is None:
                tokenizer = kwargs.get('tokenizer') or kwargs.get('processing_class')
                if tokenizer is not None:
                    self.torch_callback._log_debug("âœ… Retrieved tokenizer from kwargs")
            
            # ë°©ë²• 3: torch_callbackì— ì´ë¯¸ ì €ì¥ëœ tokenizer ì‚¬ìš©
            if tokenizer is None and hasattr(self.torch_callback, 'tokenizer') and self.torch_callback.tokenizer is not None:
                tokenizer = self.torch_callback.tokenizer
                self.torch_callback._log_debug("âœ… Using tokenizer from torch_callback.tokenizer")
            
            if tokenizer is None:
                self.torch_callback._log_debug("âš ï¸ Could not retrieve tokenizer from any source")
                self.torch_callback._log_debug(f"   - kwargs keys: {list(kwargs.keys())}")
                if trainer is not None:
                    self.torch_callback._log_debug(f"   - trainer has processing_class: {hasattr(trainer, 'processing_class')}")
                    if hasattr(trainer, 'processing_class'):
                        self.torch_callback._log_debug(f"   - trainer.processing_class: {trainer.processing_class}")
                    self.torch_callback._log_debug(f"   - trainer has tokenizer: {hasattr(trainer, 'tokenizer')}")
                    if hasattr(trainer, 'tokenizer'):
                        self.torch_callback._log_debug(f"   - trainer.tokenizer: {trainer.tokenizer}")
        
        if model is not None and not self._model_registered:
            # DeepSpeed ë˜í•‘ëœ ëª¨ë¸ ì²˜ë¦¬
            actual_model = model
            if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
                actual_model = model.module
                self.torch_callback._log_debug("âš ï¸ Detected DeepSpeed wrapped model, using model.module")
            
            self.torch_callback.register_model(actual_model, tokenizer)
            self._model_registered = True
            self.torch_callback._log_debug(f"âœ… MoE monitoring registered for model with {len(self.torch_callback.hooks)} MoE layers")
            
            # ë””ë²„ê·¸: ë“±ë¡ëœ MoE ë ˆì´ì–´ ì´ë¦„ ì¶œë ¥
            if self.torch_callback.hooks:
                moe_layer_names = []
                for name, module in actual_model.named_modules():
                    if self.torch_callback._is_moe_layer(module):
                        moe_layer_names.append(name)
                self.torch_callback._log_debug(f"ğŸ“‹ Registered MoE layers: {moe_layer_names[:5]}..." if len(moe_layer_names) > 5 else f"ğŸ“‹ Registered MoE layers: {moe_layer_names}")
            else:
                self.torch_callback._log_debug("âŒ WARNING: No MoE layers detected! Check model structure.")

            if self.torch_callback.enable_generation_logging:
                if tokenizer is not None:
                    self.torch_callback._log_debug("Generation logging enabled with tokenizer")
                else:
                    self.torch_callback._log_debug("Warning: Generation logging enabled but no tokenizer provided")
        
        # wandb.runì´ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ loggerë¥¼ wandb.runìœ¼ë¡œ ì„¤ì •
        # ì£¼ì˜: on_train_begin ì‹œì ì—ëŠ” wandb.runì´ ì•„ì§ Noneì¼ ìˆ˜ ìˆìŒ (Trainerê°€ ë‚˜ì¤‘ì— ì´ˆê¸°í™”)
        # ì‹¤ì œ ë¡œê¹…ì€ on_step_endì—ì„œ Trainerì˜ logsë¥¼ í†µí•´ ì´ë£¨ì–´ì§€ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê²½ê³ ë§Œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        try:
            import wandb
            # loggerê°€ wandb ëª¨ë“ˆ ìì²´ì´ê±°ë‚˜ Noneì¸ ê²½ìš° wandb.runìœ¼ë¡œ ì„¤ì •
            if (self.torch_callback.logger is None or 
                self.torch_callback.logger == wandb or
                (hasattr(self.torch_callback.logger, '__name__') and self.torch_callback.logger.__name__ == 'wandb')):
                if wandb.run is not None:
                    self.torch_callback.logger = wandb.run
                    self.torch_callback._log_debug("âœ… Set MoE callback logger to wandb.run")
                # wandb.runì´ Noneì´ì–´ë„ ë¬¸ì œì—†ìŒ (Trainerì˜ logsë¥¼ í†µí•´ ë¡œê¹…ë¨)
        except ImportError:
            pass
        except Exception as e:
            self.torch_callback._log_debug(f"âš ï¸ Error setting wandb logger: {e}")
        
        # VLM í…ŒìŠ¤íŠ¸: ë©€í‹°ëª¨ë‹¬ê³¼ í…ìŠ¤íŠ¸ ì „ìš© ì¼€ì´ìŠ¤ ëª¨ë‘ í…ŒìŠ¤íŠ¸
        # tokenizerì™€ modelì´ ëª¨ë‘ ìˆì„ ë•Œë§Œ í…ŒìŠ¤íŠ¸
        actual_model = model
        if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
            actual_model = model.module
        
        # if tokenizer is not None and actual_model is not None:
        #     self.torch_callback._test_vlm_capabilities(actual_model, tokenizer)
        else:
            if tokenizer is None:
                self.torch_callback._log_debug("âš ï¸ VLM test skipped: tokenizer is None")
            if actual_model is None:
                self.torch_callback._log_debug("âš ï¸ VLM test skipped: model is None")
    
    def on_step_begin(
        self, 
        args: TrainingArguments, 
        state: TrainerState, 
        control: TrainerControl, 
        **kwargs
    ):
        """Step ì‹œì‘"""
        self.torch_callback.on_step_begin()
    
    def on_step_end(
        self, 
        args: TrainingArguments, 
        state: TrainerState, 
        control: TrainerControl, 
        logs: Optional[Dict[str, float]] = None,
        **kwargs
    ):
        """Step ì¢…ë£Œ"""
        # PyTorch callback í˜¸ì¶œ - Transformersì˜ global_step ì‚¬ìš©
        self.torch_callback.on_step_end(current_step=state.global_step)

        try:
            import deepspeed.accelerator as ds_acc
            ds_acc.get_accelerator().empty_cache()
        except Exception:
            pass
    
    def on_log(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        logs: Optional[Dict[str, float]] = None,
        **kwargs
    ):
        """Trainerê°€ ë¡œê¹…í•  ë•Œ í˜¸ì¶œ - MoE ë©”íŠ¸ë¦­ì„ logsì— ì¶”ê°€í•˜ì—¬ Trainerì˜ WandbCallbackì´ ë¡œê¹…"""
        # logsê°€ Noneì´ë©´ ë¹ˆ dictë¡œ ì´ˆê¸°í™”
        if logs is None:
            logs = {}

        # global_stepì„ logsì— ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€ (wandbì—ì„œ step ì¶”ì ìš©)
        logs['train/global_step'] = float(state.global_step)
        
        # í•´ë‹¹ stepì˜ pending ë©”íŠ¸ë¦­ í™•ì¸
        # on_logëŠ” logging_stepsë§ˆë‹¤ í˜¸ì¶œë˜ë¯€ë¡œ, ìµœê·¼ stepì˜ ë©”íŠ¸ë¦­ì„ ì°¾ì•„ì•¼ í•¨
        current_metrics = None
        target_step = state.global_step
        
        # ì •í™•í•œ stepì˜ ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´, ìµœê·¼ stepì˜ ë©”íŠ¸ë¦­ì„ ì°¾ìŒ
        if target_step not in self.torch_callback.pending_metrics:
            # ìµœê·¼ stepì˜ ë©”íŠ¸ë¦­ ì°¾ê¸° (í˜„ì¬ step ì´í•˜ì˜ ê°€ì¥ ê°€ê¹Œìš´ step)
            available_steps = [s for s in self.torch_callback.pending_metrics.keys() if s <= target_step]
            if available_steps:
                target_step = max(available_steps)
                current_metrics = self.torch_callback.pending_metrics.get(target_step)
                if self.torch_callback.log_to_console and state.global_step % 10 == 0:
                    self.torch_callback._log_debug(f"ğŸ“Š Using metrics from step {target_step} for on_log at step {state.global_step}")
            else:
                # ì•„ì§ pending ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´, on_step_endì—ì„œ ìƒì„±ëœ ë©”íŠ¸ë¦­ì„ ì§ì ‘ ì‚¬ìš©
                if hasattr(self.torch_callback, 'last_log_data') and self.torch_callback.last_log_data:
                    current_metrics = self.torch_callback.last_log_data.copy()
                    if self.torch_callback.log_to_console and state.global_step % 10 == 0:
                        self.torch_callback._log_debug(f"ğŸ“Š Using last_log_data for step {state.global_step}")
        else:
            current_metrics = self.torch_callback.pending_metrics.get(target_step)
        
        if current_metrics:
            # âœ… logsì— ì¶”ê°€ (Trainerì˜ ë‹¤ë¥¸ ë¡œê¹…ê³¼ í•¨ê»˜)
            logs.update(current_metrics)
            
            # âœ… MoE ë©”íŠ¸ë¦­ì„ wandbì— ì§ì ‘ ë¡œê¹… (Trainerì˜ WandbCallbackì´ ì¼ë¶€ë§Œ ë¡œê¹…í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
            # stepì„ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ wandbê°€ ìë™ìœ¼ë¡œ Trainerì˜ stepì„ ì‚¬ìš© (ì¶©ëŒ ì—†ìŒ)
            try:
                import wandb
                if wandb.run is not None and _is_main_process():
                    # MoE ê´€ë ¨ ë©”íŠ¸ë¦­ë§Œ í•„í„°ë§
                    moe_metrics = {
                        k: v for k, v in current_metrics.items() 
                        if (k.startswith('moe/') or 
                            k.startswith('multi_modality/') or 
                            k.startswith('train/router/'))
                    }
                    
                    if moe_metrics:
                        # âœ… stepì„ ëª…ì‹œí•˜ì§€ ì•Šê³  ë¡œê¹… (wandbê°€ Trainerì˜ stepì„ ìë™ìœ¼ë¡œ ì‚¬ìš©)
                        # commit=Falseë¡œ ì„¤ì •í•˜ì—¬ Trainerì˜ ë¡œê¹…ê³¼ í•¨ê»˜ ì²˜ë¦¬
                        wandb.run.log(moe_metrics, commit=False)
                        
                        if self.torch_callback.log_to_console and state.global_step % 10 == 0:
                            self.torch_callback._log_debug(f"ğŸ“¤ on_log step {state.global_step}: directly logged {len(moe_metrics)} MoE metrics to wandb")
                            if state.global_step <= 5:
                                sample_keys = list(moe_metrics.keys())[:10]
                                self.torch_callback._log_debug(f"   Sample keys: {sample_keys}")
                    
                    # Heatmap/t-SNEëŠ” ë³„ë„ ë¡œê¹… (ì´ë¯¸ì§€ì´ë¯€ë¡œ)
                    if state.global_step in self.torch_callback.pending_heatmaps:
                        heatmap_data = self.torch_callback.pending_heatmaps[state.global_step]
                        for layer_name, image in heatmap_data.items():
                            if layer_name.endswith('_tsne'):
                                wandb.run.log({
                                    f'moe/{layer_name}/tsne_visualization': image
                                }, commit=False)
                            else:
                                wandb.run.log({
                                    f'moe/{layer_name}/usage_heatmap': image
                                }, commit=False)
                        del self.torch_callback.pending_heatmaps[state.global_step]
                    
                    # Pending alert ë¡œê¹…
                    if state.global_step in self.torch_callback.pending_alerts:
                        alert_data = self.torch_callback.pending_alerts[state.global_step]
                        for alert in alert_data:
                            wandb.run.log({
                                f'train/alerts/{alert["type"]}': 1,
                                f'train/alerts/{alert["layer"]}_severity': alert['severity']
                            }, commit=False)
                        del self.torch_callback.pending_alerts[state.global_step]
            except Exception as e:
                if self.torch_callback.log_to_console:
                    import traceback
                    self.torch_callback._log_debug(f"âš ï¸ Error logging MoE metrics to wandb in on_log: {e}")
                    if state.global_step % 50 == 0:
                        self.torch_callback._log_debug(f"   Traceback: {traceback.format_exc()}")
            
            # ë””ë²„ê·¸: current_metrics ë‚´ìš© í™•ì¸ (10 stepë§ˆë‹¤ë§Œ)
            if self.torch_callback.log_to_console and state.global_step % 10 == 0:
                moe_keys = [k for k in current_metrics.keys() if k.startswith('moe/') or k.startswith('multi_modality/') or k.startswith('train/router/')]
                self.torch_callback._log_debug(f"ğŸ“Š on_log step {state.global_step}: total {len(moe_keys)} MoE metrics available")
            
            # ë¡œê¹… í›„ pending ë©”íŠ¸ë¦­ ì œê±° (ë©”ëª¨ë¦¬ ì ˆì•½)
            try:
                steps_to_remove = [s for s in self.torch_callback.pending_metrics.keys() if s <= state.global_step]
                for step in steps_to_remove:
                    if step in self.torch_callback.pending_metrics:
                        del self.torch_callback.pending_metrics[step]
            except Exception as e:
                if self.torch_callback.log_to_console:
                    self.torch_callback._log_debug(f"âš ï¸ Error cleaning up pending metrics: {e}")
        else:
            # í•´ë‹¹ stepì˜ pending ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ê²½ê³  (log_to_consoleì¼ ë•Œë§Œ)
            if self.torch_callback.log_to_console:
                self.torch_callback._log_debug(f"âš ï¸ No pending metrics available at step {state.global_step}")
                self.torch_callback._log_debug(f"   - pending_metrics keys: {list(self.torch_callback.pending_metrics.keys())[:10]}")
                self.torch_callback._log_debug(f"   - last_log_data exists: {hasattr(self.torch_callback, 'last_log_data') and self.torch_callback.last_log_data is not None}")
                if hasattr(self.torch_callback, 'last_log_data') and self.torch_callback.last_log_data:
                    self.torch_callback._log_debug(f"   - last_log_data keys: {list(self.torch_callback.last_log_data.keys())[:10]}")
                self.torch_callback._log_debug(f"   - layer_outputs count: {len(self.torch_callback.layer_outputs)}")
            # ìµœì†Œí•œì˜ ë””ë²„ê·¸ ì •ë³´ë¼ë„ ì¶”ê°€
            logs['moe/callback_error'] = 1.0
            logs['moe/layer_outputs_count'] = len(self.torch_callback.layer_outputs) if hasattr(self.torch_callback, 'layer_outputs') else 0
            logs['moe/hooks_count'] = len(self.torch_callback.hooks) if hasattr(self.torch_callback, 'hooks') else 0
            logs['moe/pending_metrics_count'] = len(self.torch_callback.pending_metrics) if hasattr(self.torch_callback, 'pending_metrics') else 0
    
    def _run_benchmarks(
        self,
        model,
        tokenizer,
        eval_dataloader,
        state: TrainerState,
        args: TrainingArguments,
        **kwargs
    ):
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        if not _is_main_process():
            return
        
        benchmark_results = {}
        output_dir = getattr(self.torch_callback, 'log_dir', './moe_logs')
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. GramSpec MoE Analysis (ì´ë¯¸ ì‹¤í–‰ë¨, ê²°ê³¼ë§Œ ì •ë¦¬)
        if GRAMSPEC_ANALYSIS_AVAILABLE and self.torch_callback.gramspec_analyzer is not None:
            try:
                self.torch_callback._log_debug("Running GramSpec MoE Analysis benchmark...")
                analyzer = self.torch_callback.gramspec_analyzer
                aggregated = analyzer.get_aggregated_metrics()
                paper_summary = analyzer.get_paper_metrics_summary()
                
                benchmark_results['gramspec_moe_analysis'] = {
                    'aggregated_metrics': aggregated,
                    'paper_summary': paper_summary,
                }
                
                if self.torch_callback.log_to_console:
                    self.torch_callback._log_debug(f"  âœ“ GramSpec MoE Analysis completed")
            except Exception as e:
                self.torch_callback._log_debug(f"  âœ— GramSpec MoE Analysis failed: {e}")
                import traceback
                if self.torch_callback.debug_logging:
                    self.torch_callback._log_debug(traceback.format_exc())
        
        # 2. GramSpec Semantic Validation
        if GRAMSPEC_VALIDATION_AVAILABLE:
            try:
                self.torch_callback._log_debug("Running GramSpec Semantic Validation benchmark...")
                
                # Layer-wise balance ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
                if hasattr(self.torch_callback, 'layer_expert_usage_counts'):
                    layer_expert_usage = self.torch_callback.layer_expert_usage_counts
                    
                    # num_layers ì¶”ì •
                    num_layers = len(layer_expert_usage) if layer_expert_usage else 0
                    if num_layers == 0:
                        # ëª¨ë¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
                        num_layers = sum(1 for _ in model.named_modules() if 'layer' in str(_).lower() or 'block' in str(_).lower())
                    
                    if num_layers > 0 and self.torch_callback.gramspec_validator is None:
                        # Validator ì´ˆê¸°í™”
                        self.torch_callback.gramspec_validator = GramSpecSemanticValidator(
                            num_layers=num_layers,
                            num_experts=self.torch_callback.num_experts
                        )
                    
                    if self.torch_callback.gramspec_validator is not None:
                        layer_balance = self.torch_callback.gramspec_validator.analyze_layer_wise_balance(
                            layer_expert_usage_counts=layer_expert_usage
                        )
                        benchmark_results['gramspec_semantic_validation'] = {
                            'layer_wise_balance': layer_balance,
                        }
                        
                        if self.torch_callback.log_to_console:
                            self.torch_callback._log_debug(f"  âœ“ GramSpec Semantic Validation completed")
            except Exception as e:
                self.torch_callback._log_debug(f"  âœ— GramSpec Semantic Validation failed: {e}")
                import traceback
                if self.torch_callback.debug_logging:
                    self.torch_callback._log_debug(traceback.format_exc())
        
        # 3. Expert Specialization Analysis
        if EXPERT_SPECIALIZATION_AVAILABLE and eval_dataloader is not None:
            try:
                self.torch_callback._log_debug("Running Expert Specialization Analysis benchmark...")
                
                # ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘
                dataset_samples = []
                max_samples = getattr(args, 'max_eval_samples', 100)
                num_collected = 0
                
                model.eval()
                with torch.no_grad():
                    for batch in eval_dataloader:
                        if num_collected >= max_samples:
                            break
                        
                        # í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
                        if 'input_ids' in batch and tokenizer is not None:
                            input_ids = batch['input_ids']
                            for i in range(input_ids.shape[0]):
                                if num_collected >= max_samples:
                                    break
                                try:
                                    text = tokenizer.decode(input_ids[i], skip_special_tokens=True)
                                    if text.strip():
                                        dataset_samples.append(text)
                                        num_collected += 1
                                except:
                                    continue
                
                if dataset_samples:
                    # Expert activations ìˆ˜ì§‘
                    expert_activations = collect_expert_activations(
                        model=model,
                        tokenizer=tokenizer,
                        dataset=dataset_samples,
                        device=next(model.parameters()).device.type if next(model.parameters()).is_cuda else "cpu",
                        max_samples=min(len(dataset_samples), 100)  # ìµœëŒ€ 100ê°œ ìƒ˜í”Œ
                    )
                    
                    # Similarity ê³„ì‚°
                    similarity_matrix = compute_expert_similarity(expert_activations)
                    
                    benchmark_results['expert_specialization'] = {
                        'expert_activations_count': {str(k): len(v) for k, v in expert_activations.items()},
                        'similarity_matrix': similarity_matrix.tolist() if len(similarity_matrix) > 0 else [],
                    }
                    
                    if self.torch_callback.log_to_console:
                        self.torch_callback._log_debug(f"  âœ“ Expert Specialization Analysis completed ({len(dataset_samples)} samples)")
            except Exception as e:
                self.torch_callback._log_debug(f"  âœ— Expert Specialization Analysis failed: {e}")
                import traceback
                if self.torch_callback.debug_logging:
                    self.torch_callback._log_debug(traceback.format_exc())
        
        # 4. GramSpec Validation (Perplexity ë“±)
        if GRAMSPEC_VALIDATION_SCRIPT_AVAILABLE and eval_dataloader is not None:
            try:
                self.torch_callback._log_debug("Running GramSpec Validation benchmark...")
                
                # ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘
                eval_dataset = []
                max_samples = getattr(args, 'max_eval_samples', 100)
                num_collected = 0
                
                model.eval()
                with torch.no_grad():
                    for batch in eval_dataloader:
                        if num_collected >= max_samples:
                            break
                        
                        if 'input_ids' in batch and tokenizer is not None:
                            input_ids = batch['input_ids']
                            for i in range(input_ids.shape[0]):
                                if num_collected >= max_samples:
                                    break
                                try:
                                    text = tokenizer.decode(input_ids[i], skip_special_tokens=True)
                                    if text.strip():
                                        eval_dataset.append(text)
                                        num_collected += 1
                                except:
                                    continue
                
                if eval_dataset:
                    # Perplexity í‰ê°€
                    device = next(model.parameters()).device
                    device_str = device.type if device.is_cuda else "cpu"
                    try:
                        perplexity_results = evaluate_model_perplexity(
                            model=model,
                            tokenizer=tokenizer,
                            eval_dataset=eval_dataset,
                            device=device_str,
                            max_samples=len(eval_dataset)
                        )
                    except Exception as e:
                        self.torch_callback._log_debug(f"  âš ï¸ Perplexity evaluation error: {e}")
                        perplexity_results = {'perplexity': 0.0, 'loss': 0.0}
                    
                    benchmark_results['gramspec_validation'] = {
                        'perplexity': perplexity_results,
                    }
                    
                    # Trainerì— ë¡œê¹…
                    if 'trainer' in kwargs:
                        trainer = kwargs['trainer']
                        if hasattr(trainer, 'log'):
                            trainer.log({
                                'eval/benchmark/perplexity': perplexity_results.get('perplexity', 0.0),
                                'eval/benchmark/loss': perplexity_results.get('loss', 0.0),
                            })
                    
                    if self.torch_callback.log_to_console:
                        self.torch_callback._log_debug(f"  âœ“ GramSpec Validation completed (PPL: {perplexity_results.get('perplexity', 0.0):.4f})")
            except Exception as e:
                self.torch_callback._log_debug(f"  âœ— GramSpec Validation failed: {e}")
                import traceback
                if self.torch_callback.debug_logging:
                    self.torch_callback._log_debug(traceback.format_exc())
        
        # 5. Efficiency Measurement
        if EFFICIENCY_MEASUREMENT_AVAILABLE:
            try:
                self.torch_callback._log_debug("Running Efficiency Measurement benchmark...")
                
                device = next(model.parameters()).device.type if next(model.parameters()).is_cuda else "cpu"
                input_text = "The capital of France is"  # ê¸°ë³¸ ì…ë ¥ í…ìŠ¤íŠ¸
                
                # Forward throughput ì¸¡ì •
                forward_results = measure_forward_throughput(
                    model=model,
                    tokenizer=tokenizer,
                    input_text=input_text,
                    batch_sizes=[1, 4, 8],
                    seq_length=512,
                    num_runs=20,  # ë¹ ë¥¸ ì¸¡ì •ì„ ìœ„í•´ ì¤„ì„
                    warmup_runs=5,
                    device=device,
                )
                
                # Generation latency ì¸¡ì •
                generation_results = measure_generation_latency(
                    model=model,
                    tokenizer=tokenizer,
                    input_text=input_text,
                    max_new_tokens=32,
                    num_runs=20,
                    warmup_runs=5,
                    device=device,
                )
                
                # FLOPs ì¶”ì • (ì„ íƒì )
                flops_results = {}
                try:
                    flops_results = estimate_flops(
                        model=model,
                        input_shape=(1, 512),
                        device=device,
                    )
                except:
                    pass
                
                benchmark_results['efficiency'] = {
                    'forward_throughput': forward_results,
                    'generation_latency': generation_results,
                    'flops': flops_results,
                }
                
                # Trainerì— ë¡œê¹…
                if 'trainer' in kwargs:
                    trainer = kwargs['trainer']
                    if hasattr(trainer, 'log'):
                        # ì£¼ìš” ì§€í‘œë§Œ ë¡œê¹…
                        if 1 in forward_results:
                            trainer.log({
                                'eval/benchmark/tokens_per_sec': forward_results[1].get('tokens_per_sec', 0.0),
                                'eval/benchmark/latency_ms': forward_results[1].get('latency_ms_mean', 0.0),
                                'eval/benchmark/gen_latency_ms': generation_results.get('per_token_latency_ms_mean', 0.0),
                            })
                
                if self.torch_callback.log_to_console:
                    if 1 in forward_results:
                        self.torch_callback._log_debug(f"  âœ“ Efficiency Measurement completed ({forward_results[1].get('tokens_per_sec', 0.0):.2f} tokens/s)")
            except Exception as e:
                self.torch_callback._log_debug(f"  âœ— Efficiency Measurement failed: {e}")
                import traceback
                if self.torch_callback.debug_logging:
                    self.torch_callback._log_debug(traceback.format_exc())
        
        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
        if benchmark_results and self.torch_callback.save_detailed_logs:
            benchmark_file = os.path.join(
                output_dir,
                f"benchmark_results_step_{state.global_step}.json"
            )
            with open(benchmark_file, 'w') as f:
                json.dump(benchmark_results, f, indent=2)
            self.torch_callback._log_debug(f"Benchmark results saved to {benchmark_file}")
        
        if self.torch_callback.log_to_console:
            self.torch_callback._log_debug(f"Completed {len(benchmark_results)} benchmark(s)")

    def on_evaluate(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        tokenizer=None,
        eval_dataloader=None,
        **kwargs
    ):
        """Evaluation ì‹œì ì— GramSpec ì§€í‘œ ì¸¡ì •"""
        # GramSpecAnalyzerê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if self.torch_callback.gramspec_analyzer is None:
            return
        
        try:
            # Evaluation ëª¨ë“œë¡œ ì „í™˜
            original_training = model.training if model is not None else None
            if model is not None:
                model.eval()
            
            # Analyzer ì´ˆê¸°í™” (eval ì „ìš©)
            eval_analyzer = self.torch_callback.gramspec_analyzer
            eval_analyzer.reset()  # ì´ì „ ë°ì´í„° ì´ˆê¸°í™”
            
            # Routerì™€ MoE Blockì—ì„œ routing ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•œ hook ë“±ë¡
            from eval.evaluate_checkpoint_model import RoutingInfoCollector
            collector = RoutingInfoCollector(eval_analyzer)
            collector.register_hooks(model)
            
            # Eval dataloaderë¡œ forward pass ì‹¤í–‰
            # eval_dataloaderê°€ Noneì´ë©´ trainerì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            dataloader = eval_dataloader
            if dataloader is None and 'trainer' in kwargs:
                trainer = kwargs['trainer']
                if hasattr(trainer, 'get_eval_dataloader'):
                    try:
                        dataloader = trainer.get_eval_dataloader()
                    except:
                        pass
            
            if dataloader is not None:
                self.torch_callback._log_debug(f"Running evaluation metrics collection at step {state.global_step}...")
                
                num_samples = 0
                max_eval_samples = getattr(args, 'max_eval_samples', 100)  # ê¸°ë³¸ê°’ 100
                
                with torch.no_grad():
                    for batch in dataloader:
                        if num_samples >= max_eval_samples:
                            break
                        
                        # Batchë¥¼ deviceë¡œ ì´ë™
                        batch = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v 
                                for k, v in batch.items()}
                        
                        # Forward pass (routing ì •ë³´ ìˆ˜ì§‘)
                        try:
                            outputs = model(**batch)
                            # Batch size ê³„ì‚°
                            batch_size = 1
                            if 'input_ids' in batch:
                                batch_size = batch['input_ids'].shape[0]
                            elif 'pixel_values' in batch:
                                batch_size = batch['pixel_values'].shape[0]
                            num_samples += batch_size
                        except Exception as e:
                            self.torch_callback._log_debug(f"Error in eval forward pass: {e}")
                            continue
                
                # ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„
                self.torch_callback._log_debug(f"Analyzing {num_samples} eval samples...")
                eval_results = collector.analyze_collected_data(
                    num_experts=self.torch_callback.num_experts,
                    router_dim=128
                )
                
                # Hook ì œê±°
                collector.remove_hooks()
                
                # ê²°ê³¼ë¥¼ trainer logsì— ì¶”ê°€
                if 'aggregated_metrics' in eval_results:
                    eval_metrics = eval_results['aggregated_metrics']
                    
                    # ë…¼ë¬¸ìš© ì§€í‘œë“¤ì„ trainerì— ë¡œê¹…
                    eval_log_data = {}
                    
                    # Load Balancing ì§€í‘œ
                    if 'final_load_balancing_cv' in eval_metrics:
                        eval_log_data['eval/load_balancing/cv'] = eval_metrics['final_load_balancing_cv']
                    if 'final_load_imbalance_ratio' in eval_metrics:
                        eval_log_data['eval/load_balancing/imbalance_ratio'] = eval_metrics['final_load_imbalance_ratio']
                    if 'expert_utilization_rate' in eval_metrics:
                        eval_log_data['eval/load_balancing/utilization_rate'] = eval_metrics['expert_utilization_rate']
                    if 'final_maxvio' in eval_metrics:
                        eval_log_data['eval/load_balancing/maxvio'] = eval_metrics['final_maxvio']
                    if 'final_aux_loss' in eval_metrics:
                        eval_log_data['eval/load_balancing/aux_loss'] = eval_metrics['final_aux_loss']
                    
                    # Expert Specialization ì§€í‘œ
                    if 'final_expert_diversity_score' in eval_metrics:
                        eval_log_data['eval/specialization/diversity_score'] = eval_metrics['final_expert_diversity_score']
                    if 'final_expert_similarity_mean' in eval_metrics:
                        eval_log_data['eval/specialization/similarity_mean'] = eval_metrics['final_expert_similarity_mean']
                    if 'final_expert_specialization_strength' in eval_metrics:
                        eval_log_data['eval/specialization/specialization_strength'] = eval_metrics['final_expert_specialization_strength']
                    
                    # Gram Matrix Quality
                    if 'avg_gram_orthogonality' in eval_metrics:
                        eval_log_data['eval/gram_matrix/orthogonality'] = eval_metrics['avg_gram_orthogonality']
                    
                    # Paper summaryë„ ë¡œê¹…
                    if 'paper_summary' in eval_results:
                        paper_summary = eval_results['paper_summary']
                        if 'load_balancing' in paper_summary:
                            lb = paper_summary['load_balancing']
                            for key, value in lb.items():
                                if isinstance(value, (int, float)):
                                    eval_log_data[f'eval/paper/load_balancing/{key}'] = value
                    
                    # Loggerì— ì „ì†¡ (trainer.logë¥¼ í†µí•´ ì „ë‹¬)
                    if 'trainer' in kwargs:
                        trainer = kwargs['trainer']
                        if hasattr(trainer, 'log'):
                            trainer.log(eval_log_data)
                    
                    # ì½˜ì†” ì¶œë ¥
                    if self.torch_callback.log_to_console:
                        self.torch_callback._log_debug(f"\n{'='*60}")
                        self.torch_callback._log_debug(f"Evaluation Metrics (Step {state.global_step}):")
                        self.torch_callback._log_debug(f"{'='*60}")
                        for key, value in eval_log_data.items():
                            if isinstance(value, (int, float)):
                                self.torch_callback._log_debug(f"  {key}: {value:.4f}")
                        self.torch_callback._log_debug(f"{'='*60}\n")
                    
                    # ìƒì„¸ ê²°ê³¼ ì €ì¥
                    if self.torch_callback.save_detailed_logs:
                        eval_result_file = os.path.join(
                            self.torch_callback.log_dir,
                            f"eval_metrics_step_{state.global_step}.json"
                        )
                        with open(eval_result_file, 'w') as f:
                            json.dump(eval_results, f, indent=2)
                        self.torch_callback._log_debug(f"Eval metrics saved to {eval_result_file}")
            
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            if model is not None and tokenizer is not None:
                self._run_benchmarks(
                    model=model,
                    tokenizer=tokenizer,
                    eval_dataloader=dataloader,
                    state=state,
                    args=args,
                    **kwargs
                )
            else:
                self.torch_callback._log_debug("âš ï¸ No eval dataloader available for metrics collection")
            
            # ëª¨ë¸ì„ ì›ë˜ ëª¨ë“œë¡œ ë³µì›
            if model is not None and original_training is not None:
                model.train(original_training)
                
        except Exception as e:
            import traceback
            self.torch_callback._log_debug(f"Error during evaluation metrics collection: {e}")
            self.torch_callback._log_debug(traceback.format_exc())
            # ëª¨ë¸ì„ ì›ë˜ ëª¨ë“œë¡œ ë³µì›
            if model is not None and original_training is not None:
                model.train(original_training)
    
    def on_train_end(
        self, 
        args: TrainingArguments, 
        state: TrainerState, 
        control: TrainerControl, 
        **kwargs
    ):
        """í›ˆë ¨ ì¢…ë£Œ"""
        summary = self.torch_callback.get_summary()
        if self.torch_callback.log_to_console:
            self.torch_callback._log_debug("\n" + "="*50)
            self.torch_callback._log_debug("MoE Training Summary:")
            self.torch_callback._log_debug(f"Total alerts: {summary['total_alerts']}")
            if summary['alert_types']:
                self.torch_callback._log_debug("Alert breakdown:")
                for alert_type, count in summary['alert_types'].items():
                    self.torch_callback._log_debug(f"  {alert_type}: {count}")
            self.torch_callback._log_debug("="*50)
        
        # ì •ë¦¬
        self.torch_callback.cleanup()

def create_moe_callback_for_transformers(
    log_every_n_steps: int = 100,
    logger=None,
    enable_generation_logging: bool = True,
    generation_log_dir: str = "./moe_generation_logs",
    max_generation_samples: int = 3,
    generation_log_every: int = 100,
    log_tsne_every: int = 5000,
    tsne_sample_size: int = 2000,
    tokenizer=None,  # âœ… tokenizerë¥¼ ì§ì ‘ ì „ë‹¬í•  ìˆ˜ ìˆë„ë¡ ì¶”ê°€
    **kwargs
) -> TransformersMoECallbackWrapper:
    """Transformersìš© MoE ì½œë°± ìƒì„± í¸ì˜ í•¨ìˆ˜"""

    torch_callback = TorchMoECallback(
        log_every_n_steps=log_every_n_steps,
        logger=logger,
        enable_generation_logging=enable_generation_logging,
        generation_log_dir=generation_log_dir,
        max_generation_samples=max_generation_samples,
        generation_log_every=generation_log_every,
        log_tsne_every=log_tsne_every,
        tsne_sample_size=tsne_sample_size,
        force_all_ranks=True,  # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ (ì´ë¯¸ is_main_process ì²´í¬ ì œê±°ë¨)
        **kwargs
    )
    
    # âœ… tokenizerê°€ ì „ë‹¬ë˜ë©´ ë¯¸ë¦¬ ì„¤ì • (VLM í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í•„ìˆ˜)
    if tokenizer is not None:
        torch_callback.set_tokenizer(tokenizer)

    return TransformersMoECallbackWrapper(torch_callback)

def create_moe_callback_for_pytorch(
    model: torch.nn.Module,
    log_every_n_steps: int = 100,
    logger=None,
    tokenizer=None,
    enable_generation_logging: bool = True,
    generation_log_dir: str = "./moe_generation_logs",
    max_generation_samples: int = 3,
    generation_log_every: int = 100,
    **kwargs
) -> TorchMoECallback:
    """ìˆœìˆ˜ PyTorchìš© MoE ì½œë°± ìƒì„± í¸ì˜ í•¨ìˆ˜"""

    callback = TorchMoECallback(
        log_every_n_steps=log_every_n_steps,
        logger=logger,
        enable_generation_logging=enable_generation_logging,
        generation_log_dir=generation_log_dir,
        max_generation_samples=max_generation_samples,
        generation_log_every=generation_log_every,
        force_all_ranks=False,  # Multi-GPU í™˜ê²½ì—ì„œ rank 0ì—ì„œë§Œ ì‹¤í–‰
        **kwargs
    )

    return callback.register_model(model, tokenizer)
