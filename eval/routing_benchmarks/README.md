# SPECTRA 7-Day Evaluation Plan for ICML/NeurIPS Submission

Complete evaluation framework for SPECTRA (256-expert MoE model) designed for top-tier conference submission. This pipeline systematically evaluates training dynamics, benchmark performance, expert specialization, efficiency, and generates paper-ready LaTeX tables.

## ğŸ¯ Overview

This evaluation suite is structured as a 7-day plan (D-Day through D+6), with each day focusing on specific aspects:

- **D-Day**: Sanity Check - Training dynamics and perplexity validation
- **D+1~2**: Standard Benchmarks - MMLU, GSM8K, HumanEval, etc.
- **D+3~4**: Expert Analysis - Specialization, GRU trajectory, orthogonality
- **D+5**: Efficiency & Ablation - vLLM throughput and component analysis
- **D+6**: Final Tables - LaTeX table generation for paper

## ğŸ“ File Structure

```
eval/routing_benchmarks/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ evaluation_config.yaml         # Main configuration
â”‚   â””â”€â”€ baseline_models.yaml           # Baseline model specifications
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ checkpoint_loader.py           # Model loading utilities
â”‚   â”œâ”€â”€ wandb_extractor.py             # WandB log extraction
â”‚   â”œâ”€â”€ metric_tracker.py              # Results persistence
â”‚   â”œâ”€â”€ visualization.py               # Plotting functions
â”‚   â””â”€â”€ latex_formatter.py             # LaTeX table generation
â”œâ”€â”€ day0_sanity_check.py               # D-Day: Training dynamics & PPL
â”œâ”€â”€ day1_2_standard_benchmarks.py      # D+1~2: lm-eval-harness
â”œâ”€â”€ day3_4_expert_analysis.py          # D+3~4: Specialization analysis
â”œâ”€â”€ day5_efficiency_ablation.py        # D+5: Throughput & ablation
â”œâ”€â”€ day6_comparison_table.py           # D+6: Final table generation
â””â”€â”€ run_full_pipeline.py               # Automated full pipeline
```

## ğŸš€ Quick Start

### 1. Prerequisites

```bash
# Install dependencies
pip install torch transformers datasets wandb
pip install lm-eval accelerate
pip install vllm  # For throughput measurement
pip install matplotlib seaborn pandas numpy pyyaml
```

### 2. Configuration

Edit `config/evaluation_config.yaml`:

```yaml
model:
  checkpoint_path: "/path/to/your/spectra/checkpoint"  # REQUIRED
  model_name: "SPECTRA-256E"
  num_experts: 256
  active_experts: 8

wandb:
  run_id: "your_wandb_run_id"  # REQUIRED
  project: "spectra-training"

compute:
  num_gpus: 4
  batch_size_per_gpu: 8
  use_bf16: true
```

### 3. Run Full Pipeline

```bash
# Automated full evaluation (D-Day through D+6)
python eval/routing_benchmarks/run_full_pipeline.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --output_dir ./evaluation_results
```

Or use the convenience script:

```bash
# Make executable
chmod +x eval/routing_benchmarks/scripts/run_all.sh

# Run
./eval/routing_benchmarks/scripts/run_all.sh
```

## ğŸ“‹ Day-by-Day Execution

### D-Day: Sanity Check

**Purpose:** Verify model convergence before proceeding with full evaluation.

```bash
python eval/routing_benchmarks/day0_sanity_check.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --checkpoint /path/to/checkpoint \
    --output_dir ./evaluation_results/day0
```

**What it does:**
- Extracts CV curve from WandB (load balancing metric)
- Extracts MaxVio curve (constraint satisfaction)
- Measures perplexity on WikiText-103, Pile, C4
- Generates GO/NO-GO decision

**Outputs:**
- `cv_curve.png` - Coefficient of Variation over training
- `maxvio_curve.png` - Constraint violation tracking
- `training_dynamics.png` - Combined training curves
- `sanity_check_report.txt` - GO/NO-GO decision
- `perplexity.json` - Validation perplexity results

**GO Criteria:**
- CV < 0.3 (good load balancing)
- MaxVio < 0.15 (constraints satisfied)
- Reasonable perplexity (< 1000 sanity check)

### D+1~2: Standard Benchmarks

**Purpose:** Evaluate on widely-recognized benchmarks for comparison with baselines.

```bash
python eval/routing_benchmarks/day1_2_standard_benchmarks.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --output_dir ./evaluation_results/day1_2
```

**What it does:**
- Runs lm-evaluation-harness on:
  - **Knowledge:** MMLU (5-shot)
  - **Reasoning:** GSM8K (8-shot), ARC-Challenge (5-shot)
  - **Commonsense:** HellaSwag (5-shot), Winogrande (5-shot)
  - **Coding:** HumanEval (0-shot), MBPP (0-shot)
- Evaluates baseline models (Mixtral-8x7B, LLaMA-3-8B)
- Generates comparison summary

**Outputs:**
- `spectra/*.json` - SPECTRA results per task
- `mixtral/*.json` - Mixtral baseline results
- `llama3/*.json` - LLaMA-3 baseline results
- `comparison_summary.json` - Aggregated comparison

**Multi-GPU Support:**

```bash
accelerate launch --multi_gpu --num_processes=4 \
    eval/routing_benchmarks/day1_2_standard_benchmarks.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml
```

### D+3~4: Expert Analysis (Core Novelty)

**Purpose:** Demonstrate SPECTRA's unique expert specialization capabilities.

```bash
python eval/routing_benchmarks/day3_4_expert_analysis.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --output_dir ./evaluation_results/day3_4
```

**What it does:**
1. **Domain-Specific Expert Usage**
   - Collects expert activation patterns on arXiv, GitHub, novels
   - Generates heatmap showing which experts specialize in which domains

2. **GRU Trajectory Consistency**
   - Measures routing stability across layers (unique to SPECTRA)
   - Compares L1 distance between layer routing decisions

3. **Representation Orthogonality**
   - Validates OSR (Orthogonal Sinkhorn Routing)
   - Computes pairwise cosine similarity of expert representations

**Outputs:**
- `domain_heatmaps.png` - **Figure 4 for paper** - Expert specialization
- `trajectory_consistency.png` - GRU routing stability
- `orthogonality.png` - **Figure 5 for paper** - OSR validation
- `specialization_report.json` - Quantitative metrics

**Paper Writing Tips:**
- Figure 4 should clearly show distinct expert usage patterns per domain
- Emphasize that consistency is HIGHER (lower L1 distance) than standard routers
- Orthogonality histogram should center around 0 (good OSR performance)

### D+5: Efficiency & Ablation

**Purpose:** Demonstrate practical viability and validate architectural choices.

```bash
python eval/routing_benchmarks/day5_efficiency_ablation.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --output_dir ./evaluation_results/day5
```

**What it does:**
1. **vLLM Throughput Measurement**
   - Tests various input lengths (128, 512, 1024, 2048 tokens)
   - Tests various batch sizes (1, 4, 8, 16)
   - Measures TTFT (Time To First Token) and TPOT (Time Per Output Token)
   - Compares with Mixtral-8x7B and LLaMA-3-8B

2. **Ablation Study**
   - Evaluates variants: No GRU, No OSR, No Explicit Bias
   - Measures perplexity degradation for each removed component

**Outputs:**
- `throughput_comparison.png` - Inference speed vs baselines
- `ablation_table.tex` - **Table 2 for paper** - Component importance
- `efficiency_report.json` - Detailed throughput metrics

**Note:** Ablation requires pre-trained checkpoints of each variant. If unavailable, this step can be skipped with `--skip_ablation`.

### D+6: Final Comparison Tables

**Purpose:** Generate paper-ready LaTeX tables.

```bash
python eval/routing_benchmarks/day6_comparison_table.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --output_dir ./evaluation_results/day6
```

**What it does:**
- Aggregates results from all previous days
- Generates three LaTeX tables:
  1. **Table 1:** Main comparison (SPECTRA vs baselines)
  2. **Table 2:** Ablation study results
  3. **Table 3:** Model specifications

**Outputs:**
- `table1_comparison.tex` - **Main paper table** - Ready to \input
- `table2_ablation.tex` - Ablation results
- `table3_specifications.tex` - Model architecture details
- `final_summary.json` - Complete evaluation summary
- `final_report.txt` - Human-readable summary

**Using in LaTeX:**

```latex
\begin{table*}[htbp]
  \input{evaluation_results/day6/table1_comparison.tex}
\end{table*}
```

## ğŸ› ï¸ Advanced Usage

### Running Specific Days Only

```bash
# Run only Day 3-4 (expert analysis)
python eval/routing_benchmarks/run_full_pipeline.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --only day3_4
```

### Running from a Specific Day

```bash
# Start from Day 1-2 (skip sanity check)
python eval/routing_benchmarks/run_full_pipeline.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --start_from day1_2
```

### Continue on Errors

```bash
# Don't stop if a non-critical step fails
python eval/routing_benchmarks/run_full_pipeline.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --continue_on_error
```

### Skipping Baselines

```bash
# Evaluate SPECTRA only (faster)
python eval/routing_benchmarks/day1_2_standard_benchmarks.py \
    --config eval/routing_benchmarks/config/evaluation_config.yaml \
    --skip_baselines
```

## ğŸ“Š Expected Results Structure

After running the full pipeline:

```
evaluation_results/
â”œâ”€â”€ day0_sanity_check/
â”‚   â”œâ”€â”€ cv_curve.png
â”‚   â”œâ”€â”€ maxvio_curve.png
â”‚   â”œâ”€â”€ training_dynamics.png
â”‚   â”œâ”€â”€ sanity_check_report.txt
â”‚   â””â”€â”€ metrics.json
â”œâ”€â”€ day1_2_benchmarks/
â”‚   â”œâ”€â”€ spectra/
â”‚   â”‚   â”œâ”€â”€ knowledge/mmlu_results.json
â”‚   â”‚   â”œâ”€â”€ reasoning/gsm8k_results.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ mixtral/
â”‚   â”œâ”€â”€ llama3/
â”‚   â””â”€â”€ comparison_summary.json
â”œâ”€â”€ day3_4_expert_analysis/
â”‚   â”œâ”€â”€ domain_heatmaps.png          â† Figure 4
â”‚   â”œâ”€â”€ trajectory_consistency.png
â”‚   â”œâ”€â”€ orthogonality.png            â† Figure 5
â”‚   â””â”€â”€ specialization_report.json
â”œâ”€â”€ day5_efficiency/
â”‚   â”œâ”€â”€ throughput_comparison.png
â”‚   â”œâ”€â”€ ablation_table.tex           â† Table 2
â”‚   â””â”€â”€ efficiency_report.json
â”œâ”€â”€ day6_final/
â”‚   â”œâ”€â”€ table1_comparison.tex        â† Table 1 (Main)
â”‚   â”œâ”€â”€ table2_ablation.tex
â”‚   â”œâ”€â”€ table3_specifications.tex
â”‚   â””â”€â”€ final_summary.json
â””â”€â”€ full_pipeline_log.txt
```

## ğŸ› Troubleshooting

### CUDA Out of Memory

**Solution 1:** Reduce batch size in config:

```yaml
compute:
  batch_size_per_gpu: 4  # Reduce from 8
```

**Solution 2:** Use 8-bit quantization:

```python
model, tokenizer = load_spectra_checkpoint(
    checkpoint_path,
    load_in_8bit=True
)
```

### lm-eval-harness Errors

**Issue:** Task not found

```bash
# List available tasks
lm_eval --tasks list
```

**Issue:** Model loading fails

Check that `trust_remote_code=True` is set in config.

### WandB API Errors

**Issue:** Run not found

1. Check run_id in config matches WandB dashboard
2. Set `WANDB_API_KEY` environment variable
3. Or run with `--skip_wandb` flag

### Baseline Model Download Issues

**For gated models (LLaMA):**

```bash
# Login to HuggingFace
huggingface-cli login

# Or set token
export HF_TOKEN="your_token_here"
```

## ğŸ“ Paper Writing Tips

### Figure Placement Suggestions

1. **Figure 1-2:** Training dynamics (CV, MaxVio, Loss) - Shows stable training
2. **Figure 3:** Loss curve with "No Auxiliary Loss" annotation
3. **Figure 4:** Domain heatmaps - Core novelty showing specialization
4. **Figure 5:** Orthogonality histogram - OSR validation
5. **Figure 6:** Throughput comparison - Practical viability

### Table Organization

- **Table 1:** Main comparison (put in main text)
  - Active Params, MMLU, GSM8K, ARC, PPL, Throughput
  - Bold best results
  - Emphasize similar active params to baselines

- **Table 2:** Ablation study (main text or appendix)
  - Show Î” PPL for each removed component
  - Highlight that all components contribute

- **Table 3:** Model specifications (appendix)
  - Total vs Active parameters
  - Router type comparison

### Key Claims to Support

1. **Load Balancing:** "SPECTRA achieves CV < 0.1 without auxiliary loss"
   - Evidence: day0/cv_curve.png

2. **Expert Specialization:** "Experts automatically specialize by domain"
   - Evidence: day3_4/domain_heatmaps.png

3. **Trajectory Consistency:** "GRU maintains routing coherence across layers"
   - Evidence: day3_4/trajectory_consistency.png

4. **Orthogonality:** "OSR ensures diverse expert representations"
   - Evidence: day3_4/orthogonality.png (mean near 0)

5. **Efficiency:** "Comparable inference speed to Mixtral-8x7B"
   - Evidence: day5/throughput_comparison.png

6. **Performance:** "Competitive on standard benchmarks"
   - Evidence: day6/table1_comparison.tex

## ğŸ“ Citation

If you use this evaluation framework, please cite:

```bibtex
@article{spectra2024,
  title={SPECTRA: Scalable and Practical Expert Routing with Trajectory-Aware MoE},
  author={Your Name and Others},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

## ğŸ¤ Contributing

This evaluation framework is designed to be:
- **Reproducible:** All random seeds fixed, configs versioned
- **Extensible:** Easy to add new benchmarks or metrics
- **Modular:** Each day is independent
- **Documented:** Extensive inline documentation

## ğŸ“ Support

For issues or questions:
1. Check troubleshooting section above
2. Review config files for typos
3. Check WandB run logs
4. Open an issue on GitHub

## ğŸ† Success Criteria

Before submitting to ICML/NeurIPS:

- [x] D-Day sanity check passes (GO decision)
- [x] SPECTRA performance â‰¥ baseline on â‰¥50% of tasks
- [x] Clear expert specialization visible in heatmaps
- [x] CV < 0.3 and MaxVio < 0.15 throughout training
- [x] All LaTeX tables generated without errors
- [x] Paper figures are publication-quality (300 DPI)

## ğŸ“… Timeline Estimation

- **D-Day:** 2-4 hours (WandB extraction + PPL evaluation)
- **D+1~2:** 1-2 days (lm-eval is slow, especially with baselines)
- **D+3~4:** 4-8 hours (expert analysis requires multiple forward passes)
- **D+5:** 4-8 hours (vLLM setup + throughput measurement)
- **D+6:** 30 minutes (table generation from cached results)

**Total GPU Time:** ~3-4 days on 4xA100 80GB

**Cost Estimate:** ~$500-1000 on cloud GPUs (AWS p4d instances)

## ğŸ‰ Good Luck!

This framework is battle-tested and designed to help you present SPECTRA's capabilities in the best possible light for top-tier conference submission. Follow the plan systematically, and you'll have all the evidence needed for a strong paper.

---

**Last Updated:** 2024-12-18  
**Version:** 1.0  
**Maintainer:** SPECTRA Team

