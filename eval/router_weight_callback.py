# coding=utf-8
"""
Router Weight Tracking Callback for Transformers Trainer

ì´ callbackì€ transformers Trainerì™€ í†µí•©ë˜ì–´ stepë³„ë¡œ router ê°€ì¤‘ì¹˜ë¥¼ trackingí•©ë‹ˆë‹¤.
"""

import os
import logging
from typing import Optional, Dict, Any
import torch
import numpy as np
from transformers import TrainerCallback, TrainerState, TrainerControl
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR

try:
    from peft.utils.other import ModulesToSaveWrapper
except ImportError:
    ModulesToSaveWrapper = None

from eval.router_weight_tracker import RouterWeightTracker

logger = logging.getLogger(__name__)


class RouterWeightTrackingCallback(TrainerCallback):
    """
    Transformers Trainerìš© Router ê°€ì¤‘ì¹˜ tracking callback
    
    ì‚¬ìš© ì˜ˆì‹œ:
        from eval.router_weight_callback import RouterWeightTrackingCallback
        
        callback = RouterWeightTrackingCallback(
            save_dir="./router_weight_logs",
            log_every_n_steps=100,
        )
        trainer.add_callback(callback)
    """
    
    def __init__(
        self,
        save_dir: str = "./router_weight_logs",
        log_every_n_steps: int = 1,
        save_full_weights: bool = False,
        max_history: int = 1000,
        verbose: bool = True,
        check_weight_change: bool = True,
        min_change_threshold: float = 1e-8,
        check_after_steps: int = 10,
    ):
        """
        Args:
            save_dir: ê°€ì¤‘ì¹˜ ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            log_every_n_steps: N stepë§ˆë‹¤ ë¡œê·¸ ì €ì¥
            save_full_weights: Trueë©´ ì „ì²´ ê°€ì¤‘ì¹˜ í…ì„œ ì €ì¥ (ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©)
            max_history: ë©”ëª¨ë¦¬ì— ìœ ì§€í•  ìµœëŒ€ step ìˆ˜
            verbose: ìƒì„¸ ë¡œê¹… ì—¬ë¶€
            check_weight_change: weight ë³€í™” ì²´í¬ ì—¬ë¶€
            min_change_threshold: ìµœì†Œ ë³€í™” ì„ê³„ê°’ (ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ë³€í™” ì—†ìŒìœ¼ë¡œ ê°„ì£¼)
            check_after_steps: ëª‡ step í›„ë¶€í„° ë³€í™” ì²´í¬ ì‹œì‘
        """
        self.save_dir = save_dir
        self.log_every_n_steps = log_every_n_steps
        self.verbose = verbose
        self.check_weight_change = check_weight_change
        self.min_change_threshold = min_change_threshold
        self.check_after_steps = check_after_steps
        
        # ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
        os.makedirs(save_dir, exist_ok=True)
        if self.verbose:
            logger.info(f"âœ… RouterWeightTrackingCallback initialized: save_dir={save_dir}, log_every_n_steps={log_every_n_steps}")
            if self.check_weight_change:
                logger.info(f"   Weight change checking enabled: threshold={min_change_threshold}, check_after_steps={check_after_steps}")
        
        self.tracker = RouterWeightTracker(
            save_dir=save_dir,
            save_every_n_steps=log_every_n_steps,
            save_full_weights=save_full_weights,
            max_history=max_history,
        )
        
        self._first_step_logged = False
        self._last_weight_changes = {}  # layerë³„ ë§ˆì§€ë§‰ ë³€í™”ëŸ‰ ì €ì¥
        self._last_trainer = None  # ë””ë²„ê¹…ìš© trainer ì°¸ì¡°
        self._optimizer_validation_done = False  # Optimizer ê²€ì¦ ì™„ë£Œ í”Œë˜ê·¸ (DeepSpeed lazy init ëŒ€ì‘)
        self._router_forward_tracker = {}  # stepë³„ë¡œ ì‚¬ìš©ëœ router ì¶”ì : {step: [router_names]}
        self._router_hooks = []  # ë“±ë¡ëœ forward hookë“¤ (ë‚˜ì¤‘ì— ì œê±°ìš©)
        self._actual_router_weights = {}  # Forward hookì—ì„œ ì¶”ì í•œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” router weight: {router_name: {step: weight_tensor}}
        self._prev_actual_weights = {}  # ì´ì „ stepì˜ ì‹¤ì œ ì‚¬ìš©ëœ weight: {router_name: weight_tensor} (ì§ì ‘ ë¹„êµìš©)
    
    def on_train_begin(self, args, state: TrainerState, control: TrainerControl, model=None, **kwargs):
        """Training ì‹œì‘ ì‹œ router íŒŒë¼ë¯¸í„° ê²€ì¦ (requires_gradëŠ” ë³€ê²½í•˜ì§€ ì•ŠìŒ)"""
        logger.info("=" * 80)
        logger.info("ğŸ”§ Router weight tracking callback initialized (requires_grad ë³€ê²½í•˜ì§€ ì•ŠìŒ)")
        logger.info("=" * 80)
        
        # âœ… Trainer ì°¸ì¡° ì €ì¥ (ë””ë²„ê¹… ë° ê²€ì¦ìš©)
        trainer = kwargs.get('trainer')
        if trainer is not None:
            self._last_trainer = trainer
        
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
        if model is None:
            logger.error("âŒ Model is None in on_train_begin - cannot set router parameters")
            return control
        
        # Router íŒŒë¼ë¯¸í„° ê°•ì œë¡œ requires_grad=True ì„¤ì • (trainer ì—†ì–´ë„ ëª¨ë¸ì—ì„œ ì§ì ‘ ì„¤ì •)
        actual_model = model
        if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
            actual_model = model.module
        
        if actual_model is not None:
            from models.spectra_model import SPECTRARouter
            try:
                from models.g3moe_model import G3MoERouter, G3MoEGRINMoE
            except ImportError:
                G3MoERouter = None
                G3MoEGRINMoE = None
            
            fixed_count = 0
            router_modules_found = []
            seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
            
            for name, module in actual_model.named_modules():
                is_router = False
                router_module = None
                
                # 1. PEFT ModulesToSaveWrapper ì²´í¬ (ê°€ì¥ ì¤‘ìš”)
                if ModulesToSaveWrapper is not None and isinstance(module, ModulesToSaveWrapper):
                    active_adapter = getattr(module, "active_adapter", "default")
                    if hasattr(module, "modules_to_save") and active_adapter in module.modules_to_save:
                        inner_module = module.modules_to_save[active_adapter]
                        if isinstance(inner_module, SPECTRARouter):
                            is_router = True
                            router_module = inner_module
                        elif G3MoERouter is not None and isinstance(inner_module, G3MoERouter):
                            is_router = True
                            router_module = inner_module
                
                # 2. SPECTRARouter ì²´í¬
                elif isinstance(module, SPECTRARouter):
                    is_router = True
                    router_module = module
                # 3. G3MoERouter ì§ì ‘ ì²´í¬
                elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                    is_router = True
                    router_module = module
                # 4. G3MoEGRINMoE ë‚´ë¶€ì˜ router ì†ì„± ì²´í¬
                elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                    if hasattr(module, 'router'):
                        potential_router = module.router
                        
                        if ModulesToSaveWrapper is not None and isinstance(potential_router, ModulesToSaveWrapper):
                            active_adapter = getattr(potential_router, "active_adapter", "default")
                            if hasattr(potential_router, "modules_to_save") and active_adapter in potential_router.modules_to_save:
                                inner_module = potential_router.modules_to_save[active_adapter]
                                if isinstance(inner_module, G3MoERouter):
                                    is_router = True
                                    router_module = inner_module
                                    name = f"{name}.router"
                        
                        elif isinstance(potential_router, G3MoERouter):
                            is_router = True
                            router_module = potential_router
                            name = f"{name}.router"
                            
                # 5. ì¼ë°˜ì ì¸ router êµ¬ì¡° ì²´í¬ (load_balancer + expression_projector)
                elif hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
                    is_router = True
                    router_module = module
                
                if is_router and router_module is not None:
                    # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
                    router_id = id(router_module)
                    if router_id in seen_router_ids:
                        continue
                    seen_router_ids.add(router_id)
                    
                    router_modules_found.append(name)
                    # Load balancer íŒŒë¼ë¯¸í„° - requires_grad ë³€ê²½í•˜ì§€ ì•ŠìŒ (í•™ìŠµì— ì˜í–¥ ì£¼ì§€ ì•Šë„ë¡)
                    # if hasattr(router_module, 'load_balancer'):
                    #     for param_name, param in router_module.load_balancer.named_parameters(recurse=True):
                    #         if not param.requires_grad:
                    #             param.requires_grad_(True)
                    #             fixed_count += 1
                    #             logger.info(f"  âœ“ Set requires_grad=True: {name}.load_balancer.{param_name}")
                    
                    # Expression projector íŒŒë¼ë¯¸í„° - requires_grad ë³€ê²½í•˜ì§€ ì•ŠìŒ (í•™ìŠµì— ì˜í–¥ ì£¼ì§€ ì•Šë„ë¡)
                    # if hasattr(router_module, 'expression_projector'):
                    #     expr_proj = router_module.expression_projector
                    #     for param_name, param in expr_proj.named_parameters(recurse=True):
                    #         if not param.requires_grad:
                    #             param.requires_grad_(True)
                    #             fixed_count += 1
                    #             logger.info(f"  âœ“ Set requires_grad=True: {name}.expression_projector.{param_name}")
                    #     
                    #     # linear_projectionì´ ë³„ë„ë¡œ ìˆëŠ” ê²½ìš°
                    #     if hasattr(expr_proj, 'linear_projection'):
                    #         for param_name, param in expr_proj.linear_projection.named_parameters(recurse=True):
                    #             if not param.requires_grad:
                    #                 param.requires_grad_(True)
                    #                 fixed_count += 1
                    #                 logger.info(f"  âœ“ Set requires_grad=True: {name}.expression_projector.linear_projection.{param_name}")
            
            if router_modules_found:
                logger.info(f"âœ… Found {len(router_modules_found)} router module(s)")
                # requires_grad ë³€ê²½í•˜ì§€ ì•ŠìŒ (í•™ìŠµì— ì˜í–¥ ì£¼ì§€ ì•Šë„ë¡)
                # if fixed_count > 0:
                #     logger.info(f"âœ… Fixed {fixed_count} router parameters: set requires_grad=True")
                # else:
                #     logger.info(f"âœ… All router parameters already have requires_grad=True")
            else:
                logger.warning("âš ï¸ No router modules found in model")
        
        # Trainerê°€ ìˆìœ¼ë©´ optimizer ê²€ì¦ ë° ì¶”ê°€
        if trainer is not None:
            # Router íŒŒë¼ë¯¸í„° ê°•ì œ ì„¤ì • ë° ê²€ì¦
            validation_result = self._ensure_router_in_optimizer(trainer, model)
            
            if not validation_result['has_routers']:
                logger.warning("âš ï¸ No router modules found in model (from validation)")
            
            # requires_grad ë³€ê²½í•˜ì§€ ì•ŠìŒ (í•™ìŠµì— ì˜í–¥ ì£¼ì§€ ì•Šë„ë¡)
            # if not validation_result['all_trainable']:
            #     non_trainable = validation_result['non_trainable_params']
            #     logger.warning(f"âš ï¸ {len(non_trainable)} router parameters still have requires_grad=False - forcing to True...")
            #     
            #     # ì‹¤ì œ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ì„œ requires_grad=Trueë¡œ ì„¤ì •
            #     actual_model = model
            #     if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
            #         actual_model = model.module
            #     
            #     if actual_model is not None:
            #         from models.spectra_model import SPECTRARouter
            #         try:
            #             from models.g3moe_model import G3MoERouter, G3MoEGRINMoE
            #         except ImportError:
            #             G3MoERouter = None
            #             G3MoEGRINMoE = None
            #         
            #         additional_fixed = 0
            #         seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
            #         
            #         for name, module in actual_model.named_modules():
            #             is_router = False
            #             router_module = None
            #             
            #             if isinstance(module, SPECTRARouter):
            #                 is_router = True
            #                 router_module = module
            #             elif G3MoERouter is not None and isinstance(module, G3MoERouter):
            #                 is_router = True
            #                 router_module = module
            #             elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
            #                 if hasattr(module, 'router') and isinstance(module.router, G3MoERouter):
            #                     is_router = True
            #                     router_module = module.router
            #                     name = f"{name}.router"
            #             elif hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
            #                 is_router = True
            #                 router_module = module
            #             
            #             if is_router and router_module is not None:
            #                 # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
            #                 router_id = id(router_module)
            #                 if router_id in seen_router_ids:
            #                     continue
            #                 seen_router_ids.add(router_id)
            #                 # Load balancer íŒŒë¼ë¯¸í„°
            #                 if hasattr(router_module, 'load_balancer'):
            #                     for param_name, param in router_module.load_balancer.named_parameters(recurse=True):
            #                         if not param.requires_grad:
            #                             param.requires_grad_(True)
            #                             additional_fixed += 1
            #                             logger.info(f"  âœ“ Set requires_grad=True: {name}.load_balancer.{param_name}")
            #                 
            #                 # Expression projector íŒŒë¼ë¯¸í„°
            #                 if hasattr(router_module, 'expression_projector'):
            #                     expr_proj = router_module.expression_projector
            #                     for param_name, param in expr_proj.named_parameters(recurse=True):
            #                         if not param.requires_grad:
            #                             param.requires_grad_(True)
            #                             additional_fixed += 1
            #                             logger.info(f"  âœ“ Set requires_grad=True: {name}.expression_projector.{param_name}")
            #                     
            #                     # linear_projectionì´ ë³„ë„ë¡œ ìˆëŠ” ê²½ìš°
            #                     if hasattr(expr_proj, 'linear_projection'):
            #                         for param_name, param in expr_proj.linear_projection.named_parameters(recurse=True):
            #                             if not param.requires_grad:
            #                                 param.requires_grad_(True)
            #                                 additional_fixed += 1
            #                                 logger.info(f"  âœ“ Set requires_grad=True: {name}.expression_projector.linear_projection.{param_name}")
            #         
            #         if additional_fixed > 0:
            #             logger.info(f"âœ… Fixed additional {additional_fixed} router parameters: set requires_grad=True")
            #         
            #         # ì¬ê²€ì¦
            #         validation_result = self._ensure_router_in_optimizer(trainer, model)
            
            # Optimizerì— ì—†ëŠ” íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if not validation_result['all_in_optimizer']:
                missing = validation_result['missing_from_optimizer']
                logger.warning(f"âš ï¸ {len(missing)} router parameters are not in optimizer - adding to optimizer...")
                
                # ì‹¤ì œ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ì„œ optimizerì— ì¶”ê°€
                actual_model = model
                if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
                    actual_model = model.module
                
                if actual_model is not None and hasattr(trainer, 'optimizer') and trainer.optimizer is not None:
                    from models.spectra_model import SPECTRARouter
                    try:
                        from models.g3moe_model import G3MoERouter, G3MoEGRINMoE
                    except ImportError:
                        G3MoERouter = None
                        G3MoEGRINMoE = None
                    
                    optimizer_param_ids = {id(p) for group in trainer.optimizer.param_groups for p in group['params']}
                    missing_params = []
                    seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
                    
                    for name, module in actual_model.named_modules():
                        is_router = False
                        router_module = None
                        
                        # 1. PEFT ModulesToSaveWrapper ì²´í¬ (ê°€ì¥ ì¤‘ìš”)
                        if ModulesToSaveWrapper is not None and isinstance(module, ModulesToSaveWrapper):
                            active_adapter = getattr(module, "active_adapter", "default")
                            if hasattr(module, "modules_to_save") and active_adapter in module.modules_to_save:
                                inner_module = module.modules_to_save[active_adapter]
                                if isinstance(inner_module, SPECTRARouter):
                                    is_router = True
                                    router_module = inner_module
                                elif G3MoERouter is not None and isinstance(inner_module, G3MoERouter):
                                    is_router = True
                                    router_module = inner_module
                        
                        # 2. SPECTRARouter ì²´í¬
                        elif isinstance(module, SPECTRARouter):
                            is_router = True
                            router_module = module
                        # 3. G3MoERouter ì§ì ‘ ì²´í¬
                        elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                            is_router = True
                            router_module = module
                        # 4. G3MoEGRINMoE ë‚´ë¶€ì˜ router ì†ì„± ì²´í¬
                        elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                            if hasattr(module, 'router'):
                                potential_router = module.router
                                
                                if ModulesToSaveWrapper is not None and isinstance(potential_router, ModulesToSaveWrapper):
                                    active_adapter = getattr(potential_router, "active_adapter", "default")
                                    if hasattr(potential_router, "modules_to_save") and active_adapter in potential_router.modules_to_save:
                                        inner_module = potential_router.modules_to_save[active_adapter]
                                        if isinstance(inner_module, G3MoERouter):
                                            is_router = True
                                            router_module = inner_module
                                            name = f"{name}.router"
                                
                                elif isinstance(potential_router, G3MoERouter):
                                    is_router = True
                                    router_module = potential_router
                                    name = f"{name}.router"
                                    
                        # 5. ì¼ë°˜ì ì¸ router êµ¬ì¡° ì²´í¬ (load_balancer + expression_projector)
                        elif hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
                            is_router = True
                            router_module = module
                        
                        if is_router and router_module is not None:
                            # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
                            router_id = id(router_module)
                            if router_id in seen_router_ids:
                                continue
                            seen_router_ids.add(router_id)
                            # Load balancer íŒŒë¼ë¯¸í„°
                            if hasattr(router_module, 'load_balancer'):
                                for param_name, param in router_module.load_balancer.named_parameters(recurse=True):
                                    if param.requires_grad and id(param) not in optimizer_param_ids:
                                        missing_params.append(param)
                            
                            # Expression projector íŒŒë¼ë¯¸í„°
                            if hasattr(router_module, 'expression_projector'):
                                expr_proj = router_module.expression_projector
                                for param_name, param in expr_proj.named_parameters(recurse=True):
                                    if param.requires_grad and id(param) not in optimizer_param_ids:
                                        missing_params.append(param)
                                
                                # linear_projectionì´ ë³„ë„ë¡œ ìˆëŠ” ê²½ìš°
                                if hasattr(expr_proj, 'linear_projection'):
                                    for param_name, param in expr_proj.linear_projection.named_parameters(recurse=True):
                                        if param.requires_grad and id(param) not in optimizer_param_ids:
                                            missing_params.append(param)
                    
                    if missing_params and len(trainer.optimizer.param_groups) > 0:
                        trainer.optimizer.param_groups[0]['params'].extend(missing_params)
                        logger.info(f"  âœ“ Added {len(missing_params)} router parameters to optimizer param_groups[0]")
                        
                        # ì¬ê²€ì¦
                        validation_result = self._ensure_router_in_optimizer(trainer, model)
            
            # ìµœì¢… ê²€ì¦ (trainerê°€ ìˆì„ ë•Œë§Œ)
            if not validation_result['all_trainable']:
                logger.warning(
                    f"âš ï¸ {len(validation_result['non_trainable_params'])} router parameters still not trainable "
                    f"after attempts to fix. This may cause training issues."
                )
            
            if not validation_result['all_in_optimizer']:
                logger.warning(
                    f"âš ï¸ {len(validation_result['missing_from_optimizer'])} router parameters still not in optimizer "
                    f"after attempts to add. This may cause training issues."
                )
        
        logger.info("=" * 80)
        logger.info("âœ… Router parameter setup complete - all router parameters set to requires_grad=True")
        if trainer is not None:
            logger.info("âœ… Router validation passed - all router parameters are trainable and in optimizer")
        logger.info("=" * 80)
        
        # Optimizerì— ë“±ë¡ëœ íŒŒë¼ë¯¸í„° í™•ì¸ ë° ë¡œê¹… (train_SPECTRA.py í˜•ì‹ ìœ ì§€)
        if trainer is not None:
            logger.info("=" * 80)
            logger.info("ğŸ” Checking parameters registered in optimizer...")
            logger.info("=" * 80)
            
            # Optimizerì—ì„œ íŒŒë¼ë¯¸í„° ID ìˆ˜ì§‘ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
            optimizer_param_ids = set()
            optimizer_source = None
            
            # 1. ì¼ë°˜ optimizer í™•ì¸
            if hasattr(trainer, 'optimizer') and trainer.optimizer is not None:
                try:
                    optimizer_param_ids = {id(p) for group in trainer.optimizer.param_groups for p in group['params']}
                    optimizer_source = "trainer.optimizer"
                    logger.info(f"âœ… Found optimizer: trainer.optimizer with {len(optimizer_param_ids)} parameters")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to get params from trainer.optimizer: {e}")
            
            # 2. DeepSpeed optimizer í™•ì¸
            if not optimizer_param_ids and hasattr(trainer, 'deepspeed') and trainer.deepspeed is not None:
                if hasattr(trainer.deepspeed, 'optimizer') and trainer.deepspeed.optimizer is not None:
                    try:
                        optimizer_param_ids = {id(p) for group in trainer.deepspeed.optimizer.param_groups for p in group['params']}
                        optimizer_source = "trainer.deepspeed.optimizer"
                        logger.info(f"âœ… Found optimizer: trainer.deepspeed.optimizer with {len(optimizer_param_ids)} parameters")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to get params from trainer.deepspeed.optimizer: {e}")
            
            if optimizer_param_ids:
                logger.info(f"âœ… Total {len(optimizer_param_ids)} parameters in optimizer (source: {optimizer_source})")
                
                # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ìˆœíšŒí•˜ë©´ì„œ optimizerì— ë“±ë¡ëœ ê²ƒë§Œ ë¡œê¹… (train_SPECTRA.py í˜•ì‹)
                actual_model = model
                if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
                    actual_model = model.module
                
                if actual_model is not None:
                    optimizer_params_logged = 0
                    for name, param in actual_model.named_parameters():
                        # train_SPECTRA.pyì˜ í•„í„°ë§ ì¡°ê±´ ìœ ì§€
                        if param.requires_grad and not any([keyword for keyword in ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"] if keyword in name]):
                            # Optimizerì— ë“±ë¡ëœ íŒŒë¼ë¯¸í„°ë§Œ ë¡œê¹…
                            if id(param) in optimizer_param_ids:
                                logger.info(f"Trainable Layer: {name} | Shape: {param.shape} | In Optimizer: âœ“")
                                optimizer_params_logged += 1
                    
                    logger.info(f"âœ… Logged {optimizer_params_logged} trainable parameters that are in optimizer (excluding q/k/v/o/gate/up/down_proj)")
                    
                    # Router íŒŒë¼ë¯¸í„°ë§Œ ë³„ë„ë¡œ í™•ì¸ ë° ê²€ì¦
                    from models.spectra_model import SPECTRARouter
                    try:
                        from models.g3moe_model import G3MoERouter, G3MoEGRINMoE
                    except ImportError:
                        G3MoERouter = None
                        G3MoEGRINMoE = None
                    
                    router_params_in_optimizer = 0
                    router_params_not_in_optimizer = 0
                    router_params_list = []
                    router_params_not_in_optimizer_list = []
                    seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
                    
                    for name, module in actual_model.named_modules():
                        is_router = False
                        router_module = None
                        
                        if isinstance(module, SPECTRARouter):
                            is_router = True
                            router_module = module
                        elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                            is_router = True
                            router_module = module
                        elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                            if hasattr(module, 'router') and isinstance(module.router, G3MoERouter):
                                is_router = True
                                router_module = module.router
                                name = f"{name}.router"
                        elif hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
                            is_router = True
                            router_module = module
                        
                        if is_router and router_module is not None:
                            # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
                            router_id = id(router_module)
                            if router_id in seen_router_ids:
                                continue
                            seen_router_ids.add(router_id)
                            
                            # Router ëª¨ë“ˆì˜ ëª¨ë“  íŒŒë¼ë¯¸í„° í™•ì¸
                            for param_name, param in router_module.named_parameters(recurse=True):
                                full_name = f"{name}.{param_name}"
                                if param.requires_grad:
                                    param_id = id(param)
                                    if param_id in optimizer_param_ids:
                                        router_params_in_optimizer += 1
                                        router_params_list.append(full_name)
                                        logger.info(f"Trainable Layer: {full_name} | Shape: {param.shape} | In Optimizer: âœ“ | param_id={param_id}")
                                    else:
                                        router_params_not_in_optimizer += 1
                                        router_params_not_in_optimizer_list.append(full_name)
                                        logger.warning(f"Trainable Layer: {full_name} | Shape: {param.shape} | In Optimizer: âœ— | param_id={param_id}")
                    
                    # Router íŒŒë¼ë¯¸í„° ê²€ì¦ ê²°ê³¼ ìš”ì•½
                    logger.info("=" * 80)
                    logger.info(f"ğŸ“Š Router Parameters Optimizer Registration Summary:")
                    logger.info(f"   âœ… In optimizer: {router_params_in_optimizer}")
                    logger.info(f"   âŒ NOT in optimizer: {router_params_not_in_optimizer}")
                    
                    if router_params_in_optimizer > 0:
                        logger.info(f"   Router params in optimizer (first 10):")
                        for param_name in router_params_list[:10]:
                            logger.info(f"     âœ“ {param_name}")
                        if len(router_params_list) > 10:
                            logger.info(f"     ... and {len(router_params_list) - 10} more")
                    
                    if router_params_not_in_optimizer > 0:
                        logger.warning(f"   âš ï¸ Router params NOT in optimizer (first 10):")
                        for param_name in router_params_not_in_optimizer_list[:10]:
                            logger.warning(f"     âœ— {param_name}")
                        if len(router_params_not_in_optimizer_list) > 10:
                            logger.warning(f"     ... and {len(router_params_not_in_optimizer_list) - 10} more")
                        
                        # CRITICAL: Optimizerì— ì—†ëŠ” router íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ê²½ê³ 
                        logger.error(f"âŒ CRITICAL: {router_params_not_in_optimizer} router parameters are NOT in optimizer!")
                        logger.error(f"   This means these parameters will NOT be updated during training!")
                        logger.error(f"   Router will NOT learn if parameters are not in optimizer!")
                    else:
                        logger.info(f"âœ… All router parameters are in optimizer!")
                    
                    logger.info("=" * 80)
            else:
                logger.warning("âš ï¸ Optimizer not yet initialized - cannot check registered parameters")
                logger.warning("   DeepSpeed lazy initialization: optimizer will be checked at first step (step 0 or 1)")
                logger.warning("   This is normal for DeepSpeed - optimizer initializes after first forward pass")
                self._optimizer_validation_done = False  # ë‚˜ì¤‘ì— ë‹¤ì‹œ í™•ì¸ í•„ìš”

            logger.info("=" * 80)
        
        logger.info(f"âœ… RouterWeightTrackingCallback active - will track router weights every {self.log_every_n_steps} steps")
        if self.check_weight_change:
            logger.info(f"   Weight change checking will start after step {self.check_after_steps}")
        
        # Router forward hook ë“±ë¡ (ì‹¤ì œë¡œ ì‚¬ìš©ëœ router ì¶”ì )
        if actual_model is not None:
            self._register_router_forward_hooks(actual_model)
        
        return control
    
    def _register_router_forward_hooks(self, model):
        """ëª¨ë“  router ëª¨ë“ˆì— forward hookì„ ë“±ë¡í•˜ì—¬ ì‹¤ì œ ì‚¬ìš© ì—¬ë¶€ ë° ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” weight ì¶”ì """
        from models.spectra_model import SPECTRARouter
        try:
            from models.g3moe_model import G3MoERouter, G3MoEGRINMoE
        except ImportError:
            G3MoERouter = None
            G3MoEGRINMoE = None
        
        router_count = 0
        seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì  (G3MoEì—ì„œ global_routerì™€ layers[i].moe.routerê°€ ê°™ì€ ì¸ìŠ¤í„´ìŠ¤)
        
        for name, module in model.named_modules():
            is_router = False
            router_module = None
            wrapper_module = None  # ModulesToSaveWrapperì¸ ê²½ìš° wrapper ìì²´ ì €ì¥
            active_adapter = "default"
            
            # 1. PEFT ModulesToSaveWrapper ì²´í¬ (ê°€ì¥ ì¤‘ìš”)
            if ModulesToSaveWrapper is not None and isinstance(module, ModulesToSaveWrapper):
                # Wrapper ë‚´ë¶€ì˜ ì‹¤ì œ í•™ìŠµ ëª¨ë“ˆ('default' ë˜ëŠ” active_adapter) í™•ì¸
                active_adapter = getattr(module, "active_adapter", "default")
                if hasattr(module, "modules_to_save") and active_adapter in module.modules_to_save:
                    inner_module = module.modules_to_save[active_adapter]
                    
                    # ë‚´ë¶€ ëª¨ë“ˆì´ Routerì¸ì§€ í™•ì¸
                    if isinstance(inner_module, SPECTRARouter):
                        is_router = True
                        router_module = inner_module
                        wrapper_module = module  # Wrapper ìì²´ ì €ì¥
                        logger.debug(f"âœ… Found PEFT wrapped router: {name} (adapter: {active_adapter})")
                    elif G3MoERouter is not None and isinstance(inner_module, G3MoERouter):
                        is_router = True
                        router_module = inner_module
                        wrapper_module = module  # Wrapper ìì²´ ì €ì¥
                        logger.debug(f"âœ… Found PEFT wrapped G3MoE router: {name} (adapter: {active_adapter})")
            
            # 2. SPECTRARouter ì²´í¬
            elif isinstance(module, SPECTRARouter):
                is_router = True
                router_module = module
            # 3. G3MoERouter ì§ì ‘ ì²´í¬
            elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                is_router = True
                router_module = module
            # 4. G3MoEGRINMoE ë‚´ë¶€ì˜ router ì†ì„± ì²´í¬
            elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                # PEFTë¡œ ë˜í•‘ëœ routerì¼ ìˆ˜ ìˆìŒ
                if hasattr(module, 'router'):
                    potential_router = module.router
                    
                    # PEFT Wrapperì¸ì§€ í™•ì¸
                    if ModulesToSaveWrapper is not None and isinstance(potential_router, ModulesToSaveWrapper):
                        active_adapter = getattr(potential_router, "active_adapter", "default")
                        if hasattr(potential_router, "modules_to_save") and active_adapter in potential_router.modules_to_save:
                            inner_module = potential_router.modules_to_save[active_adapter]
                            if isinstance(inner_module, G3MoERouter):
                                is_router = True
                                router_module = inner_module
                                wrapper_module = potential_router  # Wrapper ìì²´ ì €ì¥
                                name = f"{name}.router"
                                logger.debug(f"âœ… Found PEFT wrapped nested router in G3MoEGRINMoE: {name}")
                    
                    # ì¼ë°˜ Routerì¸ì§€ í™•ì¸
                    elif isinstance(potential_router, G3MoERouter):
                        is_router = True
                        router_module = potential_router
                        # ì´ë¦„ì„ moe.routerë¡œ ë³€ê²½í•˜ì—¬ ì¶”ì 
                        name = f"{name}.router"
                        
            # 5. ì¼ë°˜ì ì¸ router êµ¬ì¡° ì²´í¬ (load_balancer + expression_projector)
            elif hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
                is_router = True
                router_module = module
            
            if is_router and router_module is not None:
                # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬ (G3MoEì—ì„œ global_routerì™€ layers[i].moe.routerê°€ ê°™ì€ ì¸ìŠ¤í„´ìŠ¤)
                router_id = id(router_module)
                if router_id in seen_router_ids:
                    if self.verbose:
                        logger.debug(f"â­ï¸ Skipping duplicate router instance: {name} (already processed, router_id={router_id})")
                    continue
                seen_router_ids.add(router_id)
                router_count += 1
                
                # Hookì„ ë“±ë¡í•  ëª¨ë“ˆ ê²°ì •: wrapperê°€ ìˆìœ¼ë©´ wrapperì—, ì—†ìœ¼ë©´ router_moduleì—
                hook_target_module = wrapper_module if wrapper_module is not None else router_module
                
                # Router ëª¨ë“ˆì— ëŒ€í•œ forward hook
                def make_router_forward_hook(router_name, inner_router_module, wrapper_mod, adapter_name):
                    def router_forward_hook(hooked_module, input, output):
                        # í˜„ì¬ step ê°€ì ¸ì˜¤ê¸° (trainerì—ì„œ)
                        current_step = None
                        if hasattr(self, '_last_trainer') and self._last_trainer is not None:
                            if hasattr(self._last_trainer, 'state') and self._last_trainer.state is not None:
                                current_step = self._last_trainer.state.global_step
                        
                        if current_step is not None:
                            if current_step not in self._router_forward_tracker:
                                self._router_forward_tracker[current_step] = []
                            
                            # ì¤‘ë³µ ë°©ì§€
                            if router_name not in self._router_forward_tracker[current_step]:
                                self._router_forward_tracker[current_step].append(router_name)
                                
                                # Input shape ì •ë³´ë„ ê¸°ë¡
                                input_shape = None
                                if input is not None and len(input) > 0:
                                    if isinstance(input[0], torch.Tensor):
                                        input_shape = list(input[0].shape)
                                
                                if self.verbose and current_step <= 5:
                                    logger.info(f"ğŸ” Router forward called: {router_name} at step {current_step} | input_shape={input_shape}")
                    
                    return router_forward_hook
                
                hook = hook_target_module.register_forward_hook(
                    make_router_forward_hook(name, router_module, wrapper_module, active_adapter)
                )
                self._router_hooks.append((name, hook))
                
                # Expression projectorì˜ linear_projectionì— ëŒ€í•œ forward hook (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” weight ì¶”ì )
                if hasattr(router_module, 'expression_projector'):
                    expr_proj = router_module.expression_projector
                    if hasattr(expr_proj, 'linear_projection'):
                        lin_proj = expr_proj.linear_projection
                        
                        # linear_projectionë„ ModulesToSaveWrapperë¡œ ë˜í•‘ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
                        lin_proj_wrapper = None
                        lin_proj_inner = None
                        if ModulesToSaveWrapper is not None and isinstance(lin_proj, ModulesToSaveWrapper):
                            lin_proj_adapter = getattr(lin_proj, "active_adapter", "default")
                            if hasattr(lin_proj, "modules_to_save") and lin_proj_adapter in lin_proj.modules_to_save:
                                lin_proj_inner = lin_proj.modules_to_save[lin_proj_adapter]
                                lin_proj_wrapper = lin_proj
                                logger.debug(f"  âœ… Found PEFT wrapped linear_projection: {name}.expression_projector.linear_projection (adapter: {lin_proj_adapter})")
                        
                        # Hookì„ ë“±ë¡í•  ëª¨ë“ˆ ê²°ì •
                        lin_proj_hook_target = lin_proj_wrapper if lin_proj_wrapper is not None else lin_proj
                        
                        def make_linear_projection_hook(router_name, inner_lin_proj, wrapper_lin_proj, adapter_name):
                            def linear_projection_hook(hooked_module, input, output):
                                # í˜„ì¬ step ê°€ì ¸ì˜¤ê¸°
                                current_step = None
                                if hasattr(self, '_last_trainer') and self._last_trainer is not None:
                                    if hasattr(self._last_trainer, 'state') and self._last_trainer.state is not None:
                                        current_step = self._last_trainer.state.global_step
                                
                                if current_step is not None:
                                    # ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” weight ì¶”ì 
                                    # Wrapperì¸ ê²½ìš° modules_to_save.defaultì—ì„œ weight ì¶”ì¶œ
                                    actual_weight = None
                                    if wrapper_lin_proj is not None:
                                        # ModulesToSaveWrapperì¸ ê²½ìš°, modules_to_save.defaultì˜ weight ì‚¬ìš©
                                        if hasattr(wrapper_lin_proj, "modules_to_save") and adapter_name in wrapper_lin_proj.modules_to_save:
                                            inner_mod = wrapper_lin_proj.modules_to_save[adapter_name]
                                            if hasattr(inner_mod, 'weight'):
                                                actual_weight = inner_mod.weight
                                    elif inner_lin_proj is not None and hasattr(inner_lin_proj, 'weight'):
                                        actual_weight = inner_lin_proj.weight
                                    elif hasattr(hooked_module, 'weight'):
                                        actual_weight = hooked_module.weight
                                    
                                    if actual_weight is not None:
                                        # router_nameì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ weight ì €ì¥
                                        if router_name not in self._actual_router_weights:
                                            self._actual_router_weights[router_name] = {}
                                        
                                        # í˜„ì¬ stepì˜ weight ì €ì¥ (detach & clone)
                                        self._actual_router_weights[router_name][current_step] = actual_weight.detach().clone()
                                        
                                        if self.verbose and current_step <= 5:
                                            param_id = id(actual_weight)
                                            logger.info(f"ğŸ” Actual weight tracked: {router_name}.expression_projector.linear_projection.weight at step {current_step} | param_id={param_id} | shape={actual_weight.shape} | wrapper={wrapper_lin_proj is not None}")
                            
                            return linear_projection_hook
                        
                        lin_proj_hook = lin_proj_hook_target.register_forward_hook(
                            make_linear_projection_hook(name, lin_proj_inner, lin_proj_wrapper, active_adapter if lin_proj_wrapper is None else getattr(lin_proj_wrapper, "active_adapter", "default"))
                        )
                        self._router_hooks.append((f"{name}.expression_projector.linear_projection", lin_proj_hook))
        
        if router_count > 0:
            logger.info(f"âœ… Registered forward hooks on {router_count} router modules - will track which routers are used in forward pass and actual weights")
        else:
            logger.warning("âš ï¸ No router modules found for forward hook registration")
    
    def on_step_begin(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """
        ê° training step ì‹œì‘ ì „ì— í˜¸ì¶œ
        DeepSpeed lazy initialization ëŒ€ì‘: optimizerê°€ ì´ˆê¸°í™”ëœ í›„ì— ê²€ì¦ ìˆ˜í–‰
        """
        trainer = kwargs.get('trainer')
        if trainer is not None:
            self._last_trainer = trainer
        
        # Optimizer ê²€ì¦ì´ ì•„ì§ ì•ˆ ë˜ì—ˆê³ , ì²« ë²ˆì§¸ step (0 ë˜ëŠ” 1)ì—ì„œ optimizerê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not self._optimizer_validation_done and state.global_step <= 1:
            # Optimizerê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            optimizer_available = False
            optimizer_source = None
            
            if trainer is not None:
                # 1. ì¼ë°˜ optimizer í™•ì¸
                if hasattr(trainer, 'optimizer') and trainer.optimizer is not None:
                    try:
                        param_groups = trainer.optimizer.param_groups
                        if param_groups and len(param_groups) > 0 and len(param_groups[0]['params']) > 0:
                            optimizer_available = True
                            optimizer_source = "trainer.optimizer"
                    except Exception:
                        pass
                
                # 2. DeepSpeed optimizer í™•ì¸
                if not optimizer_available and hasattr(trainer, 'deepspeed') and trainer.deepspeed is not None:
                    if hasattr(trainer.deepspeed, 'optimizer') and trainer.deepspeed.optimizer is not None:
                        try:
                            param_groups = trainer.deepspeed.optimizer.param_groups
                            if param_groups and len(param_groups) > 0 and len(param_groups[0]['params']) > 0:
                                optimizer_available = True
                                optimizer_source = "trainer.deepspeed.optimizer"
                        except Exception:
                            pass
            
            # Optimizerê°€ ì´ˆê¸°í™”ë˜ì—ˆìœ¼ë©´ ê²€ì¦ ìˆ˜í–‰
            if optimizer_available:
                logger.info("=" * 80)
                logger.info(f"ğŸ” Optimizer initialized! (source: {optimizer_source}, step: {state.global_step})")
                logger.info("   Performing router parameter optimizer registration check...")
                logger.info("=" * 80)
                
                # on_train_beginì˜ optimizer ê²€ì¦ ë¡œì§ ì¬ì‚¬ìš©
                if trainer is not None and model is not None:
                    # Optimizerì—ì„œ íŒŒë¼ë¯¸í„° ID ìˆ˜ì§‘
                    optimizer_param_ids = set()
                    
                    if optimizer_source == "trainer.optimizer":
                        try:
                            optimizer_param_ids = {id(p) for group in trainer.optimizer.param_groups for p in group['params']}
                        except Exception as e:
                            logger.warning(f"âš ï¸ Failed to get params from trainer.optimizer: {e}")
                    elif optimizer_source == "trainer.deepspeed.optimizer":
                        try:
                            optimizer_param_ids = {id(p) for group in trainer.deepspeed.optimizer.param_groups for p in group['params']}
                        except Exception as e:
                            logger.warning(f"âš ï¸ Failed to get params from trainer.deepspeed.optimizer: {e}")
                    
                    if optimizer_param_ids:
                        logger.info(f"âœ… Found {len(optimizer_param_ids)} parameters in optimizer")
                        
                        actual_model = model
                        if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
                            actual_model = model.module
                        
                        if actual_model is not None:
                            from models.spectra_model import SPECTRARouter
                            try:
                                from models.g3moe_model import G3MoERouter
                            except ImportError:
                                G3MoERouter = None
                            
                            router_params_in_optimizer = 0
                            router_params_not_in_optimizer = 0
                            router_params_list = []
                            router_params_not_in_optimizer_list = []
                            
                            try:
                                from models.g3moe_model import G3MoEGRINMoE
                            except ImportError:
                                G3MoEGRINMoE = None
                            
                            seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
                            
                            for name, module in actual_model.named_modules():
                                is_router = False
                                router_module = None
                                
                                # 1. PEFT ModulesToSaveWrapper ì²´í¬ (ê°€ì¥ ì¤‘ìš”)
                                if ModulesToSaveWrapper is not None and isinstance(module, ModulesToSaveWrapper):
                                    active_adapter = getattr(module, "active_adapter", "default")
                                    if hasattr(module, "modules_to_save") and active_adapter in module.modules_to_save:
                                        inner_module = module.modules_to_save[active_adapter]
                                        if isinstance(inner_module, SPECTRARouter):
                                            is_router = True
                                            router_module = inner_module
                                        elif G3MoERouter is not None and isinstance(inner_module, G3MoERouter):
                                            is_router = True
                                            router_module = inner_module
                                
                                # 2. SPECTRARouter ì²´í¬
                                elif isinstance(module, SPECTRARouter):
                                    is_router = True
                                    router_module = module
                                # 3. G3MoERouter ì§ì ‘ ì²´í¬
                                elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                                    is_router = True
                                    router_module = module
                                # 4. G3MoEGRINMoE ë‚´ë¶€ì˜ router ì†ì„± ì²´í¬
                                elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                                    # PEFTë¡œ ë˜í•‘ëœ routerì¼ ìˆ˜ ìˆìŒ
                                    if hasattr(module, 'router'):
                                        potential_router = module.router
                                        
                                        if ModulesToSaveWrapper is not None and isinstance(potential_router, ModulesToSaveWrapper):
                                            active_adapter = getattr(potential_router, "active_adapter", "default")
                                            if hasattr(potential_router, "modules_to_save") and active_adapter in potential_router.modules_to_save:
                                                inner_module = potential_router.modules_to_save[active_adapter]
                                                if isinstance(inner_module, G3MoERouter):
                                                    is_router = True
                                                    router_module = inner_module
                                                    name = f"{name}.router"
                                        
                                        elif isinstance(potential_router, G3MoERouter):
                                            is_router = True
                                            router_module = potential_router
                                            name = f"{name}.router"
                                            
                                # 5. ì¼ë°˜ì ì¸ router êµ¬ì¡° ì²´í¬ (load_balancer + expression_projector)
                                elif hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
                                    is_router = True
                                    router_module = module
                                
                                if is_router and router_module is not None:
                                    # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
                                    router_id = id(router_module)
                                    if router_id in seen_router_ids:
                                        continue
                                    seen_router_ids.add(router_id)
                                    
                                    for param_name, param in router_module.named_parameters(recurse=True):
                                        full_name = f"{name}.{param_name}"
                                        if param.requires_grad:
                                            param_id = id(param)
                                            if param_id in optimizer_param_ids:
                                                router_params_in_optimizer += 1
                                                router_params_list.append(full_name)
                                                logger.info(f"Trainable Layer: {full_name} | Shape: {param.shape} | In Optimizer: âœ“ | param_id={param_id}")
                                            else:
                                                router_params_not_in_optimizer += 1
                                                router_params_not_in_optimizer_list.append(full_name)
                                                logger.warning(f"Trainable Layer: {full_name} | Shape: {param.shape} | In Optimizer: âœ— | param_id={param_id}")
                            
                            # Router íŒŒë¼ë¯¸í„° ê²€ì¦ ê²°ê³¼ ìš”ì•½
                            logger.info("=" * 80)
                            logger.info(f"ğŸ“Š Router Parameters Optimizer Registration Summary (at step {state.global_step}):")
                            logger.info(f"   âœ… In optimizer: {router_params_in_optimizer}")
                            logger.info(f"   âŒ NOT in optimizer: {router_params_not_in_optimizer}")
                            
                            if router_params_in_optimizer > 0:
                                logger.info(f"   Router params in optimizer (first 10):")
                                for param_name in router_params_list[:10]:
                                    logger.info(f"     âœ“ {param_name}")
                                if len(router_params_list) > 10:
                                    logger.info(f"     ... and {len(router_params_list) - 10} more")
                            
                            if router_params_not_in_optimizer > 0:
                                logger.warning(f"   âš ï¸ Router params NOT in optimizer (first 10):")
                                for param_name in router_params_not_in_optimizer_list[:10]:
                                    logger.warning(f"     âœ— {param_name}")
                                if len(router_params_not_in_optimizer_list) > 10:
                                    logger.warning(f"     ... and {len(router_params_not_in_optimizer_list) - 10} more")
                                
                                # CRITICAL: Optimizerì— ì—†ëŠ” router íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ê²½ê³ 
                                logger.error(f"âŒ CRITICAL: {router_params_not_in_optimizer} router parameters are NOT in optimizer!")
                                logger.error(f"   This means these parameters will NOT be updated during training!")
                                logger.error(f"   Router will NOT learn if parameters are not in optimizer!")
                            else:
                                logger.info(f"âœ… All router parameters are in optimizer!")
                            
                            logger.info("=" * 80)
                
                self._optimizer_validation_done = True
            elif state.global_step == 1:
                # Step 1ê¹Œì§€ optimizerê°€ ì—†ìœ¼ë©´ ê²½ê³ 
                logger.warning(f"âš ï¸ Optimizer still not initialized at step {state.global_step}")
                logger.warning("   This may indicate an issue with optimizer initialization")
                logger.warning("   Will continue checking, but router parameters may not be in optimizer")
        
        return control
    
    def _ensure_router_in_optimizer(self, trainer, model):
        """
        Router íŒŒë¼ë¯¸í„°ê°€ ì˜¬ë°”ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥í•œì§€ ê²€ì¦
        ë°˜í™˜ê°’: dict with keys:
            - has_routers: bool
            - all_trainable: bool
            - all_in_optimizer: bool
            - non_trainable_params: list[str]
            - missing_from_optimizer: list[str]
        """
        result = {
            'has_routers': False,
            'all_trainable': False,
            'all_in_optimizer': False,
            'non_trainable_params': [],
            'missing_from_optimizer': []
        }
        
        try:
            from models.spectra_model import SPECTRARouter
            from models.spectra_model import ExpressionProjector
            try:
                from models.g3moe_model import G3MoERouter
            except ImportError:
                G3MoERouter = None
            
            # ëª¨ë¸ ì¶”ì¶œ (DeepSpeed ë˜í•‘ ì²˜ë¦¬)
            actual_model = model
            if hasattr(model, 'module'):  # DeepSpeed ë˜í•‘
                actual_model = model.module
            
            if actual_model is None:
                logger.error("âŒ Model is None, cannot validate router in optimizer")
                return result
            
            router_params = []
            router_param_names = []
            expression_projector_params = []
            load_balancer_params = []
            seen_param_ids = set()  # ì¤‘ë³µ ë°©ì§€
            
            # G3MoEGRINMoE import ì¶”ê°€
            try:
                from models.g3moe_model import G3MoEGRINMoE
            except ImportError:
                G3MoEGRINMoE = None
            
            # ëª¨ë“  router íŒŒë¼ë¯¸í„° ì°¾ê¸° (ê²€ì¦ìš©)
            seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì  (G3MoEì—ì„œ global_routerì™€ layers[i].moe.routerê°€ ê°™ì€ ì¸ìŠ¤í„´ìŠ¤)
            
            for name, module in actual_model.named_modules():
                is_router = False
                router_module = None
                
                # 1. PEFT ModulesToSaveWrapper ì²´í¬ (ê°€ì¥ ì¤‘ìš”)
                if ModulesToSaveWrapper is not None and isinstance(module, ModulesToSaveWrapper):
                    active_adapter = getattr(module, "active_adapter", "default")
                    if hasattr(module, "modules_to_save") and active_adapter in module.modules_to_save:
                        inner_module = module.modules_to_save[active_adapter]
                        if isinstance(inner_module, SPECTRARouter):
                            is_router = True
                            router_module = inner_module
                        elif G3MoERouter is not None and isinstance(inner_module, G3MoERouter):
                            is_router = True
                            router_module = inner_module
                            
                # 2. SPECTRARouter ì²´í¬
                elif isinstance(module, SPECTRARouter):
                    is_router = True
                    router_module = module
                # 3. G3MoERouter ì§ì ‘ ì²´í¬
                elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                    is_router = True
                    router_module = module
                # 4. G3MoEGRINMoE ë‚´ë¶€ì˜ router ì†ì„± ì²´í¬
                elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                    if hasattr(module, 'router'):
                        potential_router = module.router
                        
                        if ModulesToSaveWrapper is not None and isinstance(potential_router, ModulesToSaveWrapper):
                            active_adapter = getattr(potential_router, "active_adapter", "default")
                            if hasattr(potential_router, "modules_to_save") and active_adapter in potential_router.modules_to_save:
                                inner_module = potential_router.modules_to_save[active_adapter]
                                if isinstance(inner_module, G3MoERouter):
                                    is_router = True
                                    router_module = inner_module
                                    name = f"{name}.router"
                        
                        elif isinstance(potential_router, G3MoERouter):
                            is_router = True
                            router_module = potential_router
                            name = f"{name}.router"
                            
                # 5. ì¼ë°˜ì ì¸ router êµ¬ì¡° ì²´í¬ (load_balancer + expression_projector)
                elif hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
                    is_router = True
                    router_module = module
                
                if is_router and router_module is not None:
                    # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬ (G3MoEì—ì„œ global_routerì™€ layers[i].moe.routerê°€ ê°™ì€ ì¸ìŠ¤í„´ìŠ¤)
                    router_id = id(router_module)
                    if router_id in seen_router_ids:
                        continue  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ëŠ” ìŠ¤í‚µ
                    seen_router_ids.add(router_id)
                    
                    logger.info(f"âœ… Found router: {name} (router_id={router_id})")
                    # Load balancer íŒŒë¼ë¯¸í„°
                    if hasattr(router_module, 'load_balancer'):
                        for param_name, param in router_module.load_balancer.named_parameters(recurse=True):
                            param_id = id(param)
                            if param_id not in seen_param_ids:
                                router_params.append(param)
                                load_balancer_params.append(param)
                                full_name = f"{name}.load_balancer.{param_name}"
                                router_param_names.append(full_name)
                                seen_param_ids.add(param_id)
                    
                    # Expression projector íŒŒë¼ë¯¸í„°
                    if hasattr(router_module, 'expression_projector'):
                        expr_proj = router_module.expression_projector
                        for param_name, param in expr_proj.named_parameters(recurse=True):
                            param_id = id(param)
                            if param_id not in seen_param_ids:
                                router_params.append(param)
                                expression_projector_params.append(param)
                                full_name = f"{name}.expression_projector.{param_name}"
                                router_param_names.append(full_name)
                                seen_param_ids.add(param_id)
                        
                        # linear_projectionì´ ë³„ë„ë¡œ ìˆëŠ” ê²½ìš°
                        if hasattr(expr_proj, 'linear_projection'):
                            lin_proj = expr_proj.linear_projection
                            
                            # ModulesToSaveWrapperë¡œ ë˜í•‘ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            if ModulesToSaveWrapper is not None and isinstance(lin_proj, ModulesToSaveWrapper):
                                # Wrapper ë‚´ë¶€ì˜ ì‹¤ì œ í•™ìŠµ ëª¨ë“ˆì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                                lin_proj_adapter = getattr(lin_proj, "active_adapter", "default")
                                if hasattr(lin_proj, "modules_to_save") and lin_proj_adapter in lin_proj.modules_to_save:
                                    inner_lin_proj = lin_proj.modules_to_save[lin_proj_adapter]
                                    for param_name, param in inner_lin_proj.named_parameters(recurse=True):
                                        param_id = id(param)
                                        if param_id not in seen_param_ids:
                                            router_params.append(param)
                                            expression_projector_params.append(param)
                                            full_name = f"{name}.expression_projector.linear_projection.{param_name}"
                                            router_param_names.append(full_name)
                                            seen_param_ids.add(param_id)
                            else:
                                # ì¼ë°˜ ëª¨ë“ˆì¸ ê²½ìš°
                                for param_name, param in lin_proj.named_parameters(recurse=True):
                                    param_id = id(param)
                                    if param_id not in seen_param_ids:
                                        router_params.append(param)
                                        expression_projector_params.append(param)
                                        full_name = f"{name}.expression_projector.linear_projection.{param_name}"
                                        router_param_names.append(full_name)
                                        seen_param_ids.add(param_id)
            
            if not router_params:
                logger.error("âŒ No router parameters found in model")
                return result
            
            result['has_routers'] = True
            logger.info(f"âœ… Found {len(router_params)} router parameters")
            logger.info(f"   - Load balancer params: {len(load_balancer_params)}")
            logger.info(f"   - Expression projector params: {len(expression_projector_params)}")
            
            # requires_grad ê²€ì¦
            non_trainable = []
            for param, param_name in zip(router_params, router_param_names):
                if not param.requires_grad:
                    non_trainable.append(param_name)
            
            if non_trainable:
                result['non_trainable_params'] = non_trainable
                logger.error(f"âŒ {len(non_trainable)} router parameters have requires_grad=False")
                for param_name in non_trainable[:5]:
                    logger.error(f"   - {param_name}")
                if len(non_trainable) > 5:
                    logger.error(f"   ... and {len(non_trainable) - 5} more")
            else:
                result['all_trainable'] = True
                logger.info(f"âœ… All {len(router_params)} router parameters have requires_grad=True")
            
            # Optimizer í¬í•¨ ì—¬ë¶€ ê²€ì¦ (ì—¬ëŸ¬ ê²½ë¡œ í™•ì¸)
            optimizer_param_ids = set()
            optimizer_source = None
            
            # 1. ì¼ë°˜ optimizer í™•ì¸
            if hasattr(trainer, 'optimizer') and trainer.optimizer is not None:
                try:
                    optimizer_param_ids = {id(p) for group in trainer.optimizer.param_groups for p in group['params']}
                    optimizer_source = "trainer.optimizer"
                    logger.info(f"âœ… Found optimizer: trainer.optimizer with {len(optimizer_param_ids)} parameters")
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to get params from trainer.optimizer: {e}")
            
            # 2. DeepSpeed optimizer í™•ì¸
            if not optimizer_param_ids and hasattr(trainer, 'deepspeed') and trainer.deepspeed is not None:
                if hasattr(trainer.deepspeed, 'optimizer') and trainer.deepspeed.optimizer is not None:
                    try:
                        optimizer_param_ids = {id(p) for group in trainer.deepspeed.optimizer.param_groups for p in group['params']}
                        optimizer_source = "trainer.deepspeed.optimizer"
                        logger.info(f"âœ… Found optimizer: trainer.deepspeed.optimizer with {len(optimizer_param_ids)} parameters")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to get params from trainer.deepspeed.optimizer: {e}")
            
            if optimizer_param_ids:
                router_param_ids = {id(p) for p in router_params}
                in_optimizer = router_param_ids & optimizer_param_ids
                missing_ids = router_param_ids - optimizer_param_ids
                
                missing_names = [name for param, name in zip(router_params, router_param_names) if id(param) in missing_ids]
                
                logger.info(f"ğŸ“Š Optimizer registration check (source: {optimizer_source}):")
                logger.info(f"   Total optimizer params: {len(optimizer_param_ids)}")
                logger.info(f"   Router params: {len(router_params)}")
                logger.info(f"   Router params in optimizer: {len(in_optimizer)}/{len(router_params)}")
                
                if missing_names:
                    result['missing_from_optimizer'] = missing_names
                    logger.error(f"âŒ {len(missing_names)} router parameters are not in optimizer")
                    for param_name in missing_names[:10]:
                        param = next((p for p, n in zip(router_params, router_param_names) if n == param_name), None)
                        param_id = id(param) if param is not None else "unknown"
                        logger.error(f"   - {param_name} | param_id={param_id}")
                    if len(missing_names) > 10:
                        logger.error(f"   ... and {len(missing_names) - 10} more")
                else:
                    result['all_in_optimizer'] = True
                    logger.info(f"âœ… All {len(router_params)} router parameters are in optimizer")
            elif hasattr(trainer, 'deepspeed') and trainer.deepspeed is not None:
                # DeepSpeedì˜ ê²½ìš° requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°ê°€ ìë™ìœ¼ë¡œ optimizerì— í¬í•¨ë¨
                # í•˜ì§€ë§Œ ì‹¤ì œë¡œ optimizerë¥¼ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©´ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
                if hasattr(trainer.deepspeed, 'optimizer') and trainer.deepspeed.optimizer is None:
                    # DeepSpeed optimizerê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ
                    if result['all_trainable']:
                        result['all_in_optimizer'] = True
                        logger.info("âœ… DeepSpeed detected - router params with requires_grad=True will be included automatically (optimizer not yet initialized)")
                    else:
                        logger.error("âŒ DeepSpeed detected but router params are not trainable")
                else:
                    # DeepSpeed optimizerê°€ ìˆì§€ë§Œ param_groupsë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ
                    if result['all_trainable']:
                        result['all_in_optimizer'] = True
                        logger.info("âœ… DeepSpeed detected - router params with requires_grad=True will be included automatically")
                    else:
                        logger.error("âŒ DeepSpeed detected but router params are not trainable")
            else:
                logger.error("âŒ Optimizer not yet initialized - cannot validate router inclusion")
                # Optimizerê°€ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ requires_gradë§Œ í™•ì¸
                if result['all_trainable']:
                    result['all_in_optimizer'] = True  # ì¼ë‹¨ Trueë¡œ ì„¤ì • (ì´ˆê¸°í™” í›„ í™•ì¸ í•„ìš”)
            
            return result
        except Exception as e:
            import traceback
            logger.error(f"âŒ Error validating router weights: {e}")
            logger.error(f"   Traceback: {traceback.format_exc()}")
            return result
    
    def _get_actual_model(self, model):
        """ëª¨ë¸ì—ì„œ ì‹¤ì œ ëª¨ë¸ ì¶”ì¶œ (DeepSpeed, DDP ë“± ì²˜ë¦¬)"""
        if model is None:
            return None
        
        # DeepSpeedë¡œ ê°ì‹¸ì§„ ê²½ìš°
        if hasattr(model, 'module'):
            return model.module
        
        # ì¼ë°˜ ëª¨ë¸
        return model
    
    def on_after_backward(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """
        Backward ì´í›„, optimizer.step() ì´ì „ì— í˜¸ì¶œ
        ì´ ì‹œì ì—ì„œëŠ” gradientê°€ ê³„ì‚°ë˜ì—ˆì§€ë§Œ weightëŠ” ì•„ì§ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ
        Gradient ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë””ë²„ê¹…
        """
        # Trainer ì°¸ì¡° ì €ì¥ (ë””ë²„ê¹…ìš©)
        trainer = kwargs.get('trainer')
        if trainer is not None:
            self._last_trainer = trainer
        
        # Gradient í™•ì¸ (backward ì´í›„ì´ë¯€ë¡œ gradientê°€ ìˆì–´ì•¼ í•¨)
        # ì²˜ìŒ ëª‡ stepê³¼ router weightê°€ ë³€í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì— ìì„¸íˆ ë¡œê¹…
        should_log_gradients = (
            (self.verbose and state.global_step <= 5) or
            (self.check_weight_change and state.global_step >= self.check_after_steps)
        )
        
        if should_log_gradients:
            log_level = logger.info if state.global_step <= 5 else logger.debug
            log_level(f"RouterWeightTrackingCallback.on_after_backward called at step {state.global_step} (after backward, before optimizer.step())")
            try:
                actual_model = self._get_actual_model(model)
                if actual_model is not None:
                    from models.spectra_model import SPECTRARouter
                    try:
                        from models.g3moe_model import G3MoERouter
                    except ImportError:
                        G3MoERouter = None
                    
                    try:
                        from models.g3moe_model import G3MoEGRINMoE
                    except ImportError:
                        G3MoEGRINMoE = None
                    
                    router_modules = []
                    seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
                    
                    for name, module in actual_model.named_modules():
                        router_module = None
                        if isinstance(module, SPECTRARouter):
                            router_module = module
                        elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                            router_module = module
                        elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                            if hasattr(module, 'router') and isinstance(module.router, G3MoERouter):
                                router_module = module.router
                                name = f"{name}.router"
                        
                        if router_module is not None:
                            # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
                            router_id = id(router_module)
                            if router_id in seen_router_ids:
                                continue
                            seen_router_ids.add(router_id)
                            router_modules.append((name, router_module))
                    
                    if router_modules:
                        log_level(f"   Found {len(router_modules)} router modules - checking gradients...")
                        total_params_with_grad = 0
                        total_params_requires_grad = 0
                        total_grad_norm = 0.0
                        
                        for router_name, router_module in router_modules[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                            router_params_with_grad = 0
                            router_params_requires_grad = 0
                            router_grad_norm = 0.0
                            
                            # Expression projector í™•ì¸
                            if hasattr(router_module, 'expression_projector'):
                                expr_proj = router_module.expression_projector
                                for param_name, param in expr_proj.named_parameters(recurse=True):
                                    if param.requires_grad:
                                        router_params_requires_grad += 1
                                        total_params_requires_grad += 1
                                        has_grad = param.grad is not None
                                        if has_grad:
                                            router_params_with_grad += 1
                                            total_params_with_grad += 1
                                            grad_norm = param.grad.norm().item()
                                            router_grad_norm += grad_norm
                                            total_grad_norm += grad_norm
                                            if state.global_step <= 5:
                                                log_level(f"     {router_name}.expression_projector.{param_name}: has_grad={has_grad}, grad_norm={grad_norm:.2e}, param_id={id(param)}")
                                            elif not has_grad:
                                                logger.warning(f"     âš ï¸ {router_name}.expression_projector.{param_name}: requires_grad=True but grad is None! param_id={id(param)}")
                                
                                # linear_projection ìƒì„¸ í™•ì¸ (PEFT ë˜í•‘ êµ¬ì¡°)
                                if hasattr(expr_proj, 'linear_projection'):
                                    lin_proj = expr_proj.linear_projection
                                    
                                    # PEFT ModulesToSaveWrapper í™•ì¸
                                    if hasattr(lin_proj, 'original_module') and hasattr(lin_proj, 'modules_to_save'):
                                        orig_module = lin_proj.original_module
                                        modules_to_save = lin_proj.modules_to_save
                                        
                                        # original_module.weight í™•ì¸
                                        if hasattr(orig_module, 'weight'):
                                            orig_weight = orig_module.weight
                                            orig_has_grad = orig_weight.grad is not None if hasattr(orig_weight, 'grad') else False
                                            orig_grad_norm = orig_weight.grad.norm().item() if orig_has_grad and orig_weight.grad is not None else 0.0
                                            log_level(f"     {router_name}.expression_projector.linear_projection.original_module.weight: requires_grad={orig_weight.requires_grad}, has_grad={orig_has_grad}, grad_norm={orig_grad_norm:.2e}, param_id={id(orig_weight)}")
                                        
                                        # modules_to_save.default.weight í™•ì¸
                                        if hasattr(modules_to_save, 'default') and hasattr(modules_to_save.default, 'weight'):
                                            default_weight = modules_to_save.default.weight
                                            default_has_grad = default_weight.grad is not None if hasattr(default_weight, 'grad') else False
                                            default_grad_norm = default_weight.grad.norm().item() if default_has_grad and default_weight.grad is not None else 0.0
                                            log_level(f"     {router_name}.expression_projector.linear_projection.modules_to_save.default.weight: requires_grad={default_weight.requires_grad}, has_grad={default_has_grad}, grad_norm={default_grad_norm:.2e}, param_id={id(default_weight)}")
                                        
                                        # ì§ì ‘ ì ‘ê·¼ weight í™•ì¸ (forwardì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê²ƒ)
                                        if hasattr(lin_proj, 'weight'):
                                            direct_weight = lin_proj.weight
                                            direct_has_grad = direct_weight.grad is not None if hasattr(direct_weight, 'grad') else False
                                            direct_grad_norm = direct_weight.grad.norm().item() if direct_has_grad and direct_weight.grad is not None else 0.0
                                            log_level(f"     {router_name}.expression_projector.linear_projection.weight (direct): requires_grad={direct_weight.requires_grad}, has_grad={direct_has_grad}, grad_norm={direct_grad_norm:.2e}, param_id={id(direct_weight)}")
                                            
                                            # ì–´ë–¤ íŒŒë¼ë¯¸í„°ì™€ ê°™ì€ì§€ í™•ì¸
                                            if hasattr(orig_module, 'weight') and id(direct_weight) == id(orig_module.weight):
                                                logger.warning(f"     âš ï¸ CRITICAL: direct_weight is SAME as original_module.weight! PEFT is using original_module in forward!")
                                            elif hasattr(modules_to_save, 'default') and hasattr(modules_to_save.default, 'weight') and id(direct_weight) == id(modules_to_save.default.weight):
                                                log_level(f"     âœ“ direct_weight is SAME as modules_to_save.default.weight (GOOD - using modules_to_save in forward)")
                            
                            # Load balancer í™•ì¸
                            if hasattr(router_module, 'load_balancer'):
                                lb_module = router_module.load_balancer
                                for param_name, param in lb_module.named_parameters(recurse=True):
                                    if param.requires_grad:
                                        router_params_requires_grad += 1
                                        total_params_requires_grad += 1
                                        has_grad = param.grad is not None
                                        if has_grad:
                                            router_params_with_grad += 1
                                            total_params_with_grad += 1
                                            grad_norm = param.grad.norm().item()
                                            router_grad_norm += grad_norm
                                            total_grad_norm += grad_norm
                            
                            if router_params_requires_grad > 0:
                                log_level(f"   Router '{router_name}': {router_params_with_grad}/{router_params_requires_grad} params have gradients, total grad_norm={router_grad_norm:.2e}")
                                if router_params_with_grad < router_params_requires_grad:
                                    logger.warning(f"   âš ï¸ Router '{router_name}': {router_params_requires_grad - router_params_with_grad} params missing gradients!")
                        
                        # ì „ì²´ ìš”ì•½
                        if total_params_requires_grad > 0:
                            log_level(f"   Total router params: {total_params_with_grad}/{total_params_requires_grad} have gradients, total grad_norm={total_grad_norm:.2e}")
                            if total_params_with_grad < total_params_requires_grad:
                                logger.warning(f"   âš ï¸ {total_params_requires_grad - total_params_with_grad} router params missing gradients - this may cause router not to learn!")
            except Exception as e:
                logger.warning(f"   Gradient check failed: {e}")
                if self.verbose:
                    import traceback
                    logger.debug(traceback.format_exc())
        
        return control
    
    def on_step_end(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """
        ê° training step ëì—ì„œ router ê°€ì¤‘ì¹˜ tracking
        NOTE: ì´ ì‹œì ì€ backwardì™€ optimizer.step() ì´í›„ì´ë¯€ë¡œ weightê°€ ì´ë¯¸ ì—…ë°ì´íŠ¸ë¨
        """
        # Trainer ì°¸ì¡° ì €ì¥ (ë””ë²„ê¹…ìš©)
        trainer = kwargs.get('trainer')
        if trainer is not None:
            self._last_trainer = trainer
        
        # ì£¼ê¸°ì ìœ¼ë¡œë§Œ tracking (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        if state.global_step % self.log_every_n_steps == 0:
            
            # ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” weightë¥¼ RouterWeightTrackerì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ìˆ˜ì •ëœ ì¶”ì¶œ í•¨ìˆ˜ ì‚¬ìš©
            try:
                # ëª¨ë¸ ì¶”ì¶œ
                actual_model = self._get_actual_model(model)
                
                if actual_model is None:
                    if self.verbose and not self._first_step_logged:
                        logger.warning(f"âš ï¸ RouterWeightTrackingCallback: model is None at step {state.global_step}")
                    return control
                
                # Router ê°€ì¤‘ì¹˜ tracking (optimizer.step() ì´í›„ì´ë¯€ë¡œ weightê°€ ì—…ë°ì´íŠ¸ëœ ìƒíƒœ)
                # Forward hookì—ì„œ ì¶”ì í•œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” weight ì „ë‹¬
                step_stats = self.tracker.track_step(
                    model=actual_model,
                    step=state.global_step,
                    global_step=state.global_step,
                    actual_weights_dict=self._actual_router_weights,
                )
                
                # ì²« ë²ˆì§¸ ë¡œê¹… ì‹œ í™•ì¸
                if not self._first_step_logged:
                    layers_found = len(step_stats.get('layers', {}))
                    if layers_found > 0:
                        logger.info(f"âœ… RouterWeightTrackingCallback: Found {layers_found} router layers at step {state.global_step}")
                    self._first_step_logged = True
                
                # Weight ë³€í™” ì²´í¬ (check_after_steps ì´í›„ë¶€í„°)
                # Forward hookì—ì„œ ì¶”ì í•œ ì‹¤ì œ ì‚¬ìš©ëœ weightë¥¼ ì§ì ‘ ë¹„êµ
                actual_weight_changes = None
                if self.check_weight_change and state.global_step >= self.check_after_steps:
                    should_stop, actual_weight_changes = self._check_actual_weight_changes(state.global_step, model=actual_model, trainer=trainer)
                    if should_stop:
                        control.should_training_stop = True
                        control.should_epoch_stop = True
                        logger.error("ğŸ›‘ Training stopped due to router weights not changing!")
                        return control
                
                # Wandbì— ë¡œê¹… (ì„ íƒì )
                if hasattr(args, 'report_to') and args.report_to and 'wandb' in args.report_to:
                    try:
                        import wandb
                        if wandb.run is not None:
                            wandb_logs = {}
                            
                            # Forward hookì—ì„œ ì¶”ì í•œ ì‹¤ì œ weight ë³€í™” ë¡œê¹…
                            if actual_weight_changes:
                                for router_name, change_info in actual_weight_changes.items():
                                    for metric_name, metric_value in change_info.items():
                                        if isinstance(metric_value, (int, float)):
                                            wandb_logs[f"router_weight_actual/{router_name}/{metric_name}"] = metric_value
                            
                            # ê¸°ì¡´ í†µê³„ ë¡œê¹…
                            for layer_key, layer_data in step_stats.get('layers', {}).items():
                                if 'load_balancer' in layer_data:
                                    lb_stats = layer_data['load_balancer']
                                    for stat_name, stat_value in lb_stats.items():
                                        if isinstance(stat_value, (int, float)):
                                            wandb_logs[f"router_weight/{layer_key}/load_balancer/{stat_name}"] = stat_value
                                
                                if 'expression_projector' in layer_data:
                                    expr_stats = layer_data['expression_projector']
                                    for stat_name, stat_value in expr_stats.items():
                                        if isinstance(stat_value, (int, float)):
                                            wandb_logs[f"router_weight/{layer_key}/expression_projector/{stat_name}"] = stat_value
                                
                                if 'load_balancer_changes' in layer_data:
                                    changes = layer_data['load_balancer_changes']
                                    for change_name, change_value in changes.items():
                                        if isinstance(change_value, (int, float)):
                                            wandb_logs[f"router_weight/{layer_key}/load_balancer_change/{change_name}"] = change_value
                                
                                if 'expression_projector_changes' in layer_data:
                                    changes = layer_data['expression_projector_changes']
                                    for change_name, change_value in changes.items():
                                        if isinstance(change_value, (int, float)):
                                            wandb_logs[f"router_weight/{layer_key}/expression_projector_change/{change_name}"] = change_value
                            
                            # Bias balancing monitoring metrics
                            try:
                                from models.spectra_model import SPECTRARouter
                                
                                all_bias_magnitudes = []
                                all_bias_changes = []
                                total_router_count = 0
                                
                                for name, module in actual_model.named_modules():
                                    if isinstance(module, SPECTRARouter) and hasattr(module, 'expert_bias'):
                                        total_router_count += 1
                                        
                                        # Expert bias statistics
                                        expert_bias = module.expert_bias
                                        if expert_bias.numel() > 0:
                                            bias_l2_norm = torch.norm(expert_bias, p=2).item()
                                            bias_mean = expert_bias.mean().item()
                                            bias_std = expert_bias.std().item()
                                            bias_max = expert_bias.max().item()
                                            bias_min = expert_bias.min().item()
                                            
                                            all_bias_magnitudes.append(bias_l2_norm)
                                            
                                            # Per-expert bias values (first 10 experts only to avoid clutter)
                                            for expert_idx in range(min(10, expert_bias.numel())):
                                                wandb_logs[f"bias/expert_bias_{expert_idx}"] = expert_bias[expert_idx].item()
                                            
                                            # Bias change from previous step
                                            if hasattr(module, 'prev_expert_bias') and module.prev_expert_bias.numel() > 0:
                                                bias_change = expert_bias - module.prev_expert_bias
                                                bias_change_norm = torch.norm(bias_change, p=2).item()
                                                all_bias_changes.append(bias_change_norm)
                                                
                                                # Update prev_expert_bias for next step
                                                module.prev_expert_bias.copy_(expert_bias.detach())
                                            else:
                                                # Initialize prev_expert_bias
                                                if not hasattr(module, 'prev_expert_bias'):
                                                    module.register_buffer("prev_expert_bias", expert_bias.detach().clone())
                                                else:
                                                    module.prev_expert_bias.copy_(expert_bias.detach())
                                            
                                            # Expert usage statistics
                                            if hasattr(module, 'last_current_load') and module.last_current_load is not None:
                                                current_load = module.last_current_load
                                                total_tokens = current_load.sum().item()
                                                
                                                if total_tokens > 0:
                                                    usage_distribution = (current_load / total_tokens).cpu().numpy()
                                                    target_per_expert = 1.0 / float(module.num_experts)
                                                    
                                                    # Usage deviation from uniform
                                                    deviation = usage_distribution - target_per_expert
                                                    usage_deviation = float(np.linalg.norm(deviation))
                                                    
                                                    # Coefficient of variation
                                                    usage_mean = usage_distribution.mean()
                                                    usage_std = usage_distribution.std()
                                                    usage_cv = float(usage_std / (usage_mean + 1e-8))
                                                    
                                                    # Unused experts count
                                                    unused_count = int((current_load == 0).sum().item())
                                                    
                                                    # Max/min usage ratio
                                                    max_usage = float(current_load.max().item())
                                                    min_usage = float(current_load.min().item())
                                                    avg_usage = float(current_load.mean().item())
                                                    max_usage_ratio = float(max_usage / (avg_usage + 1e-8))
                                                    min_usage_ratio = float(min_usage / (avg_usage + 1e-8)) if min_usage > 0 else 0.0
                                                    
                                                    # Per-expert usage (first 10 experts only)
                                                    for expert_idx in range(min(10, current_load.numel())):
                                                        wandb_logs[f"usage/expert_usage_{expert_idx}"] = float(current_load[expert_idx].item())
                                                    
                                                    # Aggregate usage metrics
                                                    wandb_logs[f"usage/expert_usage_deviation"] = usage_deviation
                                                    wandb_logs[f"usage/expert_usage_cv"] = usage_cv
                                                    wandb_logs[f"usage/unused_experts_count"] = unused_count
                                                    wandb_logs[f"usage/max_usage_ratio"] = max_usage_ratio
                                                    wandb_logs[f"usage/min_usage_ratio"] = min_usage_ratio
                                            
                                            # Router-specific bias statistics
                                            router_layer_key = name.replace('.', '_')
                                            wandb_logs[f"router/{router_layer_key}/bias/expert_bias_l2_norm"] = bias_l2_norm
                                            wandb_logs[f"router/{router_layer_key}/bias/expert_bias_mean"] = bias_mean
                                            wandb_logs[f"router/{router_layer_key}/bias/expert_bias_std"] = bias_std
                                            wandb_logs[f"router/{router_layer_key}/bias/expert_bias_max"] = bias_max
                                            wandb_logs[f"router/{router_layer_key}/bias/expert_bias_min"] = bias_min
                                
                                # Aggregate bias metrics across all routers
                                if all_bias_magnitudes:
                                    wandb_logs["bias/expert_bias_l2_norm"] = np.mean(all_bias_magnitudes)
                                    wandb_logs["bias/expert_bias_mean"] = np.mean(all_bias_magnitudes)
                                
                                if all_bias_changes:
                                    wandb_logs["bias/expert_bias_change_norm"] = np.mean(all_bias_changes)
                                
                            except Exception as e:
                                logger.debug(f"Failed to log bias balancing metrics: {e}")
                            
                            if wandb_logs:
                                wandb.log(wandb_logs, step=state.global_step, commit=False)
                    except ImportError:
                        pass
                    except Exception as e:
                        logger.debug(f"Failed to log to wandb at step {state.global_step}: {e}")
            
            except Exception as e:
                logger.error(f"âŒ Failed to track router weights at step {state.global_step}: {e}")
                if self.verbose:
                    import traceback
                    logger.error(traceback.format_exc())
        
        return control
    
    def _check_actual_weight_changes(self, step: int, model=None, trainer=None):
        """
        Forward hookì—ì„œ ì¶”ì í•œ ì‹¤ì œ ì‚¬ìš©ëœ weightì˜ ë³€í™”ë¥¼ ì§ì ‘ ì²´í¬
        
        Returns:
            (should_stop: bool, weight_changes: dict) - í•™ìŠµ ì¤‘ë‹¨ ì—¬ë¶€ì™€ weight ë³€í™” ì •ë³´
        """
        # í˜„ì¬ stepì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ weight í™•ì¸ (forward hookì—ì„œ ì¶”ì í•œ ê²ƒ)
        current_step_weights = {}
        for router_name, step_weights in self._actual_router_weights.items():
            if step in step_weights:
                current_step_weights[router_name] = step_weights[step]
        
        if not current_step_weights:
            return False, None
        
        # ì´ì „ stepì˜ forward hook weightì™€ ë¹„êµ
        if not self._prev_actual_weights:
            self._prev_actual_weights = {k: v.detach().clone() for k, v in current_step_weights.items()}
            return False, None
        
        # Weight ë³€í™” ê³„ì‚°
        all_changes_zero = True
        max_change = 0.0
        change_details = []
        weight_changes = {}  # wandb ë¡œê¹…ìš©
        
        for router_name, current_weight in current_step_weights.items():
            if router_name not in self._prev_actual_weights:
                continue
            
            prev_weight = self._prev_actual_weights[router_name]
            
            if prev_weight.shape != current_weight.shape:
                continue
            
            try:
                diff = current_weight - prev_weight
                diff_norm = float(torch.norm(diff).item())
                diff_mean = float(diff.mean().item())
                diff_max = float(diff.abs().max().item())
                diff_std = float(diff.std().item())
                
                max_change = max(max_change, diff_norm)
                change_details.append(f"{router_name}: diff_norm={diff_norm:.2e}, diff_mean={diff_mean:.2e}, diff_max={diff_max:.2e}")
                
                # wandb ë¡œê¹…ìš© ì €ì¥
                weight_changes[router_name] = {
                    'diff_norm': diff_norm,
                    'diff_mean': diff_mean,
                    'diff_max': diff_max,
                    'diff_std': diff_std,
                }
                
                if diff_norm >= self.min_change_threshold:
                    all_changes_zero = False
            except Exception as e:
                logger.debug(f"Failed to compute weight change for {router_name}: {e}")
                continue
        
        # ë³€í™”ê°€ ì—†ìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨
        if all_changes_zero:
            error_msg = (
                f"\n{'='*80}\n"
                f"âŒ ROUTER WEIGHT CHANGE CHECK FAILED at step {step}\n"
                f"{'='*80}\n"
                f"All router weight changes are below threshold ({self.min_change_threshold:.2e})\n"
                f"This means the router is NOT LEARNING!\n"
                f"\nMax change observed: {max_change:.2e}\n"
                f"Threshold: {self.min_change_threshold:.2e}\n"
                f"\nChange details (actual weights used in forward, comparing step {step-1} vs {step}):\n"
            )
            for detail in change_details[:20]:
                error_msg += f"  {detail}\n"
            error_msg += f"\n{'='*80}\n"
            
            logger.error(error_msg)
            
            # ë””ë²„ê¹…: optimizerì— ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if trainer is not None:
                logger.error("ğŸ” Checking if router weights are in optimizer...")
                try:
                    optimizer_param_ids = set()
                    if hasattr(trainer, 'optimizer') and trainer.optimizer is not None:
                        optimizer_param_ids = {id(p) for group in trainer.optimizer.param_groups for p in group['params']}
                    elif hasattr(trainer, 'deepspeed') and trainer.deepspeed is not None:
                        if hasattr(trainer.deepspeed, 'optimizer') and trainer.deepspeed.optimizer is not None:
                            optimizer_param_ids = {id(p) for group in trainer.deepspeed.optimizer.param_groups for p in group['params']}
                    
                    for router_name, current_weight in current_step_weights.items():
                        weight_param_id = id(current_weight)
                        in_optimizer = weight_param_id in optimizer_param_ids
                        logger.error(f"   {router_name}: weight param_id={weight_param_id}, in_optimizer={in_optimizer}")
                except Exception as e:
                    logger.debug(f"Failed to check optimizer: {e}")
            
            return True, weight_changes
        
        # í˜„ì¬ stepì˜ forward hook weightë¥¼ ì´ì „ weightë¡œ ì—…ë°ì´íŠ¸
        self._prev_actual_weights = {k: v.detach().clone() for k, v in current_step_weights.items()}
        return False, weight_changes
    
    def _check_weight_changes(self, step_stats: Dict[str, Any], step: int, model=None, trainer=None) -> bool:
        """
        Weight ë³€í™”ë¥¼ ì²´í¬í•˜ê³ , ë³€í™”ê°€ ì—†ìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨
        
        Args:
            step_stats: í˜„ì¬ stepì˜ í†µê³„
            step: í˜„ì¬ step ë²ˆí˜¸
            model: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ë””ë²„ê¹…ìš©)
            trainer: Trainer ì¸ìŠ¤í„´ìŠ¤ (ë””ë²„ê¹…ìš©)
            
        Returns:
            True if should stop training, False otherwise
        """
        layers = step_stats.get('layers', {})
        
        if not layers:
            if self.verbose:
                logger.warning(f"âš ï¸ No router layers found for weight change check at step {step}")
            return False
        
        all_changes_zero = True
        max_change = 0.0
        change_details = []
        has_change_data = False  # ë³€í™” ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        
        for layer_key, layer_data in layers.items():
            # Load balancer ë³€í™” ì²´í¬
            if 'load_balancer_changes' in layer_data:
                has_change_data = True
                lb_changes = layer_data['load_balancer_changes']
                for change_name, change_value in lb_changes.items():
                    if isinstance(change_value, (int, float)):
                        abs_change = abs(change_value)
                        max_change = max(max_change, abs_change)
                        change_details.append(f"{layer_key}.load_balancer.{change_name}={change_value:.2e}")
                        
                        if abs_change >= self.min_change_threshold:
                            all_changes_zero = False
            
            # Expression projector ë³€í™” ì²´í¬
            if 'expression_projector_changes' in layer_data:
                has_change_data = True
                expr_changes = layer_data['expression_projector_changes']
                for change_name, change_value in expr_changes.items():
                    if isinstance(change_value, (int, float)):
                        abs_change = abs(change_value)
                        max_change = max(max_change, abs_change)
                        change_details.append(f"{layer_key}.expression_projector.{change_name}={change_value:.2e}")
                        
                        if abs_change >= self.min_change_threshold:
                            all_changes_zero = False
        
        # ë³€í™” ë°ì´í„°ê°€ ì—†ìœ¼ë©´ (ì²« step ë“±) ì²´í¬ ìŠ¤í‚µ
        if not has_change_data:
            if self.verbose:
                logger.debug(f"âš ï¸ No change data available at step {step} (first step?), skipping check")
            return False
        
        # ë³€í™” ë°ì´í„°ê°€ ìˆëŠ”ë° ëª¨ë‘ 0ì´ë©´ í•™ìŠµ ì¤‘ë‹¨
        if all_changes_zero:
            error_msg = (
                f"\n{'='*80}\n"
                f"âŒ ROUTER WEIGHT CHANGE CHECK FAILED at step {step}\n"
                f"{'='*80}\n"
                f"All router weight changes are below threshold ({self.min_change_threshold:.2e})\n"
                f"This means the router is NOT LEARNING!\n"
                f"\nMax change observed: {max_change:.2e}\n"
                f"Threshold: {self.min_change_threshold:.2e}\n"
                f"\nChange details (first 10):\n"
            )
            for detail in change_details[:10]:
                error_msg += f"  {detail}\n"
            error_msg += f"\n{'='*80}\n"
            
            logger.error(error_msg)
            
            # ë””ë²„ê¹…: expression_projector íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸ ë° forwardì—ì„œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„° ì¶”ì 
            logger.error("ğŸ” Debugging expression_projector parameters:")
            try:
                from models.spectra_model import SPECTRARouter
                
                # ì‹¤ì œ ëª¨ë¸ì—ì„œ router ì°¾ê¸° (ì „ë‹¬ë°›ì€ model/trainer ìš°ì„  ì‚¬ìš©)
                actual_model = None
                debug_trainer = None
                
                # 1. í•¨ìˆ˜ ì¸ìë¡œ ì „ë‹¬ë°›ì€ model/trainer ìš°ì„  ì‚¬ìš©
                if model is not None:
                    actual_model = self._get_actual_model(model)
                    debug_trainer = trainer
                # 2. ì €ì¥ëœ trainer ì°¸ì¡° ì‚¬ìš©
                elif hasattr(self, '_last_trainer') and self._last_trainer is not None:
                    debug_trainer = self._last_trainer
                    actual_model = self._get_actual_model(debug_trainer.model if hasattr(debug_trainer, 'model') else None)
                # 3. trainer ì¸ìë¡œ ì „ë‹¬ë°›ì€ ê²½ìš°
                elif trainer is not None:
                    debug_trainer = trainer
                    actual_model = self._get_actual_model(trainer.model if hasattr(trainer, 'model') else None)
                
                if actual_model is None:
                    logger.error("   âš ï¸ Cannot access model for debugging (model and trainer not available)")
                    logger.error("   This may indicate a problem with callback integration")
                
                if actual_model is not None:
                    try:
                        from models.g3moe_model import G3MoERouter, G3MoEGRINMoE
                    except ImportError:
                        G3MoERouter = None
                        G3MoEGRINMoE = None
                    
                    router_modules = []
                    seen_router_ids = set()  # ì´ë¯¸ ì²˜ë¦¬í•œ router ì¸ìŠ¤í„´ìŠ¤ ì¶”ì 
                    
                    for name, module in actual_model.named_modules():
                        router_module = None
                        if isinstance(module, SPECTRARouter):
                            router_module = module
                        elif G3MoERouter is not None and isinstance(module, G3MoERouter):
                            router_module = module
                        elif G3MoEGRINMoE is not None and isinstance(module, G3MoEGRINMoE):
                            if hasattr(module, 'router') and isinstance(module.router, G3MoERouter):
                                router_module = module.router
                                name = f"{name}.router"
                        
                        if router_module is not None:
                            # ê°™ì€ router ì¸ìŠ¤í„´ìŠ¤ëŠ” í•œ ë²ˆë§Œ ì²˜ë¦¬
                            router_id = id(router_module)
                            if router_id in seen_router_ids:
                                continue
                            seen_router_ids.add(router_id)
                            router_modules.append((name, router_module))
                    
                    logger.error(f"   Found {len(router_modules)} router modules")
                    
                    for router_name, router_module in router_modules[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                        if hasattr(router_module, 'expression_projector'):
                            expr_proj = router_module.expression_projector
                            logger.error(f"   Router {router_name}:")
                            
                            # íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸
                            expr_params = list(expr_proj.named_parameters(recurse=True))
                            logger.error(f"     Expression projector params: {len(expr_params)}")
                            
                            # ëª¨ë“  íŒŒë¼ë¯¸í„° í™•ì¸ (original_module í¬í•¨)
                            for param_name, param in expr_params:
                                has_grad = param.grad is not None if hasattr(param, 'grad') else False
                                grad_norm = param.grad.norm().item() if has_grad and param.grad is not None else 0.0
                                logger.error(f"       {param_name}: shape={param.shape}, requires_grad={param.requires_grad}, has_grad={has_grad}, grad_norm={grad_norm:.2e}, param_id={id(param)}")
                            
                            # linear_projection í™•ì¸ - PEFT ë˜í•‘ êµ¬ì¡° ë¶„ì„
                            if hasattr(expr_proj, 'linear_projection'):
                                lin_proj = expr_proj.linear_projection
                                logger.error(f"     linear_projection module type: {type(lin_proj)}")
                                logger.error(f"     linear_projection attributes: {[attr for attr in dir(lin_proj) if not attr.startswith('_')]}")
                                
                                # PEFT ModulesToSaveWrapper í™•ì¸
                                has_original_module = hasattr(lin_proj, 'original_module')
                                has_modules_to_save = hasattr(lin_proj, 'modules_to_save')
                                
                                logger.error(f"     Has original_module: {has_original_module}")
                                logger.error(f"     Has modules_to_save: {has_modules_to_save}")
                                
                                if has_original_module:
                                    orig_module = lin_proj.original_module
                                    logger.error(f"     original_module type: {type(orig_module)}")
                                    if hasattr(orig_module, 'weight'):
                                        orig_weight = orig_module.weight
                                        orig_weight_grad = orig_weight.grad is not None if hasattr(orig_weight, 'grad') else False
                                        orig_weight_grad_norm = orig_weight.grad.norm().item() if orig_weight_grad and orig_weight.grad is not None else 0.0
                                        logger.error(f"     original_module.weight: shape={orig_weight.shape}, requires_grad={orig_weight.requires_grad}, has_grad={orig_weight_grad}, grad_norm={orig_weight_grad_norm:.2e}, param_id={id(orig_weight)}")
                                
                                if has_modules_to_save:
                                    modules_to_save = lin_proj.modules_to_save
                                    logger.error(f"     modules_to_save type: {type(modules_to_save)}")
                                    logger.error(f"     modules_to_save keys: {list(modules_to_save.keys()) if hasattr(modules_to_save, 'keys') else 'N/A'}")
                                    
                                    if hasattr(modules_to_save, 'default'):
                                        default_module = modules_to_save.default
                                        logger.error(f"     modules_to_save.default type: {type(default_module)}")
                                        if hasattr(default_module, 'weight'):
                                            default_weight = default_module.weight
                                            default_weight_grad = default_weight.grad is not None if hasattr(default_weight, 'grad') else False
                                            default_weight_grad_norm = default_weight.grad.norm().item() if default_weight_grad and default_weight.grad is not None else 0.0
                                            logger.error(f"     modules_to_save.default.weight: shape={default_weight.shape}, requires_grad={default_weight.requires_grad}, has_grad={default_weight_grad}, grad_norm={default_weight_grad_norm:.2e}, param_id={id(default_weight)}")
                                
                                # ì§ì ‘ weight ì†ì„± í™•ì¸ (PEFTê°€ forwardì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒ)
                                if hasattr(lin_proj, 'weight'):
                                    direct_weight = lin_proj.weight
                                    direct_weight_grad = direct_weight.grad is not None if hasattr(direct_weight, 'grad') else False
                                    direct_weight_grad_norm = direct_weight.grad.norm().item() if direct_weight_grad and direct_weight.grad is not None else 0.0
                                    logger.error(f"     linear_projection.weight (direct access): shape={direct_weight.shape}, requires_grad={direct_weight.requires_grad}, has_grad={direct_weight_grad}, grad_norm={direct_weight_grad_norm:.2e}, param_id={id(direct_weight)}")
                                    
                                    # ì–´ë–¤ íŒŒë¼ë¯¸í„°ì™€ ê°™ì€ì§€ í™•ì¸
                                    if has_original_module and hasattr(lin_proj.original_module, 'weight'):
                                        if id(direct_weight) == id(lin_proj.original_module.weight):
                                            logger.error(f"     âš ï¸ CRITICAL: direct_weight is SAME as original_module.weight!")
                                            logger.error(f"     This confirms PEFT is using original_module in forward pass!")
                                    if has_modules_to_save and hasattr(modules_to_save, 'default') and hasattr(modules_to_save.default, 'weight'):
                                        if id(direct_weight) == id(modules_to_save.default.weight):
                                            logger.error(f"     âœ“ direct_weight is SAME as modules_to_save.default.weight (GOOD!)")
                                
                                # Forward hookì„ í†µí•œ ì‹¤ì œ ì‚¬ìš© íŒŒë¼ë¯¸í„° ì¶”ì 
                                def forward_hook(module, input, output):
                                    if hasattr(module, 'weight'):
                                        weight = module.weight
                                        logger.error(f"     ğŸ” FORWARD HOOK: linear_projection forward called with weight param_id={id(weight)}, requires_grad={weight.requires_grad}")
                                
                                # Hook ë“±ë¡ (ë‹¤ìŒ forwardì—ì„œ í™•ì¸)
                                if not hasattr(lin_proj, '_debug_hook_registered'):
                                    lin_proj.register_forward_hook(forward_hook)
                                    lin_proj._debug_hook_registered = True
                                    logger.error(f"     âœ“ Registered forward hook for linear_projection (will log on next forward pass)")
                            
                            # Optimizer í™•ì¸
                            if debug_trainer is not None:
                                # DeepSpeed ì¼€ì´ìŠ¤ í™•ì¸
                                if hasattr(debug_trainer, 'deepspeed') and debug_trainer.deepspeed is not None:
                                    if hasattr(debug_trainer.deepspeed, 'optimizer') and debug_trainer.deepspeed.optimizer is not None:
                                        ds_optimizer = debug_trainer.deepspeed.optimizer
                                        if hasattr(ds_optimizer, 'param_groups'):
                                            optimizer_param_ids = {id(p) for group in ds_optimizer.param_groups for p in group['params']}
                                            expr_param_ids = {id(p) for _, p in expr_params}
                                            in_optimizer = expr_param_ids & optimizer_param_ids
                                            logger.error(f"     Params in DeepSpeed optimizer: {len(in_optimizer)}/{len(expr_param_ids)}")
                                            
                                            # ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ optimizerì— ìˆëŠ”ì§€ ìƒì„¸ í™•ì¸
                                            for param_name, param in expr_params:
                                                param_id = id(param)
                                                in_opt = param_id in optimizer_param_ids
                                                logger.error(f"       {param_name}: in_optimizer={in_opt}, param_id={param_id}")
                                        else:
                                            logger.error(f"     âš ï¸ DeepSpeed optimizer has no param_groups")
                                    else:
                                        logger.error(f"     âš ï¸ DeepSpeed optimizer not yet initialized")
                                # ì¼ë°˜ optimizer ì¼€ì´ìŠ¤
                                elif hasattr(debug_trainer, 'optimizer') and debug_trainer.optimizer is not None:
                                    optimizer_param_ids = {id(p) for group in debug_trainer.optimizer.param_groups for p in group['params']}
                                    expr_param_ids = {id(p) for _, p in expr_params}
                                    in_optimizer = expr_param_ids & optimizer_param_ids
                                    logger.error(f"     Params in optimizer: {len(in_optimizer)}/{len(expr_param_ids)}")
                                    
                                    # ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ optimizerì— ìˆëŠ”ì§€ ìƒì„¸ í™•ì¸
                                    for param_name, param in expr_params:
                                        param_id = id(param)
                                        in_opt = param_id in optimizer_param_ids
                                        logger.error(f"       {param_name}: in_optimizer={in_opt}, param_id={param_id}")
                                else:
                                    logger.error(f"     âš ï¸ Optimizer not available in trainer")
                            else:
                                logger.error(f"     âš ï¸ Cannot check optimizer (trainer not available)")
                else:
                    logger.error("   âš ï¸ Cannot access model for debugging")
                                    
            except Exception as debug_e:
                logger.error(f"   Debug logging failed: {debug_e}")
                import traceback
                logger.error(traceback.format_exc())
            
            # í•™ìŠµ ì¤‘ë‹¨ì„ ìœ„í•´ True ë°˜í™˜
            return True
        else:
            if self.verbose and step % (self.log_every_n_steps * 10) == 0:
                logger.info(f"âœ… Router weight changes OK at step {step}: max_change={max_change:.2e}")
            return False
    
    def on_train_end(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """Training ì¢…ë£Œ ì‹œ ìµœì¢… ìš”ì•½ ì €ì¥ ë° hook ì œê±°"""
        # Forward hook ì œê±°
        for router_name, hook in self._router_hooks:
            try:
                hook.remove()
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to remove forward hook for {router_name}: {e}")
        
        if self._router_hooks:
            logger.info(f"âœ… Removed {len(self._router_hooks)} router forward hooks")
        
        # Router forward ì‚¬ìš© í†µê³„ ìš”ì•½
        if self._router_forward_tracker:
            all_used_routers = set()
            for step, routers in self._router_forward_tracker.items():
                all_used_routers.update(routers)
            
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š Router Forward Pass Summary:")
            logger.info(f"   Total steps with router usage: {len(self._router_forward_tracker)}")
            logger.info(f"   Unique routers used: {len(all_used_routers)}")
            logger.info(f"   Routers used during training:")
            for router_name in sorted(all_used_routers):
                usage_count = sum(1 for routers in self._router_forward_tracker.values() if router_name in routers)
                logger.info(f"     âœ“ {router_name} (used in {usage_count} steps)")
            logger.info("=" * 80)
        
        try:
            # ìµœì¢… ìš”ì•½ ì €ì¥
            summary = self.tracker.save_summary()
            if self.verbose:
                logger.info(f"âœ… Router weight tracking summary saved: {summary}")
                logger.info(f"   Total steps tracked: {summary.get('total_steps', 0)}")
                logger.info(f"   Layers tracked: {summary.get('layers_tracked', [])}")
        except Exception as e:
            logger.error(f"âŒ Failed to save router weight summary: {e}")
            if self.verbose:
                import traceback
                logger.error(traceback.format_exc())
        
        return control
    
    def on_save(
        self,
        args,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """Checkpoint ì €ì¥ ì‹œ router ê°€ì¤‘ì¹˜ë„ í•¨ê»˜ ì €ì¥"""
        try:
            # Checkpoint ë””ë ‰í† ë¦¬ì— router ê°€ì¤‘ì¹˜ ìš”ì•½ ì €ì¥
            if state.is_world_process_zero:
                checkpoint_dir = os.path.join(
                    args.output_dir,
                    f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}"
                )
                if os.path.exists(checkpoint_dir):
                    summary_file = os.path.join(checkpoint_dir, "router_weight_summary.json")
                    summary = self.tracker.save_summary(summary_file)
                    if self.verbose:
                        logger.info(f"âœ… Router weight summary saved to checkpoint: {summary_file}")
        except Exception as e:
            logger.error(f"âŒ Failed to save router weight summary to checkpoint: {e}")
            if self.verbose:
                import traceback
                logger.error(traceback.format_exc())
        
        return control
