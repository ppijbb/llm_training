# coding=utf-8
"""
Checkpoint ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í•™ìŠµëœ checkpoint ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ GramSpec MoE ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- Load balancing metrics
- Expert specialization metrics
- Routing quality metrics
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import json
from tqdm import tqdm
import argparse
from peft import PeftModel
from transformers import AutoConfig, AutoModel, AutoModelForCausalLM, AutoProcessor, AutoTokenizer
from transformers.generation.configuration_utils import GenerationConfig
from transformers.image_utils import load_image
import copy

from models import G3MoEModel, G3MoETextModel, G3MoEConfig, G3MoEForCausalLM, G3MoEForConditionalGeneration, G3MoETextConfig
from transformers.modeling_utils import VLMS
from eval.gramspec_moe_analysis import GramSpecAnalyzer

# Register models
AutoConfig.register("g3moe", G3MoEConfig)
AutoConfig.register("g3moe_text", G3MoETextConfig)
AutoModel.register(G3MoEConfig, G3MoEModel)
AutoModel.register(G3MoETextConfig, G3MoETextModel)
AutoModelForCausalLM.register(G3MoETextConfig, G3MoEForCausalLM)
VLMS.append("g3moe")


class RoutingInfoCollector:
    """ëª¨ë¸ forward passì—ì„œ routing ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” hook"""
    
    def __init__(self, analyzer: GramSpecAnalyzer):
        self.analyzer = analyzer
        self.hooks = []
        self.router_hooks = []
        self.routing_data = []
        self.router_internal_data = defaultdict(list)
        
    def register_hooks(self, model: nn.Module):
        """ëª¨ë¸ì˜ MoE ë ˆì´ì–´ì™€ Routerì— hook ë“±ë¡"""
        # Routerì˜ forward hook (routing_logits, expression_logits ì¶”ì¶œ)
        def create_router_hook(layer_name):
            def router_hook_fn(module, input, output):
                # Router forwardì˜ ë°˜í™˜ê°’: (multiplier, selected_experts, expression_logits, hn, speciality_penalty, cosine_similarities, expression_loss)
                if len(output) >= 7:
                    multiplier, selected_experts, expression_logits, hn, speciality_penalty, cosine_similarities, expression_loss = output
                    
                    # Router ë‚´ë¶€ì—ì„œ routing_logits ì¶”ì¶œ ì‹œë„
                    routing_logits = None
                    if hasattr(module, 'load_balancer'):
                        # GRUì˜ ì¶œë ¥ì„ routing_logitsë¡œ ì‚¬ìš©
                        # input[0]ì€ hidden_states, input[1]ì€ hn
                        if len(input) >= 1:
                            hidden_states = input[0]
                            hn_input = input[1] if len(input) > 1 else None
                            
                            # GRU forwardë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ routing_logits ì–»ê¸°
                            with torch.no_grad():
                                if hn_input is not None:
                                    routing_logits, _ = module.load_balancer(hidden_states, hn_input.to(hidden_states.dtype))
                                else:
                                    routing_logits, _ = module.load_balancer(hidden_states, None)
                                
                                # Reshape to [batch, seq, num_experts, router_dim]
                                batch_size, seq_len = hidden_states.shape[:2]
                                num_experts = module.num_experts
                                router_dim = module.router_dim
                                routing_logits = routing_logits.view(batch_size, seq_len, num_experts, router_dim)
                    
                    self.router_internal_data[layer_name].append({
                        'routing_logits': routing_logits.detach().cpu() if routing_logits is not None else None,
                        'expression_logits': expression_logits.detach().cpu() if isinstance(expression_logits, torch.Tensor) else None,
                        'selected_experts': selected_experts.detach().cpu(),
                        'routing_weights': multiplier.detach().cpu(),
                        'cosine_similarities': cosine_similarities.detach().cpu() if isinstance(cosine_similarities, torch.Tensor) else None,
                        'speciality_penalty': float(speciality_penalty) if isinstance(speciality_penalty, torch.Tensor) else speciality_penalty,
                        'expression_loss': float(expression_loss) if isinstance(expression_loss, torch.Tensor) else expression_loss,
                    })
            return router_hook_fn
        
        # MoE Blockì˜ forward hook
        def create_moe_hook(layer_name):
            def moe_hook_fn(module, input, output):
                # GramSpecMoEBlockì—ì„œ routing ì •ë³´ ì¶”ì¶œ
                if hasattr(module, '_last_routing_info'):
                    routing_info = module._last_routing_info
                    if routing_info is not None and len(routing_info) >= 6:
                        routing_weights, hn, speciality_loss, cosine_similarities, expression_loss, router_scores = routing_info
                        
                        # Routerì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì™€ ë§¤ì¹­
                        router_data = self.router_internal_data.get(layer_name, [])
                        if router_data:
                            latest_router_data = router_data[-1]  # ê°€ì¥ ìµœê·¼ ë°ì´í„° ì‚¬ìš©
                            
                            self.routing_data.append({
                                'layer': layer_name,
                                'routing_logits': latest_router_data.get('routing_logits'),
                                'expression_logits': latest_router_data.get('expression_logits'),
                                'routing_weights': latest_router_data.get('routing_weights'),
                                'selected_experts': latest_router_data.get('selected_experts'),
                                'cosine_similarities': latest_router_data.get('cosine_similarities'),
                                'speciality_penalty': latest_router_data.get('speciality_penalty', float(speciality_loss) if isinstance(speciality_loss, torch.Tensor) else speciality_loss),
                                'expression_loss': latest_router_data.get('expression_loss', float(expression_loss) if isinstance(expression_loss, torch.Tensor) else expression_loss),
                                'router_scores': router_scores.detach().cpu(),
                            })
            return moe_hook_fn
        
        # Routerì™€ MoE Blockì— hook ë“±ë¡
        for name, module in model.named_modules():
            # Router hook
            if hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector'):
                # GramSpecRouter
                hook = module.register_forward_hook(create_router_hook(name))
                self.router_hooks.append(hook)
                print(f"Registered router hook: {name}")
            
            # MoE Block hook
            if isinstance(module, nn.Module) and hasattr(module, '_last_routing_info'):
                hook = module.register_forward_hook(create_moe_hook(name))
                self.hooks.append(hook)
                print(f"Registered MoE block hook: {name}")
    
    def remove_hooks(self):
        """Hook ì œê±°"""
        for hook in self.hooks + self.router_hooks:
            hook.remove()
        self.hooks = []
        self.router_hooks = []
    
    def analyze_collected_data(self, num_experts: int, router_dim: int = 128) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„"""
        if not self.routing_data:
            print("âš ï¸  No routing data collected. Make sure hooks are registered correctly.")
            return {}
        
        all_metrics = []
        
        for data in self.routing_data:
            routing_logits = data.get('routing_logits')
            expression_logits = data.get('expression_logits')
            selected_experts = data.get('selected_experts')
            routing_weights = data.get('routing_weights')
            cosine_similarities = data.get('cosine_similarities')
            
            if routing_logits is None or expression_logits is None:
                # Router scoresë¡œë¶€í„° ê·¼ì‚¬ê°’ ìƒì„±
                router_scores = data.get('router_scores')
                if router_scores is not None:
                    batch_size, seq_len, num_experts_actual = router_scores.shape
                    # Router scoresë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¼ì‚¬
                    routing_logits = router_scores.unsqueeze(-1).expand(-1, -1, -1, router_dim)
                    expression_logits = routing_logits.clone()
                else:
                    continue
            
            if selected_experts is None or routing_weights is None:
                # router_scoresì—ì„œ ì¶”ì¶œ
                router_scores = data.get('router_scores')
                if router_scores is not None:
                    batch_size, seq_len, num_experts_actual = router_scores.shape
                    top_k = routing_weights.shape[-1] if routing_weights is not None else 2
                    routing_scores_flat = router_scores.view(batch_size * seq_len, num_experts_actual)
                    top_k_values, selected_experts = torch.topk(routing_scores_flat, k=min(top_k, num_experts_actual), dim=-1)
                    routing_weights = torch.softmax(top_k_values, dim=-1)
                else:
                    continue
            
            # Shape í™•ì¸ ë° ë³€í™˜
            if routing_logits.dim() == 4:
                batch_size, seq_len, num_experts_actual, router_dim_actual = routing_logits.shape
                routing_logits = routing_logits.view(batch_size * seq_len, num_experts_actual, router_dim_actual)
                expression_logits = expression_logits.view(batch_size * seq_len, num_experts_actual, router_dim_actual)
            
            if cosine_similarities is None:
                batch_size, seq_len = routing_logits.shape[0] // num_experts, 1
                cosine_similarities = torch.zeros(batch_size, seq_len, num_experts)
            
            # Analyzerì— ì „ë‹¬
            try:
                metrics = self.analyzer.analyze_routing_step(
                    routing_logits=routing_logits,
                    expression_logits=expression_logits,
                    selected_experts=selected_experts,
                    routing_weights=routing_weights,
                    speciality_penalty=data.get('speciality_penalty', 0.0),
                    cosine_similarities=cosine_similarities,
                    expression_loss=data.get('expression_loss', 0.0),
                )
                
                metrics['layer'] = data['layer']
                all_metrics.append(metrics)
            except Exception as e:
                print(f"Error analyzing data for layer {data.get('layer', 'unknown')}: {e}")
                continue
        
        return {
            'per_layer_metrics': all_metrics,
            'aggregated_metrics': self.analyzer.get_aggregated_metrics(),
            'paper_summary': self.analyzer.get_paper_metrics_summary(),
        }


def load_checkpoint_model(
    checkpoint_path: str,
    base_model_name: str,
    model_architecture,
    moe_config: Dict[str, Any],
    device: str = "cuda",
) -> Tuple[nn.Module, Any]:
    """Checkpointì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print(f"Loading base model: {base_model_name}")
    base_config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)
    base_config = base_config.to_dict()
    
    if "text_config" not in base_config:
        base_config['text_config'] = copy.deepcopy(base_config)
    
    base_config['text_config'].update(moe_config)
    base_config.update(base_config['text_config'])
    model_config = G3MoEConfig(**base_config)
    model_config.model_type = "gemma3"
    model_config.text_config.model_type = "gemma3_text"
    model_config.architectures = ["G3MoEForConditionalGeneration"]
    
    print(f"Loading checkpoint from: {checkpoint_path}")
    model = PeftModel.from_pretrained(
        model=model_architecture.from_pretrained(
            pretrained_model_name_or_path=base_model_name,
            config=model_config,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            offload_state_dict=True,
            use_cache=False,
            attn_implementation="flash_attention_3",
        ).to(device),
        model_id=checkpoint_path,
    )
    model.merge_and_unload()
    model.eval()
    
    # Tokenizer ë¡œë“œ
    try:
        tokenizer = AutoProcessor.from_pretrained(base_model_name, use_fast=True)
        if hasattr(tokenizer, 'chat_template'):
            chat_template_path = "/home/conan/workspace/llm_training/sft/config/chat_template.txt"
            if os.path.exists(chat_template_path):
                with open(chat_template_path, "r") as f:
                    tokenizer.chat_template = f.read()
    except:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)
    
    print("âœ… Model and tokenizer loaded successfully")
    return model, tokenizer


def prepare_evaluation_data(
    tokenizer: Any,
    dataset_name: Optional[str] = None,
    num_samples: int = 100,
    max_length: int = 512,
    use_training_eval_set: bool = True,
) -> List[Dict[str, torch.Tensor]]:
    """
    í‰ê°€ìš© ë°ì´í„° ì¤€ë¹„
    
    Options:
    1. í•™ìŠµì— ì‚¬ìš©í•œ test split ì‚¬ìš© (use_training_eval_set=True)
    2. HuggingFace ë°ì´í„°ì…‹ ì‚¬ìš© (dataset_name ì§€ì •)
    3. VLM í‰ê°€ ë°ì´í„°ì…‹ ì‚¬ìš© (MME, VQAv2 ë“±)
    """
    inputs_list = []
    
    # Option 1: í•™ìŠµì— ì‚¬ìš©í•œ test split ì‚¬ìš©
    if use_training_eval_set and dataset_name:
        try:
            from data.simple_sft_dataset import get_simple_sft_dataset
            print(f"Loading training eval set from: {dataset_name}")
            dataset = get_simple_sft_dataset(
                dataset_name=dataset_name,
                tokenizer=tokenizer,
                max_length=max_length,
                max_samples=num_samples,
                test_size=0.1,
                use_streaming=False
            )
            
            eval_dataset = dataset.get("test", None)
            if eval_dataset is not None:
                print(f"âœ… Loaded {len(eval_dataset)} eval samples from training dataset")
                eval_dataset = eval_dataset.select(range(min(num_samples, len(eval_dataset))))
                
                for sample in tqdm(eval_dataset, desc="Preparing eval data"):
                    # VLM ë°ì´í„°ì…‹ì¸ ê²½ìš° ì´ë¯¸ì§€ í¬í•¨
                    if 'images' in sample and sample['images']:
                        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°
                        messages = sample.get('messages', [])
                        if messages:
                            # Chat template ì ìš©
                            if hasattr(tokenizer, 'apply_chat_template'):
                                text = tokenizer.apply_chat_template(
                                    messages,
                                    add_generation_prompt=True,
                                    tokenize=False
                                )
                            else:
                                text = str(messages)
                            
                            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í•¨ê»˜ ì²˜ë¦¬
                            images = sample['images']
                            if isinstance(images, list) and len(images) > 0:
                                inputs = tokenizer(
                                    text=text,
                                    images=images[0] if len(images) == 1 else images,
                                    return_tensors="pt",
                                    truncation=True,
                                    max_length=max_length,
                                    padding="max_length",
                                )
                            else:
                                inputs = tokenizer(
                                    text=text,
                                    return_tensors="pt",
                                    truncation=True,
                                    max_length=max_length,
                                    padding="max_length",
                                )
                        else:
                            continue
                    else:
                        # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                        messages = sample.get('messages', [])
                        if messages:
                            if hasattr(tokenizer, 'apply_chat_template'):
                                text = tokenizer.apply_chat_template(
                                    messages,
                                    add_generation_prompt=True,
                                    tokenize=False
                                )
                            else:
                                text = str(messages)
                            
                            inputs = tokenizer(
                                text=text,
                                return_tensors="pt",
                                truncation=True,
                                max_length=max_length,
                                padding="max_length",
                            )
                        else:
                            continue
                    
                    if "token_type_ids" in inputs:
                        del inputs["token_type_ids"]
                    inputs_list.append(inputs)
                
                return inputs_list
        except Exception as e:
            print(f"âš ï¸ Failed to load training eval set: {e}")
            print("Falling back to default samples...")
    
    # Option 2: VLM í‰ê°€ ë°ì´í„°ì…‹ ì‚¬ìš©
    if dataset_name and dataset_name.lower() in ['mme', 'vqav2', 'textvqa', 'imagenet1k', 'imagenet-1k']:
        try:
            from datasets import load_dataset
            from PIL import Image
            import requests
            from io import BytesIO
            
            print(f"Loading VLM evaluation dataset: {dataset_name}")
            
            if dataset_name.lower() == 'mme':
                dataset = load_dataset("MMMU/MME")
                # MMEëŠ” ì—¬ëŸ¬ taskë¡œ êµ¬ì„±
                tasks = ['color', 'count', 'position', 'posters', 'ocr']
                for task in tasks[:2]:  # ì²˜ìŒ 2ê°œ taskë§Œ ì‚¬ìš©
                    if task in dataset:
                        task_data = dataset[task].select(range(min(num_samples // 2, len(dataset[task]))))
                        for sample in tqdm(task_data, desc=f"Loading {task}"):
                            image = sample['image']
                            question = sample['question']
                            prompt = f"<image>\n{question}\nAnswer:"
                            
                            inputs = tokenizer(
                                text=[prompt],
                                images=[image],
                                return_tensors="pt",
                                truncation=True,
                                max_length=max_length,
                            )
                            if "token_type_ids" in inputs:
                                del inputs["token_type_ids"]
                            inputs_list.append(inputs)
            
            elif dataset_name.lower() == 'vqav2':
                print("Loading VQAv2 dataset from HuggingFace...")
                try:
                    # VQAv2 ë°ì´í„°ì…‹ ë¡œë“œ
                    dataset = load_dataset("lmms-lab/VQAv2", split="validation")
                except:
                    # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
                    try:
                        dataset = load_dataset("datalab/vqa_v2", split="validation")
                    except:
                        dataset = load_dataset("Antonio/vqa_v2", split="validation")
                
                dataset = dataset.select(range(min(num_samples, len(dataset))))
                
                for sample in tqdm(dataset, desc="Loading VQAv2"):
                    try:
                        # ì´ë¯¸ì§€ ë¡œë“œ
                        if 'image' in sample:
                            image = sample['image']
                        elif 'image_url' in sample:
                            # URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                            img_url = sample['image_url']
                            response = requests.get(img_url, timeout=10)
                            image = Image.open(BytesIO(response.content)).convert('RGB')
                        elif 'image_path' in sample:
                            image = Image.open(sample['image_path']).convert('RGB')
                        else:
                            continue
                        
                        # ì§ˆë¬¸ ì¶”ì¶œ
                        question = sample.get('question', sample.get('text', ''))
                        if not question:
                            continue
                        
                        # í”„ë¡¬í”„íŠ¸ ìƒì„±
                        prompt = f"<image>\nQuestion: {question}\nAnswer:"
                        
                        inputs = tokenizer(
                            text=[prompt],
                            images=[image],
                            return_tensors="pt",
                            truncation=True,
                            max_length=max_length,
                        )
                        if "token_type_ids" in inputs:
                            del inputs["token_type_ids"]
                        inputs_list.append(inputs)
                    except Exception as e:
                        print(f"âš ï¸ Error loading VQAv2 sample: {e}")
                        continue
            
            elif dataset_name.lower() == 'textvqa':
                print("Loading TextVQA dataset from HuggingFace...")
                try:
                    dataset = load_dataset("lmms-lab/TextVQA", split="validation")
                except:
                    try:
                        dataset = load_dataset("textvqa", split="validation")
                    except:
                        dataset = load_dataset("HuggingFaceM4/TextVQA", split="validation")
                
                dataset = dataset.select(range(min(num_samples, len(dataset))))
                
                for sample in tqdm(dataset, desc="Loading TextVQA"):
                    try:
                        # ì´ë¯¸ì§€ ë¡œë“œ
                        if 'image' in sample:
                            image = sample['image']
                        elif 'image_url' in sample:
                            img_url = sample['image_url']
                            response = requests.get(img_url, timeout=10)
                            image = Image.open(BytesIO(response.content)).convert('RGB')
                        elif 'image_path' in sample:
                            image = Image.open(sample['image_path']).convert('RGB')
                        else:
                            continue
                        
                        # ì§ˆë¬¸ ì¶”ì¶œ
                        question = sample.get('question', sample.get('text', ''))
                        if not question:
                            continue
                        
                        # í”„ë¡¬í”„íŠ¸ ìƒì„± (TextVQAëŠ” í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸)
                        prompt = f"<image>\nQuestion: {question}\nAnswer the question based on the text visible in the image:"
                        
                        inputs = tokenizer(
                            text=[prompt],
                            images=[image],
                            return_tensors="pt",
                            truncation=True,
                            max_length=max_length,
                        )
                        if "token_type_ids" in inputs:
                            del inputs["token_type_ids"]
                        inputs_list.append(inputs)
                    except Exception as e:
                        print(f"âš ï¸ Error loading TextVQA sample: {e}")
                        continue
            
            elif dataset_name.lower() in ['imagenet1k', 'imagenet-1k']:
                print("Loading ImageNet-1k dataset from HuggingFace...")
                try:
                    # ImageNet-1k ë°ì´í„°ì…‹ ë¡œë“œ
                    dataset = load_dataset("imagenet-1k", split="validation")
                except:
                    try:
                        dataset = load_dataset("Maysee/tiny-imagenet", split="validation")
                        print("âš ï¸ Using tiny-imagenet as fallback")
                    except:
                        # ImageNet ì§ì ‘ ê²½ë¡œ ì‹œë„
                        try:
                            dataset = load_dataset("laion/laion400m", split="train", streaming=True)
                            print("âš ï¸ Using LAION-400M as fallback (will sample first N)")
                            dataset = list(dataset.take(num_samples))
                        except Exception as e:
                            raise Exception(f"Could not load ImageNet-1k: {e}")
                
                # ImageNetì€ ì´ë¯¸ì§€ ë¶„ë¥˜ì´ë¯€ë¡œ í´ë˜ìŠ¤ ì´ë¦„ì„ ì§ˆë¬¸ìœ¼ë¡œ ì‚¬ìš©
                if not isinstance(dataset, list):
                    dataset = dataset.select(range(min(num_samples, len(dataset))))
                
                for sample in tqdm(dataset, desc="Loading ImageNet-1k"):
                    try:
                        # ì´ë¯¸ì§€ ë¡œë“œ
                        if 'image' in sample:
                            image = sample['image']
                        elif 'img' in sample:
                            image = sample['img']
                        else:
                            continue
                        
                        # ë ˆì´ë¸” ì¶”ì¶œ
                        label = sample.get('label', sample.get('labels', None))
                        label_text = sample.get('label_text', sample.get('class_name', ''))
                        
                        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ì´ë¯¸ì§€ ë¶„ë¥˜)
                        if label_text:
                            prompt = f"<image>\nWhat is the main object or class in this image? Answer with a single word or short phrase:"
                        else:
                            prompt = f"<image>\nWhat is the main object or class in this image? Answer with a single word or short phrase:"
                        
                        inputs = tokenizer(
                            text=[prompt],
                            images=[image],
                            return_tensors="pt",
                            truncation=True,
                            max_length=max_length,
                        )
                        if "token_type_ids" in inputs:
                            del inputs["token_type_ids"]
                        
                        # ë ˆì´ë¸” ì •ë³´ë„ í•¨ê»˜ ì €ì¥ (ì •í™•ë„ ê³„ì‚°ìš©)
                        inputs['label'] = label
                        inputs['label_text'] = label_text
                        inputs_list.append(inputs)
                    except Exception as e:
                        print(f"âš ï¸ Error loading ImageNet-1k sample: {e}")
                        continue
            
            if inputs_list:
                print(f"âœ… Successfully loaded {len(inputs_list)} samples from {dataset_name}")
                return inputs_list
        except Exception as e:
            import traceback
            print(f"âš ï¸ Failed to load VLM dataset {dataset_name}: {e}")
            traceback.print_exc()
            print("Falling back to default samples...")
    
    # Option 3: ê¸°ë³¸ ìƒ˜í”Œ (fallback)
    print("Using default text samples...")
    sample_texts = [
        "The capital of France is Paris.",
        "Machine learning is a subset of artificial intelligence.",
        "Python is a popular programming language.",
        "The Earth orbits around the Sun.",
        "Water boils at 100 degrees Celsius at sea level.",
    ] * (num_samples // 5 + 1)
    
    sample_texts = sample_texts[:num_samples]
    
    for text in tqdm(sample_texts, desc="Preparing evaluation data"):
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=max_length,
            padding="max_length",
        )
        if "token_type_ids" in inputs:
            del inputs["token_type_ids"]
        inputs_list.append(inputs)
    
    return inputs_list


def evaluate_model(
    model: nn.Module,
    tokenizer: Any,
    eval_data: List[Dict[str, torch.Tensor]],
    analyzer: GramSpecAnalyzer,
    device: str = "cuda",
    max_samples: Optional[int] = None,
) -> Dict[str, Any]:
    """ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
    print(f"\n{'='*60}")
    print("Starting Model Evaluation")
    print(f"{'='*60}")
    
    collector = RoutingInfoCollector(analyzer)
    collector.register_hooks(model)
    
    model.eval()
    eval_data = eval_data[:max_samples] if max_samples else eval_data
    
    print(f"Evaluating on {len(eval_data)} samples...")
    with torch.no_grad():
        for i, inputs in enumerate(tqdm(eval_data, desc="Evaluating")):
            # Move to device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Forward pass
            try:
                outputs = model(**inputs)
            except Exception as e:
                print(f"Error in forward pass for sample {i}: {e}")
                continue
    
    # ë¶„ì„ ìˆ˜í–‰
    print("\nAnalyzing collected routing data...")
    results = collector.analyze_collected_data(
        num_experts=analyzer.num_experts,
        router_dim=analyzer.router_dim,
    )
    
    collector.remove_hooks()
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Evaluate checkpoint model with GramSpec analysis")
    parser.add_argument("--checkpoint_path", type=str, required=True,
                       help="Path to checkpoint directory")
    parser.add_argument("--base_model", type=str, default="Gunulhona/Gemma-3-4B",
                       help="Base model name")
    parser.add_argument("--num_samples", type=int, default=100,
                       help="Number of evaluation samples")
    parser.add_argument("--max_length", type=int, default=512,
                       help="Maximum sequence length")
    parser.add_argument("--output_dir", type=str, default="./evaluation_results",
                       help="Output directory for results")
    parser.add_argument("--device", type=str, default="cuda",
                       help="Device to use")
    parser.add_argument("--num_experts", type=int, default=8,
                       help="Number of experts (from config)")
    parser.add_argument("--router_dim", type=int, default=128,
                       help="Router dimension")
    parser.add_argument("--eval_dataset", type=str, default=None,
                       help="Evaluation dataset name (e.g., 'HuggingFaceTB/smoltalk' for training eval set, or 'mme', 'vqav2', 'textvqa', 'imagenet1k' for VLM benchmarks)")
    parser.add_argument("--use_training_eval_set", action="store_true",
                       help="Use test split from training dataset")
    
    args = parser.parse_args()
    
    # MoE config (checkpointì—ì„œ ê°€ì ¸ì™€ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
    moe_config = {
        "n_shared_experts": 1,
        "n_routed_experts": args.num_experts,
        "n_group": 2,
        "topk_group": 2,
        "num_experts_per_tok": 2,
        "first_k_dense_replace": 8,
        "router_aux_loss_coef": 9e-1,
        "router_jitter_noise": 1e-05,
        "input_jitter_noise": 1e-05,
        "router_z_loss_coef": 1e-2,
        "ema_alpha": 0.99,
        "balancing_strength": 5e-2,
        "no_rope_layer_interval": 4,
        "use_sliding_window": True,
        "rope_scaling": {
            "rope_type": "yarn",
            "factor": 8.0
        },
        "use_bfloat16": True
    }
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_checkpoint_model(
        checkpoint_path=args.checkpoint_path,
        base_model_name=args.base_model,
        model_architecture=G3MoEForConditionalGeneration,
        moe_config=moe_config,
        device=args.device,
    )
    
    # Analyzer ì´ˆê¸°í™”
    analyzer = GramSpecAnalyzer(
        num_experts=args.num_experts,
        router_dim=args.router_dim,
    )
    
    # í‰ê°€ ë°ì´í„° ì¤€ë¹„
    eval_data = prepare_evaluation_data(
        tokenizer=tokenizer,
        dataset_name=args.eval_dataset,
        num_samples=args.num_samples,
        max_length=args.max_length,
        use_training_eval_set=args.use_training_eval_set,
    )
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluate_model(
        model=model,
        tokenizer=tokenizer,
        eval_data=eval_data,
        analyzer=analyzer,
        device=args.device,
    )
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs(args.output_dir, exist_ok=True)
    output_file = os.path.join(args.output_dir, "evaluation_results.json")
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n{'='*60}")
    print("Evaluation Complete!")
    print(f"{'='*60}")
    print(f"Results saved to: {output_file}")
    
    # ë…¼ë¬¸ìš© ìš”ì•½ ì¶œë ¥
    if 'paper_summary' in results:
        print("\nğŸ“Š Paper Summary Metrics:")
        print(json.dumps(results['paper_summary'], indent=2))
    
    # ì§‘ê³„ ì§€í‘œ ì¶œë ¥
    if 'aggregated_metrics' in results:
        print("\nğŸ“ˆ Aggregated Metrics:")
        agg = results['aggregated_metrics']
        print(f"  Load Balancing CV: {agg.get('final_load_balancing_cv', 'N/A'):.4f}")
        print(f"  Load Imbalance Ratio: {agg.get('final_load_imbalance_ratio', 'N/A'):.4f}")
        print(f"  Expert Utilization Rate: {agg.get('expert_utilization_rate', 'N/A'):.4f}")
        print(f"  Gram Orthogonality: {agg.get('avg_gram_orthogonality', 'N/A'):.4f}")


if __name__ == "__main__":
    main()

