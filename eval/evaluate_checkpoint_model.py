# coding=utf-8
"""
Checkpoint ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í•™ìŠµëœ checkpoint ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ SPECTRA MoE ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- Load balancing metrics
- Expert specialization metrics
- Routing quality metrics
"""

import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import json
from tqdm import tqdm
import argparse
from peft import PeftModel
from transformers import AutoConfig, AutoModel, AutoModelForCausalLM, AutoProcessor, AutoTokenizer
from transformers.generation.configuration_utils import GenerationConfig
from transformers.image_utils import load_image
import copy

from models import G3MoEModel, G3MoETextModel, G3MoEConfig, G3MoEForCausalLM, G3MoEForConditionalGeneration, G3MoETextConfig
from transformers.modeling_utils import VLMS
from eval.spectra_analysis import SPECTRAAnalyzer


# Register models
AutoConfig.register("g3moe", G3MoEConfig)
AutoConfig.register("g3moe_text", G3MoETextConfig)
AutoModel.register(G3MoEConfig, G3MoEModel)
AutoModel.register(G3MoETextConfig, G3MoETextModel)
AutoModelForCausalLM.register(G3MoEConfig, G3MoEForConditionalGeneration)
VLMS.append("g3moe")


class RoutingInfoCollector:
    """ëª¨ë¸ forward passì—ì„œ routing ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” hook"""
    
    def __init__(self, analyzer: SPECTRAAnalyzer):
        self.analyzer = analyzer
        self.hooks = []
        self.router_hooks = []
        self.routing_data = []
        self.router_internal_data = defaultdict(list)
        
    def register_hooks(self, model: nn.Module):
        """ëª¨ë¸ì˜ MoE ë ˆì´ì–´ì™€ Routerì— hook ë“±ë¡"""
        from models.g3moe_model import G3MoERouter, G3MoEGRINMoE
        from models.spectra import SPECTRARouter, SPECTRABlock
        
        # Routerì˜ forward hook (routing_logits, expression_logits ì¶”ì¶œ)
        def create_router_hook(layer_name):
            def router_hook_fn(module, input, output):
                # Router forwardì˜ ë°˜í™˜ê°’: (multiplier, selected_experts, expression_logits, hn, speciality_penalty, cosine_similarities, expression_loss, routing_probs_full)
                # 8ê°œ ê°’ì„ ë°˜í™˜ (routing_probs_full ì¶”ê°€ë¨)
                if len(output) >= 8:
                    multiplier, selected_experts, expression_logits, hn, speciality_penalty, cosine_similarities, expression_loss, routing_probs_full = output
                elif len(output) >= 7:
                    # ì´ì „ ë²„ì „ í˜¸í™˜ì„± (7ê°œë§Œ ë°˜í™˜í•˜ëŠ” ê²½ìš°)
                    multiplier, selected_experts, expression_logits, hn, speciality_penalty, cosine_similarities, expression_loss = output
                    routing_probs_full = None
                else:
                    # outputì´ íŠœí”Œì´ ì•„ë‹ˆê±°ë‚˜ ê¸¸ì´ê°€ ë¶€ì¡±í•œ ê²½ìš°
                    return
                
                # Router ë‚´ë¶€ì—ì„œ routing_logits ì¶”ì¶œ ì‹œë„
                routing_logits = None
                if hasattr(module, 'load_balancer'):
                    # GRUì˜ ì¶œë ¥ì„ routing_logitsë¡œ ì‚¬ìš©
                    # input[0]ì€ hidden_states, input[1]ì€ hn
                    if len(input) >= 1:
                        hidden_states = input[0]
                        hn_input = input[1] if len(input) > 1 else None
                        
                        # GRU forwardë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ routing_logits ì–»ê¸°
                        with torch.no_grad():
                            if hn_input is not None:
                                routing_logits, _ = module.load_balancer(hidden_states, hn_input.to(hidden_states.dtype))
                            else:
                                routing_logits, _ = module.load_balancer(hidden_states, None)
                            
                            # Reshape to [batch, seq, num_experts, router_dim]
                            batch_size, seq_len = hidden_states.shape[:2]
                            num_experts = module.num_experts
                            router_dim = module.router_dim
                            routing_logits = routing_logits.view(batch_size, seq_len, num_experts, router_dim)
                
                # expression_logitsì˜ shape í™•ì¸ ë° ìˆ˜ì •
                # Router forwardì—ì„œ expression_logitsëŠ” view(hidden_shape)ë¥¼ ê±°ì³ [batch, seq, num_experts, router_dim] í˜•íƒœê°€ ë˜ì–´ì•¼ í•¨
                # í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” [batch, seq, 1, router_dim] ë˜ëŠ” [batch*seq, 1, router_dim] í˜•íƒœì¼ ìˆ˜ ìˆìŒ
                expression_logits_fixed = expression_logits
                if isinstance(expression_logits, torch.Tensor):
                    # expression_logitsì˜ shape í™•ì¸
                    if expression_logits.dim() == 4:
                        exp_batch, exp_seq, exp_num_exp, exp_router_dim = expression_logits.shape
                        if exp_num_exp == 1 and routing_logits is not None:
                            # [batch, seq, 1, router_dim] -> [batch, seq, num_experts, router_dim]ë¡œ expand
                            if routing_logits.dim() == 4:
                                _, _, num_experts, router_dim = routing_logits.shape
                                expression_logits_fixed = expression_logits.expand(exp_batch, exp_seq, num_experts, router_dim)
                    elif expression_logits.dim() == 3:
                        exp_batch_seq, exp_dim1, exp_dim2 = expression_logits.shape
                        if routing_logits is not None:
                            if routing_logits.dim() == 4:
                                batch_size, seq_len, num_experts, router_dim = routing_logits.shape
                                if exp_dim1 == 1 and exp_dim2 == router_dim:
                                    # [batch*seq, 1, router_dim] -> [batch*seq, num_experts, router_dim]ë¡œ expand
                                    expression_logits_fixed = expression_logits.expand(exp_batch_seq, num_experts, router_dim)
                                elif exp_dim1 * exp_dim2 == num_experts * router_dim:
                                    # [batch*seq, num_experts*router_dim] -> [batch*seq, num_experts, router_dim]ë¡œ reshape
                                    expression_logits_fixed = expression_logits.view(exp_batch_seq, num_experts, router_dim)
                            elif routing_logits.dim() == 3:
                                batch_seq_len, num_experts, router_dim = routing_logits.shape
                                if exp_dim1 == 1 and exp_dim2 == router_dim:
                                    expression_logits_fixed = expression_logits.expand(batch_seq_len, num_experts, router_dim)
                                elif exp_dim1 * exp_dim2 == num_experts * router_dim:
                                    expression_logits_fixed = expression_logits.view(batch_seq_len, num_experts, router_dim)
                
                self.router_internal_data[layer_name].append({
                    'routing_logits': routing_logits.detach().cpu() if routing_logits is not None else None,
                    'expression_logits': expression_logits_fixed.detach().cpu() if isinstance(expression_logits_fixed, torch.Tensor) else None,
                    'selected_experts': selected_experts.detach().cpu(),
                    'routing_weights': multiplier.detach().cpu(),
                    'cosine_similarities': cosine_similarities.detach().cpu() if isinstance(cosine_similarities, torch.Tensor) else None,
                    'speciality_penalty': float(speciality_penalty) if isinstance(speciality_penalty, torch.Tensor) else speciality_penalty,
                    'expression_loss': float(expression_loss) if isinstance(expression_loss, torch.Tensor) else expression_loss,
                })
            return router_hook_fn
        
        # MoE Blockì˜ forward hook (G3MoEGRINMoEì™€ SPECTRABlock ëª¨ë‘ ì§€ì›)
        def create_moe_hook(layer_name):
            def moe_hook_fn(module, input, output):
                # G3MoEGRINMoE: output = (final_hidden_states, (routing_weights, hn, speciality_loss, cosine_similarities, expression_loss))
                # ì£¼ì˜: forwardì—ì„œ router_logitsë¡œ ì´ë¦„ì„ ë°”ê¾¸ì§€ë§Œ ì‹¤ì œë¡œëŠ” routing_weightsì…ë‹ˆë‹¤
                if isinstance(output, tuple) and len(output) == 2:
                    final_hidden_states, routing_info_tuple = output
                    if isinstance(routing_info_tuple, tuple) and len(routing_info_tuple) >= 5:
                        routing_weights_from_moe, hn, speciality_loss, cosine_similarities, expression_loss = routing_info_tuple[:5]
                        
                        # Routerì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì™€ ë§¤ì¹­
                        # Router hookì´ ë¨¼ì € ì‹¤í–‰ë˜ì–´ router_internal_dataì— ë°ì´í„°ê°€ ì €ì¥ë˜ì–´ì•¼ í•¨
                        # Global routerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ëª¨ë“  router ë°ì´í„° ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒì„ ì‚¬ìš©
                        latest_router_data = None
                        if self.router_internal_data:
                            # ëª¨ë“  router ë°ì´í„° ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒ ì°¾ê¸°
                            all_router_data = []
                            for router_name, router_data_list in self.router_internal_data.items():
                                if router_data_list:
                                    all_router_data.extend([(router_name, data) for data in router_data_list])
                            
                            if all_router_data:
                                # ê°€ì¥ ìµœê·¼ ë°ì´í„° ì‚¬ìš© (ë§ˆì§€ë§‰ í•­ëª©)
                                _, latest_router_data = all_router_data[-1]
                        
                        if latest_router_data:
                            
                            # routing_weightsëŠ” top-kì— ëŒ€í•œ ê°€ì¤‘ì¹˜ì´ë¯€ë¡œ, router_scoresë¥¼ ì¬êµ¬ì„±
                            # Router hookì—ì„œ ìˆ˜ì§‘í•œ selected_expertsì™€ routing_weightsë¥¼ ì‚¬ìš©
                            router_scores = None
                            selected_experts = latest_router_data.get('selected_experts')
                            routing_weights = latest_router_data.get('routing_weights')
                            
                            if selected_experts is not None and routing_weights is not None:
                                # selected_experts: [batch*seq, top_k]
                                # routing_weights: [batch*seq, top_k]
                                batch_size, seq_len = input[0].shape[:2] if len(input) > 0 and isinstance(input[0], torch.Tensor) else (1, 1)
                                num_experts = module.num_experts if hasattr(module, 'num_experts') else selected_experts.max().item() + 1
                                
                                # ëª¨ë“  expertì— ëŒ€í•œ ì ìˆ˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                                router_scores = torch.zeros(batch_size * seq_len, num_experts, dtype=routing_weights.dtype)
                                
                                # selected_expertsì— í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì— routing_weights í• ë‹¹
                                batch_seq_indices = torch.arange(batch_size * seq_len, device=selected_experts.device).unsqueeze(1).expand(-1, selected_experts.shape[-1])
                                router_scores[batch_seq_indices, selected_experts] = routing_weights
                                
                                router_scores = router_scores.view(batch_size, seq_len, num_experts)
                            
                            self.routing_data.append({
                                'layer': layer_name,
                                'routing_logits': latest_router_data.get('routing_logits'),
                                'expression_logits': latest_router_data.get('expression_logits'),
                                'routing_weights': latest_router_data.get('routing_weights'),
                                'selected_experts': latest_router_data.get('selected_experts'),
                                'cosine_similarities': latest_router_data.get('cosine_similarities'),
                                'speciality_penalty': latest_router_data.get('speciality_penalty', float(speciality_loss) if isinstance(speciality_loss, torch.Tensor) else speciality_loss),
                                'expression_loss': latest_router_data.get('expression_loss', float(expression_loss) if isinstance(expression_loss, torch.Tensor) else expression_loss),
                                'router_scores': router_scores.detach().cpu() if router_scores is not None else None,
                            })
                            return
                
                # SPECTRABlock: _last_routing_info ì‚¬ìš©
                if hasattr(module, '_last_routing_info'):
                    routing_info = module._last_routing_info
                    if routing_info is not None and len(routing_info) >= 6:
                        routing_weights, hn, speciality_loss, cosine_similarities, expression_loss, router_scores = routing_info
                        
                        # Routerì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì™€ ë§¤ì¹­
                        # Global routerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ëª¨ë“  router ë°ì´í„° ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒì„ ì‚¬ìš©
                        latest_router_data = None
                        if self.router_internal_data:
                            all_router_data = []
                            for router_name, router_data_list in self.router_internal_data.items():
                                if router_data_list:
                                    all_router_data.extend([(router_name, data) for data in router_data_list])
                            
                            if all_router_data:
                                _, latest_router_data = all_router_data[-1]
                        
                        if latest_router_data:
                            self.routing_data.append({
                                'layer': layer_name,
                                'routing_logits': latest_router_data.get('routing_logits'),
                                'expression_logits': latest_router_data.get('expression_logits'),
                                'routing_weights': latest_router_data.get('routing_weights'),
                                'selected_experts': latest_router_data.get('selected_experts'),
                                'cosine_similarities': latest_router_data.get('cosine_similarities'),
                                'speciality_penalty': latest_router_data.get('speciality_penalty', float(speciality_loss) if isinstance(speciality_loss, torch.Tensor) else speciality_loss),
                                'expression_loss': latest_router_data.get('expression_loss', float(expression_loss) if isinstance(expression_loss, torch.Tensor) else expression_loss),
                                'router_scores': router_scores.detach().cpu(),
                            })
            return moe_hook_fn
        
        # Routerì™€ MoE Blockì— hook ë“±ë¡
        router_count = 0
        moe_count = 0
        
        for name, module in model.named_modules():
            # Router hook (G3MoERouter ë˜ëŠ” SPECTRARouter)
            if isinstance(module, (G3MoERouter, SPECTRARouter)) or (hasattr(module, 'load_balancer') and hasattr(module, 'expression_projector')):
                hook = module.register_forward_hook(create_router_hook(name))
                self.router_hooks.append(hook)
                router_count += 1
                print(f"Registered router hook: {name}")
            
            # MoE Block hook (G3MoEGRINMoE ë˜ëŠ” SPECTRABlock)
            if isinstance(module, (G3MoEGRINMoE, SPECTRABlock)) or hasattr(module, '_last_routing_info'):
                hook = module.register_forward_hook(create_moe_hook(name))
                self.hooks.append(hook)
                moe_count += 1
                print(f"Registered MoE block hook: {name}")
        
        print(f"\nâœ… Hook registration complete: {router_count} routers, {moe_count} MoE blocks")
    
    def remove_hooks(self):
        """Hook ì œê±°"""
        for hook in self.hooks + self.router_hooks:
            hook.remove()
        self.hooks = []
        self.router_hooks = []
    
    def analyze_collected_data(self, num_experts: int, router_dim: int = 128) -> Dict[str, Any]:
        """ìˆ˜ì§‘ëœ ë°ì´í„° ë¶„ì„"""
        if not self.routing_data:
            print("âš ï¸  No routing data collected. Make sure hooks are registered correctly.")
            return {}
        
        all_metrics = []
        
        for data in self.routing_data:
            routing_logits = data.get('routing_logits')
            expression_logits = data.get('expression_logits')
            selected_experts = data.get('selected_experts')
            routing_weights = data.get('routing_weights')
            cosine_similarities = data.get('cosine_similarities')
            
            if routing_logits is None or expression_logits is None:
                # Router scoresë¡œë¶€í„° ê·¼ì‚¬ê°’ ìƒì„±
                router_scores = data.get('router_scores')
                if router_scores is not None:
                    batch_size, seq_len, num_experts_actual = router_scores.shape
                    # Router scoresë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê·¼ì‚¬
                    routing_logits = router_scores.unsqueeze(-1).expand(-1, -1, -1, router_dim)
                    expression_logits = routing_logits.clone()
                else:
                    continue
            
            if selected_experts is None or routing_weights is None:
                # router_scoresì—ì„œ ì¶”ì¶œ
                router_scores = data.get('router_scores')
                if router_scores is not None:
                    batch_size, seq_len, num_experts_actual = router_scores.shape
                    top_k = routing_weights.shape[-1] if routing_weights is not None else 2
                    routing_scores_flat = router_scores.view(batch_size * seq_len, num_experts_actual)
                    top_k_values, selected_experts = torch.topk(routing_scores_flat, k=min(top_k, num_experts_actual), dim=-1)
                    routing_weights = torch.softmax(top_k_values, dim=-1)
                else:
                    continue
            
            # Shape í™•ì¸ ë° ë³€í™˜
            # routing_logitsì™€ expression_logitsì˜ shapeì„ ì¼ì¹˜ì‹œí‚´
            if routing_logits is not None and expression_logits is not None:
                # routing_logits shape í™•ì¸
                batch_seq_len = None
                num_experts_actual = None
                router_dim_actual = None
                
                if routing_logits.dim() == 4:
                    batch_size, seq_len, num_experts_actual, router_dim_actual = routing_logits.shape
                    batch_seq_len = batch_size * seq_len
                    routing_logits = routing_logits.view(batch_seq_len, num_experts_actual, router_dim_actual)
                elif routing_logits.dim() == 3:
                    # [batch*seq, num_experts, router_dim] í˜•íƒœ
                    batch_seq_len, num_experts_actual, router_dim_actual = routing_logits.shape
                else:
                    print(f"âš ï¸ Unexpected routing_logits shape: {routing_logits.shape}")
                    continue
                
                # expression_logits shape í™•ì¸ ë° ë³€í™˜
                if expression_logits.dim() == 4:
                    # [batch, seq, num_experts, router_dim]
                    exp_batch_size, exp_seq_len, exp_num_experts, exp_router_dim = expression_logits.shape
                    exp_batch_seq_len = exp_batch_size * exp_seq_len
                    if exp_batch_seq_len == batch_seq_len:
                        expression_logits = expression_logits.view(exp_batch_seq_len, exp_num_experts, exp_router_dim)
                    else:
                        # Shapeì´ ë‹¤ë¥´ë©´ ì¬êµ¬ì„± ì‹œë„
                        expression_logits = expression_logits.view(-1, exp_num_experts, exp_router_dim)
                elif expression_logits.dim() == 3:
                    # [batch*seq, num_experts, router_dim] ë˜ëŠ” [batch*seq, num_experts*router_dim]
                    exp_batch_seq_len, dim1, dim2 = expression_logits.shape
                    
                    if dim1 == num_experts_actual and dim2 == router_dim_actual:
                        # ì´ë¯¸ ì˜¬ë°”ë¥¸ shape
                        pass
                    elif dim1 * dim2 == num_experts_actual * router_dim_actual:
                        # [batch*seq, num_experts*router_dim] í˜•íƒœë¥¼ [batch*seq, num_experts, router_dim]ë¡œ ë³€í™˜
                        expression_logits = expression_logits.view(exp_batch_seq_len, num_experts_actual, router_dim_actual)
                    else:
                        # Shapeì´ ë§ì§€ ì•Šìœ¼ë©´ ì¬êµ¬ì„± ì‹œë„
                        total_elements = expression_logits.numel()
                        expected_elements = batch_seq_len * num_experts_actual * router_dim_actual
                        
                        if total_elements == expected_elements:
                            expression_logits = expression_logits.view(batch_seq_len, num_experts_actual, router_dim_actual)
                        else:
                            print(f"âš ï¸ Cannot reshape expression_logits: shape={expression_logits.shape}, expected elements={expected_elements}, actual={total_elements}")
                            # ìµœì„ ì˜ ë…¸ë ¥ìœ¼ë¡œ ì¬êµ¬ì„±
                            if total_elements % (num_experts_actual * router_dim_actual) == 0:
                                new_batch_seq = total_elements // (num_experts_actual * router_dim_actual)
                                expression_logits = expression_logits.view(new_batch_seq, num_experts_actual, router_dim_actual)
                            else:
                                continue
                elif expression_logits.dim() == 2:
                    # [batch*seq, num_experts*router_dim] í˜•íƒœ
                    exp_batch_seq_len, exp_total_dim = expression_logits.shape
                    if exp_total_dim == num_experts_actual * router_dim_actual:
                        expression_logits = expression_logits.view(exp_batch_seq_len, num_experts_actual, router_dim_actual)
                    else:
                        print(f"âš ï¸ Cannot reshape expression_logits from 2D: shape={expression_logits.shape}, expected dim={num_experts_actual * router_dim_actual}")
                        continue
                else:
                    print(f"âš ï¸ Unexpected expression_logits shape: {expression_logits.shape}")
                    continue
                
                # ìµœì¢… shape í™•ì¸
                if routing_logits.shape != expression_logits.shape:
                    print(f"âš ï¸ Shape mismatch after conversion: routing_logits={routing_logits.shape}, expression_logits={expression_logits.shape}")
                    # ìµœì†Œí•œì˜ shapeìœ¼ë¡œ ë§ì¶¤
                    min_batch_seq = min(routing_logits.shape[0], expression_logits.shape[0])
                    routing_logits = routing_logits[:min_batch_seq]
                    expression_logits = expression_logits[:min_batch_seq]
            
            if cosine_similarities is None:
                batch_size, seq_len = routing_logits.shape[0] // num_experts, 1
                cosine_similarities = torch.zeros(batch_size, seq_len, num_experts)
            
            # Analyzerì— ì „ë‹¬
            try:
                metrics = self.analyzer.analyze_routing_step(
                    routing_logits=routing_logits,
                    expression_logits=expression_logits,
                    selected_experts=selected_experts,
                    routing_weights=routing_weights,
                    speciality_penalty=data.get('speciality_penalty', 0.0),
                    cosine_similarities=cosine_similarities,
                    expression_loss=data.get('expression_loss', 0.0),
                )
                
                metrics['layer'] = data['layer']
                all_metrics.append(metrics)
            except Exception as e:
                print(f"Error analyzing data for layer {data.get('layer', 'unknown')}: {e}")
                continue
        
        return {
            'per_layer_metrics': all_metrics,
            'aggregated_metrics': self.analyzer.get_aggregated_metrics(),
            'paper_summary': self.analyzer.get_paper_metrics_summary(),
        }


def load_checkpoint_model(
    checkpoint_path: str,
    base_model_name: str,
    model_architecture,
    moe_config: Dict[str, Any],
    device: str = "cuda",
) -> Tuple[nn.Module, Any]:
    """Checkpointì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print(f"Loading base model: {base_model_name}")
    base_config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)
    base_config = base_config.to_dict()
    
    if "text_config" not in base_config:
        base_config['text_config'] = copy.deepcopy(base_config)
    
    base_config['text_config'].update(moe_config)
    base_config.update(base_config['text_config'])
    model_config = G3MoEConfig(**base_config)
    model_config.model_type = "gemma3"
    model_config.text_config.model_type = "gemma3_text"
    model_config.architectures = ["G3MoEForConditionalGeneration"]
    
    print(f"Loading checkpoint from: {checkpoint_path}")
    model = PeftModel.from_pretrained(
        model=model_architecture.from_pretrained(
            pretrained_model_name_or_path=base_model_name,
            config=model_config,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            offload_state_dict=True,
            use_cache=False,
            attn_implementation="flash_attention_3",
        ).to(device),
        model_id=checkpoint_path,
    )
    model.merge_and_unload()
    model.eval()
    
    # Tokenizer ë¡œë“œ
    try:
        tokenizer = AutoProcessor.from_pretrained(base_model_name, use_fast=True)
        if hasattr(tokenizer, 'chat_template'):
            chat_template_path = "/home/conan/workspace/llm_training/sft/config/chat_template.txt"
            if os.path.exists(chat_template_path):
                with open(chat_template_path, "r") as f:
                    tokenizer.chat_template = f.read()
    except:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)
    
    print("âœ… Model and tokenizer loaded successfully")
    return model, tokenizer


def prepare_evaluation_data(
    tokenizer: Any,
    dataset_name: Optional[str] = None,
    num_samples: int = 500,
    max_length: int = 512,
    use_training_eval_set: bool = True,
) -> List[Dict[str, torch.Tensor]]:
    """
    í‰ê°€ìš© ë°ì´í„° ì¤€ë¹„ (streaming ëª¨ë“œ)
    
    HuggingFace Hubì—ì„œ streaming ëª¨ë“œë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    from datasets import load_dataset
    from datasets.iterable_dataset import IterableDataset
    from itertools import islice
    
    inputs_list = []
    
    if not dataset_name:
        raise ValueError("dataset_name must be provided")
    
    # Option 1: í•™ìŠµì— ì‚¬ìš©í•œ test split ì‚¬ìš©
    if use_training_eval_set:
        from data.simple_sft_dataset import get_simple_sft_dataset
        print(f"Loading training eval set from: {dataset_name} (streaming mode)")
        
        dataset = get_simple_sft_dataset(
            dataset_name=dataset_name,
            tokenizer=tokenizer,
            max_length=max_length,
            max_samples=num_samples,
            test_size=0.1,
            use_streaming=True
        )
        
        eval_dataset = dataset.get("test", None)
        if eval_dataset is None:
            raise ValueError(f"No test split found in dataset: {dataset_name}")
        
        print(f"âœ… Loaded eval dataset (streaming mode)")
        
        # Streaming ë°ì´í„°ì…‹ ì²˜ë¦¬
        sample_count = 0
        for sample in tqdm(eval_dataset, desc="Preparing eval data", total=num_samples):
            if sample_count >= num_samples:
                break
            
            # VLM ë°ì´í„°ì…‹ì¸ ê²½ìš° ì´ë¯¸ì§€ í¬í•¨
            if 'images' in sample and sample['images']:
                messages = sample.get('messages', [])
                if not messages:
                    continue
                
                # Chat template ì ìš©
                if hasattr(tokenizer, 'apply_chat_template'):
                    text = tokenizer.apply_chat_template(
                        messages,
                        add_generation_prompt=True,
                        tokenize=False
                    )
                else:
                    text = str(messages)
                
                # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í•¨ê»˜ ì²˜ë¦¬
                images = sample['images']
                if isinstance(images, list) and len(images) > 0:
                    inputs = tokenizer(
                        text=text,
                        images=images[0] if len(images) == 1 else images,
                        return_tensors="pt",
                        truncation=True,
                        max_length=max_length,
                        padding="max_length",
                    )
                else:
                    inputs = tokenizer(
                        text=text,
                        return_tensors="pt",
                        truncation=True,
                        max_length=max_length,
                        padding="max_length",
                    )
            else:
                # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                messages = sample.get('messages', [])
                if not messages:
                    continue
                
                if hasattr(tokenizer, 'apply_chat_template'):
                    text = tokenizer.apply_chat_template(
                        messages,
                        add_generation_prompt=True,
                        tokenize=False
                    )
                else:
                    text = str(messages)
                
                inputs = tokenizer(
                    text=text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=max_length,
                    padding="max_length",
                )
            
            if "token_type_ids" in inputs:
                del inputs["token_type_ids"]
            inputs_list.append(inputs)
            sample_count += 1
        
        print(f"âœ… Successfully loaded {len(inputs_list)} samples from {dataset_name}")
        return inputs_list
    
    # Option 2: HuggingFace Hubì—ì„œ ì§ì ‘ ë¡œë“œ (streaming ëª¨ë“œ)
    print(f"Loading dataset from HuggingFace Hub: {dataset_name} (streaming mode)")
    
    # ë°ì´í„°ì…‹ ì´ë¦„ì—ì„œ split ì¶”ì¶œ (í˜•ì‹: "dataset_name:split" í•„ìˆ˜)
    if ':' not in dataset_name:
        raise ValueError(
            f"Dataset name must include split in format 'dataset_name:split'. "
            f"Got: {dataset_name}. Example: 'lmms-lab/VQAv2:validation'"
        )
    
    dataset_path, split_name = dataset_name.split(':', 1)
    
    try:
        # Streaming ëª¨ë“œë¡œ ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = load_dataset(
            dataset_path,
            split=split_name,
            streaming=True
        )
        
        if not isinstance(dataset, IterableDataset):
            raise ValueError(f"Expected IterableDataset, got {type(dataset)}")
        
        print(f"âœ… Loaded dataset in streaming mode")
        
        # Streaming ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
        sample_count = 0
        for sample in tqdm(islice(dataset, num_samples), desc="Preparing eval data", total=num_samples):
            try:
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                image = None
                if 'image' in sample:
                    image = sample['image']
                elif 'images' in sample:
                    images = sample['images']
                    if isinstance(images, list) and len(images) > 0:
                        image = images[0]
                    elif images:
                        image = images
                
                # í…ìŠ¤íŠ¸/ì§ˆë¬¸ ì²˜ë¦¬
                question = None
                if 'question' in sample:
                    question = sample['question']
                elif 'text' in sample:
                    question = sample['text']
                elif 'prompt' in sample:
                    question = sample['prompt']
                elif 'messages' in sample:
                    messages = sample['messages']
                    if hasattr(tokenizer, 'apply_chat_template'):
                        question = tokenizer.apply_chat_template(
                            messages,
                            add_generation_prompt=True,
                            tokenize=False
                        )
                    else:
                        question = str(messages)
                
                if not question:
                    continue
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                if image is not None:
                    prompt =  tokenizer.apply_chat_template(
                            [
                                {
                                    "role": "system",
                                    "content": [
                                        {"type": "text", "text": "You are a helpful assistant."}
                                    ]
                                },
                                {
                                    "role": "user",
                                    "content": [
                                        {"type": "text", "text": question},
                                        {"type": "image"}
                                    ]
                                }
                            ],
                            # tokenize=True,
                            add_generation_prompt=True,
                            # return_tensors="pt",
                            # return_dict=True,
                        )
                    inputs = tokenizer(
                        text=prompt,
                        images=[image],
                        return_tensors="pt",
                        truncation=True,
                        max_length=max_length,
                        padding="max_length",
                    )
                else:
                    prompt =  tokenizer.apply_chat_template(
                            [
                                {
                                    "role": "system",
                                    "content": [
                                        {"type": "text", "text": "You are a helpful assistant."}
                                    ]
                                },
                                {
                                    "role": "user",
                                    "content": [
                                        {"type": "text", "text": question}
                                    ]
                                }
                            ],
                            # tokenize=True,
                            add_generation_prompt=True,
                            # return_tensors="pt",
                            # return_dict=True,
                        )
                    inputs = tokenizer(
                        text=prompt,
                        return_tensors="pt",
                        truncation=True,
                        max_length=max_length,
                        padding="max_length",
                    )
                
                if "token_type_ids" in inputs:
                    del inputs["token_type_ids"]
                
                # ë ˆì´ë¸” ì •ë³´ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì €ì¥
                if 'label' in sample:
                    inputs['label'] = sample['label']
                if 'label_text' in sample:
                    inputs['label_text'] = sample['label_text']
                if 'answer' in sample:
                    inputs['answer'] = sample['answer']
                
                inputs_list.append(inputs)
                sample_count += 1
                
            except Exception as e:
                print(f"âš ï¸ Error processing sample: {e}")
                continue
        
        print(f"âœ… Successfully loaded {len(inputs_list)} samples from {dataset_name}")
        return inputs_list
        
    except Exception as e:
        import traceback
        print(f"âŒ Failed to load dataset {dataset_name}: {e}")
        traceback.print_exc()
        raise


def evaluate_model(
    model: nn.Module,
    tokenizer: Any,
    eval_data: List[Dict[str, torch.Tensor]],
    analyzer: SPECTRAAnalyzer,
    device: str = "cuda",
    max_samples: Optional[int] = None,
) -> Dict[str, Any]:
    """ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
    print(f"\n{'='*60}")
    print("Starting Model Evaluation")
    print(f"{'='*60}")
    
    collector = RoutingInfoCollector(analyzer)
    collector.register_hooks(model)
    
    model.eval()
    
    # max_samplesê°€ ì§€ì •ëœ ê²½ìš° ì œí•œ
    if max_samples:
        eval_data = eval_data[:max_samples]
    
    num_samples = len(eval_data)
    print(f"Evaluating on {num_samples} samples...")
    
    with torch.no_grad():
        for i, inputs in enumerate(tqdm(eval_data, desc="Evaluating", total=num_samples)):
            # Move to device
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            # Forward pass
            try:
                outputs = model(**inputs)
            except Exception as e:
                print(f"Error in forward pass for sample {i}: {e}")
                continue
    
    # ë¶„ì„ ìˆ˜í–‰
    print("\nAnalyzing collected routing data...")
    results = collector.analyze_collected_data(
        num_experts=analyzer.num_experts,
        router_dim=analyzer.router_dim,
    )
    
    collector.remove_hooks()
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate checkpoint model with SPECTRA analysis")
    parser.add_argument(
        "--checkpoint_path",
        type=str,
        required=True,
        help="Path to checkpoint directory")
    parser.add_argument(
        "--base_model",
        type=str,
        default="Gunulhona/Gemma-3-4B",
        help="Base model name")
    parser.add_argument(
        "--num_samples",
        type=int,
        default=100,
        help="Number of evaluation samples")
    parser.add_argument(
        "--max_length",
        type=int,
        default=512,
        help="Maximum sequence length")
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./evaluation_results",
        help="Output directory for results")
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="Device to use")
    parser.add_argument(
        "--eval_dataset",
        type=str,
        required=True,
        help="Evaluation dataset name from HuggingFace Hub. Format: 'dataset_name:split' (e.g., 'lmms-lab/VQAv2:validation'). All datasets are loaded in streaming mode.")
    parser.add_argument(
        "--use_training_eval_set",
        action="store_true",
        help="Use test split from training dataset (streaming mode)")
    
    args = parser.parse_args()
    
    # MoE config (checkpointì—ì„œ ê°€ì ¸ì™€ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
    moe_config = {
        "n_shared_experts": 1,
        "n_routed_experts": 8,
        "n_group": 2,
        "topk_group": 2,
        "num_experts_per_tok": 2,
        "first_k_dense_replace": 8,
        "router_aux_loss_coef": 9e-1,
        "router_jitter_noise": 1e-05,
        "input_jitter_noise": 1e-05,
        "router_z_loss_coef": 1e-2,
        "ema_alpha": 0.99,
        "balancing_strength": 5e-2,
        "no_rope_layer_interval": 4,
        "use_sliding_window": True,
        "rope_scaling": {
            "rope_type": "yarn",
            "factor": 8.0
        },
        "use_bfloat16": True
    }
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_checkpoint_model(
        checkpoint_path=args.checkpoint_path,
        base_model_name=args.base_model,
        model_architecture=G3MoEForConditionalGeneration,
        moe_config=moe_config,
        device=args.device,
    )
    
    # Analyzer ì´ˆê¸°í™”
    analyzer = SPECTRAAnalyzer(
        num_experts=moe_config.get('n_routed_experts', 8),
        router_dim=moe_config.get('router_dim', 128),
    )
    
    # í‰ê°€ ë°ì´í„° ì¤€ë¹„
    eval_data = prepare_evaluation_data(
        tokenizer=tokenizer,
        dataset_name=args.eval_dataset,
        num_samples=args.num_samples,
        max_length=args.max_length,
        use_training_eval_set=args.use_training_eval_set,
    )
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluate_model(
        model=model,
        tokenizer=tokenizer,
        eval_data=eval_data,
        analyzer=analyzer,
        device=args.device,
    )
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs(args.output_dir, exist_ok=True)
    output_file = os.path.join(args.output_dir, "evaluation_results.json")
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n{'='*60}")
    print("Evaluation Complete!")
    print(f"{'='*60}")
    print(f"Results saved to: {output_file}")
    
    # ë…¼ë¬¸ìš© ìš”ì•½ ì¶œë ¥
    if 'paper_summary' in results:
        print("\nğŸ“Š Paper Summary Metrics:")
        print(json.dumps(results['paper_summary'], indent=2))
    
    # ì§‘ê³„ ì§€í‘œ ì¶œë ¥
    if 'aggregated_metrics' in results:
        print("\nğŸ“ˆ Aggregated Metrics:")
        agg = results['aggregated_metrics']
        
        def format_metric(value, default='N/A'):
            if value == default or value is None:
                return default
            try:
                return f"{float(value):.4f}"
            except (ValueError, TypeError):
                return str(value)
        
        print("\nğŸ“Š ì£¼ìš” Load Balancing ì§€í‘œ:")
        print(f"  Load Balancing CV: {format_metric(agg.get('final_load_balancing_cv', 'N/A'))}")
        print(f"  Load Imbalance Ratio: {format_metric(agg.get('final_load_imbalance_ratio', 'N/A'))}")
        print(f"  MaxVio (Maximum Violation): {format_metric(agg.get('final_maxvio', 'N/A'))}")
        print(f"  Aux Loss: {format_metric(agg.get('final_aux_loss', 'N/A'))}")
        print(f"  Expert Utilization Rate: {format_metric(agg.get('expert_utilization_rate', 'N/A'))}")
        
        print("\nğŸ“ˆ ìµœê·¼ ë…¼ë¬¸ ì§€í‘œ:")
        print(f"  LPR (Layer-wise Performance Ratio): {format_metric(agg.get('final_lpr', 'N/A'))}")
        print(f"  Expert Efficiency (DeepSpeed MoE): {format_metric(agg.get('final_expert_efficiency', 'N/A'))}")
        print(f"  Expert Capacity Utilization: {format_metric(agg.get('avg_expert_capacity_utilization', 'N/A'))}")
        print(f"  Load Variance: {format_metric(agg.get('avg_load_variance', 'N/A'))}")
        
        print("\nğŸ”¬ Gram Matrix & Specialization ì§€í‘œ:")
        print(f"  Gram Orthogonality (í‰ê· ): {format_metric(agg.get('avg_gram_orthogonality', 'N/A'))}")
        if 'std_gram_orthogonality' in agg:
            print(f"  Gram Orthogonality (í‘œì¤€í¸ì°¨): {format_metric(agg.get('std_gram_orthogonality', 'N/A'))}")


if __name__ == "__main__":
    main()

