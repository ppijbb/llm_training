=================================================================
INTRODUCTION
=================================================================

The advancement of large language models (LLMs) has significantly advanced natural language processing. Guided by the "Scaling Laws" [Kaplan et al., 2020], which correlate model performance with the number of parameters, the field has made substantial progress. However, this progress requires substantial computational resources, as the resources required for training and inference grow quadratically with model size, creating a barrier to further scaling [Brown et al., 2020]. Mixture-of-Experts (MoE) architectures have emerged as a standard approach to address this challenge [Shazeer et al., 2017]. MoE models operationalize the concept of 'sparse activation' by using only a fraction of their total parameters (the activated 'experts') to process each token. This allows them to scale their model capacity while keeping computational costs manageable. The success of models like GShard [Lepikhin et al., 2020], Switch Transformer [Fedus et al., 2021], and more recently, the open-source Mixtral [Jiang et al., 2024], has established MoE as a standard approach for next-generation LLMs.

However, the full potential of an MoE model depends entirely on the efficiency of a small network called the 'router,' which decides which expert(s) should handle each incoming token. Current routing mechanisms suffer from several persistent issues. The most significant is 'expert collapse,' a phenomenon where the router learns to favor a small subset of popular experts, leaving the vast majority of parameters underutilized. This effectively reduces the model's operational capacity, defeating the purpose of having a large number of experts. A related problem is 'functional redundancy,' where multiple experts learn to perform similar functions, wasting model capacity. Furthermore, most routers are stateless and make decisions for each token in isolation, ignoring the sequential context of a sentence, which is a critical limitation for tasks requiring complex reasoning or long-form generation.

In this work, we propose **SPECTRA (Sinkhorn Projected Experts for Consistent TRAjectory routing)**, a new routing mechanism designed to address these limitations. We redefine routing from being merely a problem of 'selecting the best expert' to a process of 'structurally enforcing expert specialization while maintaining optimal load balance.' The core idea behind SPECTRA combines the Gram matrix from linear algebra with OSR (Orthogonal Sinkhorn Routing). We treat the knowledge representation of each expert as a vector and use the Gram matrix to measure the similarity between these vectors. By applying a repulsive cost function that penalizes similarity during routing, we structurally enforce that experts develop distinct functional specializations. Simultaneously, our parameter-free Sinkhorn algorithm ensures uniform expert utilization through constraint satisfaction.

SPECTRA combines several components to realize this approach. First, to enable context-aware routing, it employs a **GRU (Gated Recurrent Unit)-based sequential router** that leverages information from previous tokens to inform subsequent routing decisions, creating consistent trajectories across the sequence. Second, it uses an **Orthogonal Expression Projector** that learns a representation space where expert specializations are orthogonal to one another. Third, **OSR (Orthogonal Sinkhorn Routing)** uses a repulsive cost function and pure mathematical Sinkhorn algorithm to ensure optimal load distribution and automatic expert separation without learned parameters. Finally, an optional **Orthogonal Loss** on the projector weights provides initial guidance during early training.

This paper makes the following contributions. First, we introduce SPECTRA, a novel routing mechanism that explicitly and simultaneously enforces expert specialization, diversity, and load balancing. Second, we provide a modular design that allows for high reusability, enabling practitioners to easily convert a dense model to an MoE architecture or replace the router in existing MoE models. Third, we validate our approach through extensive experiments and ablation studies, systematically demonstrating the positive impact of each component of SPECTRA on model performance. In the following sections, we will review related work, detail the technical aspects of SPECTRA, and demonstrate its superiority through empirical results.
