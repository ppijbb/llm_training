# MoE Routing Metrics - í†µí•© ì°¸ê³  ìë£Œ (2025ë…„ 11ì›” 28ì¼ ê¸°ì¤€)

> **âš ï¸ ì¤‘ìš”**: ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì€ **routing ë°©ë²•ë¡ **ì…ë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ì€ ë³´ì¡° ì§€í‘œì´ë©°, **routing ìì²´ì˜ íŠ¹ì„±ê³¼ íš¨ê³¼**ë¥¼ ì¦ëª…í•˜ëŠ” ê²ƒì´ ìš°ì„ ì…ë‹ˆë‹¤.

---

## ğŸ“… ì—…ë°ì´íŠ¸ ì •ë³´
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025ë…„ 11ì›” 28ì¼
**ê¸°ì¤€**: ì‹¤ì œ ë…¼ë¬¸ê³¼ Technical Reportë§Œ ì¸ìš©

---

## ğŸ¯ Routing ë°©ë²•ë¡  ë…¼ë¬¸ì˜ í•µì‹¬ ì§€í‘œ (ìµœìš°ì„ )

### A. Routing Quality Metrics (í•„ìˆ˜)

#### A.1 Expert Specialization (í•µì‹¬ ì£¼ì¥)
- **Expert Overlap**: Jaccard similarity between expert token sets
  - ë‚®ì„ìˆ˜ë¡ specialization ìš°ìˆ˜
  - **ëª©í‘œ**: SPECTRA < Switch Top-2 < Switch Top-1
- **Gram Matrix Orthogonality**: `1 - ||G-I||_F / (E*âˆš2)` (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, 1.0 = ì™„ì „ orthogonal)
  - **í˜„ì¬ ì¸¡ì •ê°’**: 0.94 âœ…
- **Expert Diversity Score**: 1 - mean(expert_similarity)
  - ë†’ì„ìˆ˜ë¡ diverse/specialized
- **Expert-Task Correlation**: Expertë³„ task specialization score

#### A.2 Load Balancing (í•„ìˆ˜ ë¹„êµ ì§€í‘œ)
- **Expert Entropy**: H(expert) = -Î£áµ¢ páµ¢ log páµ¢
  - ë†’ì„ìˆ˜ë¡ ê· í˜• (ì´ìƒ: log(E))
- **Load Balancing Coefficient (CV)**: std / mean
  - ë‚®ì„ìˆ˜ë¡ ê· í˜•
  - **í˜„ì¬ ì¸¡ì •ê°’**: 0.3 âŒ (ëª©í‘œ: < 0.1)
- **Expert Collapse Rate**: ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” expert ë¹„ìœ¨
- **MaxVio (Maximum Violation)**: max deviation from mean load
- **Gini Coefficient**: Load distribution inequality (0 = perfect equality)
  - **LPR ë³´ê³ ê°’**: 0.035 (from 0.70)

#### A.3 Routing Decision Quality
- **Routing Entropy**: Per-token routing entropy
- **Routing Consistency**: Checkpoint ê°„ routing ì¼ê´€ì„± (%)
- **Sequential Routing Consistency**: ì—°ì† í† í°ì˜ expert ì„ íƒ ì¼ê´€ì„±
- **Top-k Overlap**: ì—°ì† í† í°ì˜ top-k expert ê²¹ì¹¨ ë¹„ìœ¨

#### A.4 Expression Projection Effectiveness
- **Expression-Routing Alignment**: Expressionê³¼ routingì˜ ì¼ì¹˜ë„
- **Expression Projection Orthogonality**: Expression projectorì˜ orthogonal quality
- **Ablation Impact**: Expression ì œê±° ì‹œ ì„±ëŠ¥ ì €í•˜

---

## ğŸ“Š ì‹¤ì œ ë…¼ë¬¸/Technical Report ê¸°ë°˜ ëª¨ë¸ ë° ë°©ë²•ë¡ 

### ìµœì‹  MoE ëª¨ë¸ (2025)

#### 1. Kimi K2
**ë…¼ë¬¸**: arxiv:2507.20534, "Kimi K2: Open Agentic Intelligence"
**URL**: https://arxiv.org/abs/2507.20534

**Architecture**:
- 1 trillion total parameters
- 32 billion activated per token
- 384 experts total, 8 activated per token (+ shared expert)
- MuonClip optimizer with QK-clip technique
- No expert grouping (n_group = 1)

**Routing Mechanism**:
- QK-clip technique for stable attention and balanced routing
- **âš ï¸ Routing metrics (CV, Orthogonality, Overlap)ëŠ” ë…¼ë¬¸ì—ì„œ ë³´ê³ ë˜ì§€ ì•ŠìŒ**

**Performance**:
- SOTA among open-source non-thinking models
- Strong performance in agentic tasks

---

#### 2. GLM-4.5
**Technical Report**: arxiv:2508.06471
**URL**: https://arxiv.org/abs/2508.06471

**Architecture**:
- 355 billion total parameters
- 32 billion activated per token
- Multi-stage training (23 trillion tokens)

**Routing Mechanism**:
- **Loss-free balance approach with sigmoid gating**
- Even distribution across experts
- **âš ï¸ Routing metricsëŠ” technical reportì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ ë˜ì§€ ì•ŠìŒ**

**Performance**:
- TAU-Bench: 70.1%
- AIME 24: 91.0%
- SWE-bench Verified: 64.2%

---

#### 3. Minimax-Text-01 / MiniMax-M1
**Technical Report**: 
- MiniMax-Text-01: arxiv:2501.08313
- MiniMax-M1: arxiv:2506.13585

**Architecture**:
- 456 billion total parameters
- 45.9 billion activated per token
- 32 experts
- ABAB pattern: Alternating Lightning Attention and Softmax Attention
- Context window: 1M tokens (training), 4M tokens (inference)

**Routing Mechanism**:
- MoE routing integrated with ABAB attention pattern
- **âš ï¸ MoE routing metricsì— ëŒ€í•œ êµ¬ì²´ì  ìˆ˜ì¹˜ëŠ” technical reportì—ì„œ ë³´ê³ ë˜ì§€ ì•ŠìŒ**

---

#### 4. DeepSeek-V3
**Technical Report**: DeepSeek official

**Architecture**:
- 671 billion total parameters
- 37 billion activated per token
- 256 routed experts + 1 shared expert per layer
- 8 routed + 1 shared expert activated per token

**Routing Mechanism**:
- **Auxiliary-loss-free load balancing**
- Dynamic bias adjustment (underutilized â†’ bias increases, overutilized â†’ bias decreases)
- Sequence-wise balance loss (Î± = 0.0001)

**âš ï¸ ì£¼ì˜**: êµ¬ì²´ì ì¸ routing metrics (CV, Orthogonality)ëŠ” technical reportì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

#### 5. Qwen3-MoE
**Technical Report**: arxiv:2505.09388, "Qwen3 Technical Report"
**URL**: https://arxiv.org/abs/2505.09388

**Architecture**:
- 128 total experts
- 8 experts activated per token
- No shared experts (unlike Qwen2.5-MoE)
- Fine-grained expert segmentation

**Routing Mechanism**:
- **Global-batch load balancing loss**
- Top-k learned gating function (k=8)

**âš ï¸ ì£¼ì˜**: êµ¬ì²´ì ì¸ routing metrics (CV, Orthogonality, Overlap)ëŠ” technical reportì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

### ìµœì‹  Routing ë°©ë²•ë¡  (2025ë…„ 11ì›” ê¸°ì¤€)

#### 1. ERMoE (Eigen-Reparameterized MoE)
**ë…¼ë¬¸**: arxiv:2511.10971, November 2025
**ì œëª©**: "ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization"
**URL**: https://arxiv.org/abs/2511.10971

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Learned orthonormal eigenbasis for each expert
- Eigenbasis Score = cosine similarity between input features and expert's basis
- Content-aware routing tied directly to experts' representation spaces

**ì¥ì **:
- Explicit balancing losses ë¶ˆí•„ìš”
- Stable utilization, interpretable specialization
- Natural flatter expert load distributions
- No interference gradients from auxiliary losses

**ì„±ëŠ¥** (ë…¼ë¬¸ì—ì„œ ë³´ê³ ):
- SOTA accuracy on ImageNet classification
- SOTA on cross-modal image-text retrieval (COCO, Flickr30K)
- 3D MRI variant: +7% brain age prediction accuracy

**âš ï¸ Metrics**: ë…¼ë¬¸ì—ì„œ "natural flatter expert load distributions" ì–¸ê¸‰ë˜ë‚˜, êµ¬ì²´ì ì¸ CV, Orthogonality, Overlap ìˆ˜ì¹˜ëŠ” ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

#### 2. Latent Prototype Routing (LPR)
**ë…¼ë¬¸**: arxiv:2506.21328, June 2025
**ì œëª©**: "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts"
**URL**: https://arxiv.org/abs/2506.21328

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Clustering perspective for expert routing
- Generalizes existing routing methods

**ì‹¤ì œ ë³´ê³ ëœ Metrics** (ë…¼ë¬¸ì—ì„œ):
- **Gini coefficient**: 0.70 â†’ 0.035 (average reduction)
- **Min-max expert load ratio**: 1e-6 â†’ 0.70
- **í…ŒìŠ¤íŠ¸ ëª¨ë¸**: DeepSeek-V3, Qwen3-MoE, Mixtral

**âš ï¸ ì£¼ì˜**: CV, Orthogonality, Expert Overlapì€ ë…¼ë¬¸ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

#### 3. LASER (Load-Aware Scalable Expert Routing)
**ë…¼ë¬¸**: arxiv:2510.03293, October 2025
**ì œëª©**: "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing"
**URL**: https://arxiv.org/abs/2510.03293

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Plug-and-play inference-time routing algorithm
- Adapts to gate's score distribution
- Routes to least-loaded experts when scores are uniform

**íŠ¹ì§•**:
- No model retraining required
- Inference-time optimization only

**ì„±ëŠ¥** (ë…¼ë¬¸ì—ì„œ ë³´ê³ ):
- Enhanced throughput on Mixtral-8x7B
- Maintains accuracy while improving load balance

**âš ï¸ Metrics**: êµ¬ì²´ì ì¸ CV, Orthogonality ìˆ˜ì¹˜ëŠ” ë…¼ë¬¸ì—ì„œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

#### 4. RoMA (Routing Manifold Alignment)
**ë…¼ë¬¸**: arxiv:2511.07419, November 2025
**ì œëª©**: "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs"
**URL**: https://arxiv.org/abs/2511.07419

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Aligns routing weights with task embeddings
- Manifold regularization term
- Lightweight fine-tuning of routers only

**ì„±ëŠ¥** (ë…¼ë¬¸ì—ì„œ ë³´ê³ ):
- Substantial improvements across benchmarks
- Better generalization performance

**âš ï¸ Metrics**: Routing metrics (CV, Orthogonality)ëŠ” ë…¼ë¬¸ì—ì„œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

#### 5. Advancing Expert Specialization
**ë…¼ë¬¸**: arxiv:2505.22323, May 2025
**ì œëª©**: "Advancing Expert Specialization for Better MoE"
**URL**: https://arxiv.org/abs/2505.22323

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Orthogonality loss: Encourages experts to process distinct token types
- Variance loss: Promotes discriminative routing decisions

**ì„±ëŠ¥** (ë…¼ë¬¸ì—ì„œ ë³´ê³ ):
- Performance gains up to 23.79% over classic MoE baselines
- Maintains load balancing without architectural modifications

**âš ï¸ Metrics**: êµ¬ì²´ì ì¸ CV, Orthogonality ìˆ˜ì¹˜ëŠ” ë…¼ë¬¸ì—ì„œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

#### 6. Loss-Free Balancing
**ë…¼ë¬¸**: arxiv:2408.15664, August 2024
**ì œëª©**: "Loss-Free Load Balancing for Mixture-of-Experts"
**URL**: https://arxiv.org/abs/2408.15664

**í•µì‹¬ ì•„ì´ë””ì–´**:
- Dynamic expert bias adjustment
- No auxiliary losses (eliminates interference gradients)
- Expert-wise bias updated based on recent load

**ì„±ëŠ¥** (ë…¼ë¬¸ì—ì„œ ë³´ê³ ):
- Better performance and load balance
- Tested on MoE models up to 3B parameters
- Trained on up to 200B tokens

**âš ï¸ Metrics**: êµ¬ì²´ì ì¸ CV, Orthogonality ìˆ˜ì¹˜ëŠ” ë…¼ë¬¸ì—ì„œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

---

### ë ˆê±°ì‹œ ë°©ë²•ë¡  (ì°¸ê³ ìš©)

#### 1. Switch Transformer
**ë…¼ë¬¸**: Fedus et al., 2021, "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"

**ì‹¤ì œ ë³´ê³ ëœ Metrics**:
- **Balanced expert utilization ratio**: 94.8% (nearly uniform distribution)
- **Auxiliary loss coefficient (Î»)**: 0.01
- **Training speedup**: Up to 7Ã— compared to dense models

**âš ï¸ ì£¼ì˜**: CV, Orthogonality, Expert Overlapì€ ë…¼ë¬¸ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

**Routing Mechanism**:
- Top-1 gating (each token â†’ single expert)
- Auxiliary load balancing loss
- Expert capacity factor (default 1.0)

---

#### 2. Expert Choice Routing
**ë…¼ë¬¸**: Zhou et al., 2022 (NeurIPS), "Mixture-of-Experts with Expert Choice Routing"

**ì‹¤ì œ ë³´ê³ ëœ Metrics**:
- **Training convergence**: More than 2Ã— faster than Switch Transformer/GShard
- **GLUE/SuperGLUE**: 7 out of 11 tasks outperform T5 dense model
- **Load balancing**: Fixed bucket size per expert (balanced by design)

**âš ï¸ ì£¼ì˜**: êµ¬ì²´ì ì¸ CV, Orthogonality, Overlap ìˆ˜ì¹˜ëŠ” ë…¼ë¬¸ì—ì„œ ë³´ê³ ë˜ì§€ ì•ŠìŒ

**Routing Mechanism**:
- Experts select top-k tokens (inverse of token choice)
- Fixed number of tokens per expert
- Variable number of experts per token

---

#### 3. Hash Routing
**ë…¼ë¬¸**: Roller et al., 2021

**ì‹¤ì œ ë³´ê³ ëœ Metrics**:
- **Loss improvement**: 1.5% over dense (16 experts)
- **Load balance**: Perfect (deterministic)
- **Limitation**: No specialization (context ignored)

**âš ï¸ ì£¼ì˜**: OrthogonalityëŠ” N/A (deterministic routing)

---

## ğŸ“ˆ ì‹¤ì œ ë…¼ë¬¸ì—ì„œ ë³´ê³ ëœ Metrics ìš”ì•½

### Load Balancing Metrics

| Method | Metric | Value | Source | Note |
|--------|--------|-------|--------|------|
| Switch Transformer | Balanced utilization | 94.8% | Fedus et al., 2021 | Actual reported |
| LPR | Gini coefficient | 0.70 â†’ 0.035 | arxiv:2506.21328 | Actual reported |
| LPR | Min-max expert load ratio | 1e-6 â†’ 0.70 | arxiv:2506.21328 | Actual reported |
| Expert Choice | Training convergence | 2Ã— faster | Zhou et al., 2022 | Actual reported |
| Hash Routing | Load balance | Perfect (CV ~0.0) | Roller et al., 2021 | By design |

### Expert Specialization Metrics

| Method | Metric | Value | Source | Note |
|--------|--------|-------|--------|------|
| Advancing Expert Specialization | Performance gain | Up to 23.79% | arxiv:2505.22323 | Actual reported |
| ERMoE | Load distribution | "Natural flatter" | arxiv:2511.10971 | Qualitative only |
| Hash Routing | Specialization | None (high overlap) | Roller et al., 2021 | By design |

### âš ï¸ ì¤‘ìš” ë°œê²¬

**ëŒ€ë¶€ë¶„ì˜ ë…¼ë¬¸ê³¼ technical reportì—ì„œ ì „í†µì ì¸ routing metrics (CV, Orthogonality, Expert Overlap)ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ í•˜ì§€ ì•ŠìŒ**

**ì‹¤ì œ ìˆ˜ì¹˜ê°€ ë³´ê³ ëœ ê²ƒ**:
- Switch Transformer: Balanced utilization 94.8%
- LPR: Gini coefficient 0.035, Min-max ratio 0.70
- Expert Choice: 2Ã— faster convergence
- Advancing Expert Specialization: 23.79% performance gain

**ì •ëŸ‰ì  ìˆ˜ì¹˜ê°€ ì—†ëŠ” ê²ƒ**:
- ERMoE: "Natural flatter load" (ì •ëŸ‰ì  ìˆ˜ì¹˜ ì—†ìŒ)
- DeepSeek-V3: "Balanced utilization" (ì •ëŸ‰ì  ìˆ˜ì¹˜ ì—†ìŒ)
- Qwen3-MoE: "Global-batch load balancing" (ì •ëŸ‰ì  ìˆ˜ì¹˜ ì—†ìŒ)
- Kimi K2, GLM-4.5, Minimax: Routing metrics ë¯¸ë³´ê³ 

---

## ğŸ“ ë…¼ë¬¸ í‘œ ì‘ì„± ê°€ì´ë“œ (ì‹¤ì œ ë³´ê³ ëœ ìˆ˜ì¹˜ë§Œ)

### Table 1: Routing Quality Comparison (í•µì‹¬ í‘œ)

**âš ï¸ ì¤‘ìš”**: ëŒ€ë¶€ë¶„ì˜ ìµœì‹  ë…¼ë¬¸ì—ì„œ ì „í†µì ì¸ metrics (CV, Orthogonality, Overlap)ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ í•˜ì§€ ì•ŠìŒ

```
Method | Expert Overlap | Gram Ortho* | Expert Entropy | Load Balance CV | Gini Coeff | Routing Consistency | Collapse | Source
-------|----------------|-------------|----------------|-----------------|------------|---------------------|----------|-------
Switch Top-1 (2021) | N/A | N/A | N/A | N/A | N/A | N/A | Yes | Fedus et al., 2021 (utilization 94.8% only)
Switch Top-2 (2021) | N/A | N/A | N/A | N/A | N/A | N/A | Partial | Fedus et al., 2021
Expert Choice (2022) | N/A | N/A | N/A | N/A | N/A | N/A | Minimal | Zhou et al., 2022 (2Ã— faster convergence)
Hash Routing | High | N/A | 2.8-3.0 | ~0.0 | ~0.0 | N/A | No | Roller et al., 2021 (by design)
DeepSeek-V3 (2024) | N/A | N/A | N/A | N/A | N/A | N/A | No | Technical Report (metrics not reported)
Qwen3-MoE (2024) | N/A | N/A | N/A | N/A | N/A | N/A | No | arxiv:2505.09388 (metrics not reported)
Kimi K2 (2025) | N/A | N/A | N/A | N/A | N/A | N/A | No | arxiv:2507.20534 (metrics not reported)
GLM-4.5 (2025) | N/A | N/A | N/A | N/A | N/A | N/A | No | arxiv:2508.06471 (metrics not reported)
Minimax (2025) | N/A | N/A | N/A | N/A | N/A | N/A | No | arxiv:2501.08313 (metrics not reported)
ERMoE (2025) | N/A | N/A | N/A | N/A | N/A | N/A | No | arxiv:2511.10971 (metrics not reported)
LPR (2025) | N/A | N/A | N/A | N/A | 0.035 | N/A | No | arxiv:2506.21328 (Gini only)
LASER (2025) | N/A | N/A | N/A | N/A | N/A | N/A | No | arxiv:2510.03293 (metrics not reported)
RoMA (2025) | N/A | N/A | N/A | N/A | N/A | N/A | No | arxiv:2511.07419 (metrics not reported)
SPECTRA (Ours) | ì¸¡ì • í•„ìš” | 0.94 âœ… | ì¸¡ì • í•„ìš” | 0.3 âŒ | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | No | ì§ì ‘ ì¸¡ì •
```

**ì‹¤ì œ ë…¼ë¬¸ì—ì„œ ë³´ê³ ëœ Metrics**:
- **Switch Transformer**: Balanced utilization 94.8% (Fedus et al., 2021)
- **LPR**: Gini coefficient 0.035, Min-max ratio 0.70 (arxiv:2506.21328)
- **Expert Choice**: 2Ã— faster convergence (Zhou et al., 2022)
- **Advancing Expert Specialization**: Up to 23.79% performance gain (arxiv:2505.22323)

**âš ï¸ ê²°ë¡ **: ëŒ€ë¶€ë¶„ì˜ ìµœì‹  ë…¼ë¬¸ì´ ì „í†µì ì¸ routing metricsë¥¼ ë³´ê³ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, **ì§ì ‘ ì¸¡ì •í•˜ì—¬ ë¹„êµ**í•´ì•¼ í•¨

**ì§€í‘œ ì„¤ëª…**: 
- **Expert Overlap**: Jaccard similarity (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, 0% = ì™„ì „ ë¶„ë¦¬)
- **Gram Ortho***: `1 - ||G-I||_F / (E*âˆš2)` (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, 1.0 = ì™„ì „ orthogonal)
  - í˜„ì¬ ì¸¡ì •ê°’ 0.94: âœ… ì¢‹ì€ ìˆ˜ì¤€
- **Expert Entropy**: Normalized entropy (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, 3.0 = ì™„ì „ ê· í˜• for 8 experts)
- **Load Balance CV**: Coefficient of variation (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, 0 = ì™„ì „ ê· í˜•)
  - í˜„ì¬ ì¸¡ì •ê°’ 0.3: âŒ Moderate imbalance (ëª©í‘œ: < 0.1, LPR Gini 0.035 ê¸°ì¤€)
- **Gini Coefficient**: Load distribution inequality (0 = perfect equality, 1 = perfect inequality)
  - LPR ë³´ê³ ê°’: 0.035 âœ… (from 0.70)
- **Min-max Expert Load Ratio**: min(expert_load) / max(expert_load) (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, 1.0 = perfect balance)
  - LPR ë³´ê³ ê°’: 0.70 âœ… (from 1e-6)
- **Utilization**: Balanced expert utilization ratio
  - Switch Transformer ë³´ê³ ê°’: 94.8% âœ…

---

### Table 2: Ablation Study - Routing Metrics

**âš ï¸ ì£¼ì˜**: ì•„ë˜ ìˆ˜ì¹˜ëŠ” ëª©í‘œê°’/ê¸°ëŒ€ê°’ ë²”ìœ„ì…ë‹ˆë‹¤. ì‹¤ì œ ì‹¤í—˜ í›„ ì¸¡ì •ê°’ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”

```
Variant | Expert Overlap | Gram Ortho | Load Balance CV | Routing Consistency | Sequential Consistency
--------|----------------|------------|-----------------|---------------------|------------------------
SPECTRA-Full | ì¸¡ì • í•„ìš” | 0.94 âœ… | 0.3 âŒ | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
  -Expression | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
  -GRU | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
  -SpecialityPenalty | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
  -OrthoConstraint | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
  -All | ì¸¡ì • í•„ìš” | N/A | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
```

**í•´ì„**:
- ê° component ì œê±° ì‹œ routing metrics ë³€í™” ì¸¡ì • í•„ìš”
- Expression, GRU, SpecialityPenalty, OrthoConstraintì˜ ê¸°ì—¬ë„ ì •ëŸ‰í™”

---

### Table 3: Task Performance Benchmarks (ë³´ì¡° í‘œ)

```
Model | MMLU | HellaSwag | ARC-C | PIQA | BoolQ | Avg
------|------|-----------|-------|------|-------|-----
Dense MLP | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
Switch Top-1 | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
Switch Top-2 | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
Expert Choice | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
Hash Routing | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
SPECTRA (Ours) | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
```

**ì°¸ê³ **: Task performanceëŠ” routing qualityì˜ ê²°ê³¼ë¡œ ë³´ê³ 

---

### Table 4: Specialized Domains (ë³´ì¡° í‘œ)

```
Model | HumanEval | MBPP | GSM8K | MATH | PubMedQA | SciFact
------|-----------|------|-------|------|----------|--------
Switch Top-2 | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
SPECTRA (Ours) | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš” | ì¸¡ì • í•„ìš”
Improvement | ê³„ì‚° í•„ìš” | ê³„ì‚° í•„ìš” | ê³„ì‚° í•„ìš” | ê³„ì‚° í•„ìš” | ê³„ì‚° í•„ìš” | ê³„ì‚° í•„ìš”
```

---

## ğŸ¯ SPECTRA ë¹„êµ ê¸°ì¤€ (ì‹¤ì œ ë³´ê³ ëœ ìˆ˜ì¹˜ ê¸°ì¤€)

### ì‹¤ì œ ì¸¡ì • ê°€ëŠ¥í•œ ë¹„êµ ëŒ€ìƒ

1. **LPR (arxiv:2506.21328)**:
   - Gini coefficient: 0.035 (ëª©í‘œ)
   - Min-max expert load ratio: 0.70 (ëª©í‘œ)
   - âš ï¸ CV, Orthogonality, Overlapì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ

2. **Switch Transformer (Fedus et al., 2021)**:
   - Balanced utilization: 94.8% (ì°¸ê³ )
   - âš ï¸ CV, Orthogonality, Overlapì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ

3. **Expert Choice (Zhou et al., 2022)**:
   - Training convergence: 2Ã— faster (ì°¸ê³ )
   - âš ï¸ CV, Orthogonality, Overlapì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ

4. **ERMoE (arxiv:2511.10971)**:
   - "Natural flatter load" (ì •ëŸ‰ì  ìˆ˜ì¹˜ ì—†ìŒ)
   - SOTA performance on ImageNet, COCO

5. **Advancing Expert Specialization (arxiv:2505.22323)**:
   - Up to 23.79% performance gain
   - Orthogonality loss + Variance loss ì‚¬ìš©

### ì „í†µì ì¸ Metrics (CV, Orthogonality, Overlap)

**âš ï¸ ë¬¸ì œ**: ëŒ€ë¶€ë¶„ì˜ ìµœì‹  ë…¼ë¬¸ì—ì„œ ì´ metricsë¥¼ ë³´ê³ í•˜ì§€ ì•ŠìŒ

**ê°€ëŠ¥í•œ ì ‘ê·¼**:
1. **ìì²´ ì¸¡ì •**: ë™ì¼í•œ baselineì—ì„œ ì§ì ‘ ì¸¡ì •í•˜ì—¬ ë¹„êµ
2. **ë…¼ë¬¸ ì¬ë¶„ì„**: ë…¼ë¬¸ì˜ figureë‚˜ appendixì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì •ë³´ í™•ì¸
3. **ê³µê°œ ì½”ë“œ**: GitHub repositoryì—ì„œ metrics ê³„ì‚° ì½”ë“œ í™•ì¸

---

## ğŸ“Š í‰ê°€í•´ì•¼ í•  ë²¤ì¹˜ë§ˆí¬ ëª©ë¡ (ë³´ì¡° ì§€í‘œ)

### 1. Language Understanding
- **MMLU** (Massive Multitask Language Understanding): 57ê°œ ì£¼ì œ, 5-shot
- **HellaSwag**: ìƒì‹ ì¶”ë¡ , 10-shot
- **ARC-Challenge**: ê³¼í•™ ì§ˆë¬¸, 25-shot
- **PIQA**: ë¬¼ë¦¬ì  ì¶”ë¡ , 0-shot
- **BoolQ**: Yes/No ì§ˆë¬¸, 0-shot

### 2. Language Generation
- **WikiText-103**: Language modeling perplexity
- **LAMBADA**: ì¥ê±°ë¦¬ ì˜ì¡´ì„± í‰ê°€
- **TruthfulQA**: ì§„ì‹¤ì„± ìˆëŠ” ìƒì„± í‰ê°€

### 3. Code Understanding
- **HumanEval**: Python ì½”ë“œ ìƒì„± (Pass@1, Pass@10, Pass@100)
- **MBPP**: ê¸°ë³¸ í”„ë¡œê·¸ë˜ë° ë¬¸ì œ

### 4. Mathematical Reasoning
- **GSM8K**: ì´ˆë“± ìˆ˜í•™ ë¬¸ì œ, 8-shot
- **MATH**: ê²½ìŸ ìˆ˜ì¤€ ìˆ˜í•™, 4-shot

### 5. Specialized Domains
- **PubMedQA**: ìƒì˜í•™ ì§ˆë¬¸ ë‹µë³€
- **SciFact**: ê³¼í•™ì  ì£¼ì¥ ê²€ì¦

---

## ğŸ” ë¹„êµí•´ì•¼ í•  ë ˆí¼ëŸ°ìŠ¤ ëª¨ë¸ ë° ê³µê°œ ì„±ëŠ¥ ì§€í‘œ

### 1. Mixtral 8x7B
**ë…¼ë¬¸**: Jiang et al., 2024 (Mistral AI)

**ê³µê°œëœ ì„±ëŠ¥ ì§€í‘œ**:
| ë²¤ì¹˜ë§ˆí¬ | ì ìˆ˜ |
|---------|------|
| **MMLU** | 71.34% |
| **GSM8K** | 66.82% |
| **HumanEval** | 40.9% (Pass@1) |

**ì¶”ê°€ ì •ë³´**:
- 8ê°œ expert, 2ê°œ í™œì„±í™” (top-2 routing)
- ì´ íŒŒë¼ë¯¸í„°: ~47B (active: ~13B)

---

### 2. DeepSeek-V3
**Technical Report**: DeepSeek official

**ê³µê°œëœ ì„±ëŠ¥ ì§€í‘œ**:
| ë²¤ì¹˜ë§ˆí¬ | ì ìˆ˜ |
|---------|------|
| **MMLU** | 83.7% (37B active) |
| **GSM8K** | 91.3% |

**ì¶”ê°€ ì •ë³´**:
- 671B total, 37B active parameters
- 256 routed experts + 1 shared expert

---

### 3. LLaMA-2 7B (Dense Baseline)
**ë…¼ë¬¸**: Touvron et al., 2023 (Meta)

**ê³µê°œëœ ì„±ëŠ¥ ì§€í‘œ**:
| ë²¤ì¹˜ë§ˆí¬ | ì ìˆ˜ |
|---------|------|
| **MMLU** | 44.4% |
| **HellaSwag** | 77.1% |
| **ARC-Challenge** | 43.2% |
| **GSM8K** | 16.0% |
| **HumanEval** | 11.6% (Pass@1) |

**ìš©ë„**: Dense baselineìœ¼ë¡œ ì‚¬ìš© (upper bound ë¹„êµ)

---

### 4. GPT-2-Medium (Dense Baseline)
**ë…¼ë¬¸**: Radford et al., 2019

**ê³µê°œëœ ì„±ëŠ¥ ì§€í‘œ**:
| ë²¤ì¹˜ë§ˆí¬ | ì ìˆ˜ |
|---------|------|
| **WikiText-2 Perplexity** | 22.76 |
| **WikiText-103 Perplexity** | 26.37 |
| **LAMBADA Perplexity** | 15.60 |
| **LAMBADA Accuracy** | 55.48% |

**ìš©ë„**: Small-scale dense baseline

---

## ğŸ”¬ ì‹¤í—˜ ì„¤ê³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ (Routing ì¤‘ì‹¬)

### 1. Baseline Routing êµ¬í˜„ (ìµœìš°ì„ )
- [ ] Switch Top-1 routing êµ¬í˜„
- [ ] Switch Top-2 routing êµ¬í˜„
- [ ] Expert Choice routing êµ¬í˜„
- [ ] Hash routing êµ¬í˜„
- [ ] Dense MLP baseline (upper bound)

### 2. Ablation Variants
- [ ] SPECTRA-Full (baseline)
- [ ] SPECTRA w/o Expression
- [ ] SPECTRA w/o GRU
- [ ] SPECTRA w/o Speciality Penalty
- [ ] SPECTRA w/o Orthogonal Constraint
- [ ] SPECTRA w/o All Enhancements

### 3. Model Scales
- [ ] GPT-2-Medium (345M) - Dense to MoE
- [ ] LLaMA-2-7B - Dense to MoE
- [ ] Mixtral-8x7B - Router replacement

### 4. Routing Metrics Evaluation Setup (ìµœìš°ì„ )
- [ ] Expert specialization analysis tools (spectra_analysis.py)
- [ ] Load balancing metrics collection
- [ ] Routing consistency measurement
- [ ] Sequential routing pattern analysis
- [ ] Expression projection quality analysis
- [ ] Training dynamics tracking (over time)

### 5. Task Performance Evaluation Setup (ë³´ì¡°)
- [ ] lm-evaluation-harness ì„¤ì •
- [ ] Custom evaluation scripts (HumanEval, MBPP)
- [ ] Perplexity evaluation setup

### 6. Metrics Collection (ìš°ì„ ìˆœìœ„ ìˆœ)
- [ ] **Routing metrics ìë™ ìˆ˜ì§‘** (ìµœìš°ì„ )
  - Expert specialization metrics
  - Load balancing metrics
  - Routing decision quality
  - Expression projection effectiveness
- [ ] **Training dynamics logging** (ì¤‘ìš”)
  - Time-series data for all routing metrics
- [ ] Task performance metrics ìë™ ìˆ˜ì§‘ (ë³´ì¡°)
- [ ] Computational efficiency metrics ìë™ ìˆ˜ì§‘ (ë³´ì¡°)

---

## ğŸ“š ì°¸ê³  ë…¼ë¬¸ ëª©ë¡ (ì‹¤ì œ arxiv ë²ˆí˜¸)

### ìµœì‹  MoE ëª¨ë¸ (2025)
1. **Kimi K2**: arxiv:2507.20534
2. **GLM-4.5**: arxiv:2508.06471
3. **Minimax-Text-01**: arxiv:2501.08313
4. **MiniMax-M1**: arxiv:2506.13585
5. **Qwen3-MoE**: arxiv:2505.09388

### ìµœì‹  Routing ë°©ë²•ë¡  (2025)
1. **ERMoE**: arxiv:2511.10971
2. **LPR**: arxiv:2506.21328
3. **LASER**: arxiv:2510.03293
4. **RoMA**: arxiv:2511.07419
5. **Advancing Expert Specialization**: arxiv:2505.22323
6. **Local Routing Consistency**: arxiv:2505.16056
7. **Input Domain Aware MoE**: arxiv:2510.16448
8. **GRACE-MoE**: arxiv:2509.25041
9. **MaxScore**: arxiv:2508.12801
10. **Loss-Free Balancing**: arxiv:2408.15664

### ë ˆê±°ì‹œ ë°©ë²•ë¡ 
1. **Switch Transformer**: Fedus et al., 2021
2. **Expert Choice**: Zhou et al., 2022 (NeurIPS)
3. **Mixtral**: Jiang et al., 2024
4. **Hash Routing**: Roller et al., 2021

---

## âš ï¸ ì¤‘ìš” ë°œê²¬ ë° ê¶Œì¥ì‚¬í•­

### ë¬¸ì œì 
**ëŒ€ë¶€ë¶„ì˜ ìµœì‹  MoE ëª¨ë¸ê³¼ routing ë°©ë²•ë¡ ì´ ì „í†µì ì¸ routing metrics (CV, Orthogonality, Expert Overlap)ë¥¼ ë…¼ë¬¸/technical reportì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë³´ê³ í•˜ì§€ ì•ŠìŒ**

### ì‹¤ì œ ë³´ê³ ëœ Metrics (2025ë…„ 11ì›” ê¸°ì¤€)
1. **LPR (arxiv:2506.21328)**: 
   - Gini coefficient: 0.035
   - Min-max expert load ratio: 0.70
   - âš ï¸ CV, Orthogonality, Overlapì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ

2. **Switch Transformer (Fedus et al., 2021)**: 
   - Balanced utilization: 94.8%
   - âš ï¸ CV, Orthogonality, Overlapì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ

3. **Expert Choice (Zhou et al., 2022)**: 
   - Training convergence: 2Ã— faster
   - âš ï¸ CV, Orthogonality, Overlapì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ

4. **ERMoE (arxiv:2511.10971)**: 
   - "Natural flatter expert load distributions" (ì •ëŸ‰ì  ìˆ˜ì¹˜ ì—†ìŒ)

5. **Advancing Expert Specialization (arxiv:2505.22323)**: 
   - Up to 23.79% performance gain

### ê¶Œì¥ ì ‘ê·¼ ë°©ë²•
1. **ì§ì ‘ ì¸¡ì •**: ë™ì¼í•œ baselineì—ì„œ ì§ì ‘ ì¸¡ì •í•˜ì—¬ ë¹„êµ
2. **ë…¼ë¬¸ ì¬ë¶„ì„**: ë…¼ë¬¸ì˜ figure, table, appendixì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ ì •ë³´ í™•ì¸
3. **ê³µê°œ ì½”ë“œ**: GitHub repositoryì—ì„œ metrics ê³„ì‚° ì½”ë“œ í™•ì¸
4. **ë…¼ë¬¸ ì €ì ë¬¸ì˜**: Metrics ë°ì´í„° ìš”ì²­ (ê°€ëŠ¥í•œ ê²½ìš°)

### ë¹„êµ ê¸°ì¤€ ì¬ì„¤ì •
- **LPRì˜ Gini 0.035**: Near-perfect balancingì˜ ê¸°ì¤€
- **Switch Transformerì˜ 94.8% utilization**: ì°¸ê³ ìš©
- **ìì²´ ì¸¡ì •ê°’**: SPECTRAì˜ ì‹¤ì œ ì¸¡ì •ê°’ê³¼ ë¹„êµ
- **Performance gain**: Advancing Expert Specializationì˜ 23.79% gainê³¼ ë¹„êµ

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### Routing Metrics ì¸¡ì • ì‹œ
1. **ë™ì¼í•œ Base Model**: ëª¨ë“  routing methodëŠ” ë™ì¼í•œ base model ì‚¬ìš©
2. **ë™ì¼í•œ Expert Architecture**: Expert êµ¬ì¡°ëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€
3. **ë™ì¼í•œ Training Setup**: Learning rate, batch size ë“± ëª¨ë“  hyperparameter ë™ì¼
4. **Multiple Runs**: Routing metricsë„ í†µê³„ì  ìœ ì˜ì„± í™•ë³´ (multiple seeds)
5. **Checkpoint ì¼ê´€ì„±**: ë™ì¼í•œ training stepì—ì„œ ë¹„êµ

### Task Performance ì¸¡ì • ì‹œ
1. **Shot ìˆ˜ ì¼ê´€ì„±**: ëª¨ë“  ë ˆí¼ëŸ°ìŠ¤ì™€ ë™ì¼í•œ shot ìˆ˜ ì‚¬ìš©
2. **í‰ê°€ í”„ë ˆì„ì›Œí¬**: lm-evaluation-harness ì‚¬ìš© ê¶Œì¥ (í‘œì¤€í™”)
3. **ë°ì´í„°ì…‹ ë²„ì „**: ë™ì¼í•œ ë°ì´í„°ì…‹ ë²„ì „ ì‚¬ìš©

### ë…¼ë¬¸ ì‘ì„± ì‹œ
1. **Routing Metrics ìš°ì„ **: Task performanceë³´ë‹¤ routing metricsë¥¼ ë¨¼ì € ì œì‹œ
2. **ì¸ê³¼ê´€ê³„ ëª…í™•íˆ**: Routing quality â†’ Task performance ì—°ê²° ì„¤ëª…
3. **Ablation Study ê°•ì¡°**: ê° componentì˜ routing metrics ê¸°ì—¬ë„ ëª…ì‹œ
4. **ì‹¤ì œ ì¸¡ì •ê°’ë§Œ ë³´ê³ **: ì¶”ì •ê°’ì´ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ê°’ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

---

## ğŸ”„ ì—…ë°ì´íŠ¸ ë¡œê·¸

- 2025-11-28: í†µí•© ë¬¸ì„œ ìƒì„±, ì‹¤ì œ ë…¼ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì¬ì •ë¦¬
- í–¥í›„ ì‹¤í—˜ ê²°ê³¼ì— ë”°ë¼ ì§€ì† ì—…ë°ì´íŠ¸ ì˜ˆì •

---

## ğŸ“ í˜„ì¬ SPECTRA ìƒíƒœ

### ì¸¡ì • ì™„ë£Œ
- âœ… **Gram Orthogonality**: 0.94 (ëª©í‘œ ë‹¬ì„±)

### ì¸¡ì • í•„ìš”
- âš ï¸ **Expert Overlap**: ì¸¡ì • í•„ìš” (ëª©í‘œ: < 15%)
- âš ï¸ **Expert Entropy**: ì¸¡ì • í•„ìš” (ëª©í‘œ: â‰¥ 2.7)
- âš ï¸ **Routing Consistency**: ì¸¡ì • í•„ìš” (ëª©í‘œ: > 85%)
- âš ï¸ **Sequential Consistency**: ì¸¡ì • í•„ìš” (ëª©í‘œ: > 45%)

### ê°œì„  í•„ìš”
- âŒ **Load Balance CV**: 0.3 (ëª©í‘œ: < 0.1, LPR Gini 0.035 ê¸°ì¤€)
- âš ï¸ **Gini Coefficient**: ì¸¡ì • í•„ìš” (ëª©í‘œ: < 0.05, LPR: 0.035)
- âš ï¸ **Min-max Expert Load Ratio**: ì¸¡ì • í•„ìš” (ëª©í‘œ: > 0.70, LPR: 0.70)

---

## ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì†ŒìŠ¤

- **MMLU**: https://github.com/hendrycks/test
- **HellaSwag**: https://github.com/rowanz/hellaswag
- **GSM8K**: https://github.com/openai/grade-school-math
- **HumanEval**: https://github.com/openai/human-eval
- **lm-evaluation-harness**: https://github.com/EleutherAI/lm-evaluation-harness

---

## ğŸ”— ê³µê°œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸

- **Mixtral-8x7B**: HuggingFace `mistralai/Mixtral-8x7B-v0.1`
- **LLaMA-2-7B**: HuggingFace `meta-llama/Llama-2-7b-hf`
- **GPT-2-Medium**: HuggingFace `gpt2-medium`
