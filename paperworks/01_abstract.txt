=================================================================
ABSTRACT
=================================================================

Mixture-of-Experts (MoE) architectures offer an efficient path to scaling large language models, yet practical MoE training often suffers from routing pathologies such as expert collapse, redundant experts, and unstable routing over long contexts. To address these limitations, we introduce SPECTRA (Sinkhorn Projected Experts for Consistent TRAjectory routing), a routing mechanism that combines (i) a GRU-based sequential router for context-aware routing trajectories, (ii) an orthogonal expression projector for capability discovery, and (iii) OSR (Orthogonal Sinkhorn Routing), a parameter-free optimal-transport routing layer that enforces balanced utilization and expert separation through a repulsive cost. In the current implementation, OSR penalizes expert correlations regardless of sign and additionally discourages overly confident alignments, preventing degenerate anti-parallel solutions. We focus evaluation on routing-quality metrics (orthogonality, overlap, CV, stability) and outline a sweep plan to close the remaining load-balance gap (target CV < 0.1) before finalizing full benchmark tables. We provide a modular implementation that can upcycle pretrained dense models to MoE or replace routing in existing MoE models.

Keywords: Mixture-of-Experts, Routing Mechanism, Gram Matrix, Expert Specialization, Large Language Models


=================================================================
CONTRIBUTIONS
=================================================================

1.  **SPECTRA Routing:** We introduce a novel routing mechanism using OSR (Orthogonal Sinkhorn Routing) and Gram matrix orthogonalization to enforce both expert diversity and specialization, addressing common pitfalls in MoE models. OSR employs a repulsive cost function that mathematically guarantees expert separation without requiring learned parameters.

2.  **In-depth Component Analysis:** We provide a systematic ablation study that validates the contribution of each key component of SPECTRA: the Expression Projector (with orthogonal initialization), GRU-based Sequential Routing, OSR (Orthogonal Sinkhorn Routing), Gram Matrix Repulsive Cost, and Orthogonal Constraints.

3.  **Selection-Stage Balancing + Stability Knobs (Implementation-Aligned):** We evaluate quota-constrained expert-choice selection (capacity-limited top-k) and stabilized Sinkhorn variants (log-domain, adaptive temperature/iteration sweeps) as first-order levers to reduce load-balance CV without sacrificing specialization.

4.  **Open-Source, Modular, and Production-Ready:** We release our implementation as open-source code, complete with backward compatibility and extensive documentation to facilitate adoption and further research. Our framework is designed for broad applicability, enabling the conversion of dense pretrained models to MoE architectures and the replacement of routing mechanisms in existing MoE models like GLM-4.5, Kimi-K2, MiniMax-M2, Qwen3, DeepSeek-V3, and GPT-OSS.
