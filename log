[0;32m================================================================================[0m
[0;32m        SPECTRA SFT DeepSpeed Training with Small Model Configuration      [0m
[0;32m================================================================================[0m
[1;33mProject Root:[0m /home/conan/workspace/llm_training
[1;33mConfig File:[0m /home/conan/workspace/llm_training/spectra_sft/config/spectra_qwen_config.json
[1;33mNumber of GPUs:[0m 2
[0;34mDeepSpeed Config:[0m /home/conan/workspace/llm_training/spectra_sft/config/deepspeed_zero3.json
[1;33mOutput Directory:[0m /mls/conan/training_logs/spectra_qwen_outputs
[1;33mCUDA Devices:[0m 0,1
[0;32mStarting DeepSpeed training with 2 GPUs...[0m
‚úÖ Qwen3 Input Injection Guard (v19) enabled
‚úÖ Qwen3 Participation Guard (v16 - Global Pool) applied
‚úÖ Qwen3 Input Injection Guard (v19) enabled
‚úÖ Qwen3 Participation Guard (v16 - Global Pool) applied
‚úÖ [Pre-Init] Set CUDA device to 0
   üîç Verifying: torch.cuda.current_device() == 0
‚úÖ Siglip Ï¥àÍ∏∞Ìôî Ìï®ÏàòÎì§ Ìå®Ïπò Ï†ÅÏö©Îê®
üö® `vision_tower` is part of SPECTRAModel.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /home/conan/workspace/llm_training/models/spectra_model.py.
üö® `language_model` is part of SPECTRAModel.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /home/conan/workspace/llm_training/models/spectra_model.py.
‚úÖ Siglip Ï¥àÍ∏∞Ìôî Ìï®ÏàòÎì§ Ìå®Ïπò Ï†ÅÏö©Îê®
üö® `vision_tower` is part of SPECTRAModel.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /home/conan/workspace/llm_training/models/spectra_model.py.
üö® `language_model` is part of SPECTRAModel.__init__'s signature, but not documented. Make sure to add it to the docstring of the function in /home/conan/workspace/llm_training/models/spectra_model.py.
‚úì Config loaded from: /home/conan/workspace/llm_training/spectra_sft/config/spectra_qwen_config.json
‚úì Config loaded from: /home/conan/workspace/llm_training/spectra_sft/config/spectra_qwen_config.json
00:34:22 | INFO     | ‚ÑπÔ∏è Autograd anomaly detection DISABLED
NCCL version 2.27.3+cuda12.9
00:34:23 | INFO     | ‚úÖ Manually initialized distributed process group (with device_id for NCCL)
00:34:23 | INFO     | üöÄ Starting tokenizer setup...
00:34:23 | INFO     | üî§ Loading tokenizer from: Qwen/Qwen3-VL-30B-A3B-Instruct
DeepSpeed environment variables set
00:34:28 | INFO     |   ‚úÖ AutoProcessor loaded successfully
00:34:28 | INFO     | üöÄ Starting model setup...
00:34:28 | INFO     | üîß Setting up DeepSpeed environment...
DeepSpeed environment variables set
00:34:28 | INFO     |   ‚ÑπÔ∏è  No rope_scaling found in config, using default
00:34:28 | INFO     |   ‚úÖ flash-attn-3 package detected - using flash_attention_3
00:34:28 | INFO     | Loading base model configuration...
00:34:28 | INFO     |   üîì Explicitly set tie_word_embeddings = False for NVMe offloading compatibility
00:34:28 | INFO     | SPECTRA configuration created successfully
00:34:28 | INFO     |   Shared experts: 0, Routed experts: 128, Experts per token: 8
00:34:28 | INFO     | Using DeepSpeed - letting DeepSpeed handle device placement
00:34:28 | INFO     | ü§ñ Loading SPECTRA model...
00:34:28 | INFO     | ü§ñ Model path: Qwen/Qwen3-VL-30B-A3B-Instruct
00:34:28 | INFO     | ü§ñ Device map: None
00:34:28 | INFO     | ü§ñ Attention implementation: flash_attention_3
00:34:28 | INFO     | üîß GPU Memory [BEFORE_MODEL_LOAD] - Allocated: 0.00GB, Reserved: 0.00GB
00:34:28 | INFO     |   üöÄ Using HfDeepSpeedConfig for direct meta device initialization (no CPU load)
00:34:28 | INFO     | ‚úÖ Relying on JSON/Accelerate config for DeepSpeed Buffer and NVMe settings
00:34:28 | INFO     | ‚ö†Ô∏è  autotp_size already set in DeepSpeed config, skipping tensor_model_parallel_size from Accelerate
00:34:28 | INFO     |   ‚ÑπÔ∏è  Will set tp_injection_policy to exclude embedding layers during DeepSpeed init
00:34:28 | INFO     | ‚úÖ ZeRO-3 configured to work with TP size 2
00:34:28 | INFO     |   üìä World size for batch calculation: 2
00:34:28 | INFO     |   üóëÔ∏è  Removing train_batch_size from DeepSpeed config to bypass assertion checks
00:34:28 | INFO     |   üíæ Saved modified DeepSpeed config to temporary file: /tmp/tmpnl40739t.json
00:34:28 | INFO     |   üì§ Exported DEEPSPEED_CONFIG_FILE=/tmp/tmpnl40739t.json
00:34:28 | INFO     |   ‚ö° Loading base model with ZeRO-3 setup...
00:34:28 | INFO     | üìç Resolved repo ID to local snapshot: /home/conan/.cache/huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct/snapshots/9c4b90e1e4ba969fd3b5378b57d966d725f1b86c
00:34:28 | INFO     | üöÄ Loading base model from: /home/conan/.cache/huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct/snapshots/9c4b90e1e4ba969fd3b5378b57d966d725f1b86c
00:34:28 | WARNING  | ‚ö†Ô∏è  FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead (at /home/conan/workspace/llm_training/spectra_sft/train_spectra.py:639)
üö® Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
üö® Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
üö® Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
üö® Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
üö® Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
üö® Config not found for parakeet. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py
00:34:32 | INFO     | Unable to initialize backend 'cuda': 
00:34:32 | INFO     | Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
00:34:32 | INFO     | Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
00:34:32 | WARNING  | An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.
INFO 02-13 00:34:35 [__init__.py:216] Automatically detected platform cuda.
00:34:35 | WARNING  | ‚ö†Ô∏è  FutureWarning: `BestOfNSampler` is deprecated and will be removed in TRL 0.25. (at /home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/trl/extras/best_of_n_sampler.py:24)
00:34:35 | WARNING  | ‚ö†Ô∏è  UserWarning: TRL currently only supports vLLM version `0.10.2`. You have version 0.11.0 installed. We recommend to install this version to avoid compatibility issues. (at /home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/trl/import_utils.py:91)
INFO 02-13 00:34:35 [__init__.py:216] Automatically detected platform cuda.
00:34:36 | WARNING  | ‚ö†Ô∏è  FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead (at /home/conan/workspace/llm_training/spectra_sft/train_spectra.py:656)
00:34:36 | INFO     | ‚úÖ Applied Universal Exoskeleton Guard to fix size 0 mismatch issues.
00:34:36 | INFO     |   ‚ö° Using ZeRO-3 meta device initialization...
00:34:37 | INFO     |   ‚ö° Instantiating base model (searching for correct AutoModel class)...
00:34:37 | WARNING  | ‚ö†Ô∏è  FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead. (at /home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/models/auto/modeling_auto.py:2284)
00:37:39 | INFO     |   ‚úÖ Loaded base model successfully using AutoModelForVision2Seq
00:37:39 | INFO     |   üîß Injecting SPECTRA Exoskeleton (OSR/LDR Router Swap)...
00:37:39 | INFO     | üíâ Injecting SPECTRA Exoskeleton (Type: qwen3_vl_moe) into base model...
00:37:39 | INFO     | ‚úÖ SPECTRA Exoskeleton injection complete (48 layers hijacked)
00:37:39 | INFO     |  ‚úÖ Model loaded.
00:37:39 | INFO     | üîß Initializing Multi-Modal Projector...
00:37:39 | INFO     | üîß Verifying and Initializing Router Components...
00:37:39 | INFO     | üîß Initializing new LayerNorm components...
00:37:39 | INFO     |   [setup_model] Router parameters: 13/13 trainable
00:37:39 | INFO     | üîß Ensuring all experts (SPECTRAMLP) are trainable...
00:37:39 | INFO     | ‚úÖ Applied SliceBackward0 fix to Qwen3VLMoeTextModel
00:37:39 | INFO     | üöÄ [PEFT] Applying LoRA wrapping...
00:37:39 | INFO     | üöÄ Dynamically identified 10 LoRA target modules:
00:37:39 | INFO     |    ['expression_projector', 'gate', 'gate_proj1', 'gate_proj2', 'intent_proj', 'k_proj', 'o_proj', 'priority_proj', 'q_proj', 'v_proj']
00:37:40 | INFO     | ‚úÖ [PEFT] LoRA wrapping applied successfully
00:37:40 | INFO     | üîì [PEFT] Manually unfreezing Experts and Routers for FFT...
00:37:40 | INFO     | ‚úÖ [PEFT] Unfrozen 97 MoE/Router modules for Full Fine-Tuning
00:37:40 | INFO     | üìä [PEFT] Post-LoRA Trainable: 0.03B / 0.03B (100.00%)
üõ†Ô∏è Monkey-patching: Registering 'List' feature type as alias for 'Sequence'
üõ†Ô∏è Monkey-patching: Registering 'List' feature type as alias for 'Sequence'
00:37:41 | INFO     | üíæ Loading Multi-Modal Dataset: HuggingFaceTB/smoltalk (Max: 100000)
00:37:42 | INFO     | [0vs2048] Collator: pure_qwen_collate_fn | image resize 224x224 ‚Üí uniform vision patch count across ranks | CHECK: COLLATOR CHECK line = first batch shape, STEP0 BATCH CHECK = per-rank step0 input
00:37:42 | INFO     | ‚úÖ All ranks finished dataset load (barrier passed).
00:37:42 | INFO     | ‚úÖ Found 48 MoE layers in model
00:37:42 | INFO     |    All MoE layers (48):
00:37:42 | INFO     |      [0] model.language_model.layers.0.mlp (Qwen3VLMoeTextSparseMoeBlock)
00:37:42 | INFO     |      [1] model.language_model.layers.1.mlp (Qwen3VLMoeTextSparseMoeBlock)
00:37:42 | INFO     |      [2] model.language_model.layers.2.mlp (Qwen3VLMoeTextSparseMoeBlock)
00:37:42 | INFO     |      [3] model.language_model.layers.3.mlp (Qwen3VLMoeTextSparseMoeBlock)
00:37:42 | INFO     |      [4] model.language_model.layers.4.mlp (Qwen3VLMoeTextSparseMoeBlock)
00:37:42 | INFO     |      ... and 43 more
00:37:42 | INFO     |    MoE class names found: {'Qwen3VLMoeTextSparseMoeBlock'}
00:37:42 | INFO     | Dataset already setup.
00:37:42 | INFO     | üßπ Î™®Îç∏ Î∞è Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú ÌõÑ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨...
00:37:42 | INFO     | üßπ Starting GPU memory cleanup...
00:37:42 | INFO     | üîß GPU Memory [BEFORE_CLEANUP] - Allocated: 29.03GB, Reserved: 29.95GB
00:37:43 | INFO     | üîß GPU Memory [AFTER_CLEANUP] - Allocated: 29.03GB, Reserved: 29.95GB
00:37:43 | INFO     | üßπ Memory cleanup completed - Freed: 0.00GB allocated, 0.00GB reserved
00:37:43 | INFO     | üîß [DeepSpeed] Requested Trainer to skip automatic model placement

==================================================
TRAINING CONFIGURATION
==================================================
Model: Qwen/Qwen3-VL-30B-A3B-Instruct
Model parameters: 339.03M total, 339.03M trainable
Dataset: HuggingFaceTB/smoltalk
Max sequence length: 131072
Use LoRA: True
LoRA rank: 16
DeepSpeed config: /tmp/tmpbudxoprm.json
Training epochs: 1
Batch size per device: 1
Gradient accumulation steps: 1
Learning rate: 0.0005
FP16: False
BF16: True
==================================================
Starting training...
00:37:43 | INFO     | DeepSpeed config set: /tmp/tmpnl40739t.json
00:37:43 | INFO     | DeepSpeed zero stage: 3, offload_optimizer: nvme, offload_param: none
00:37:43 | WARNING  | ‚ö†Ô∏è WARNING: Using ZeRO-3 with CPU offload may cause router learning issues!
00:37:43 | INFO     | Setting up trainer...
00:37:43 | INFO     |   üîß Registering SPECTRAMoE with DeepSpeed...
00:37:43 | INFO     |   üìã Using detected MoE classes from model: ['Qwen3VLMoeTextSparseMoeBlock', 'SPECTRAMoE']
00:37:43 | INFO     |   üìã Detected Vision classes from model: {'Qwen3VLMoeVisionModel'}
00:37:43 | INFO     |   ‚úÖ Added MoE classes to leaf_module.classes: ['MixtralSparseMoeBlock', 'Qwen2MoeSparseMoeBlock', 'DeepseekV3MoE', 'Qwen3VLMoeTextSparseMoeBlock']
00:37:43 | INFO     |   ‚úÖ Updated DeepSpeed config: /tmp/tmpnl40739t.json
00:37:43 | INFO     |      moe_leaf_modules: []
00:37:43 | INFO     |   üîß Patching Accelerate DeepSpeed plugin to handle missing MoE classes...
00:37:43 | INFO     |   üìã Found actual MoE classes in model: {'Qwen3VLMoeTextSparseMoeBlock'}
00:37:43 | INFO     |   üöÄ Standard DeepSpeed initialization (manual patches bypassed)
00:37:43 | INFO     | ‚úÖ Dataset validation: train=1400000, eval=8360
00:37:43 | INFO     |   üîß Forcing idempotent patching of DeepSpeedPlugin...
00:37:43 | INFO     |   ‚úÖ Applied final patch to Accelerate DeepSpeedPlugin.set_moe_leaf_modules (no-op)
00:37:43 | INFO     |   üçÉ Identified Decoder Layer Class: Qwen3VLMoeTextDecoderLayer
00:37:43 | INFO     |   üçÉ Identified Vision Block Class: Qwen3VLMoeVisionBlock
00:37:43 | INFO     |   ‚úÖ Forced ZeRO-3 leaf modules: ['Qwen3VLMoeTextExperts', 'Qwen3VLMoeVisionModel', 'Qwen3VLMoeTextDecoderLayer', 'Qwen3VLMoeTextSparseMoeBlock', 'Qwen3VLMoeVisionBlock', 'Qwen3VLMoeVisionPatchMerger']
00:37:43 | INFO     | ‚úÖ Enabled input require grads for gradient checkpointing (pre-trainer)
00:37:43 | WARNING  | [0vs2048] Vision encoder gradient_checkpointing disabled (rankÎ≥Ñ patch Ïàò Î∂àÏùºÏπò Î∞©ÏßÄ)
00:37:43 | INFO     |   ‚ÑπÔ∏è  Skipping embedding layer verification - DeepSpeed autotp will handle embeddings
00:37:43 | WARNING  | ‚ö†Ô∏è  Embedding model.model.language_model.embed_tokens has empty weight
00:37:43 | INFO     |   üîß Reinitializing embedding model.model.language_model.embed_tokens
00:37:43 | INFO     |   ‚úÖ Fixed empty embedding model.model.language_model.embed_tokens
00:37:43 | INFO     |   ‚úÖ Verified and fixed embedding layers after Trainer creation
00:37:43 | INFO     |   ‚ÑπÔ∏è  Vision encoder/tower is trainable (freeze_vision=False)
00:37:43 | INFO     | ================================================================================

================================================================================
üöÄ STARTING TRAINING
================================================================================
00:37:47 | INFO     | wandb version 0.24.2 is available!  To upgrade, please run:
 $ pip install wandb --upgrade
00:37:47 | INFO     | weave version 0.52.26 is available!  To upgrade, please run:
 $ pip install weave --upgrade
00:37:47 | INFO     | Logged in as Weights & Biases user: kevintb.
View Weave data at https://wandb.ai/kevintb/spectra-sft/weave
00:37:47 | INFO     | ‚úÖ wandb initialized after Trainer creation
00:37:47 | INFO     | ‚úÖ MoE monitoring callback added
00:37:47 | INFO     | ‚úÖ ModulesToSaveSyncCallback added
00:37:47 | INFO     | ‚úÖ Enabling benchmark callback (mode=step, freq=1000, tasks=['mmlu', 'hellaswag', 'gsm8k', 'truthfulqa', 'arc', 'ifeval', 'mme', 'vqav2', 'textvqa'])
00:37:47 | INFO     | ‚úÖ All callbacks restored

==================================================
TRAINING CONFIGURATION
==================================================
Model: Qwen/Qwen3-VL-30B-A3B-Instruct
Model parameters: 339.03M total, 339.03M trainable
Dataset: HuggingFaceTB/smoltalk
Max sequence length: 131072
Use LoRA: True
LoRA rank: 16
DeepSpeed config: /tmp/tmpnl40739t.json
Training epochs: 1
Batch size per device: 1
Gradient accumulation steps: 1
Learning rate: 0.0005
FP16: False
BF16: True
==================================================
Starting training...
00:37:47 | INFO     | üöÄ Starting training...
00:37:47 | INFO     | üîß Training configuration:
00:37:47 | INFO     |   - Epochs: 1
00:37:47 | INFO     |   - Batch size per device: 1
00:37:47 | INFO     |   - Gradient accumulation steps: 1
00:37:47 | INFO     |   - Learning rate: 0.0005
00:37:47 | INFO     |   - Max sequence length: 131072
00:37:47 | INFO     | üîß Setting up memory-optimized evaluation...
00:37:47 | INFO     | üßπ ÌïôÏäµ ÏãúÏûë Ï†Ñ GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨...
00:37:47 | INFO     | üßπ Starting GPU memory cleanup...
00:37:47 | INFO     | üîß GPU Memory [BEFORE_CLEANUP] - Allocated: 29.61GB, Reserved: 30.54GB
00:37:48 | INFO     | üîß GPU Memory [AFTER_CLEANUP] - Allocated: 29.32GB, Reserved: 30.54GB
00:37:48 | INFO     | üßπ Memory cleanup completed - Freed: 0.29GB allocated, 0.00GB reserved
00:37:48 | INFO     | üîß GPU Memory [TRAINING_START] - Allocated: 29.32GB, Reserved: 30.54GB
00:37:48 | INFO     | üîß DeepSpeed gradient patch skipped (causes shape mismatch in ZeRO-3 leaf modules)
00:37:48 | INFO     | üîç Configuring gradient checkpointing (debug mode disabled for stability)...

================================================================================
üöÄ STARTING TRAINING
================================================================================
00:37:48 | INFO     | üöÄ Starting training...
00:37:48 | INFO     | ‚úÖ Rank 0: Dist initialized. World size: 2
üîç Rank 0: Running pre-training consistency check...
üîç Rank 1: Running pre-training consistency check...00:37:48 | INFO     | üîç Running pre-training consistency check...

00:37:48 | INFO     | üîç Verifying model parameter consistency across ranks...
00:37:48 | INFO     | ‚úÖ Model parameter consistency check passed (all ranks match).
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000500, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Stage 3 initialize beginning
MA 29.29 GB         Max_MA 30.06 GB         CA 30.25 GB         Max_CA 78 GB 
CPU Virtual Memory:  used = 31.96 GB, percent = 6.3%
DeepSpeedZeRoOffload initialize [begin]
MA 29.29 GB         Max_MA 29.29 GB         CA 30.25 GB         Max_CA 30 GB 
CPU Virtual Memory:  used = 32.04 GB, percent = 6.4%
Parameter Offload - Persistent parameters statistics: param_count = 1070, numel = 41641218
DeepSpeedZeRoOffload initialize [end]
MA 28.97 GB         Max_MA 29.58 GB         CA 30.52 GB         Max_CA 31 GB 
CPU Virtual Memory:  used = 32.04 GB, percent = 6.4%
Before creating fp16 partitions
MA 28.97 GB         Max_MA 28.97 GB         CA 30.52 GB         Max_CA 31 GB 
CPU Virtual Memory:  used = 32.03 GB, percent = 6.4%
After creating fp16 partitions: 98
MA 28.97 GB         Max_MA 28.97 GB         CA 29.93 GB         Max_CA 31 GB 
CPU Virtual Memory:  used = 37.23 GB, percent = 7.4%
00:38:17 | INFO     | gcc -pthread -B /home/conan/miniconda3/envs/llm_train/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/conan/miniconda3/envs/llm_train/include -fPIC -O2 -isystem /home/conan/miniconda3/envs/llm_train/include -fPIC -c /tmp/tmpx9imk6lr/test.c -o /tmp/tmpx9imk6lr/test.o
00:38:17 | INFO     | gcc -pthread -B /home/conan/miniconda3/envs/llm_train/compiler_compat /tmp/tmpx9imk6lr/test.o -laio -o /tmp/tmpx9imk6lr/a.out
Before creating fp32 partitions
MA 28.97 GB         Max_MA 28.97 GB         CA 29.93 GB         Max_CA 30 GB 
CPU Virtual Memory:  used = 49.29 GB, percent = 9.8%
After creating fp32 partitions
MA 28.97 GB         Max_MA 28.97 GB         CA 29.93 GB         Max_CA 30 GB 
CPU Virtual Memory:  used = 42.1 GB, percent = 8.4%
Before initializing optimizer states
MA 28.97 GB         Max_MA 28.97 GB         CA 29.93 GB         Max_CA 30 GB 
CPU Virtual Memory:  used = 43.04 GB, percent = 8.5%
After initializing optimizer states
MA 28.97 GB         Max_MA 28.97 GB         CA 29.93 GB         Max_CA 30 GB 
CPU Virtual Memory:  used = 41.76 GB, percent = 8.3%
[2026-02-13 02:08:04,848] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[0vs2048] COLLATOR CHECK: first batch | pixel_values.shape=torch.Size([256, 1536]), image_grid_thw.shape=(1, 3) | num_patches(seq)=256 (must be same on all ranks)
After initializing ZeRO optimizer
MA 29.16 GB         Max_MA 29.91 GB         CA 30.85 GB         Max_CA 31 GB 
CPU Virtual Memory:  used = 96.86 GB, percent = 19.2%
[2026-02-13 02:08:05,532] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
[0vs2048] COLLATOR CHECK: first batch | pixel_values.shape=torch.Size([256, 1536]), image_grid_thw.shape=(1, 3) | num_patches(seq)=256 (must be same on all ranks)
02:08:07 | WARNING  | [0vs2048] STEP0 BATCH CHECK: rank=0, pixel_values=(256, 1536), num_patches=256, image_grid_thw=(1, 3), input_ids=(1, 186), num_image_tokens_in_input_ids=64
02:08:07 | WARNING  | [0vs2048] STEP0 CHECK: Compare num_patches and num_image_tokens_in_input_ids across ranks; if any rank has 0 image tokens ‚Üí 0vs2048 (see traceback).
02:08:08 | WARNING  | ‚ö†Ô∏è  UserWarning: None of the inputs have requires_grad=True. Gradients will be None (at /home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/utils/checkpoint.py:85)

================================================================================
üö® TRAINING ERROR DETECTED INSIDE trainer.train()
================================================================================
Error type: OutOfMemoryError
Error message: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[0vs2048] rank that raised: 0
[0vs2048] SUMMARY: If STEP0 BATCH CHECK showed same num_patches on all ranks, 0vs2048 is NOT from vision patch mismatch ‚Üí see Full traceback for exact op (AccumulateGrad).

Full traceback:
Traceback (most recent call last):
  File "/home/conan/workspace/llm_training/spectra_sft/train_spectra.py", line 2876, in main
    trainer.train()
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/trl/trainer/sft_trainer.py", line 1185, in training_step
    return super().training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2726, in backward
    self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2532, in backward
    loss.backward(**backward_kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

================================================================================
03:17:06 | ERROR    | ‚ùå Training error inside trainer.train(): OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
03:17:06 | ERROR    | ‚ùå Traceback:
Traceback (most recent call last):
  File "/home/conan/workspace/llm_training/spectra_sft/train_spectra.py", line 2876, in main
    trainer.train()
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/trl/trainer/sft_trainer.py", line 1185, in training_step
    return super().training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2726, in backward
    self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2532, in backward
    loss.backward(**backward_kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

03:17:06 | ERROR    | ‚ùå CUDA OOM: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
03:17:06 | INFO     | üîß GPU Memory [OOM] - Allocated: 74.76GB, Reserved: 77.17GB
03:17:06 | ERROR    | Step: 0, Epoch: 0.000
03:17:56 | INFO     | üîß GPU Memory [OOM] - Allocated: 74.76GB, Reserved: 77.17GB
03:17:56 | ERROR    | üíæ OOM ÏóêÎü¨ Ï†ïÎ≥¥ Ï†ÄÏû•: /mls/conan/training_logs/spectra_qwen_outputs/oom_error_20260213_031756.json
03:17:56 | INFO     | üßπ Starting GPU memory cleanup...
03:17:56 | INFO     | üîß GPU Memory [BEFORE_CLEANUP] - Allocated: 74.76GB, Reserved: 77.17GB
03:17:57 | INFO     | üîß GPU Memory [AFTER_CLEANUP] - Allocated: 74.76GB, Reserved: 76.04GB
03:17:57 | INFO     | üßπ Memory cleanup completed - Freed: 0.00GB allocated, 1.13GB reserved
03:17:57 | ERROR    | üí° Ìï¥Í≤∞ Î∞©Î≤ï: batch_size Í∞êÏÜå, gradient_accumulation_steps Ï¶ùÍ∞Ä, max_length Í∞êÏÜå

================================================================================
‚ö†Ô∏è  FINALLY BLOCK: Exception detected during training
================================================================================
Exception type: OutOfMemoryError
Exception value: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Traceback:
Traceback (most recent call last):
  File "/home/conan/workspace/llm_training/spectra_sft/train_spectra.py", line 2908, in main
    raise e
  File "/home/conan/workspace/llm_training/spectra_sft/train_spectra.py", line 2903, in main
    raise train_e
  File "/home/conan/workspace/llm_training/spectra_sft/train_spectra.py", line 2876, in main
    trainer.train()
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/trl/trainer/sft_trainer.py", line 1185, in training_step
    return super().training_step(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/transformers/trainer.py", line 4071, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/accelerate/accelerator.py", line 2726, in backward
    self.deepspeed_engine_wrapped.backward(loss, sync_gradients=self.sync_gradients, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/accelerate/utils/deepspeed.py", line 270, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/deepspeed/runtime/engine.py", line 2532, in backward
    loss.backward(**backward_kwargs)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/function.py", line 311, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/home/conan/miniconda3/envs/llm_train/lib/python3.12/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

================================================================================
03:17:57 | WARNING  | ‚ö†Ô∏è  Exception detected in finally block: OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 637.06 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 75.89 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Saving final model...
03:17:57 | INFO     | üíæ Saving final model...
03:17:57 | INFO     | üíæ Using DeepSpeed save_checkpoint (NVMe-safe)...
