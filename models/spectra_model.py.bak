import copy
import os
import sys
import inspect
import gc
import traceback
import math
from functools import partial
from collections.abc import Callable
from dataclasses import dataclass
from typing import List, Optional, Tuple, Union, Type, TYPE_CHECKING

from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
# Add dynamo import for torch.compile compatibility
import torch._dynamo
import torch.distributed as dist

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, HybridCache, StaticCache, DynamicCache
from transformers.generation.utils import GenerationMixin
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask
from transformers.processing_utils import Unpack
from transformers.utils import logging
from transformers.utils.doc import (
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
    add_start_docstrings,
)
from transformers.utils.generic import (
    ModelOutput,
    can_return_tuple,
)
from transformers.utils import auto_docstring
from transformers.utils.import_utils import (
    is_torchdynamo_compiling,
    is_torch_flex_attn_available,
    is_flash_attn_2_available
)
from transformers.modeling_utils import (
    restore_default_dtype,
    SpecificPreTrainedModelType,
)
from transformers.configuration_utils import PretrainedConfig
from transformers import logging
from transformers.utils.deprecation import deprecate_kwarg
from transformers import AutoModel, AutoConfig, AutoModelForCausalLM, AutoModelForVision2Seq
from .spectra_config import SPECTRAConfig, SPECTRATextConfig

if is_torch_flex_attn_available():
    from torch.nn.attention.flex_attention import BlockMask
    from transformers.integrations.flex_attention import make_flex_block_causal_mask

    

try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False

# ===== Transformers Compatibility Patch (Module Level) =====
# Some models (Kimi-VL, etc.) use legacy activation imports that were renamed/removed
# Patch transformers.activations immediately on module load
try:
    import transformers.activations as _ta
    
    # PytorchGELUTanh was renamed/removed in newer transformers versions
    if not hasattr(_ta, 'PytorchGELUTanh'):
        class _PytorchGELUTanh(nn.Module):
            """Compatibility shim for legacy PytorchGELUTanh activation."""
            def forward(self, input):
                return F.gelu(input, approximate="tanh")
        _ta.PytorchGELUTanh = _PytorchGELUTanh
    
    # NewGELUActivation might also be missing in some versions
    if not hasattr(_ta, 'NewGELUActivation') and hasattr(_ta, 'GELUActivation'):
        _ta.NewGELUActivation = _ta.GELUActivation
except Exception:
    pass  # Non-critical, will fail later if actually needed

def is_deepspeed_initialized():

    """Check if DeepSpeed is initialized and active for the current process."""
    return DEEPSPEED_AVAILABLE and dist.is_initialized() and hasattr(deepspeed, 'comm') and deepspeed.comm.is_initialized()

def _safe_init_weight(param, init_fn):
    """
    Safely initialize a parameter that might be sharded by DeepSpeed ZeRO-3.
    Use GatheredParameters to gather, initialize on rank 0, and scatter back.
    Also handles casting to FP32 for initialization if the param is BF16/FP16, 
    as operations like orthogonal_ (QR decomposition) do not support BF16 on CUDA.
    """
    if param is None:
        return

    def _apply_init_fp32(p):
        if p.dtype in [torch.bfloat16, torch.float16]:
            # Create FP32 copy for safe initialization
            p_fp32 = p.data.float()
            init_fn(p_fp32)
            # Copy back to original dtype
            p.data.copy_(p_fp32)
        else:
            init_fn(p)
        
    if DEEPSPEED_AVAILABLE and hasattr(param, "ds_id"):
        # ZeRO-3 sharded parameter
        with deepspeed.zero.GatheredParameters(param, modifier_rank=0):
            if deepspeed.comm.get_rank() == 0:
                _apply_init_fp32(param)
    else:
        # Normal parameter (or ZeRO < 3)
        _apply_init_fp32(param)
    
logger = logging.get_logger(__name__)
_CONFIG_FOR_DOC = "SPECTRAConfig"


def calculate_ortho_loss_for_experts(expert_weights: List[torch.Tensor]) -> torch.Tensor:
    """
    Calculates the orthogonalization loss for a set of expert weights from a single MoE layer.
    This loss encourages functional diversity among experts by penalizing similarity
    in their weight spaces. The loss is the squared Frobenius norm
    of (VV' - I) where V is the matrix of normalized expert weights.
    """
    if not expert_weights:
        return torch.tensor(0.0, device=expert_weights[0].device)

    flattened_weights = [w.view(-1) for w in expert_weights]
    V = torch.stack(flattened_weights)

    # Normalize rows to be unit vectors, preventing weights from collapsing to zero
    # [수정] 엡실론 추가하여 0으로 나누기 방지
    V = V / (V.norm(p=2, dim=1, keepdim=True) + 1e-6)
    
    # Gram matrix: V @ V.T
    gram_matrix = torch.matmul(V, V.t())
    
    # Target: identity matrix
    identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
    
    # Loss: squared Frobenius norm of (VV' - I)
    ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
    return ortho_loss


def _orthogonal_constraint_loss(
    num_experts: int, 
    gate_logits: torch.Tensor
) -> torch.Tensor:
        """라우터 출력값들의 직교성 제약 손실"""
        # router_outputs: [batch*seq, num_experts]
        
        # 각 토큰별로 expert 방향이 직교하도록
        # [수정] 엡실론 추가하여 0으로 나누기 방지
        normalized_outputs = gate_logits / (gate_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # Gram matrix: [num_experts, num_experts]
        gram_matrix = torch.matmul(normalized_outputs.T, normalized_outputs)
        
        # Target: identity matrix
        identity = torch.eye(num_experts, device=gate_logits.device, dtype=gate_logits.dtype)
        
        # Loss: squared Frobenius norm of (GG' - I)
        constraint_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
        return constraint_loss


class ContrastiveRouterLoss(nn.Module):
    """
    Encourages experts to process tokens from distinct semantic spaces (Hidden States).
    Uses Soft Probabilities for differentiability.
    """
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.epsilon = 1e-6

    def forward(self, hidden_states, routing_weights):
        """
        hidden_states: [batch, seq, hidden_dim]
        routing_weights: [batch, seq, num_experts] (Softmax probabilities)
        """
        # Flatten: [batch, seq, hidden] -> [N, hidden], [batch*seq, experts] -> [N, experts]
        flattened_states = hidden_states.reshape(-1, hidden_states.size(-1))
        flattened_weights = routing_weights.reshape(-1, routing_weights.size(-1))
        
        # Ensure dtype consistency for matmul
        flattened_weights = flattened_weights.to(dtype=flattened_states.dtype)
        
        # Calculate weighted centroids for each expert
        # [experts, N] @ [N, hidden] -> [experts, hidden]
        expert_centroids = torch.matmul(flattened_weights.t(), flattened_states)
        
        # Normalize by total weight assigned to expert
        # CRITICAL FIX: Detach denominator to prevent invalid gradient flow
        expert_weight_sums = (flattened_weights.sum(dim=0) + self.epsilon).detach()
        expert_centroids = expert_centroids / expert_weight_sums.unsqueeze(-1)
        
        # Cosine similarity between centroids
        # [수정] 엡실론 추가하여 0으로 나누기 방지
        normalized_centroids = expert_centroids / (expert_centroids.norm(p=2, dim=1, keepdim=True) + 1e-6)
        similarity = torch.matmul(normalized_centroids, normalized_centroids.t())
        
        # Minimize off-diagonal similarity (contrastive)
        num_experts = similarity.size(0)
        mask = torch.eye(num_experts, device=similarity.device).bool()
        contrastive_loss = similarity[~mask].mean()
        
        return contrastive_loss

def load_balancing_loss_func(
    gate_logits: torch.Tensor,
    num_experts: int,
    top_k: int = 2,
    attention_mask: Optional[torch.Tensor] = None,
    router_z_loss_coef: Optional[float] = None,
    router_entropy_coef: Optional[float] = None,
    usage_uniformity_coef: Optional[float] = None,
) -> torch.Tensor:
    if gate_logits is None:
        return torch.tensor(0.0)

    if isinstance(gate_logits, tuple):
        # Add a check for empty tuple
        if not gate_logits:
            return torch.tensor(0.0)
        compute_device = gate_logits[0].device
        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)
    else:
        # handle tensor input (single tensor from global router)
        concatenated_gate_logits = gate_logits
        compute_device = concatenated_gate_logits.device

    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)

    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)

    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)

    if attention_mask is None:
        # Compute the percentage of tokens routed to each experts
        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.mean(routing_weights, dim=0)
    else:
        batch_size, sequence_length = attention_mask.shape
        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)

        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask
        expert_attention_mask = (
            attention_mask[None, :, :, None, None]
            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))
            .reshape(-1, top_k, num_experts)
            .to(compute_device)
        )

        # Compute the percentage of tokens routed to each experts
        # Sum over batch*seq and top_k dimensions to get [num_experts]
        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=(0, 1)) / torch.sum(
            expert_attention_mask, dim=(0, 1)
        )

        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert
        router_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))
            .reshape(-1, num_experts)
            .to(compute_device)
        )

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(
            router_per_expert_attention_mask, dim=0
        )

    # Core Switch-style load balancing loss
    aux_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0)) * num_experts

    # Router z-loss (Switch Transformer) to prevent overconfident routers
    if router_z_loss_coef is not None and router_z_loss_coef > 0:
        log_z = torch.logsumexp(concatenated_gate_logits, dim=-1)
        z_loss = torch.square(log_z).mean()
        aux_loss = aux_loss + router_z_loss_coef * z_loss

    # Entropy regularization to avoid routing collapse (maximize entropy)
    if router_entropy_coef is not None and router_entropy_coef > 0:
        token_entropy = -(routing_weights * torch.log(routing_weights.clamp_min(1e-12))).sum(dim=-1)
        # Normalize by log(num_experts) for scale invariance across different expert counts
        normalized_entropy = token_entropy / math.log(max(num_experts, 2))
        entropy_reg = -normalized_entropy.mean()
        aux_loss = aux_loss + router_entropy_coef * entropy_reg

    # Usage uniformity (optional): encourage average routing probability per expert to be near-uniform
    if usage_uniformity_coef is not None and usage_uniformity_coef > 0:
        # router_prob_per_expert already accounts for attention mask when provided
        target = torch.full_like(router_prob_per_expert, 1.0 / float(num_experts))
        usage_uniformity_loss = torch.mean(torch.square(router_prob_per_expert - target))
        aux_loss = aux_loss + usage_uniformity_coef * usage_uniformity_loss

    return aux_loss


def spectra_lb_loss(
    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
    num_experts: int,
    lb_l2_coef: float = 1.0,
    lb_cv_coef: float = 0.5,
    lb_entropy_floor_coef: float = 0.0,
    top_k: int = 2,
    lb_topk_l2_coef: float = 0.0,
    lb_topk_cv_coef: float = 0.0,
    attention_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    SPECTRA-aligned low-overhead load balancing loss.
    Uses per-expert statistics only (O(E)) to minimize compute overhead.
    No shape changes or fallback zeros/nan - skips invalid terms.

    Args:
        gate_logits: Routing logits, either tensor [N, E] or tuple of tensors
        num_experts: Number of experts E
        lb_l2_coef: Weight for L2 uniformity loss
        lb_cv_coef: Weight for CV minimization
        lb_entropy_floor_coef: Weight for entropy floor (optional)
        top_k: Number of experts selected per token
        lb_topk_l2_coef: Weight for top-k token count uniformity loss
        lb_topk_cv_coef: Weight for top-k token count CV minimization
        attention_mask: Attention mask for valid tokens

    Returns:
        Scalar loss tensor (sum of weighted terms)
    """
    # Handle tuple input (concatenate if needed)
    if isinstance(gate_logits, tuple):
        if not gate_logits:  # Empty tuple - skip entirely
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        tensors = [gl for gl in gate_logits if gl is not None and gl.numel() > 0]
        if not tensors:
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        gate_logits = torch.cat(tensors, dim=0)

    if gate_logits is None or gate_logits.numel() == 0:
        return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)

    # Get routing weights using existing softmax path
    routing_weights = torch.nn.functional.softmax(gate_logits, dim=-1)
    device, dtype = routing_weights.device, routing_weights.dtype

    # Compute per-expert mean probabilities
    routing_per_expert_attention_mask = None
    if attention_mask is not None:
        # Use existing masking logic from load_balancing_loss_func
        batch_size, seq_length = attention_mask.shape
        tokens = routing_weights.shape[0]
        denom = batch_size * seq_length
        num_hidden_layers = max(tokens // denom, 1) if denom > 0 else 1

        routing_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, seq_length, num_experts))
            .reshape(-1, num_experts)
            .to(device)
        )

        # If shapes mismatch (e.g., tokens not divisible), fall back to full mask
        if routing_per_expert_attention_mask.shape[0] != routing_weights.shape[0]:
            routing_per_expert_attention_mask = torch.ones_like(routing_weights, device=device, dtype=dtype)

        # Mean per expert (masked)
        denom_mask = torch.sum(routing_per_expert_attention_mask, dim=0).clamp_min(1.0)
        p_bar = torch.sum(routing_weights * routing_per_expert_attention_mask, dim=0) / denom_mask
    else:
        # Simple mean across batch dimension
        p_bar = routing_weights.mean(dim=0)  # [E]

    # Uniform target distribution
    u = torch.full_like(p_bar, 1.0 / float(num_experts))

    # Numerical stability
    eps = torch.finfo(dtype).eps

    total_loss = torch.zeros((), dtype=dtype, device=device)

    # L2 uniformity loss
    if lb_l2_coef > 0:
        l2_loss = torch.sum((p_bar - u) ** 2)
        total_loss = total_loss + lb_l2_coef * l2_loss

    # CV minimization (approximate)
    if lb_cv_coef > 0:
        mean_p = p_bar.mean()
        var_p = p_bar.var(unbiased=False)  # Population variance
        cv_loss = var_p / (mean_p + eps)
        total_loss = total_loss + lb_cv_coef * cv_loss

    # Entropy floor (optional)
    if lb_entropy_floor_coef > 0:
        # Token-level entropy floor
        token_entropy = -torch.sum(routing_weights * torch.log(routing_weights + eps), dim=-1)
        entropy_floor_loss = -token_entropy.mean()  # Minimize negative entropy = maximize entropy
        total_loss = total_loss + lb_entropy_floor_coef * entropy_floor_loss

    # Top-k token count based losses (works on discrete expert selection)
    if (lb_topk_l2_coef > 0 or lb_topk_cv_coef > 0) and top_k > 0:
        k = min(top_k, num_experts)
        # top-k indices based on routing probabilities (monotonic with logits)
        topk_probs, topk_indices = torch.topk(routing_weights, k=k, dim=-1)

        if attention_mask is not None and routing_per_expert_attention_mask is not None:
            token_mask = routing_per_expert_attention_mask.any(dim=-1)
            if token_mask.any():
                topk_indices = topk_indices[token_mask]
                topk_probs = topk_probs[token_mask]
            else:
                topk_indices = topk_indices[:0]
                topk_probs = topk_probs[:0]

        flat_indices = topk_indices.reshape(-1)
        flat_probs = topk_probs.reshape(-1)

        counts = torch.zeros(num_experts, dtype=dtype, device=device)
        if flat_indices.numel() > 0:
            ones = torch.ones_like(flat_indices, dtype=dtype, device=device)
            counts.scatter_add_(0, flat_indices, ones)

            weighted_counts = torch.zeros_like(counts)
            weighted_counts.scatter_add_(0, flat_indices, flat_probs.to(dtype))

            total_counts = counts.sum()
            if total_counts > 0 and lb_topk_l2_coef > 0:
                usage_distribution = counts / total_counts
                l2_topk_loss = torch.sum((usage_distribution - u) ** 2)
                total_loss = total_loss + lb_topk_l2_coef * l2_topk_loss

            mean_weight = weighted_counts.mean()
            var_weight = weighted_counts.var(unbiased=False) if weighted_counts.numel() > 0 else torch.tensor(0.0, device=device, dtype=dtype)
            if lb_topk_cv_coef > 0 and mean_weight > 0:
                topk_cv_loss = var_weight / (mean_weight + eps)
                total_loss = total_loss + lb_topk_cv_coef * topk_cv_loss

    return total_loss


# Copied from Phi-3.5-MoE
def _get_unpad_data(attention_mask):
    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)
    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
    max_seqlen_in_batch = seqlens_in_batch.max().item()
    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))
    return (
        indices,
        cu_seqlens,
        max_seqlen_in_batch,
    )
    
@dataclass
class SPECTRAModelOutputWithPast(BaseModelOutputWithPast):
    """
    Base class for SPECTRA outputs, with hidden states and attentions.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.
    """

    image_hidden_states: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    aux_loss: Optional[torch.FloatTensor] = None
    routing_uncertainty: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None

@dataclass
class SPECTRACausalLMOutputWithPast(ModelOutput):
    """
    Base class for SPECTRA causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    image_hidden_states: Optional[torch.FloatTensor] = None
    # this is moe specific
    aux_loss: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    routing_uncertainty: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None

class mp(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx, 
        scores: torch.Tensor, 
        multiplier: torch.Tensor, 
        selected_experts: torch.Tensor,
        masked_gates: torch.Tensor,
        mask_for_one: torch.Tensor,
    ):
        ctx.save_for_backward(multiplier, selected_experts, masked_gates)
        return multiplier * mask_for_one
        
    @staticmethod
    def backward(
        ctx, 
        grad_at_output: torch.Tensor, 
    ):
        multiplier, selected_experts, masked_gates = ctx.saved_tensors
        
        grad_at_output = grad_at_output * multiplier
        
        grad_at_scores_expaned = masked_gates * grad_at_output.mul(-1)
        grad_at_scores_expaned.scatter_add_(
            dim=-1,
            index=selected_experts,
            src=grad_at_output,
        )
        
        return (
            grad_at_scores_expaned, 
            None, 
            None, 
            None, 
            None, 
            None, 
        )


def enhanced_soft_orthogonality_loss(
    expert_embeddings: torch.Tensor,
    lambda_so: float = 1e-4,
    use_srip: bool = True,
) -> torch.Tensor:
    """
    Enhanced Soft Orthogonality Loss with SRIP variant.
    
    핵심 차별점:
    1. Frobenius Norm + Spectral Norm 결합
    2. 양방향 억제: cos(e_i, e_j)^2로 +1과 -1 모두 페널티
    3. Warm-up 지원
    
    Args:
        expert_embeddings: [num_experts, dim] or [batch, num_experts, dim]
        lambda_so: loss coefficient
        use_srip: use spectral norm variant
    
    Returns:
        loss: scalar orthogonality loss
    """
    if expert_embeddings.dim() == 3:
        # Batch case: average over batch
        # This is important: we want the EXPERTS to be orthogonal on average,
        # but momentarily they can shift. Averaging the REPRESENTATION first
        # stabilizes the gradient.
        expert_embeddings = expert_embeddings.mean(dim=0)  # [E, dim]
    
    # Check for empty or invalid input
    if expert_embeddings.numel() == 0:
        return torch.tensor(0.0, device=expert_embeddings.device, requires_grad=True)

    # Normalize to unit vectors
    E_norm = F.normalize(expert_embeddings, p=2, dim=-1)  # [E, dim]
    
    # Gram matrix (pairwise cosine similarities)
    # G_ij = cos(e_i, e_j)
    G = torch.matmul(E_norm, E_norm.t())  # [E, E]
    
    # Target: Identity matrix
    I = torch.eye(G.size(0), device=G.device, dtype=G.dtype)
    
    # Frobenius norm loss: ||G - I||_F^2
    # This penalizes off-diagonals (both positive and negative correlations)
    # (cos)^2 will be minimized
    frob_loss = torch.pow(torch.norm(G - I, p='fro'), 2)
    
    if use_srip:
        # SRIP: Spectral norm of (G - I), bounds Lipschitz constant
        # Approximate with power iteration for efficiency? 
        # For typical E (e.g., 8-64), exact computation via svd or matrix_norm is feasible/fast enough on GPU.
        # If E is very large (e.g., 256+), might be slow, but usually done once per step.
        diff = G - I
        # Use spectral norm (2-norm)
        # torch.linalg.matrix_norm with ord=2 computes the spectral norm (largest singular value)
        spectral_loss = torch.linalg.matrix_norm(diff, ord=2) ** 2
        loss = 0.7 * frob_loss + 0.3 * spectral_loss
    else:
        loss = frob_loss
    
    return lambda_so * loss


class ExpressionProjector(nn.Module):
    """
    [Verified] Newton-Schulz Orthogonal Projector
    학습/추론 모두에서 가중치를 강제로 직교화하여 붕괴를 방지합니다.
    """

    def __init__(
        self,
        input_dim,
        output_dim,
        num_experts,
        method="newton_schulz",
        iterations=3,
        **kwargs
    ):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_experts = num_experts
        self.method = method
        self.iterations = iterations

        # 단일 선형층 + 정직교 초기화
        self.exp_proj = nn.Linear(input_dim, output_dim, bias=False)
        _safe_init_weight(self.exp_proj.weight, nn.init.orthogonal_)

    def newton_schulz(self, W: torch.Tensor, steps: int = 3) -> torch.Tensor:
        """SVD-free orthogonalization."""
        # CRITICAL FIX: Detach norm to prevent invalid gradient flow
        # The norm is only used for numerical stability, not for learning
        norms = (W.norm(p="fro") + 1e-8).detach()
        X = W / norms  # Always normalize for stability

        transpose = X.shape[0] < X.shape[1]
        if transpose:
            X = X.t()

        for _ in range(steps):
            A = torch.matmul(X.t(), X)
            X = 1.5 * X - 0.5 * torch.matmul(X, A)

        if transpose:
            X = X.t()
        return X

    def enforce_orthogonality(self, beta=0.1):
        """
        Manifold constraint: Stiefel retraction.
        W = W - beta * (W@W.T - I) @ W
        학습 파라미터를 직접 수정하여 전문가 벡터들이 항상 직교하도록 유지합니다.
        """
        with torch.no_grad():
            W = self.exp_proj.weight
            I = torch.eye(W.size(0), device=W.device, dtype=W.dtype)
            # W: [num_experts, hidden_dim]
            # W @ W.T: [num_experts, num_experts]
            delta = torch.matmul(torch.matmul(W, W.t()) - I, W)
            W.sub_(beta * delta)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # CRITICAL FIX: Newton-Schulz causes gradient issues with checkpointing
        # Use direct linear projection instead - orthogonality is enforced via loss
        # W_ortho = self.newton_schulz(self.exp_proj.weight, steps=self.iterations)
        # return F.linear(x, W_ortho)
        return self.exp_proj(x)

    def orthogonal_loss(self):
        # 강제 직교화 경로이므로 별도 손실 불필요
        return torch.tensor(0.0, device=self.exp_proj.weight.device)


class ManualGRUCell(nn.Module):
    """
    LoRA-friendly GRU cell implemented purely with Linear layers.
    Gate weights are explicitly named for PEFT targeting.
    """

    def __init__(self, input_size: int, hidden_size: int):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        self.weight_ih_gates = nn.Linear(input_size, hidden_size * 2)
        self.weight_hh_gates = nn.Linear(hidden_size, hidden_size * 2)

        self.weight_ih_cand = nn.Linear(input_size, hidden_size)
        self.weight_hh_cand = nn.Linear(hidden_size, hidden_size)

        _safe_init_weight(self.weight_hh_gates.weight, nn.init.orthogonal_)
        _safe_init_weight(self.weight_hh_cand.weight, nn.init.orthogonal_)
        _safe_init_weight(self.weight_ih_gates.weight, nn.init.xavier_uniform_)
        _safe_init_weight(self.weight_ih_cand.weight, nn.init.xavier_uniform_)

    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:
        gates_x = self.weight_ih_gates(x)
        gates_h = self.weight_hh_gates(h)
        gates = gates_x + gates_h

        r_gate, z_gate = gates.chunk(2, dim=1)
        r_gate = torch.sigmoid(r_gate)
        z_gate = torch.sigmoid(z_gate)

        cand_h = self.weight_hh_cand(h * r_gate)
        cand_x = self.weight_ih_cand(x)
        n = torch.tanh(cand_x + cand_h)

        next_h = (1 - z_gate) * n + z_gate * h
        return next_h


# NOT USE IN CURRENT IMPLEMENTATION
def differentiable_sinkhorn(cost: torch.Tensor, num_experts: int, epsilon: float = 0.05, iterations: int = 3) -> torch.Tensor:
    """
    [Pure Math Sinkhorn - No Learning, Just Computing]
    Differentiable Sinkhorn algorithm for optimal transport.
    학습 파라미터가 0개이므로 망가지고 싶어도 망가질 수 없습니다.
    
    Args:
        cost: [Batch, Experts] cost matrix (lower is better)
        num_experts: number of experts
        epsilon: temperature parameter
        iterations: number of Sinkhorn iterations
    
    Returns:
        Q: [Batch, Experts] doubly stochastic matrix (assignment probabilities)
    """
    batch_tokens = cost.shape[0]
    device = cost.device
    dtype = cost.dtype
    
    # Validate epsilon
    if epsilon <= 0:
        raise ValueError(f"epsilon must be positive, got {epsilon}")
    if iterations <= 0:
        raise ValueError(f"iterations must be positive, got {iterations}")
    
    # Initialize Q: exp(-cost / epsilon)
    # cost_float = cost.float()
    cost_min, _ = cost.min(dim=-1, keepdim=True)
    cost_float = (cost - cost_min).float()  # 이제 최솟값은 0이 됨 -> exp(0) = 1 (Safe!)
    
    # Clamp cost to prevent exp overflow
    max_cost_ratio = 50.0
    cost_clamped = cost_float / epsilon
    cost_clamped = torch.clamp(cost_clamped, min=-max_cost_ratio, max=max_cost_ratio)
    
    Q = torch.exp(-cost_clamped).to(dtype=dtype)
    
    # Store original shape
    original_shape = Q.shape
    N, E = original_shape[0], original_shape[1]
    target_load = float(N) / float(E)
    
    # Sinkhorn iterations (standard Sinkhorn-Knopp)
    for i in range(iterations):
        # Row normalization: 각 토큰의 확률 합 = 1
        row_sum = Q.sum(dim=-1, keepdim=True) + 1e-8
        Q = Q / row_sum
        
        # NaN/Inf check
        if torch.isnan(Q).any() or torch.isinf(Q).any():
            raise ValueError(f"NaN/Inf in Sinkhorn iteration {i} after row norm")
        
        # Column normalization: 각 전문가의 기대 토큰 수 = N/E
        col_sum = Q.sum(dim=0, keepdim=True) + 1e-8
        Q = Q / col_sum * target_load
        
        # NaN/Inf check
        if torch.isnan(Q).any() or torch.isinf(Q).any():
            raise ValueError(f"NaN/Inf in Sinkhorn iteration {i} after col norm")
        
        # Shape consistency check
        assert Q.shape == original_shape, f"Shape changed in iteration {i}: {Q.shape} vs {original_shape}"
    
    # Final row normalization to ensure row sums = 1.0
    row_sum = Q.sum(dim=-1, keepdim=True) + 1e-8
    Q = Q / row_sum
    
    return Q


def log_sinkhorn_stabilized(
    logits: torch.Tensor,
    num_experts: int,
    epsilon: float = 0.05,
    iterations: int = 5,
    adaptive_epsilon: bool = True,
    cv_ema: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Log-Domain Stabilized Sinkhorn Algorithm.
    
    핵심: exp(-C/epsilon) 대신 log-space에서 연산하여 underflow/overflow 방지.
    
    Args:
        logits: [N, E] routing logits (higher = better)
        num_experts: number of experts
        epsilon: base temperature (adaptive하게 조절됨)
        iterations: Sinkhorn iterations
        adaptive_epsilon: CV에 따라 epsilon 조절
        cv_ema: CV EMA value for adaptive epsilon
    
    Returns:
        P: [N, E] doubly stochastic matrix
        max_vio: maximum constraint violation (for monitoring)
    """
    N, E = logits.shape
    device, dtype = logits.device, logits.dtype
    
    # Adaptive Epsilon: CV가 높으면 epsilon 증가 (더 부드러운 할당)
    if adaptive_epsilon and cv_ema is not None:
        cv_val = cv_ema.item() if torch.is_tensor(cv_ema) else cv_ema
        # Max scaling factor: 4.0 (1 + 3.0)
        # If CV explodes (e.g. > 10.0), clamp it.
        epsilon = epsilon * (1.0 + min(cv_val, 3.0))
    
    # Safety clamp for epsilon
    epsilon = max(epsilon, 1e-4) # Avoid division by zero
    
    # Log-space cost matrix: M = logits / epsilon
    # We want to maximize logits <-> minimize cost
    # Cost = -logits
    # K = exp(-Cost/eps) = exp(logits/eps)
    # working in log domain: M = log(K) = logits/eps
    M = logits.float() / epsilon
    
    # Numerical stability: subtract max per row (log-sum-exp trick preparation)
    # Does not change the resulting distribution P
    M_max = M.max(dim=-1, keepdim=True).values
    M = M - M_max
    
    # Initialize dual potentials
    f = torch.zeros(N, 1, device=device, dtype=torch.float32)  # row potential
    g = torch.zeros(1, E, device=device, dtype=torch.float32)  # column potential
    
    # Target marginals
    # Row target: 1 (actually 1/N but we are working with probabilities summing to 1 per row)
    # Wait, Sinkhorn usually projects to DSM where rowsum=1, colsum=N/E?
    # Or rowsum=1/N, colsum=1/E?
    # Standard Attention/Routing: row_sum = 1 (each token goes somewhere)
    # Col sum = N/E (uniform load)
    
    target_row = torch.zeros(N, 1, device=device, dtype=torch.float32) # log(1) = 0
    target_col = torch.log(torch.tensor(N / E, device=device, dtype=torch.float32) + 1e-10) # log(N/E)
    
    for _ in range(iterations):
        # Row normalization in log-space
        # u = 1 ./ (K @ v) => log(u) = -log(K @ exp(log_v))
        # log_sum_exp_row = logsumexp(M + g^T)
        log_sum_exp_row = torch.logsumexp(M + g, dim=-1, keepdim=True)
        # f update: f = log(target_row) - log_sum_exp_row
        # But wait, original M includes f and g implicitly?
        # Usually Sinkhorn updates are:
        # u <- target_r / (K @ v)
        # v <- target_c / (K.T @ u)
        # In log domain:
        # log_u <- log_target_r - logsumexp(M + log_v)
        # log_v <- log_target_c - logsumexp(M.T + log_u)
        
        f = target_row - log_sum_exp_row
        
        # Column normalization in log-space  
        log_sum_exp_col = torch.logsumexp(M + f, dim=0, keepdim=True)
        g = target_col - log_sum_exp_col
    
    # Compute final transport plan P = diag(u) K diag(v)
    # log P = log u + M + log v
    log_P = f + M + g
    
    # Convert back to probability space
    # Since we normalized rows to sum to 1 (target_row=0 => log(1)),
    # P rows should sum to 1.
    P = torch.exp(log_P).to(dtype)
    
    # Final cleanup: ensure row sums are exactly 1
    # (sometimes small errors accumulate)
    P = P / (P.sum(dim=-1, keepdim=True) + 1e-8)
    
    # Compute MaxVio for monitoring
    if iterations > 0:
        with torch.no_grad():
            row_sum = P.sum(dim=-1) # Should be 1.0
            col_sum = P.sum(dim=0)  # Should be N/E
            
            # Use float64 for precision in check
            row_vio = (row_sum.float() - 1.0).abs().max()
            col_vio = (col_sum.float() - (N/E)).abs().max()
            max_vio = torch.max(row_vio, col_vio)
    else:
        max_vio = torch.tensor(0.0, device=device)
    
    return P, max_vio


# [OSR] Neural Solver 제거 완료 - Pure Math Sinkhorn만 사용
# [OSR] Neural Solver 클래스 제거 완료
# NeuralGradientProjector와 DualPotentialLinearSolver는 제거됨
# 이제 differentiable_sinkhorn 함수만 사용 (학습 파라미터 0개)

class SPECTRARouter(nn.Module):
    def __init__(self, config: SPECTRATextConfig, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.router_dim = config.router_dim
        self.layernorm_eps = getattr(config, "router_layernorm_eps", 1e-5)

        # ------------------------------------------------------------------
        # Expert-Choice (Quota) Routing
        # - Keep sparse MoE compute (still runs only top-k experts per token)
        # - But make expert loads stable by enforcing per-expert capacity at the
        #   *selection* stage (token-choice top-k tends to break Sinkhorn balance).
        # ------------------------------------------------------------------
        self.expert_choice_routing = bool(getattr(config, "expert_choice_routing", True))
        # Capacity factor (typical: 1.0~2.0). We also accept `capacity_factor` for convenience.
        self.expert_choice_capacity_factor = float(
            getattr(config, "expert_choice_capacity_factor", getattr(config, "capacity_factor", 1.25))
        )
        
        # ===== 개선된 Loss 가중치 설정 =====
        # 1. Speciality: 너무 강하지 않게 (기존 0.02 → 0.001)
        self.speciality_strength = getattr(config, "speciality_strength", 0.001)
        
        # 2. Sinkhorn: Teacher 강도 조절 (기존 0.1 → 0.05)
        self.sinkhorn_distillation_coef = getattr(config, "sinkhorn_distillation_coef", 0.05)
        
        # [전략 1] Balance Loss 가중치: CV를 0으로 만들기 위한 "가중치 폭탄"
        # GRU Solver의 balance_loss에 강한 가중치를 줘서 밸런싱을 최우선으로 만듦
        self.balance_loss_coef = getattr(config, "balance_loss_coef", 2.0)  # 기본값 2.0 (1.0 ~ 5.0 권장)
        
        # 3. Adaptive weighting: CV에 따라 loss 가중치 자동 조절
        self.adaptive_loss_scaling = getattr(config, "adaptive_loss_scaling", True)
        
        self.register_buffer("training_step", torch.tensor(0, dtype=torch.long))
        
        # EMA 설정
        self.balancing_strength = getattr(config, "balancing_strength", 0.01)
        self.ema_alpha = getattr(config, "ema_alpha", 0.99)
        self.register_buffer("expert_load_ema", torch.zeros(self.num_experts), persistent=True)
        
        # CV 추적
        self.register_buffer("cv_ema", torch.tensor(1.0), persistent=True)
        self.cv_ema_alpha = 0.99
        
        # Sinkhorn 설정
        self.sinkhorn_epsilon = max(getattr(config, "sinkhorn_epsilon", 0.1), 1e-6)
        self.sinkhorn_iterations = max(getattr(config, "sinkhorn_iterations", 3), 1)
        
        # Global routing GRU cell (depth-shared)
        # GRU는 작게 유지: 상황 파악만 하면 되고, expert별 표현은 ExpressionProjector가 담당
        # Expert별 orthogonal 표현 공간은 ExpressionProjector가 hidden_size -> num_experts * router_dim으로 확장
        gru_hidden_size = getattr(config, "router_intent_hidden_size", self.router_dim)
        self.load_balancer = ManualGRUCell(
            input_size=self.hidden_size,
            hidden_size=gru_hidden_size,
        )
        
        # Projection from small GRU hidden to num_experts * router_dim for routing
        # This allows GRU to be small while maintaining full expert space for routing
        self.gru_to_routing = nn.Linear(
            gru_hidden_size,
            self.num_experts * self.router_dim,
            bias=False
        )
        nn.init.zeros_(self.gru_to_routing.weight)  # Start with zero projection
        
        # ===== GRU → Expert Bias Projection (Lightweight) =====
        # GRU hidden state를 expert space로 투영하는 단순 선형 변환
        # bias 없음, zero-init으로 시작 (GRU state를 왜곡 없이 읽기만 함)
        self.bias_proj = nn.Linear(
            gru_hidden_size,
            self.num_experts,
            bias=False
        )
        # Zero initialization: GRU state를 그대로 읽기만 함
        nn.init.zeros_(self.bias_proj.weight)
        
        # ===== Expression Projector (Linear + Ortho Init) =====
        # 복잡한 Newton-Schulz도 일단 뺍시다. OSR 척력이면 충분합니다.
        self.expression = ExpressionProjector(
            self.hidden_size,
            self.num_experts * self.router_dim,
            self.num_experts,
            method="linear",
            iterations=getattr(config, "osr_iter", 3),
        )
        # Orthogonal initialization for better starting point
        if hasattr(self.expression, 'projection'):
            _safe_init_weight(self.expression.exp_proj.weight, nn.init.orthogonal_)
        self.expression.ortho_strength = 0.0  # OSR 척력이 직교성을 강제하므로 추가 제약 불필요
        
        # [DeepSeek-V3 Style] Simple Persistent Bias
        # 학습되는 파라미터가 아니라, 피드백 루프로 업데이트되는 버퍼입니다.
        # self.register_buffer("accumulated_bias", torch.zeros(self.num_experts), persistent=True) # DEPRECATED
        
        # Contrastive loss
        self.contrastive_loss = ContrastiveRouterLoss()

        # CRITICAL FIX for CheckpointError (use_reentrant=False):
        # In-place updates to buffers (expert_bias, cv_ema) in forward pass cause metadata mismatch
        # during recomputation. We move these updates to a backward hook which runs only once.
        self.register_full_backward_hook(self._backward_hook_update)
        self.bias_update_rate = getattr(config, "balancing_strength", 0.01)

        # OSR Hyperparameters
        self.repulsion_weight = getattr(config, "osr_repulsion_weight", 0.5)  # [핵심] OSR 척력 가중치
        
        # SOS-RMoE Configuration
        self.log_sinkhorn_enabled = getattr(config, "log_sinkhorn_enabled", True)
        self.srip_enabled = getattr(config, "srip_enabled", True)
        self.so_warmup_steps = getattr(config, "so_warmup_steps", 100)
        self.so_lambda_max = getattr(config, "so_lambda_max", 5e-2)
        
        # MaxVio tracking
        self.register_buffer("max_vio_ema", torch.tensor(0.0), persistent=True)
        self.max_vio_ema_alpha = 0.95

        # [NEW] Aux-Loss-Free Balancing을 위한 Global Bias Buffer
        # 학습 파라미터가 아닌 버퍼로 등록 (Optimizer가 건드리지 않음)
        self.register_buffer("expert_bias", torch.zeros(self.num_experts), persistent=True)
         
        # DeepSeek-V3 / Loss-Free Balancing 권장 설정
        # 0.001은 너무 느릴 수 있으나 진동을 막기 위해 안전함.
        self.bias_update_rate = getattr(config, "bias_update_rate", 0.001) # Set to 0.001 for stability
        
        # [Explicit Feedback Loop] Expert Bias Buffer
        # Initialized to zero, updated via feedback loop to penalize overloaded experts.
        self.register_buffer("expert_bias", torch.zeros(self.num_experts), persistent=True)

        # ===== [Fancy Hybrid] Orthogonal Penalty Scheduling =====
        # 초반: "비슷하면 죽어!" (강제 분리) -> 후반: "실력대로 해." (미세 조정)
        self.penalty_alpha_init = getattr(config, "penalty_alpha_init", 0.5)
        self.penalty_alpha_min = getattr(config, "penalty_alpha_min", 0.05)
        
        # [CheckpointError Fix] Delayed Update Buffers
        # Backward hook accumulates updates here; Forward applies them (step_update).
        # This prevents Shared Router State from mutating mid-backward-pass.
        self.register_buffer("pending_expert_bias_delta", torch.zeros_like(self.expert_bias), persistent=False)
        self.register_buffer("pending_cv_sum", torch.tensor(0.0, device=self.expert_bias.device), persistent=False)
        self.register_buffer("pending_cv_count", torch.tensor(0.0, device=self.expert_bias.device), persistent=False)

        # ===== [VL-GRU] Vertical Layer-wise Recurrence Enhancement =====
        # Enable true vertical recurrence where layer-wise GRU state forms a chain
        self.vl_gru_enabled = getattr(config, "vl_gru_enabled", True)
        self.vl_gru_layer_norm = getattr(config, "vl_gru_layer_norm", True)
        self.vl_gru_depth_encoding = getattr(config, "vl_gru_depth_encoding", True)
        self.vl_gru_residual_alpha = getattr(config, "vl_gru_residual_alpha", 0.1)
        
        # Layer normalization for GRU state stability (prevents drift across layers)
        if self.vl_gru_layer_norm:
            self.gru_state_norm = nn.LayerNorm(gru_hidden_size, eps=self.layernorm_eps)
        
        # Depth encoding: learnable embeddings for each layer position
        # Allows GRU to be "depth-aware" - knows which layer it's processing
        max_layers = getattr(config, "num_hidden_layers", 32)
        if self.vl_gru_depth_encoding:
            self.depth_embeddings = nn.Embedding(max_layers, gru_hidden_size)
            nn.init.normal_(self.depth_embeddings.weight, std=0.02)

    def get_ortho_alpha(self, step_frac: float) -> float:
        """
        [OSR Cosine Annealing] Orthogonal penalty strength scheduling.
        초반: 강한 직교성 강제 (전문가 분리) → 후반: 약화하여 미세조정
        
        Args:
            step_frac: Training progress (0.0 ~ 1.0)
        
        Returns:
            alpha: Current orthogonal penalty strength
        """
        import math
        return self.penalty_alpha_min + 0.5 * (self.penalty_alpha_init - self.penalty_alpha_min) * (1.0 + math.cos(math.pi * step_frac))

    def step_update(self):
        """
        Apply pending updates to state buffers.
        Called at the start of forward() to ensure state is stable for the duration of the step
        (Forward + Backward + Recompute).
        """
        if not self.training:
            return

        with torch.no_grad():
             # 1. Apply Expert Bias Delta
             if self.pending_expert_bias_delta.abs().sum() > 0:
                 self.expert_bias += self.pending_expert_bias_delta
                 self.pending_expert_bias_delta.zero_()
                 # Mean centering
                 self.expert_bias -= self.expert_bias.mean()
             
             # 2. Apply CV EMA Update
             if self.pending_cv_count > 0:
                 avg_cv = self.pending_cv_sum / self.pending_cv_count
                 # Update EMA once per step (or batch of layers) using average CV
                 self.cv_ema.mul_(self.max_vio_ema_alpha).add_(avg_cv * (1.0 - self.max_vio_ema_alpha))
                 self.pending_cv_sum.zero_()
                 self.pending_cv_count.zero_()

    def _backward_hook_update(self, module, grad_input, grad_output):
        """
        Accumulate state updates (EMA, Bias) into pending buffers.
        Do NOT mutate expert_bias/cv_ema in-place here.
        """
        if not self.training:
            return

        with torch.no_grad():
            # 1. Update Expert Bias (Load Balancing)
            if hasattr(self, 'last_routing_probs_full') and self.last_routing_probs_full is not None:
                # Use the stats from the last forward pass (recomputed or original)
                routing_probs = self.last_routing_probs_full
                last_routing_probs_mean = routing_probs.mean(dim=(0, 1))
                target_load = 1.0 / self.num_experts
                load_error = last_routing_probs_mean - target_load
                
                # Accumulate Bias Delta
                delta = -self.bias_update_rate * load_error.sign()
                self.pending_expert_bias_delta += delta
                
                # Update Expert Load EMA (This is less critical for consistency, can be direct or delayed)
                # But to be safe, let's keep it direct as it's purely monitoring/logging mostly? 
                # Wait, expert_load_ema MIGHT be used in forward?
                # Checked forward: It is commented out in usage (lines 1483).
                # So direct update is safe-ish, but for purity let's leave it direct (per-layer stats).
                self.expert_load_ema = 0.9 * self.expert_load_ema + 0.1 * last_routing_probs_mean
                
                # Setup for CV update
                expert_load = routing_probs.sum(dim=(0, 1))
                total_load = expert_load.sum()
                if total_load > 0:
                    expert_dist = expert_load / total_load
                else:
                    expert_dist = torch.ones_like(expert_load) / self.num_experts
                
                # CV Calculation
                var_p = expert_dist.var(unbiased=False)
                mean_p = expert_dist.mean()
                current_cv = (var_p.sqrt() / (mean_p + 1e-6))
                
                # Accumulate CV stats
                self.pending_cv_sum += current_cv
                self.pending_cv_count += 1

                
                # CV Update
                var_p = expert_dist.var(unbiased=False)
                mean_p = expert_dist.mean()
                current_cv = (var_p.sqrt() / (mean_p + 1e-6))
                
                self.cv_ema.mul_(self.cv_ema_alpha).add_(current_cv * (1.0 - self.cv_ema_alpha))
                
            # 2. Update MaxVio EMA
            if hasattr(self, 'last_max_vio') and self.last_max_vio is not None:
                self.max_vio_ema.mul_(self.max_vio_ema_alpha).add_(
                    self.last_max_vio * (1.0 - self.max_vio_ema_alpha)
                )

    def _expert_choice_topk_selection(
        self,
        routing_probs_full: torch.Tensor,  # [B, S, E], Sinkhorn probs
        top_k: int,
        capacity_factor: float,
    ) -> Tuple[torch.Tensor, torch.Tensor, int, torch.Tensor]:
        """
        Expert-choice (quota) sparse selection.

        We keep the *scores/probabilities* for all experts (cheap) but choose a sparse top-k
        set under a per-expert capacity constraint:
          capacity ≈ capacity_factor * (N_tokens * top_k) / num_experts

        Returns:
          multiplier: [B, S, top_k] normalized weights (sum=1 over top_k)
          selected_experts: [B, S, top_k] expert indices
          cap: per-expert capacity in tokens
          fallback_mask: [N] mask where quota selection failed and token-choice fallback was used
        """
        if routing_probs_full.dim() != 3:
            raise ValueError(f"routing_probs_full must be [B,S,E], got {routing_probs_full.shape}")

        batch_size, seq_len, num_experts = routing_probs_full.shape
        if num_experts != self.num_experts:
            raise ValueError(f"Expected num_experts={self.num_experts}, got {num_experts}")

        k = int(min(max(int(top_k), 1), num_experts))
        N = int(batch_size * seq_len)
        scores = routing_probs_full.reshape(N, num_experts)

        # Per-expert capacity in terms of *slots* (N tokens * k experts per token).
        # Add slack via capacity_factor. We cap by N*k slots, not N tokens.
        cap = int(math.ceil(float(capacity_factor) * (float(N) * float(k)) / float(num_experts)))
        cap = max(1, min(cap, N * k))

        # Capacity-masked k-round selection (hard constraint):
        # - Each round picks 1 expert per token among experts with remaining capacity
        # - Prevents any expert from exceeding cap (=> CV/MaxVio drift from selection stage is stopped)
        cap_left = torch.full((num_experts,), cap, dtype=torch.long, device=scores.device)
        selected = torch.empty((N, k), dtype=torch.long, device=scores.device)
        selected_vals = torch.empty((N, k), dtype=scores.dtype, device=scores.device)
        fallback_mask = torch.zeros((N,), dtype=torch.bool, device=scores.device)

        neg_inf = torch.tensor(float("-inf"), device=scores.device, dtype=scores.dtype)

        for r in range(k):
            # Start from original scores
            ms = scores

            # Mask experts with no remaining capacity
            avail = cap_left > 0  # [E]
            # CRITICAL FIX: Use tensor operations instead of .item() for deterministic behavior
            # in forward vs backward pass with use_reentrant=False
            # Always compute masked_scores to ensure consistent tensor shapes
            masked_scores = ms.clone()
            masked_scores[:, ~avail] = neg_inf

            # Mask already chosen experts for this token
            if r > 0:
                row_ids = torch.arange(N, device=scores.device).unsqueeze(1).expand(N, r)
                masked_scores[row_ids, selected[:, :r]] = neg_inf

            # CRITICAL FIX: Use tensor operations instead of conditional logic
            # to ensure deterministic behavior in forward vs backward pass
            # Check if any expert is available (tensor operation, not .item())
            avail_any = avail.any()  # Tensor, not scalar
            
            # Check if tokens have any valid scores (tensor operation)
            has_any = torch.isfinite(masked_scores).any(dim=-1)  # [N]
            has_any_all = has_any.all()  # Tensor, not scalar
            
            # Use torch.where to handle both cases deterministically
            # If no experts available, use fallback; otherwise use masked choice
            # CRITICAL: Always compute both paths to ensure consistent tensor shapes
            fallback_scores = scores.clone()
            if r > 0:
                row_ids = torch.arange(N, device=scores.device).unsqueeze(1).expand(N, r)
                fallback_scores[row_ids, selected[:, :r]] = neg_inf
            fallback_choice = fallback_scores.argmax(dim=-1)
            masked_choice = masked_scores.argmax(dim=-1)
            
            # Use tensor operations to combine choices deterministically
            # If no experts available OR token has no valid scores, use fallback
            use_fallback = ~avail_any | ~has_any
            chosen = torch.where(use_fallback, fallback_choice, masked_choice)
            
            # Update fallback_mask using tensor operations
            fallback_mask |= use_fallback
            
            # Get chosen values
            chosen_vals = scores.gather(1, chosen.unsqueeze(1)).squeeze(1)

            selected[:, r] = chosen
            selected_vals[:, r] = chosen_vals

            # Update capacities
            used = torch.bincount(chosen, minlength=num_experts).to(cap_left.dtype)
            cap_left = cap_left - used
            cap_left = torch.clamp(cap_left, min=0)

        # Normalize weights over top-k for stable scaling.
        # CRITICAL FIX: Detach denominator to prevent invalid gradient flow
        # The normalization denominator doesn't need gradients - only the routing scores do
        selected_vals = selected_vals.clamp_min(0.0)
        denom = selected_vals.sum(dim=-1, keepdim=True).clamp_min(1e-8).detach()
        multiplier = (selected_vals / denom).to(dtype=routing_probs_full.dtype)

        multiplier = multiplier.view(batch_size, seq_len, k)
        selected_experts = selected.view(batch_size, seq_len, k)
        return multiplier, selected_experts, cap, fallback_mask

    def compute_improved_speciality_loss(
        self, 
        routing_logits: torch.Tensor,
        expression_logits: torch.Tensor
    ) -> torch.Tensor:
        """개선된 Speciality Loss: Soft orthogonality + Diversity"""
        batch_size, seq_len, num_experts, router_dim = routing_logits.shape
        
        # 1. Routing space의 soft orthogonality (기존보다 약하게)
        # [수정] 엡실론 추가하여 0으로 나누기 방지
        routing_normalized = routing_logits / (routing_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        gram = torch.matmul(
            routing_normalized.view(-1, num_experts, router_dim),
            routing_normalized.view(-1, num_experts, router_dim).transpose(-2, -1)
        )
        identity = torch.eye(num_experts, device=gram.device)
        
        # Off-diagonal만 페널티 (diagonal은 1이어도 됨)
        mask = ~torch.eye(num_experts, dtype=torch.bool, device=gram.device)
        off_diag_loss = (gram[:, mask] ** 2).mean()
        
        # 2. Expression space의 diversity (전체 배치에서 다양성)
        # expression_logits의 shape을 routing_logits와 맞춤
        # [수정] 엡실론 추가하여 0으로 나누기 방지
        expr_normalized = expression_logits / (expression_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # expression_logits의 shape 확인 및 변환
        if expr_normalized.shape == routing_logits.shape:
            # 이미 올바른 shape: [batch_size, seq_len, num_experts, router_dim]
            expr_flat = expr_normalized.view(-1, num_experts, router_dim)
        else:
            # shape이 다르면 변환 시도
            # expression_logits: [batch_size, seq_len, num_experts, router_dim] 또는 다른 형태
            expr_total_elements = expr_normalized.numel()
            expected_elements = batch_size * seq_len * num_experts * router_dim
            
            if expr_total_elements == expected_elements:
                # 전체 요소 수가 같으면 reshape만 하면 됨
                expr_flat = expr_normalized.view(batch_size, seq_len, num_experts, router_dim).view(-1, num_experts, router_dim)
            else:
                # 요소 수가 다르면 마지막 router_dim 차원을 기준으로 처리
                # expr_normalized를 [..., router_dim] 형태로 가정
                if expr_normalized.dim() >= 2 and expr_normalized.size(-1) == router_dim:
                    # 마지막 차원이 router_dim이면, 앞부분을 flatten
                    expr_flat_all = expr_normalized.view(-1, router_dim)
                    # 필요한 개수만큼만 사용
                    needed_count = batch_size * seq_len * num_experts
                    if expr_flat_all.size(0) >= needed_count:
                        expr_flat = expr_flat_all[:needed_count].view(-1, num_experts, router_dim)
                    else:
                        # 부족하면 반복
                        repeat_times = (needed_count + expr_flat_all.size(0) - 1) // expr_flat_all.size(0)
                        expr_flat = expr_flat_all.repeat(repeat_times, 1)[:needed_count].view(-1, num_experts, router_dim)
                else:
                    # shape이 맞지 않으면 routing_logits만 사용
                    return off_diag_loss
        
        # Expert별 평균 방향
        expert_directions = expr_flat.mean(dim=0)  # [num_experts, router_dim]
        # [수정] 엡실론 추가하여 0으로 나누기 방지
        expert_directions = expert_directions / (expert_directions.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # Expert간 cosine similarity의 분산 최소화 (더 uniform하게)
        similarity_matrix = torch.matmul(expert_directions, expert_directions.T)
        target_similarity = torch.ones_like(similarity_matrix) * (1.0 / num_experts)
        target_similarity.diagonal().fill_(1.0)
        diversity_loss = F.mse_loss(similarity_matrix, target_similarity)
        
        return off_diag_loss * 0.5 + diversity_loss * 0.5

    @torch._dynamo.disable  # Disable torch.compile for gradient checkpointing compatibility
    def sinkhorn_algorithm(self, cost: torch.Tensor, epsilon: float = None, iterations: int = None) -> torch.Tensor:
        """
        Sinkhorn-Knopp algorithm for Optimal Transport.
        Computes a doubly stochastic matrix Q that ensures uniform expert load distribution.
        
        Args:
            cost: [N, E] cost matrix (negative logits, higher score = lower cost)
            epsilon: Temperature parameter (default: self.sinkhorn_epsilon)
            iterations: Number of Sinkhorn iterations (default: self.sinkhorn_iterations)
        
        Returns:
            Q: [N, E] doubly stochastic matrix (assignment probabilities)
        """
        if self.log_sinkhorn_enabled:
            # Use Log-Domain Sinkhorn for stability
            P, max_vio = log_sinkhorn_stabilized(
                logits=cost,  # cost -> logits (negate), higher logits = lower cost
                num_experts=self.num_experts,
                epsilon=epsilon,
                iterations=iterations,
                adaptive_epsilon=self.adaptive_loss_scaling,
                cv_ema=self.cv_ema,
            )
        else:
            # Standard Sinkhorn (legacy) - only if explicitly disabled
            # ... (omitted, assuming log_sinkhorn is preferred)
            # Just fallback to log_sinkhorn for safety
            P, max_vio = log_sinkhorn_stabilized(
                logits=cost,
                num_experts=self.num_experts,
                epsilon=epsilon,
                iterations=iterations,
                adaptive_epsilon=False, # Disable adaptive if legacy mode
                cv_ema=None,
            )
            
        # Update MaxVio EMA
        
        # Save max_vio for backward hook update
        if self.training:
            self.last_max_vio = max_vio.detach()
            # In-place update moved to _backward_hook_update

        
        return P
    
    # NOT USE IN CURRENT IMPLEMENTATION
    def compute_routing_uncertainty(self, routing_probs: torch.Tensor) -> torch.Tensor:
        """
        Compute normalized entropy of routing probabilities as uncertainty measure.
        Returns: [batch, seq_len] or [batch * seq_len] depending on input
        """
        # routing_probs: [..., num_experts]
        probs = routing_probs + 1e-8
        
        # Entropy: -sum(p * log(p))
        entropy = -torch.sum(probs * torch.log(probs), dim=-1)
        
        # Normalize by max entropy (log(num_experts))
        max_entropy = torch.log(torch.tensor(self.num_experts, device=probs.device, dtype=probs.dtype))
        normalized_entropy = entropy / max_entropy
        
        return normalized_entropy

    def compute_osr_cost(self, similarity: torch.Tensor, expert_embeddings: torch.Tensor) -> torch.Tensor:
        """
        [OSR Core Logic - Stabilized: Repulsion penalizes |expert-expert cosine| to drive orthogonality]
        Cost = -Similarity + lambda * Repulsion
        Cost = -Similarity + lambda * Repulsion
        Repulsion: 토큰이 선호(|similarity|)하는 전문가들이 서로 상관(|cos|)이 크면 페널티를 줌.
        (핵심) expert-expert cos의 부호(양/음)와 무관하게 |cos|가 크면 "직교(0)"에서 멀기 때문에 페널티.
        
        Args:
            similarity: [Batch, Experts] cosine similarity between routing and expression vectors
            expert_embeddings: [Experts, Dim] expert representation vectors
        
        Returns:
            cost: [Batch, Experts] cost matrix (lower is better for Sinkhorn)
        """
        # 1. 전문가 간 유사도 (Gram Matrix)
        # expert_embeddings: [Experts, Dim]
        # G: [Experts, Experts]
        expert_sim_matrix = torch.matmul(expert_embeddings, expert_embeddings.t())
        
        # 자기 자신과의 유사도(대각선)는 0으로 만듦 (자기 자신을 밀어내면 안 되니까)
        identity = torch.eye(self.num_experts, device=similarity.device, dtype=similarity.dtype)
        expert_sim_matrix = expert_sim_matrix * (1 - identity)
        
        # |cos|를 벌점: square는 부호를 제거하고 큰 상관(양/음)을 강하게 벌점
        # -> anti-parallel(-1)로 "음수 수렴"하는 해도 막고, 목표를 orthogonal(0)로 둠
        repulsion_matrix = torch.square(expert_sim_matrix)
        similarity_penalty = torch.relu(torch.abs(similarity) - 0.5) ** 2  # 0.7 이상이면 페널티
        # 2. 척력 계산 (Lateral Inhibition)
        # 토큰-전문가 관련성(|similarity|) 크기로 가중: 강하게 관련된 전문가들끼리만 더 강하게 밀어냄
        similarity_magnitude = torch.abs(similarity)
        repulsion_score = torch.matmul(similarity_magnitude, repulsion_matrix)
        
        # 3. 최종 Cost (Sinkhorn은 Cost가 낮을수록 좋아함)
        # Similarity가 높으면 Cost 낮춤 (-Sim)
        # Repulsion이 높으면 Cost 높임 (+Rep)
        cost = -similarity + self.repulsion_weight * repulsion_score + 0.25 * similarity_penalty
        
        return cost

    @torch._dynamo.disable
    def forward(self, x, hn, top_k=2, jitter_eps=0.01, step_frac=0.0, layer_idx: int = 0):
        """
        [OSR (Orthogonal Sinkhorn Routing) - Fancy Hybrid Version]
        Efficient Orthogonal Penalty + Alpha Scheduling.
        
        Args:
            step_frac: Training progress (0.0 ~ 1.0) for alpha scheduling
        """

        batch_size, seq_len, _ = x.shape
        tokens = batch_size * seq_len
        
        # Apply pending state updates (Stable State for Step)
        if self.training:
            self.step_update()

        # Flatten for processing
        x_flat = x.view(tokens, -1)  # [tokens, hidden]

        # GRU hidden size (small, not num_experts * router_dim)
        gru_hidden_size = getattr(self.config, "router_intent_hidden_size", self.router_dim)

        if hn is None:
            hn_flat = torch.zeros(
                tokens, gru_hidden_size, device=x.device, dtype=x.dtype
            )
        else:
            hn_flat = hn.view(tokens, -1)
            # [VL-GRU] Apply layer normalization to incoming hidden state for stability
            if self.vl_gru_enabled and self.vl_gru_layer_norm and hasattr(self, 'gru_state_norm'):
                hn_flat = self.gru_state_norm(hn_flat)

        # [VL-GRU] Depth encoding: inject layer position information into GRU
        # This allows the GRU to be "depth-aware" and form consistent expert paths
        if self.vl_gru_enabled and self.vl_gru_depth_encoding and hasattr(self, 'depth_embeddings'):
            layer_idx_clamped = min(layer_idx, self.depth_embeddings.num_embeddings - 1)
            depth_emb = self.depth_embeddings.weight[layer_idx_clamped]  # [gru_hidden_size]
            depth_emb = depth_emb.unsqueeze(0).expand(tokens, -1)  # [tokens, gru_hidden_size]
            # Add depth encoding to hidden state (not input, to preserve token information)
            hn_flat = hn_flat + depth_emb

        # 1. GRU & Projector (Context Aware)
        # 얘는 그냥 "상황 파악"만 하면 됩니다. 밸런싱은 수학이 합니다.
        # GRU는 작게 유지하고, projection으로 expert space로 확장
        gru_output_flat = self.load_balancer(x_flat, hn_flat)  # [tokens, router_dim]
        
        # [VL-GRU] Residual connection with alpha scaling for smooth layer-wise state propagation
        if self.vl_gru_enabled and hn is not None:
            gru_output_flat = gru_output_flat + self.vl_gru_residual_alpha * hn_flat
        
        hn_next = gru_output_flat  # Update hn (small size) - this flows to next layer
        # Project to full expert space for routing
        routing_output_flat = self.gru_to_routing(gru_output_flat)  # [tokens, E*R]
        routing_output = routing_output_flat.view(batch_size, seq_len, self.num_experts, self.router_dim)

        # Expression projection
        proj = self.expression(x_flat)  # [tokens, E*R]
        proj = proj.view(batch_size, seq_len, self.num_experts, self.router_dim)


        # 2. Normalize & Similarity
        # Force FP32 for stability in normalize and dot product
        routing_output_f32 = routing_output.float()
        proj_f32 = proj.float()
        
        routing_vec = F.normalize(routing_output_f32, p=2, dim=-1)  # [B, S, E, R]
        expression_vec = F.normalize(proj_f32, p=2, dim=-1)        # [B, S, E, R]
        
        # Cosine Similarity [B, S, E]
        similarity = (routing_vec * expression_vec).sum(dim=-1).to(x.dtype)  # [B, S, E]
        
        # Sanitize similarity (remove NaNs)
        if torch.isnan(similarity).any():
            similarity = torch.nan_to_num(similarity, nan=0.0)

        domain_orthogonality = similarity  # Alias for compatibility

        # 3. [OSR] Repulsive Cost Calculation
        # 전문가 간의 유사도를 계산하기 위해 대표 벡터 추출
        # 계산 효율성을 위해 현재 배치의 expression_vec 평균을 사용 (Dynamic Orthogonality)
        # [Experts, Dim]
        # Use FP32 for expert representation mean calculation
        current_expert_repr = expression_vec.mean(dim=(0, 1))  # [E, R]
        current_expert_repr = F.normalize(current_expert_repr, p=2, dim=-1)
        
        # Expert 간 similarity matrix 계산 (pairwise_expert_similarity용)
        expert_sim_matrix = torch.matmul(current_expert_repr, current_expert_repr.t())  # [E, E]
        
        flat_sim = similarity.view(-1, self.num_experts)  # [B*S, E]
        
        # [OSR] Cosine annealing orthogonal penalty
        # 초반: 강한 직교성 강제 (전문가 분리) → 후반: 약화하여 미세조정
        ortho_alpha = self.get_ortho_alpha(step_frac)
        
        # Compute orthogonal penalty: penalize tokens that prefer similar experts
        # expert_sim_matrix: [E, E] pairwise expert similarity (diagonal = 1)
        ortho_penalty_matrix = expert_sim_matrix.abs().to(x.dtype)  # Use absolute to penalize both +/- correlation
        ortho_penalty_diag = torch.eye(self.num_experts, device=x.device, dtype=x.dtype)
        ortho_penalty_matrix = ortho_penalty_matrix * (1 - ortho_penalty_diag)  # Zero diagonal
        
        # Compute per-token penalty: similarity to experts * expert-expert correlation
        ortho_penalty = torch.matmul(flat_sim.abs(), ortho_penalty_matrix)  # [B*S, E]
        
        # Routing cost: similarity + scaled orthogonal penalty
        # Lower cost = better choice, so we negate similarity and add penalty
        cost_matrix = (2.0 - 2.0 * flat_sim) + ortho_alpha * ortho_penalty
        
        # Sanitize cost matrix before Sinkhorn
        if torch.isnan(cost_matrix).any():
             cost_matrix = torch.nan_to_num(cost_matrix, nan=10.0) # High cost for NaNs

        
        # Expert similarity matrix 저장 (callback에서 사용)
        if self.training:
            with torch.no_grad():
                self.last_expert_sim_matrix = expert_sim_matrix.detach().to(x.dtype)  # Cast back to model dtype

        # [NEW] Bias Injection (Contextual + Load Balancing)
        # 1. GRU Contextual Bias (Existing)
        # "이 문맥에서는 이 전문가가 좋아"
        gru_bias = self.bias_proj(hn_next)  # [tokens, num_experts]
        context_bias = gru_bias.mean(dim=0) # [num_experts] (Token-wise bias를 global bias로 집계)

        # 2. [NEW] Load Balancing Bias (Explicit Feedback Loop)
        # "너 너무 많이 먹었어, 좀 빠져" (Aux Loss Free)
        # if self.training:
        #     # 현재 배치의 실제 부하 계산 (Sinkhorn 적용 전이든 후든 경향성은 비슷함)
        #     # 여기서는 이전 스텝의 routing_probs_full을 쓰거나, 
        #     # 혹은 현재 배치의 'routing_vec * expression_vec' (Similarity) 기반으로 추정 가능하지만,
        #     # 가장 정확한 건 Sinkhorn을 돌린 결과임.
        #     # 하지만 Sinkhorn 돌리기 전에 Bias를 넣어야 하므로, '이전 스텝의 분포'나 '누적 EMA'를 쓰는 게 정석.
        #     # 다만 DeepSeek-V3는 'current step'의 load를 쓴다고 명시되어 있지는 않음.
        #     # 여기서는 편의상 self.expert_load_ema (이미 계산됨) 사용
            
        #     # expert_load_ema는 이미 update됨 (forward 끝단에서).
        #     # 하지만 여기는 forward 중간임.
        #     # 따라서 '직전 배치'까지의 EMA를 기준점으로 삼음.
            
        #     current_load = self.expert_load_ema / (self.expert_load_ema.sum() + 1e-6)
        #     target_load = 1.0 / self.num_experts
            
        #     # Feedback: 많이 쓰였으면(+) -> Bias 감소(-) -> Cost 증가(+)
        #     # bias -= rate * sign(load - target)
        #     error = current_load - target_load
            
        #     # Update Buffer (In-place)
        #     # sign()을 쓰면 과도한 반응을 막고 일정하게 밀어냄 (Bang-Bang Control 느낌)
        #     # 값 자체를 쓰면 P-Control. 0.001 rate면 값 자체를 쓰는 게 부드러움.
        #     self.expert_bias -= self.bias_update_rate * error.sign() 
            
        #     # Drift 방지 (Mean Centering) - 전체 Bias의 평균은 0으로 유지
        #     self.expert_bias -= self.expert_bias.mean()
            
        # # 3. Apply Bias to Cost
        # # Cost = -Similarity + Repulsion - (ContextBias + ExpertBias)
        # # Bias가 높을수록 Cost가 낮아져서 선택 확률 증가
        # # ExpertBias는 많이 쓰이면 음수가 되므로, -(-Minus) = +Plus Cost => 선택 확률 감소 (Correct)
        # total_bias = context_bias + self.expert_bias
        # cost_matrix = cost_matrix - total_bias.unsqueeze(0)
        
        # 4. Sinkhorn (Pass-through Gradient) with Dynamic Epsilon
        # Dynamic Epsilon based on CV (ASR Logic)
        if self.training and self.adaptive_loss_scaling:
            # CV가 높으면 epsilon을 키워서(Temperature Up) 더 Flat하게 만듭니다.
            # Base: 0.1, CV=1.0 -> 0.1 * (1 + 1.0) = 0.2
            # 척력이 강하면 이미 Flat해지므로, epsilon 증가폭을 조절합니다.
            cv_val = self.cv_ema.item()
            epsilon_factor = 1.0 + min(cv_val, 5.0)  # Max factor 6x
            dynamic_epsilon = self.sinkhorn_epsilon * epsilon_factor
        else:
            dynamic_epsilon = self.sinkhorn_epsilon

        
        # Pure Math Sinkhorn - 학습 파라미터 0개
        Q_flat = self.sinkhorn_algorithm(
            cost_matrix,
            epsilon=dynamic_epsilon,
            iterations=self.sinkhorn_iterations
        )
        
        routing_probs_full = Q_flat.view(batch_size, seq_len, self.num_experts)
        
        # Store routing_probs_full for next batch's P-control feedback
        if self.training:
            with torch.no_grad():
                last_routing_probs_full = routing_probs_full.detach().mean(dim=(0, 1))
                target_load = 1.0 / self.num_experts
                
                # 2. 오차 계산 (양수면 과부하)
                load_error = last_routing_probs_full - target_load
                
                # 3. Bias 업데이트 (DeepSeek-V3 방식)
                # 많이 먹었으면(error > 0) -> Bias 감소(-) -> 다음 스텝 Cost 증가(+) -> 선택 확률 하락
                # sign()을 써서 급격한 변화 방지 (안정적 CV 감소)
                # CRITICAL: Moved to backward hook to prevent CheckpointError
                # self.expert_bias -= self.bias_update_rate * load_error.sign()
                pass
                
                # 4. Drift 방지 (Mean Centering)
                # self.expert_bias -= self.expert_bias.mean()
                pass
                
                # (선택) EMA 업데이트도 여기서 같이 수행
                # self.expert_load_ema = 0.9 * self.expert_load_ema + 0.1 * last_routing_probs_full
                pass

        # 5. Sparse Selection (Top-k) with optional Expert-Choice quota
        quota_cap = None
        quota_fallback_frac = None
        if self.training and self.expert_choice_routing:
            multiplier, selected_experts, quota_cap, fallback_mask = self._expert_choice_topk_selection(
                routing_probs_full=routing_probs_full,
                top_k=top_k,
                capacity_factor=self.expert_choice_capacity_factor,
            )
            quota_fallback_frac = float(fallback_mask.float().mean().item()) if fallback_mask.numel() > 0 else 0.0
        else:
            top_k_probs, selected_experts = torch.topk(routing_probs_full, min(int(top_k), self.num_experts), dim=-1)
            top_k_probs = top_k_probs.clamp_min(0.0)
            # CRITICAL FIX: Detach denominator to prevent invalid gradient flow
            multiplier = top_k_probs / (top_k_probs.sum(dim=-1, keepdim=True) + 1e-8).detach()

        # Expose last routing for monitoring callback (so it never falls back to argmax(routing_probs_full))
        if self.training:
            with torch.no_grad():
                tok = batch_size * seq_len
                k = selected_experts.size(-1)
                self.last_selected_experts = selected_experts.reshape(tok, k).detach()
                self.last_routing_weights = multiplier.reshape(tok, k).detach()
                self.last_num_experts = int(self.num_experts)
                # Quota stats (for debugging/plots)
                self.last_quota_cap = int(quota_cap) if quota_cap is not None else None
                self.last_quota_fallback_frac = float(quota_fallback_frac) if quota_fallback_frac is not None else 0.0
                self.last_expert_choice_enabled = bool(self.training and self.expert_choice_routing)
                # Also log the ingredients so cap is interpretable downstream
                self.last_quota_tokens = int(tok)
                self.last_quota_top_k = int(k)
                self.last_quota_num_experts = int(self.num_experts)
                self.last_quota_capacity_factor = float(self.expert_choice_capacity_factor)

        # 6. Loss 계산
        zero = torch.tensor(0.0, device=x.device, dtype=x.dtype, requires_grad=True)
        
        # [OSR] Speciality (Diversity) Loss
        # cost에 포함되지 않으므로, 아주 약한 residual loss로만 유지 (Optional)
        speciality_loss = 1e-6 * self.expression.orthogonal_loss()

        # Routing Uncertainty (direct computation)
        # Always compute or only if needed? User wants "directly calculated".
        # It's useful for monitoring.
        routing_uncertainty = self.compute_routing_uncertainty(routing_probs_full)

        # Ortho Loss (Expression Projector의 직교성)
        # 1. Projector 자체의 직교성
        ortho_loss = self.expression.orthogonal_loss()
        
        # 2. [NEW] Enhanced Soft Orthogonality (Representation-level)
        # Manifold constraint가 직접 작용하므로, 이제는 residual loss로만 둡니다.
        if self.training:
            warmup_progress = min(1.0, float(self.training_step) / max(self.so_warmup_steps, 1))
            so_lambda = 1e-6 * warmup_progress
            
            if so_lambda > 0:
                 ortho_loss = ortho_loss + so_lambda * speciality_loss
        
        # [Sharpening] Entropy Minimization: "한 놈만 패라" (확실한 전문가 선택)
        # routing_probs_full은 이미 Sinkhorn을 거친 확률입니다.
        if self.training:
            # Entropy 계산: 낮을수록 좋음 (Sharp = 확실한 선택)
            probs = routing_probs_full + 1e-8  # Numerical stability
            entropy = -(probs * torch.log(probs)).sum(dim=-1).mean()  # [B, S, E] -> scalar
            entropy_loss = entropy
        else:
            entropy_loss = zero

        # 8. Contrastive Loss
        if self.training:
            contrastive_loss = self.contrastive_loss(x, routing_probs_full)
        else:
            contrastive_loss = zero

        # 7. Update CV EMA (Thermostat Feedback Loop)
        if self.training:
            with torch.no_grad():
                # 배치의 Expert 사용량 계산 (Soft Probabilities 합)
                # routing_probs_full: [Batch, Seq, Experts]
                expert_load = routing_probs_full.sum(dim=(0, 1))  # [Experts]
                
                # Normalize
                total_load = expert_load.sum()
                if total_load > 0:
                    expert_dist = expert_load / total_load
                else:
                    expert_dist = torch.ones_like(expert_load) / self.num_experts
                
                # CV Calculation: std / mean
                # mean is 1/E
                # std = sqrt(mean((x - mean)^2))
                # CV = sqrt(E * sum((p - 1/E)^2))  (simplification for prob distribution)
                # Actual definition: sigma / mu. mu = 1/E.
                # sigma = sqrt(mean(p^2) - mean(p)^2) = sqrt(mean(p^2) - (1/E)^2)
                # CV = sqrt(mean(p^2) - (1/E)^2) / (1/E)
                #    = sqrt(E * sum(p^2) - 1)
                
                # Using standard definition on the counts directly might be more numerically stable if counts are large?
                # Using probabilities is fine.
                
                # Variance of probabilities p_i
                var_p = expert_dist.var(unbiased=False)
                mean_p = expert_dist.mean()
                current_cv = (var_p.sqrt() / (mean_p + 1e-6))
                
                # Update EMA
                # CRITICAL: Moved to backward hook to prevent CheckpointError
                pass
                
                # Update Expert Load EMA for Bias Predictor
                # self.expert_load_ema.mul_(self.ema_alpha).add_(expert_dist * (1.0 - self.ema_alpha))
                pass
                
                # Save losses/metrics for callback (training only)
                self.last_speciality_loss = speciality_loss.detach()
                self.last_ortho_loss = ortho_loss.detach()
                self.last_cosine_similarities = domain_orthogonality.detach()
                self.last_entropy_loss = entropy_loss.detach()
                self.last_routing_uncertainty = routing_uncertainty.detach() if torch.is_tensor(routing_uncertainty) else routing_uncertainty
                self.last_contrastive_loss = contrastive_loss.detach() if torch.is_tensor(contrastive_loss) else contrastive_loss
                
                # Save routing decisions for MoE monitoring callback
                self.last_selected_experts = selected_experts.detach()
                self.last_routing_weights = multiplier.detach()  # top-k routing weights
                self.last_routing_probs_full = routing_probs_full.detach() if routing_probs_full is not None else None
                self.last_num_experts = self.num_experts
        else:
            # Validation/eval mode: also save metrics for callback (detached)
            with torch.no_grad():
                self.last_speciality_loss = speciality_loss.detach() if torch.is_tensor(speciality_loss) else speciality_loss
                self.last_ortho_loss = ortho_loss.detach() if torch.is_tensor(ortho_loss) else ortho_loss
                self.last_cosine_similarities = domain_orthogonality.detach() if torch.is_tensor(domain_orthogonality) else domain_orthogonality
                self.last_entropy_loss = entropy_loss.detach() if torch.is_tensor(entropy_loss) else entropy_loss
                self.last_routing_uncertainty = routing_uncertainty.detach() if torch.is_tensor(routing_uncertainty) else routing_uncertainty
                self.last_contrastive_loss = contrastive_loss.detach() if torch.is_tensor(contrastive_loss) else contrastive_loss
                
                # Save routing decisions for MoE monitoring callback
                self.last_selected_experts = selected_experts.detach()
                self.last_routing_weights = multiplier.detach()  # top-k routing weights
                self.last_routing_probs_full = routing_probs_full.detach() if routing_probs_full is not None else None
                self.last_num_experts = self.num_experts
                
                # Quota stats (for debugging/plots)
                self.last_quota_cap = int(quota_cap) if quota_cap is not None else None
                self.last_quota_fallback_frac = float(quota_fallback_frac) if quota_fallback_frac is not None else 0.0
                self.last_expert_choice_enabled = bool(self.expert_choice_routing)
                self.last_quota_tokens = int(tokens)  # tokens is defined at the beginning of forward()
                self.last_quota_top_k = int(selected_experts.size(-1))
                self.last_quota_num_experts = int(self.num_experts)
                self.last_quota_capacity_factor = float(self.expert_choice_capacity_factor)
                
                # Expert similarity matrix 저장 (callback에서 사용)
                self.last_expert_sim_matrix = expert_sim_matrix.detach() if torch.is_tensor(expert_sim_matrix) else expert_sim_matrix

        # CRITICAL FIX: Ensure all secondary outputs are scalars to prevent CheckpointError
        # Position 4 in flattened layer outputs is usually speciality_loss or cosine_similarities
        def _to_scalar(t, name):
            if t is None:
                return torch.tensor(0.0, device=x.device, dtype=x.dtype, requires_grad=True)
            if not torch.is_tensor(t):
                return torch.tensor(float(t), device=x.device, dtype=x.dtype, requires_grad=True)
            if t.numel() == 0:
                return torch.tensor(0.0, device=x.device, dtype=x.dtype, requires_grad=True)
            # Force scalar 0D tensor
            return t.flatten().mean()

        if self.training:
            # During training, we force ALL secondary outputs to be dummy scalars
            # to satisfy gradient checkpointing metadata checks [B, S, E] vs [0].
            # Only keep losses that need gradients.
            zero_scalar = torch.tensor(0.0, device=x.device, dtype=x.dtype, requires_grad=True)
        return (
            multiplier,
            selected_experts,
            hn_next,
            routing_probs_full,
                _to_scalar(speciality_loss, "spec"),
                zero_scalar, # cosine_similarities (Not needed for backprop)
                _to_scalar(ortho_loss, "ortho"),
                _to_scalar(entropy_loss, "ent"),
                zero_scalar, # routing_uncertainty (Not needed for backprop)
                _to_scalar(contrastive_loss, "cont"),
            )
        
        return (
            multiplier,
            selected_experts,
            hn_next,
            routing_probs_full,
            _to_scalar(speciality_loss, "spec"),
            _to_scalar(domain_orthogonality, "cos"),
            _to_scalar(ortho_loss, "ortho"),
            _to_scalar(entropy_loss, "ent"),
            _to_scalar(routing_uncertainty, "unc"),
            _to_scalar(contrastive_loss, "cont"),
        )


class SwitchRouterAdapter(nn.Module):
    """
    Adapter to make SwitchRouter compatible with SPECTRAMoE's expected 15-tuple return format.
    
    Wraps SwitchRouter and converts its output to match SPECTRARouter's signature.
    """
    def __init__(self, config: SPECTRATextConfig, top_k: int = 1):
        super().__init__()
        # Lazy import to avoid circular dependency
        from .standard_moe_upcycle import SwitchRouter
        
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.top_k = top_k
        self.router_dim = getattr(config, "router_dim", 128)
        
        # Create SwitchRouter instance
        self.switch_router = SwitchRouter(
            hidden_size=self.hidden_size,
            num_experts=self.num_experts,
            load_balance_loss_coef=getattr(config, "balance_loss_coef", 0.01),
        )
        
        # Register buffer for hidden state (not used but needed for compatibility)
        self.register_buffer("dummy_hn", torch.zeros(1, self.num_experts * self.router_dim))
        
        # Mark as router for compatibility with monitoring callbacks
        setattr(self, "_is_spectra_router", True)
    
    def forward(self, hidden_states: torch.Tensor, global_routing_logits: Optional[torch.Tensor] = None, top_k: Optional[int] = None, jitter_eps: float = 0.01, layer_idx: int = 0):
        """
        Forward pass compatible with SPECTRARouter signature.
        
        Args:
            hidden_states: [batch, seq, hidden_size]
            global_routing_logits: Ignored (for compatibility)
            top_k: Number of experts per token (default: self.top_k)
            jitter_eps: Jitter noise (passed to SwitchRouter)
            layer_idx: Layer index for deterministic RNG
        
        Returns:
            15-tuple matching SPECTRARouter output format
        """
        if top_k is None:
            top_k = self.top_k
        
        # Create deterministic generator for this layer/step to ensure recompute consistency
        generator = None
        if self.training and jitter_eps > 0:
            step = getattr(self, "_step", 0)
            # deterministic seed: step * 1000 + layer
            seed = (int(step) * 1000) + int(layer_idx)
            generator = torch.Generator(device=hidden_states.device).manual_seed(seed)
        
        # Call SwitchRouter
        # SwitchRouter.forward returns: (multiplier, selected_experts, router_logits, hn, load_balance_loss, router_scores, expression_loss)
        switch_output = self.switch_router.forward(
            hidden_states,
            hn=None,
            top_k=top_k,
            jitter_eps=jitter_eps,
            training=self.training,
            generator=generator
        )
        
        multiplier, selected_experts, router_logits, hn, load_balance_loss, router_scores, _ = switch_output
        
        # Reshape to match expected shapes
        # multiplier: [batch*seq, top_k] -> keep as is
        # selected_experts: [batch*seq, top_k] -> keep as is
        
        # Create dummy hn_next (SwitchRouter doesn't use sequential state)
        # Expected shape: [batch*seq, num_experts * router_dim]
        batch_size, seq_len = hidden_states.shape[:2]
        batch_seq_len = batch_size * seq_len
        hn_next = torch.zeros(
            batch_seq_len, 
            self.num_experts * self.router_dim,
            device=hidden_states.device,
            dtype=hidden_states.dtype
        )
        
        # Create zero tensors for unused losses/metrics
        # SwitchRouter does not have Expression Projector or Speciality mechanism,
        # so these specific losses are inherently zero.
        zero = torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
        
        # router_scores: [batch, seq, num_experts] -> use as routing_probs_full
        routing_probs_full = router_scores  # Already in correct shape
        
        # CRITICAL FIX: Ensure all secondary outputs are scalars to prevent CheckpointError
        def _to_scalar(t):
            if t is None:
                return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
            if not torch.is_tensor(t):
                return torch.tensor(float(t), device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
            if t.numel() == 0:
                return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
            if t.dim() > 0:
                return t.mean()
            return t

        # Return 10-tuple matching SPECTRARouter format
        return (
            multiplier,                    # routing_weights
            selected_experts,              # selected_experts
            hn_next,                       # hn_next (dummy)
            routing_probs_full,            # routing_probs_full
            _to_scalar(load_balance_loss),  # speciality_loss alias
            _to_scalar(zero),               # domain_orthogonality (N/A)
            _to_scalar(zero),               # ortho_loss (N/A)
            _to_scalar(zero),               # entropy_loss (N/A)
            _to_scalar(zero),               # routing_uncertainty (N/A)
            _to_scalar(zero),               # contrastive_loss (N/A)
        )


# SPECTRA Exoskeleton Architecture: 
# All MoE and Routing logic is encapsulated in SPECTRAMoE, which acts as a wrapper 
# around any base layer (Dense or MoE).
import torch.nn.functional as F
try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False

iterations = 0
class SPECTRAMoE(GradientCheckpointingLayer):
    """
    SPECTRAMoE: The "Exoskeleton" wrapper.
    It wraps a base module (Attention or MLP) and transforms it into a routed MoE layer.
    
    CRITICAL: Inherits from GradientCheckpointingLayer to disable checkpointing.
    Expert choice routing can produce different tensor shapes in forward vs backward,
    causing "Recomputed values have different metadata" errors with use_reentrant=False.
    """

    def __init__(self, config, experts, router=None, shared_experts=None, return_tuple=True, upcycle=False, **kwargs):
        super().__init__()
        # Normalize config to handle SPECTRAConfig wrapping text_config
        if hasattr(config, "text_config") and config.text_config is not None:
            self.config = config.text_config
        else:
            self.config = config
        config = self.config
        
        self.return_tuple = return_tuple
        
        # Experts: Managed as ModuleList
        if isinstance(experts, (nn.ModuleList, list)):
            self.experts = nn.ModuleList(experts) if isinstance(experts, list) else experts
            self.num_experts = len(self.experts)
        elif isinstance(experts, nn.Module):
            if upcycle:
                # Upcycling: Clone one-by-one to save RAM
                self.num_experts = getattr(config, "n_routed_experts", getattr(config, "num_local_experts", 8))
                self.experts = nn.ModuleList([experts]) # Expert 0
                for i in range(1, self.num_experts):
                    self.experts.append(copy.deepcopy(experts))
            else:
                self.experts = nn.ModuleList([experts])
                # Detect if the expert module is monolithic (e.g. Qwen3/DeepSeek group modules)
                # These modules handle multiple experts internally and expect indices/weights for all experts.
                self.num_experts = getattr(experts, "num_experts", getattr(experts, "n_experts", 1))
                if self.num_experts == 1:
                    # Fallback to config if not explicitly stated on the module
                    self.num_experts = getattr(config, "n_routed_experts", getattr(config, "num_local_experts", 1))
                
                # Check for monolithic forward signature: (x, weights, indices, ...)
                # Monolithic check: one module handling multiple experts
                self.is_monolithic = (self.num_experts > 1 and len(self.experts) == 1)
                
                if not self.is_monolithic:
                    # Double check via signature if num_experts check was inconclusive
                    import inspect
                    try:
                        sig = inspect.signature(experts.forward)
                        if len(sig.parameters) >= 3:
                            self.is_monolithic = True
                    except:
                        pass
        else:
            raise ValueError(f"Experts must be nn.Module or nn.ModuleList, got {type(experts)}")
            
        self.shared_experts = shared_experts
        self.top_k = min(getattr(config, "num_experts_per_tok", 2), self.num_experts)
        
        # Router: Shared global router. 
        # CRITICAL: Single global router must be provided - no per-module routers allowed
        if router is None:
            raise ValueError(
                "SPECTRAMoE requires a global_router to be provided. "
                "All MoE layers must share the same global router instance from SPECTRATextModel.global_router"
            )
        # CRITICAL: We don't register it as a submodule to avoid ZeRO-3 multiple-registration errors.
        self._router = [router] # Reference in a list to hide from nn.Module
        
        self.router_jitter_noise = getattr(config, 'router_jitter_noise', 0.01)
        self.input_jitter_noise = getattr(config, 'input_jitter_noise', 0.0)
        
        # Adaptive filter parameters and EMA tracking
        hidden_size = getattr(config, "hidden_size", None)
        if hidden_size is None and hasattr(config, "text_config"):
            hidden_size = getattr(config.text_config, "hidden_size", None)
            
        self.register_buffer("expert_specialization_ema", torch.zeros(self.num_experts, hidden_size), persistent=True)
        self.routing_temperature = nn.Parameter(torch.ones(1))
        
        self.freeze_shared_experts = getattr(config, 'freeze_shared_experts', True)
        if self.freeze_shared_experts and self.shared_experts is not None:
            self._freeze_shared_experts()
            
        # CRITICAL FIX for CheckpointError: 
        # Register backward hook to handle state updates (EMA) avoiding in-place ops in forward
        self.register_full_backward_hook(self._backward_hook_update)

    def _backward_hook_update(self, module, grad_input, grad_output):
        """
        Perform state updates (Expert Specialization EMA) during backward pass.
        """
        if not self.training or not hasattr(self, 'last_expert_means'):
            return

        with torch.no_grad():
            for expert_idx, current_mean in self.last_expert_means.items():
                ema_alpha = getattr(self.config, "ema_alpha", 0.99)
                self.expert_specialization_ema[expert_idx].mul_(ema_alpha).add_(current_mean, alpha=1.0 - ema_alpha)
        
        # Clear cache
        self.last_expert_means = {}

    @property
    def router(self):
        return self._router[0]

    def _freeze_shared_experts(self):
        if self.shared_experts is not None:
            for param in self.shared_experts.parameters():
                param.requires_grad = False

    def _gradient_checkpointing_func(self, user_function, *args, **kwargs):
        """
        Override to disable gradient checkpointing for SPECTRAMoE.
        Expert choice routing can produce different tensor shapes in forward vs backward,
        causing "Recomputed values have different metadata" errors with use_reentrant=False.
        """
        # CRITICAL: Bypass checkpointing completely - Expert choice routing produces
        # different tensor shapes in forward vs backward, causing shape mismatch errors.
        # Also, checkpointing can break requires_grad chain with ZeRO-3.
        return user_function(*args, **kwargs)

    @torch._dynamo.disable
    def forward(self, hidden_states: torch.Tensor, hn_state: torch.Tensor = None, layer_idx: int = 0) -> Tuple[torch.Tensor, Tuple]:
        original_shape = hidden_states.shape
        if len(original_shape) == 3:
            batch_size, seq_len, hidden_dim = original_shape
        else:
            # Handle 2D flattened inputs [N, H]
            batch_size = 1
            seq_len = original_shape[0]
            hidden_dim = original_shape[1]
        
        if self.training and self.input_jitter_noise > 0:
            # Deterministic jitter for input
            start_step = getattr(self.router, "_step", 0) # Access step from shared router
            seed = (int(start_step) * 1000) + int(layer_idx) + 1 # Offset +1 to differ from router seed
            generator = torch.Generator(device=hidden_states.device).manual_seed(seed)
            jitter = torch.empty_like(hidden_states).uniform_(1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise, generator=generator)
            hidden_states = hidden_states * jitter
        
        # 1. Routing
        # CRITICAL FIX for ZeRO-3 + Gradient Checkpointing:
        # Router forward is called directly. The router is shared globally across all layers,
        # and is registered only once at SPECTRATextModel level (not as submodule in SPECTRAMoE).
        # This prevents ZeRO-3 from partitioning router parameters multiple times.
        # The router parameters are stored in _router list (not registered as submodule) to avoid
        # multiple registrations that cause "TBackward0 returned invalid gradient" errors.
        # 
        # CRITICAL FIX for Qwen + Expert Choice Routing + use_reentrant=False:
        # Router must be excluded from gradient checkpointing because expert choice routing
        # can produce different tensor shapes in forward vs backward pass (due to dynamic
        # expert selection based on quota). This causes "Recomputed values have different
        # metadata" errors when use_reentrant=False.
        # 
        # Solution: Router is called directly. Since SPECTRAMoE._gradient_checkpointing_func
        # bypasses checkpointing and SPECTRADecoderLayer._gradient_checkpointing_func also
        # bypasses checkpointing, router should never be recomputed. However, if issues
        # persist, the problem is in _expert_choice_topk_selection which uses conditional
        # logic that may evaluate differently in forward vs backward.
        # 
        # To ensure router is never checkpointed, we rely on:
        # 1. SPECTRAMoE._gradient_checkpointing_func bypassing checkpointing
        # 2. SPECTRADecoderLayer._gradient_checkpointing_func bypassing checkpointing  
        # 3. SPECTRATextModel._gradient_checkpointing_func detecting SPECTRADecoderLayer
        # 4. SPECTRAMoE and SPECTRADecoderLayer in _no_split_modules

        try:
            from deepspeed.runtime.zero.partition_parameters import GatheredParameters
            is_zero3 = False
            if is_deepspeed_initialized():
                # Check if stage 3 is active
                ds_config = getattr(self.config, "deepspeed_config", None)
                if ds_config:
                    if ds_config.get("zero_optimization", {}).get("stage") == 3:
                        is_zero3 = True

            with GatheredParameters(self.router.parameters(), enabled=is_zero3):
                router_output = self.router(
                    hidden_states, 
                    hn_state,
                    top_k=self.top_k,
                    jitter_eps=self.router_jitter_noise,
                    layer_idx=layer_idx
                )
        except ImportError:
            router_output = self.router(
                hidden_states, 
                hn_state,
                top_k=self.top_k,
                jitter_eps=self.router_jitter_noise,
                layer_idx=layer_idx
            )
        
        # Initialize storage for expert means for backward hook
        if self.training:
            self.last_expert_means = {}
        
        # Unpack: multiplier, selected_experts, hn_next, routing_probs_full, speciality_loss, 
        # cosine_similarities, ortho_loss, entropy_loss, routing_uncertainty, contrastive_loss
        routing_weights, selected_experts, hn_next, routing_probs_full, speciality_loss, \
        cosine_similarities, router_ortho_loss, entropy_loss, routing_uncertainty, contrastive_loss = router_output

        # CRITICAL FIX: Ensure routing_probs_full and hn_next always have consistent shapes
        # to prevent "Recomputed values have different metadata" errors in gradient checkpointing
        batch_seq_len = batch_size * seq_len
        
        # Ensure routing_probs_full is never None and has consistent shape
        if routing_probs_full is None:
            routing_probs_full = torch.zeros(
                batch_size, seq_len, self.num_experts,
                device=hidden_states.device, dtype=hidden_states.dtype
            )
        elif routing_probs_full.shape != (batch_size, seq_len, self.num_experts):
            # Reshape to expected shape if needed
            if routing_probs_full.numel() == 0:
                routing_probs_full = torch.zeros(
                    batch_size, seq_len, self.num_experts,
                    device=hidden_states.device, dtype=hidden_states.dtype
                )
            else:
                routing_probs_full = routing_probs_full.view(batch_size, seq_len, self.num_experts)
        
        # Ensure hn_next is never None and has consistent shape
        if hn_next is None:
            hn_next = torch.zeros(
                batch_seq_len, self.num_experts * getattr(self, 'router_dim', 256),
                device=hidden_states.device, dtype=hidden_states.dtype
            )
        elif hn_next.numel() == 0:
            # If empty tensor, create zero tensor with expected shape
            expected_hn_dim = self.num_experts * getattr(self, 'router_dim', 256)
            hn_next = torch.zeros(
                batch_seq_len, expected_hn_dim,
                device=hidden_states.device, dtype=hidden_states.dtype
            )
        elif len(hn_next.shape) == 1:
            # If 1D, reshape to 2D
            expected_hn_dim = self.num_experts * getattr(self, 'router_dim', 256)
            if hn_next.shape[0] == batch_seq_len * expected_hn_dim:
                hn_next = hn_next.view(batch_seq_len, expected_hn_dim)
            else:
                # Pad or truncate to expected shape
                expected_size = batch_seq_len * expected_hn_dim
                if hn_next.shape[0] < expected_size:
                    padding = torch.zeros(
                        expected_size - hn_next.shape[0],
                        device=hidden_states.device, dtype=hidden_states.dtype
                    )
                    hn_next = torch.cat([hn_next, padding])
                else:
                    hn_next = hn_next[:expected_size]
                hn_next = hn_next.view(batch_seq_len, expected_hn_dim)
        elif hn_next.shape[0] != batch_seq_len:
            # Reshape to expected batch_seq_len
            expected_hn_dim = hn_next.shape[-1] if len(hn_next.shape) > 1 else self.num_experts * getattr(self, 'router_dim', 256)
            if hn_next.numel() == batch_seq_len * expected_hn_dim:
                hn_next = hn_next.view(batch_seq_len, expected_hn_dim)
            else:
                # Create zero tensor with expected shape
                hn_next = torch.zeros(
                    batch_seq_len, expected_hn_dim,
                    device=hidden_states.device, dtype=hidden_states.dtype
                )

        # 2. Expert Dispatch
        routing_weights_flat = routing_weights.view(batch_size * seq_len, -1)
        selected_experts_flat = selected_experts.view(batch_size * seq_len, -1)
        hidden_states_flat = hidden_states.view(batch_size * seq_len, hidden_dim)
        
        final_hidden_states = torch.zeros_like(hidden_states_flat)
        
        # Uncertainty-based broadcasting (Exploration)
        if self.training and getattr(self, "enable_uncertainty_broadcast", False) and routing_uncertainty is not None:
            uncertainty_flat = routing_uncertainty.view(-1)
            uncertain_mask = uncertainty_flat > getattr(self, "uncertainty_threshold", 0.7)
            expert_mask = F.one_hot(selected_experts_flat, num_classes=self.num_experts).bool()
            if uncertain_mask.any():
                uncertain_token_mask = uncertain_mask.view(-1, 1).expand(-1, self.top_k)
                expert_mask[uncertain_token_mask] = True
            else:
                expert_mask = F.one_hot(selected_experts_flat, num_classes=self.num_experts).bool()
        else:
            expert_mask = F.one_hot(selected_experts_flat, num_classes=self.num_experts).bool()
            
        expert_mask = expert_mask.permute(2, 0, 1) # [E, tokens, top_k]
        
        # 2a. Monolithic Dispatch (e.g. Qwen3-VL-MoE)
        # Qwen experts require (hidden_states, routing_weights, router_indices) signature
        if getattr(self, "is_monolithic", False):
            # Pass everything at once if the module handles internal routing
            # [CRITICAL] Fix for CUDA device-side assert: 
            # Monolithic blocks often expect full-width weights [N, num_experts]
            # to accommodate indexing by global expert IDs (seen in Qwen3-VL-MoE traceback).
            full_routing_weights = torch.zeros(
                batch_size * seq_len, self.num_experts, 
                device=hidden_states.device, dtype=hidden_states.dtype
            )
            # Scatter SPECTRA's sparse top-k weights into the full-width tensor
            full_routing_weights.scatter_(1, selected_experts_flat, routing_weights_flat)
            
            # Call monolithic expert block directly
            # NOTE: GatheredParameters removed - causes rank mismatch due to different param lists per rank
            final_hidden_states = self.experts[0](hidden_states_flat, full_routing_weights, selected_experts_flat)
            
            # If monolithic dispatch failed, fall back to standard dispatch
            if final_hidden_states is None:
                logger.warning("⚠️ Monolithic dispatch failed, falling back to standard dispatch")
                # Fallback to standard loop-based dispatch
                final_hidden_states = torch.zeros_like(hidden_states_flat)
                for expert_idx in range(self.num_experts):
                    expert_module = self.experts[expert_idx]
                    token_idx, topk_idx = torch.where(expert_mask[expert_idx])

                    # CRITICAL FIX: Always call expert_module to ensure consistent tensor shapes
                    # in forward vs backward pass with gradient checkpointing
                    has_tokens = token_idx.numel() > 0
                    if has_tokens:
                        current_state = hidden_states_flat[token_idx]
                        expert_out = expert_module(current_state)
                        weighted_out = expert_out * routing_weights_flat[token_idx, topk_idx].unsqueeze(-1)
                        final_hidden_states.index_add_(0, token_idx, weighted_out.to(final_hidden_states.dtype))
                    else:
                        # CRITICAL FIX for NCCL desync in fallback path
                        dummy = torch.zeros(1, hidden_dim, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
                        dummy_out = expert_module(dummy)
                        zero_contribution = dummy_out.sum() * 0.0
                        final_hidden_states[0, 0] = final_hidden_states[0, 0] + zero_contribution
        else:
            # 2b. Standard Loop-based Dispatch (Individual Experts)
            # CRITICAL for DeepSpeed ZeRO-3: Deterministic submodule visit order
            # CRITICAL FIX for gradient checkpointing: Remove conditional logic that can
            # evaluate differently in forward vs backward pass. Always call expert_module
            # to ensure consistent tensor shapes.
            for expert_idx in range(self.num_experts):
                expert_module = self.experts[expert_idx]
                token_idx, topk_idx = torch.where(expert_mask[expert_idx])
                
                # CRITICAL: Use tensor operations instead of .numel() > 0 check
                # to ensure deterministic behavior in forward vs backward pass
                has_tokens = token_idx.numel() > 0
                # Always call expert_module to ensure consistent tensor shapes
                # If no tokens, use dummy input but don't add to final_hidden_states
                if has_tokens:
                    current_state = hidden_states_flat[token_idx]
                    expert_out = expert_module(current_state)
                    # Apply routing weights
                    weighted_out = expert_out * routing_weights_flat[token_idx, topk_idx].unsqueeze(-1)
                    final_hidden_states.index_add_(0, token_idx, weighted_out.to(final_hidden_states.dtype))
                    
                    # Update Specialization EMA
                    # Update Specialization EMA
                    if self.training:
                        # Save mean for backward hook update
                        current_mean = current_state.mean(dim=0).detach()
                        self.last_expert_means[expert_idx] = current_mean
                        # In-place update moved to _backward_hook_update
                else:
                    # CRITICAL FIX for NCCL desync: Unused experts must also generate gradients!
                    # If expert gets no tokens, it won't participate in gradient computation,
                    # causing other ranks to wait indefinitely during all_gather.
                    # Solution: Use dummy input and add zero-weighted output to maintain gradient chain.
                    dummy = torch.zeros(1, hidden_dim, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
                    dummy_out = expert_module(dummy)
                    # Add zero-weighted output to maintain gradient chain (0 * output = 0, but gradient flows)
                    zero_contribution = dummy_out.sum() * 0.0
                    # Add to first position with zero weight to maintain gradient chain
                    final_hidden_states[0, 0] = final_hidden_states[0, 0] + zero_contribution

        final_hidden_states = final_hidden_states.view(batch_size, seq_len, hidden_dim)
        
        # 3. Shared Experts Integration
        if self.shared_experts is not None:
            if isinstance(self.shared_experts, nn.ModuleList):
                shared_out = sum(expert(hidden_states) for expert in self.shared_experts) / len(self.shared_experts)
            else:
                shared_out = self.shared_experts(hidden_states)
            
            # Upcycling trick: maintain relative norm
            pretrained_norm = torch.linalg.vector_norm(shared_out, dim=-1, keepdim=True)
            combined = final_hidden_states + shared_out
            combined_norm = torch.linalg.vector_norm(combined, dim=-1, keepdim=True)
            scale = pretrained_norm / combined_norm.clamp_min(torch.finfo(combined.dtype).tiny)
            final_hidden_states = combined * scale
            
        # 4. Connecting Expert Weights to Orthogonal Loss
        # This addresses the "disconnected experts" concern by ensuring actual weights are penalized.
        if self.training:
            # We add a small portion of the raw weight orthogonal loss to the router's ortho signal
            try:
                # Monolithic blocks usually have their own internal orthogonal logic or sharded weights
                # For safety and memory, we skip raw weight ortho for monolithic group modules
                if not getattr(self, "is_monolithic", False):
                    expert_params = []
                    for expert in self.experts:
                        # Collect weights (usually gate/up/down projections)
                        w = [p.view(-1) for p in expert.parameters() if p.requires_grad and p.dim() > 1]
                        if w:
                            expert_params.append(torch.cat(w))
                    
                    if len(expert_params) == self.num_experts:
                        weight_ortho_loss = calculate_ortho_loss_for_experts(expert_params)
                        router_ortho_loss = router_ortho_loss + weight_ortho_loss
            except Exception as e:
                # Fallback to avoid training interruption
                pass

        if self.training:
            # Re-force scalarization in SPECTRAMoE just in case
            def force_scalar_train(t):
                return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
            
            # Keep losses but force them to scalars
            def force_scalar_loss(t):
                if t is None or not torch.is_tensor(t):
                    return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
                if t.numel() == 0:
                    return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
                return t.flatten().mean()

            speciality_loss = force_scalar_loss(speciality_loss)
            router_ortho_loss = force_scalar_loss(router_ortho_loss)
            entropy_loss = force_scalar_loss(entropy_loss)
            contrastive_loss = force_scalar_loss(contrastive_loss)
            
            # Return dummies for non-loss metrics during training
            cosine_similarities = force_scalar_train(cosine_similarities)
            routing_uncertainty = force_scalar_train(routing_uncertainty)
        else:
            def force_scalar_val(t):
                if t is None or not torch.is_tensor(t):
                    return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
                if t.numel() == 0:
                    return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
                return t.flatten().mean()

            speciality_loss = force_scalar_val(speciality_loss)
            cosine_similarities = force_scalar_val(cosine_similarities)
            router_ortho_loss = force_scalar_val(router_ortho_loss)
            entropy_loss = force_scalar_val(entropy_loss)
            routing_uncertainty = force_scalar_val(routing_uncertainty)
            contrastive_loss = force_scalar_val(contrastive_loss)
            
        router_info = (
            routing_probs_full, hn_next, speciality_loss, cosine_similarities, 
            router_ortho_loss, entropy_loss, routing_uncertainty, contrastive_loss
        )
        
        # Store routing info for MoE monitoring callback (SPECTRAMoE level)
        # This allows callback to access routing info even if router is not directly accessible
        self.last_selected_experts = selected_experts.detach()
        self.last_routing_weights = routing_weights.detach()
        self.last_routing_probs_full = routing_probs_full.detach() if routing_probs_full is not None else None
        self.last_num_experts = self.num_experts
        
        # Also store router metrics for callback (if router has them)
        router = self.router
        if router is not None:
            if hasattr(router, 'last_speciality_loss'):
                self.last_speciality_loss = router.last_speciality_loss
            if hasattr(router, 'last_ortho_loss'):
                self.last_ortho_loss = router.last_ortho_loss
            if hasattr(router, 'last_cosine_similarities'):
                self.last_cosine_similarities = router.last_cosine_similarities
            if hasattr(router, 'last_entropy_loss'):
                self.last_entropy_loss = router.last_entropy_loss
            if hasattr(router, 'last_routing_uncertainty'):
                self.last_routing_uncertainty = router.last_routing_uncertainty
            if hasattr(router, 'last_contrastive_loss'):
                self.last_contrastive_loss = router.last_contrastive_loss
            if hasattr(router, 'last_expert_sim_matrix'):
                self.last_expert_sim_matrix = router.last_expert_sim_matrix
            # Quota stats
            for attr in ['last_quota_cap', 'last_quota_fallback_frac', 'last_expert_choice_enabled',
                        'last_quota_tokens', 'last_quota_top_k', 'last_quota_num_experts', 'last_quota_capacity_factor']:
                if hasattr(router, attr):
                    setattr(self, attr, getattr(router, attr))
        
        # Reshape back to original input shape
        if len(original_shape) == 2:
            final_hidden_states = final_hidden_states.view(*original_shape)
        
        if self.return_tuple:
            return final_hidden_states, router_info
        return final_hidden_states

    def compute_pairwise_expert_similarity(self):
        if self.expert_specialization_ema is not None:
            normalized_specs = F.normalize(self.expert_specialization_ema, dim=-1)
            sim_matrix = torch.matmul(normalized_specs, normalized_specs.t())
            mask = torch.eye(self.num_experts, device=sim_matrix.device).bool()
            off_diagonal = sim_matrix[~mask]
            return off_diagonal.mean()
        return torch.tensor(0.0)


class SPECTRARMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6, **kwargs):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.zeros(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        # Llama does x.to(float16) * w whilst SPECTRA is (x * w).to(float16)
        # See https://github.com/huggingface/transformers/pull/29402
        output = output * (1.0 + self.weight.float())
        return output.type_as(x)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.eps}"


class SPECTRARotaryEmbedding(nn.Module):
    def __init__(self, config: SPECTRATextConfig, device=None, **kwargs):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)



# 
# Integrated Exoskeleton: SPECTRA classes (Attention/MLP) are removed to keep the core lean.
# The design relies on wrapping existing model blocks (upcycling/swapping) rather than 
# hardcoding architecture-specific logic here.
# 


# SPECTRAAttention and SPECTRAMLP are deleted as they are redundant in an exoskeleton architecture.


class MoEFactory:
    """
    Factory for instantiating the correct MoE architecture based on config.
    CRITICAL: All MoE layers must use the same global_router instance from SPECTRATextModel.
    """
    @staticmethod
    def create_moe(config: SPECTRATextConfig, experts: Union[nn.Module, nn.ModuleList], global_router: Optional[nn.Module] = None):
        impl = getattr(config, "moe_implementation", "spectra")
        
        if impl == "spectra":
            # CRITICAL: global_router is required - single router shared across all layers
            if global_router is None:
                raise ValueError(
                    "MoEFactory.create_moe requires global_router to be provided. "
                    "All MoE layers must share the same global router instance from SPECTRATextModel.global_router"
                )
            return SPECTRAMoE(config, experts=experts, router=global_router)
        elif impl == "switch":
            # SwitchRouterAdapter might need experts too if we want it to be generic
            return SwitchRouterAdapter(config, experts=experts) if hasattr(SwitchRouterAdapter, '__init__') and 'experts' in inspect.signature(SwitchRouterAdapter.__init__).parameters else SwitchRouterAdapter(config)
        else:
            raise ValueError(f"Unknown moe_implementation: {impl}")


class SPECTRADecoderLayer(GradientCheckpointingLayer):
    """
    SPECTRA Decoder Layer: A generic shell that uses integrated exoskeleton wrapping.
    For scratch builds, it initializes standard attention and MLP modules.
    For upcycling, these are replaced by the base model's modules wrapped in SPECTRAMoE.
    """
    def __init__(self, config: SPECTRATextConfig, layer_idx: int, global_router: nn.Module, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx
        
        # Store global_router for use in MoE creation
        # CRITICAL: Single global router shared across all layers
        self.global_router = global_router
        
        # Skeletal placeholders: Functional modules should be injected via upcycling or swapping.
        # Direct scratch instantiation without upcycling will result in non-functional layers.
        self.self_attn = nn.Identity()
        self.mlp = nn.Identity()
        
        self.input_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.pre_feedforward_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_feedforward_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        
        self.is_dense_replacement = layer_idx >= config.first_k_dense_replace
        self.attention_type = "full_attention"
    
    def _gradient_checkpointing_func(self, user_function, *args, **kwargs):
        """
        Override to completely disable gradient checkpointing for SPECTRADecoderLayer.
        
        CRITICAL: SPECTRADecoderLayer contains SPECTRAMoE which uses expert choice routing.
        Expert choice routing can produce different tensor shapes in forward vs backward pass,
        causing "Recomputed values have different metadata" errors with use_reentrant=False.
        
        Solution: Completely disable checkpointing for SPECTRADecoderLayer to avoid shape mismatches.
        This ensures SPECTRAMoE forward is never recomputed during backward pass.
        """
        # CRITICAL: Bypass checkpointing completely - Expert choice routing produces
        # different tensor shapes in forward vs backward, causing shape mismatch errors.
        return user_function(*args, **kwargs)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings_global: torch.Tensor,
        position_embeddings_local: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        global_routing_hn: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:

        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        # 1. Attention Block
        # If wrapped in SPECTRAMoE exoskeleton
        if isinstance(self.self_attn, SPECTRAMoE):
            # Disable checkpointing for SPECTRAMoE by calling it outside checkpoint context
            # This is done by ensuring SPECTRAMoE._gradient_checkpointing_func bypasses checkpointing
            # Pass layer_idx for deterministic RNG
            hidden_states, attn_router_info = self.self_attn(hidden_states, global_routing_hn, layer_idx=self.layer_idx)
            self_attn_weights = None # Metadata handling
        else:
            # Fallback for native modules (Transplanted Idefics3/Llama Attention)
            attn_outputs = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
                position_embeddings=position_embeddings_global,
            **kwargs,
        )
            
            # Robust unpacking
            if len(attn_outputs) == 3:
                hidden_states, self_attn_weights, past_key_value = attn_outputs
            elif len(attn_outputs) == 2:
                hidden_states, past_key_value = attn_outputs
                self_attn_weights = None
            else:
                hidden_states = attn_outputs[0]
                self_attn_weights = None
                past_key_value = None
            attn_router_info = (None,) * 8

        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = residual + hidden_states
        
        # 2. Feed-forward (MoE / MLP) Block
        residual = hidden_states
        hidden_states = self.pre_feedforward_layernorm(hidden_states)
        
        # CRITICAL: SPECTRAMoE must be excluded from gradient checkpointing because
        # expert choice routing can produce different tensor shapes in forward vs backward.
        if isinstance(self.mlp, SPECTRAMoE):
            # Disable checkpointing for SPECTRAMoE by calling it outside checkpoint context
            # This is done by ensuring SPECTRAMoE._gradient_checkpointing_func bypasses checkpointing
            hidden_states, moe_router_info = self.mlp(hidden_states, global_routing_hn)
        else:
            hidden_states = self.mlp(hidden_states)
            moe_router_info = (None,) * 8
            
        hidden_states = self.post_feedforward_layernorm(hidden_states)
        hidden_states = residual + hidden_states
        
        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)
        if use_cache:
            outputs += (past_key_value,)
        
        outputs += (moe_router_info,)
        return outputs


SPECTRA_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`SPECTRAConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""

@auto_docstring
class SPECTRAPreTrainedModel(PreTrainedModel):
    config: SPECTRAConfig
    base_model_prefix = ""
    supports_gradient_checkpointing = True
    _no_split_modules = [
        "SPECTRARouter",
        "SiglipVisionEmbeddings",
        "SiglipEncoderLayer",
        "SiglipMultiheadAttentionPoolingHead",
    ]
    _skip_keys_device_placement = [
        "past_key_values",
        "global_router.training_step",
        "global_router.expert_load_ema",
        "global_router.cv_ema",
        "global_router.max_vio_ema",
        "global_router.expert_bias",
    ]
    _supports_flash_attn = True
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True
    
    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": SPECTRADecoderLayer
    }

    def _initialize_moe_router_and_temperature(self) -> None:
        """Initialize only MoE router weights and routing temperature if they were not loaded from a checkpoint.

        - Router linear weights: Xavier uniform for stable logits
        - Routing temperature: ones (softplus(1) ~ 1.313) keeps scale reasonable
        """
        with torch.no_grad():
            for module in self.modules():
                # Initialize router linears ONLY if not already initialized/loaded
                router = getattr(module, "router", None)
                if isinstance(router, nn.Linear):
                    already_init = getattr(router, "_is_hf_initialized", False)
                    if not already_init:
                        nn.init.xavier_uniform_(router.weight)
                        if router.bias is not None:
                            router.bias.zero_()
                # Initialize routing temperature ONLY if looks uninitialized/bad
                routing_temp = getattr(module, "routing_temperature", None)
                if isinstance(routing_temp, nn.Parameter):
                    if not torch.isfinite(routing_temp).all() or routing_temp.abs().sum() == 0:
                        routing_temp.data.fill_(1.0)

    def _init_weights(self, module):
        # important: this ported version of Gemma2 isn't meant for training from scratch - only
        # inference and fine-tuning - so the proper init weights code has been removed

        # Only initialize router linears explicitly; skip expert MLPs and other dense layers during fine-tuning
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):
            if getattr(module, "_is_spectra_router", False):
                logging.get_logger('transformers').debug(f"Initializing router layer with Xavier uniform: {module}")
                _safe_init_weight(module.weight, nn.init.xavier_uniform_)
                if module.bias is not None:
                    # bias is usually handled by zero.Init (init with 0 is default behavior for many), 
                    # but explicit zeroing is fine. Check safe init.
                    # Since zero_() works on 1D tensor, we can just call it directly or safe init.
                    # module.bias.data.zero_() works on sharded param usually?
                    # Let's use safe init to be sure.
                    _safe_init_weight(module.bias, lambda p: p.data.zero_())
            else:
                # Do not touch non-router linears here to avoid re-initializing experts or base MLPs
                pass
        elif isinstance(module, ExpressionProjector):
            logging.get_logger('transformers').debug(f"Initializing expression projector layer with orthogonal: {module}")
            # original_dtype = module.exp_proj.weight.dtype # safe init handles it on modifier rank
            # module.to(torch.float32) # Init logic: DeepSpeed gather brings it in its storage dtype? 
            # If we want float32 init, we should do it inside safe init? 
            # For simplicity, just run orthogonal_.
            _safe_init_weight(module.exp_proj.weight, nn.init.orthogonal_)
            # module.to(original_dtype)
        elif isinstance(module, SPECTRAMultiModalProjector):
            _safe_init_weight(module.mm_input_projection_weight, nn.init.xavier_uniform_)
        elif isinstance(module, nn.GRU):
            for name, param in module.named_parameters():
                if 'weight_ih' in name:
                    _safe_init_weight(param, nn.init.xavier_uniform_)
                elif 'weight_hh' in name:
                    _safe_init_weight(param, nn.init.xavier_uniform_)
                elif 'weight_hr' in name:
                    _safe_init_weight(param, nn.init.xavier_uniform_)
                elif 'bias' in name:
                    # param.data.fill_(0)
                    _safe_init_weight(param, lambda p: p.data.fill_(0))
        else:
            super()._init_weights(module)

    @staticmethod
    def _is_multimodal_model(checkpoint_config) -> bool:
        """Check if checkpoint config represents a multimodal model using transformers internal mappings"""
        model_type = getattr(checkpoint_config, 'model_type', '').lower()
        
        # Method 1: Check transformers internal mapping
        try:
            from transformers.models.auto.modeling_auto import MODELS_FOR_VISION_2_SEQ_GENERATION_MAPPING
            if model_type in MODELS_FOR_VISION_2_SEQ_GENERATION_MAPPING:
                return True
        except (ImportError, AttributeError):
            pass
        
        # Method 2: Check AutoModelForVision2Seq registry
        try:
            from transformers import AutoModelForVision2Seq
            # Check if model_type is in AutoModelForVision2Seq's supported models
            if hasattr(AutoModelForVision2Seq, '_model_mapping'):
                if model_type in AutoModelForVision2Seq._model_mapping:
                    return True
        except (AttributeError, ImportError):
            pass
        
        # Method 3: Fallback to existing heuristic
        has_vision = hasattr(checkpoint_config, 'vision_config') and checkpoint_config.vision_config is not None
        return has_vision or 'vision' in model_type or 'vl' in model_type

    @staticmethod
    def _get_model_class_for_loading(checkpoint_config, logger):
        """Get appropriate AutoModel class for loading based on model type"""
        if SPECTRAPreTrainedModel._is_multimodal_model(checkpoint_config):
            logger.info("  🖼️  Detected multimodal model, using AutoModelForVision2Seq...")
            from transformers import AutoModelForVision2Seq, AutoModel
            return AutoModelForVision2Seq, AutoModel  # primary, fallback
        else:
            logger.info("  📝 Detected text-only model, using AutoModelForCausalLM...")
            from transformers import AutoModelForCausalLM, AutoModel
            return AutoModelForCausalLM, AutoModel  # primary, fallback

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: Type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        force_upcycle: bool = False,  # CRITICAL: 모듈 교체는 from_pretrained 외부에서 수행해야 함 (DeepSpeed ZeRO-3 호환성)
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        """
        [SPECTRA Upcycling Strategy]
        1. pretrained 모델 path가 SPECTRA checkpoint인지 확인
        2. SPECTRA checkpoint면 → 그대로 로딩
        3. Base model checkpoint면 → Base model로 로딩 후 MoE upcycling
        """
        import time
        import os
        from pathlib import Path
        
        logger = logging.get_logger('transformers')
        
        # ===== Transformers Compatibility Patch =====
        # Some models (Kimi-VL, etc.) use legacy activation imports that were renamed/removed
        # Patch transformers.activations to provide missing classes before loading any model
        try:
            import transformers.activations as ta
            import torch.nn as nn
            
            # PytorchGELUTanh was renamed to gelu_pytorch_tanh in newer transformers
            if not hasattr(ta, 'PytorchGELUTanh'):
                class PytorchGELUTanh(nn.Module):
                    def forward(self, input):
                        return nn.functional.gelu(input, approximate="tanh")
                ta.PytorchGELUTanh = PytorchGELUTanh
                logger.info("  🔧 Patched transformers.activations.PytorchGELUTanh for compatibility")
            
            # NewGELUActivation might also be missing in some versions
            if not hasattr(ta, 'NewGELUActivation'):
                ta.NewGELUActivation = ta.GELUActivation
        except Exception as patch_err:
            logger.warning(f"  ⚠️ Activation patch failed (non-critical): {patch_err}")
        
        # Step 1: Config 로딩 (SPECTRA인지 Base model인지 판단)

        # CRITICAL: MoE 모델의 경우, checkpoint의 native config를 먼저 로드해야 함
        # SPECTRA config를 전달하면 MoE 모델을 SPECTRA로 잘못 인식할 수 있음
        checkpoint_native_config = None
        if config is None:
            try:
                checkpoint_native_config = AutoConfig.from_pretrained(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    trust_remote_code=kwargs.get("trust_remote_code", True),
                    **kwargs.get("config_kwargs", {})
                )
                config = checkpoint_native_config
            except Exception as e:
                logger.warning(f"  ⚠️  Failed to load native config: {e}")
                # If explicit config was not passed, we can't proceed with detection using it
                checkpoint_native_config = None
        else:
            # Config가 전달되었지만, MoE 모델인지 확인하기 위해 checkpoint config도 로드
            try:
                checkpoint_native_config = AutoConfig.from_pretrained(
                    pretrained_model_name_or_path,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    local_files_only=local_files_only,
                    token=token,
                    revision=revision,
                    trust_remote_code=kwargs.get("trust_remote_code", True),
                    **kwargs.get("config_kwargs", {})
                )
            except Exception as e:
                logger.warning(f"  ⚠️  Failed to load native config from checkpoint: {e}")
                if not isinstance(config, SPECTRAConfig):
                    checkpoint_native_config = config
                else:
                    checkpoint_native_config = None
        
        # SPECTRA config로 변환 (필요시) - 하지만 MoE 판단은 checkpoint_native_config 사용
        # CRITICAL: Vision-Language 모델인 경우 vision_config를 포함해야 함
        if isinstance(config, SPECTRAConfig):
            spectra_config = config
        else:
            config_dict = config.to_dict()
            # Vision-Language 모델인 경우 checkpoint_native_config에서 vision_config 추출
            if checkpoint_native_config is not None:
                if hasattr(checkpoint_native_config, 'vision_config') and checkpoint_native_config.vision_config is not None:
                    # Vision-Language 모델: vision_config 포함하여 SPECTRAConfig 생성
                    if 'vision_config' not in config_dict:
                        config_dict['vision_config'] = checkpoint_native_config.vision_config.to_dict() if hasattr(checkpoint_native_config.vision_config, 'to_dict') else checkpoint_native_config.vision_config
                    logger.info("  🖼️  Including vision_config from checkpoint for Vision-Language model")
            spectra_config = SPECTRAConfig(**config_dict)
        
        # Attention implementation 설정
        if spectra_config.text_config.attn_implementation == None:
            if is_flash_attn_2_available():
                spectra_config.text_config.attn_implementation = "flash_attention_2"
            elif is_torch_flex_attn_available():
                spectra_config.text_config.attn_implementation = "flex_attention"
            elif kwargs.get("_fast_init", False):
                spectra_config.text_config.attn_implementation = "sdpa"
            else:
                spectra_config.text_config.attn_implementation = "eager"
        
        # Step 2: Checkpoint 타입 분석
        # - SPECTRA checkpoint: SPECTRA Router + MoE
        # - MoE checkpoint: 다른 Router + MoE (DeepSeek, Mixtral 등)
        # - Dense checkpoint: MLP만 있음
        checkpoint_type = "dense"  # default
        
        # CRITICAL: Use checkpoint_native_config for type detection, NOT SPECTRA config
        # SPECTRA config를 사용하면 MoE 모델을 SPECTRA로 잘못 인식할 수 있음
        config_for_detection = checkpoint_native_config if checkpoint_native_config is not None else config
        original_config = config  # Save SPECTRA config for router creation later
        
        try:
            checkpoint_path = Path(pretrained_model_name_or_path)
            if checkpoint_path.exists():
                # Local checkpoint - use checkpoint_native_config for detection
                if isinstance(config_for_detection, SPECTRAConfig):
                    checkpoint_type = "spectra"
                else:
                    # weight 파일에서 구조 분석
                    import glob
                    weight_files = glob.glob(str(checkpoint_path / "*.safetensors")) + glob.glob(str(checkpoint_path / "*.bin"))
                    if weight_files:
                        try:
                            if weight_files[0].endswith('.safetensors'):
                                from safetensors import safe_open
                                with safe_open(weight_files[0], framework="pt", device="cpu") as f:
                                    keys = list(f.keys())
                            else:
                                state_dict = torch.load(weight_files[0], map_location="cpu", weights_only=True)
                                keys = list(state_dict.keys())
                            
                            # Check for SPECTRA router
                            if any('global_router' in k or 'load_balancer' in k for k in keys):
                                checkpoint_type = "spectra"
                            else:
                                # CRITICAL: Sparse MoE 모델 (Kimi-VL 등) 감지
                                # 일부 레이어만 MoE인 경우도 체크해야 함
                                # 여러 레이어 인덱스를 샘플링해서 체크 (sparse MoE 지원)
                                
                                # MoE router 패턴 체크 (더 포괄적으로)
                                has_moe_router = any(
                                    'gate.weight' in k or 
                                    'gate.bias' in k or
                                    'block_sparse_moe.gate' in k or
                                    'mlp.gate.' in k or  # Qwen, Kimi-VL 스타일: layer.mlp.gate.weight
                                    'router' in k.lower()  # gate_proj 방지를 위해 gate 뒤에 . 포함
                                    for k in keys
                                )
                                
                                # MoE experts 패턴 체크 (더 포괄적으로)
                                has_moe_experts = any(
                                    'experts.' in k or 
                                    'block_sparse_moe.experts' in k or
                                    'mlp.experts' in k or  # Qwen, Kimi-VL 스타일: layer.mlp.experts
                                    '.expert.' in k.lower()
                                    for k in keys
                                )
                                
                                # Sparse MoE 체크: 일부 레이어만 MoE인 경우
                                # 예: layers.1.mlp.gate는 있지만 layers.0.mlp.gate는 없을 수 있음
                                # 레이어 인덱스 패턴 추출
                                layer_indices_with_moe = set()
                                for k in keys:
                                    # layers.N.mlp.gate 또는 layers.N.mlp.experts 패턴 찾기
                                    import re
                                    # layers.숫자.mlp.gate 또는 layers.숫자.mlp.experts
                                    match = re.search(r'layers\.(\d+)\.mlp\.(gate|experts)\.', k)
                                    if match:
                                        layer_idx = int(match.group(1))
                                        layer_indices_with_moe.add(layer_idx)
                                
                                # Sparse MoE 감지: 일부 레이어에만 MoE가 있으면 MoE 모델
                                # (first_k_dense_replace나 moe_layer_frequency로 인해)
                                if layer_indices_with_moe:
                                    checkpoint_type = "moe"
                                    logger.info(f"  🔍 Detected sparse MoE: MoE layers at indices {sorted(list(layer_indices_with_moe))[:10]}...")
                                elif has_moe_router or has_moe_experts:
                                    checkpoint_type = "moe"
                                else:
                                    checkpoint_type = "dense"

                            # DeepSpeed rank sync for checkpoint_type
                            if is_deepspeed_initialized() and dist.get_rank() == 0:
                                dist.broadcast_object_list([checkpoint_type], src=0)
                            elif is_deepspeed_initialized():
                                broadcast_list = [None]
                                dist.broadcast_object_list(broadcast_list, src=0)
                                checkpoint_type = broadcast_list[0]
                                logger.info(f"Rank {dist.get_rank()} synchronized checkpoint_type: {checkpoint_type}")
                        except:
                            pass
            else:
                # Remote checkpoint - use checkpoint_native_config for detection, NOT SPECTRA config
                if isinstance(config_for_detection, SPECTRAConfig):
                    checkpoint_type = "spectra"
                else:
                    # CRITICAL: Remote checkpoint도 weight 파일에서 직접 확인 시도
                    # Kimi-VL 같은 경우 config에 MoE 속성이 없을 수 있음
                    try:
                        # HuggingFace Hub에서 첫 번째 weight 파일 다운로드 시도
                        from huggingface_hub import hf_hub_download, list_repo_files
                        import os
                        
                        # repo_id 추출 (path에서)
                        if "/" in str(pretrained_model_name_or_path):
                            repo_id = str(pretrained_model_name_or_path)
                        else:
                            repo_id = pretrained_model_name_or_path
                        
                        # Weight 파일 목록 가져오기
                        try:
                            repo_files = list_repo_files(repo_id, token=token)
                            weight_files = [f for f in repo_files if f.endswith(('.safetensors', '.bin'))]
                            
                            if weight_files:
                                # 첫 번째 weight 파일 다운로드 (메타데이터만)
                                first_weight = weight_files[0]
                                temp_path = hf_hub_download(
                                    repo_id=repo_id,
                                    filename=first_weight,
                                    cache_dir=cache_dir,
                                    token=token,
                                    local_files_only=local_files_only,
                                )
                                
                                # Weight 파일에서 MoE 구조 확인
                                if first_weight.endswith('.safetensors'):
                                    from safetensors import safe_open
                                    with safe_open(temp_path, framework="pt", device="cpu") as f:
                                        keys = list(f.keys())[:1000]  # 처음 1000개만 체크 (속도)
                                else:
                                    state_dict = torch.load(temp_path, map_location="cpu", weights_only=True)
                                    keys = list(state_dict.keys())[:1000]
                                
                                # MoE 구조 체크 (더 포괄적으로)
                                has_moe_router = any(
                                    'gate.weight' in k or 
                                    'gate.bias' in k or
                                    'block_sparse_moe.gate' in k or
                                    'mlp.gate.' in k or  # Qwen, Kimi-VL 스타일
                                    'router' in k.lower()
                                    for k in keys
                                )
                                
                                has_moe_experts = any(
                                    'experts.' in k or 
                                    'block_sparse_moe.experts' in k or
                                    'mlp.experts' in k or  # Qwen, Kimi-VL 스타일
                                    '.expert.' in k.lower()
                                    for k in keys
                                )
                                
                                # Sparse MoE 체크: 일부 레이어만 MoE인 경우 (Kimi-VL 등)
                                layer_indices_with_moe = set()
                                import re
                                for k in keys:
                                    # layers.N.mlp.gate 또는 layers.N.mlp.experts 패턴 찾기
                                    match = re.search(r'layers\.(\d+)\.mlp\.(gate|experts)\.', k)
                                    if match:
                                        layer_idx = int(match.group(1))
                                        layer_indices_with_moe.add(layer_idx)
                                
                                if layer_indices_with_moe:
                                    checkpoint_type = "moe"
                                    logger.info(f"  🔍 Detected sparse MoE from weight file: MoE layers at indices {sorted(list(layer_indices_with_moe))[:10]}...")
                                elif has_moe_router or has_moe_experts:
                                    checkpoint_type = "moe"
                                    logger.info(f"  🔍 Detected MoE from weight file: router={has_moe_router}, experts={has_moe_experts}")
                                
                                # 임시 파일 정리 (선택적)
                                # os.remove(temp_path)  # 캐시에 남겨두는 게 나을 수도 있음
                        except Exception as weight_check_err:
                            logger.debug(f"  ⚠️  Weight file check failed (non-critical): {weight_check_err}")
                            # Fallback to config-based detection
                            pass
                    except Exception as hf_hub_err:
                        logger.debug(f"  ⚠️  HuggingFace Hub check failed (non-critical): {hf_hub_err}")
                        # Fallback to config-based detection
                        pass
                    
                    # Config 기반 MoE 체크 (fallback)
                    if checkpoint_type == "dense":
                        # CRITICAL: Kimi-VL 같은 sparse MoE 모델 감지
                        # moe_layer_frequency나 first_k_dense_replace가 있으면 MoE 모델
                        has_moe_layer_freq = hasattr(config_for_detection, 'moe_layer_frequency') or \
                                           hasattr(config_for_detection, 'moe_layer_freq')
                        has_first_k_dense = hasattr(config_for_detection, 'first_k_dense_replace')
                        
                        if has_moe_layer_freq or has_first_k_dense:
                            # Sparse MoE 모델 (일부 레이어만 MoE)
                            checkpoint_type = "moe"
                            logger.info(f"  🔍 Detected sparse MoE from config: moe_layer_frequency={has_moe_layer_freq}, first_k_dense_replace={has_first_k_dense}")
                        
                        # MoE config 체크 (다양한 MoE architectures) - checkpoint_native_config 사용
                        elif hasattr(config_for_detection, 'num_local_experts'):  # Mixtral, Qwen-MoE, Kimi-VL
                            checkpoint_type = "moe"
                        elif hasattr(config_for_detection, 'n_routed_experts'):  # SPECTRA, DeepSeek
                            checkpoint_type = "moe"
                        elif hasattr(config_for_detection, 'num_experts'):  # Generic MoE
                            checkpoint_type = "moe"
                        elif hasattr(config_for_detection, 'moe_intermediate_size'):  # Qwen2MoE
                            checkpoint_type = "moe"
                        elif hasattr(config_for_detection, 'n_shared_experts'):  # DeepSeek
                            checkpoint_type = "moe"
                        # Model type 체크
                        elif hasattr(config_for_detection, 'model_type') and 'moe' in config_for_detection.model_type.lower():
                            checkpoint_type = "moe"
                        # 추가: config_dict에서 직접 체크 (일부 모델은 속성으로 노출 안 할 수 있음)
                        elif hasattr(config_for_detection, 'to_dict'):
                            config_dict = config_for_detection.to_dict()
                            # moe_layer_frequency나 first_k_dense_replace 체크
                            if 'moe_layer_frequency' in config_dict or 'moe_layer_freq' in config_dict or \
                               'first_k_dense_replace' in config_dict:
                                checkpoint_type = "moe"
                                logger.info(f"  🔍 Detected sparse MoE from config dict: {[k for k in ['moe_layer_frequency', 'moe_layer_freq', 'first_k_dense_replace'] if k in config_dict]}")
                            elif any(('expert' in str(v).lower() or 'moe' in str(v).lower()) 
                                   for k, v in config_dict.items() 
                                   if (isinstance(v, int) and v > 1) or (isinstance(v, str) and len(v) > 0)):
                                # 숫자 값이 1보다 크고 'expert'나 'moe' 관련이면 MoE로 간주
                                checkpoint_type = "moe"
                                logger.info(f"  🔍 Detected MoE from config dict values")
        except Exception as e:
            logger.warning(f"⚠️  Checkpoint type detection failed: {e}, defaulting to dense")
            checkpoint_type = "dense"
        
        logger.info(f"📊 Found checkpoint type: {checkpoint_type}")
        if checkpoint_type == "moe":
            # Log config attributes for debugging - use checkpoint_native_config
            moe_attrs = [attr for attr in ['num_local_experts', 'n_routed_experts', 'num_experts', 'moe_intermediate_size', 'n_shared_experts'] if hasattr(config_for_detection, attr)]
            if moe_attrs:
                logger.info(f"   MoE config attributes: {', '.join(moe_attrs)}")
            logger.info(f"   ✅ Detected as MoE model - will load pretrained weights and swap router only")
        
        logging.set_verbosity_error()
        start_time = time.time()
        
        # Step 3: 로딩 전략 분기
        if checkpoint_type == "spectra":
            # Loading SPECTRA checkpoint
            logger.info("  📂 SPECTRA checkpoint detected. Loading...")
            load_start = time.time()
            
            load_kwargs = {
                "config": spectra_config,
                "cache_dir": cache_dir,
                "ignore_mismatched_sizes": ignore_mismatched_sizes,
                "force_download": force_download,
                "local_files_only": local_files_only,
                "token": token,
                "revision": revision,
                "use_safetensors": use_safetensors,
                "weights_only": weights_only,
                "trust_remote_code": kwargs.get("trust_remote_code", True),
            }
            
            # Weight 로딩 최적화 파라미터
            for param in ["device_map", "torch_dtype", "low_cpu_mem_usage", "offload_state_dict", "max_memory"]:
                if param in kwargs:
                    load_kwargs[param] = kwargs[param]
            
            # Use fast init for SPECTRA load too
            if "_fast_init" not in load_kwargs:
                load_kwargs["_fast_init"] = True

            for k, v in kwargs.items():
                if k not in load_kwargs and k not in ["config_kwargs", "force_upcycle"]:
                    load_kwargs[k] = v
            
            base_model = super().from_pretrained(
                pretrained_model_name_or_path,
                *model_args,
                **load_kwargs
            )
            
            # CRITICAL: Verify that base_model is a SPECTRAPreTrainedModel instance
            if not isinstance(base_model, SPECTRAPreTrainedModel):
                logger.error(f"  ❌ Expected SPECTRAPreTrainedModel instance, but got {type(base_model).__name__}")
                logger.error("     This indicates that super().from_pretrained did not return a SPECTRA model.")
                logger.error("     This may happen if checkpoint_type was incorrectly detected as 'spectra'.")
                raise TypeError(
                    f"Expected SPECTRAPreTrainedModel instance from from_pretrained, "
                    f"but got {type(base_model).__name__}. "
                    f"Checkpoint type detection may have failed."
                )
            
            logger.info(f"  ✅ SPECTRA checkpoint loaded in {time.time() - load_start:.2f}s")
            logging.set_verbosity_warning()
            return base_model
            
        elif checkpoint_type == "moe":
            # MoE model checkpoint → Router만 SPECTRA Router로 교체
            logger.info("  🔄 Loading MoE model for router swapping")
            logger.info("  ⚠️  CRITICAL: Using model's native config (SPECTRA config ignored to prevent model recreation)")
            moe_load_start = time.time()
            
            # CRITICAL: Do NOT use SPECTRA config - it will recreate the model with new MoE layers
            # Load the MoE model with its native config from checkpoint, then swap only the router
            # Save original_config for router creation later
            router_config_for_swap = original_config  # SPECTRA config for router creation
            
            # Save original device_map for later use
            original_device_map = kwargs.get("device_map")
            original_torch_dtype = kwargs.get("torch_dtype", torch.bfloat16)
            
            load_kwargs = {
                "cache_dir": cache_dir,
                # CRITICAL: ignore_mismatched_sizes=False to ensure all pretrained weights are loaded
                # Router weights will be replaced later, but experts must be loaded from pretrained
                "ignore_mismatched_sizes": False,  # Load all pretrained weights, router will be swapped
                "force_download": force_download,
                "local_files_only": local_files_only,
                "token": token,
                "revision": revision,
                "use_safetensors": use_safetensors,
                "weights_only": weights_only,
                "trust_remote_code": kwargs.get("trust_remote_code", True),
                "torch_dtype": original_torch_dtype,
            }
            
            # Weight 로딩 최적화 파라미터만 추가 (모델 초기화 파라미터 제외)
            # These are safe to pass to from_pretrained
            safe_load_params = ["low_cpu_mem_usage", "offload_state_dict", "max_memory"]
            for param in safe_load_params:
                if param in kwargs:
                    load_kwargs[param] = kwargs[param]
            
            # CRITICAL: Exclude model initialization parameters that are NOT part of from_pretrained signature
            # These should be set AFTER model loading, not during initialization
            excluded_init_params = [
                "use_cache",  # Set via model.config.use_cache after loading
                "gradient_checkpointing",  # Set via model.gradient_checkpointing_enable() after loading
                "attn_implementation",  # Should be in config, not passed to __init__
            ]
            
            # Default to fast loading if not specified
            if "low_cpu_mem_usage" not in load_kwargs:
                load_kwargs["low_cpu_mem_usage"] = True
            
            # Ensure fast init is used (skips random initialization of weights)
            if "_fast_init" not in load_kwargs:
                load_kwargs["_fast_init"] = True

            # Filter kwargs for AutoModel
            # CRITICAL: Do NOT pass config to AutoModel - it will recreate the model with SPECTRA config
            # and create new MoE layers, causing OOM. We want to load the original MoE model as-is.
            spectra_specific_args = ["n_routed_experts", "router_dim", "moe_implementation", "router_impl"]
            spectra_overrides = {}
            
            # Exclude config and config_kwargs to prevent model recreation
            # Also exclude model initialization parameters that are NOT part of from_pretrained signature
            excluded_keys = ["config", "config_kwargs", "force_upcycle"]
            excluded_init_params = [
                "use_cache",  # Set via model.config.use_cache after loading
                "gradient_checkpointing",  # Set via model.gradient_checkpointing_enable() after loading
                "attn_implementation",  # Should be in config, not passed to __init__
            ]
            
            # Store excluded init params to set after model loading
            post_load_settings = {}
            for param in excluded_init_params:
                if param in kwargs:
                    post_load_settings[param] = kwargs[param]
            
            for k, v in kwargs.items():
                if k in spectra_specific_args:
                    spectra_overrides[k] = v
                elif k not in load_kwargs and k not in excluded_keys and k not in excluded_init_params:
                    load_kwargs[k] = v
            
            # CRITICAL: Ensure config is NOT in load_kwargs (it would recreate the model)
            if "config" in load_kwargs:
                logger.warning("  ⚠️  Removing 'config' from load_kwargs to prevent model recreation")
                del load_kwargs["config"]
            
            # CRITICAL: 모든 rank에서 동일한 모델 구조를 보장하기 위해
            # 모델 로딩 전에 동기화
            try:
                import torch.distributed as dist
                if dist.is_available() and dist.is_initialized():
                    dist.barrier()
                    logger.info("  🔄 Synchronized all ranks before model loading")
            except:
                pass
            
            # MoE model 로딩 (device_map 사용 시 바로 GPU로)
            if original_device_map is not None:
                logger.info(f"  📥 Loading MoE model with device_map={original_device_map} and weights_only={weights_only}...")
            else:
                logger.info(f"  📥 Loading MoE model with weights_only={weights_only}...")
            
            # CRITICAL: Load checkpoint's native config first, then use appropriate AutoModel class
            # Vision-Language models use AutoModelForVision2Seq, not AutoModelForCausalLM
            logger.info("  📥 Loading checkpoint's native config to determine model type...")
            checkpoint_config = AutoConfig.from_pretrained(
                pretrained_model_name_or_path,
            cache_dir=cache_dir,
            force_download=force_download,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
                trust_remote_code=kwargs.get("trust_remote_code", True),
            )
            
            # Determine model class based on checkpoint config using transformers internal mappings
            # Use checkpoint's native config for loading
            # CRITICAL: Pass config to ensure correct model class is used, but this is the checkpoint's native config
            # so it won't recreate the model structure
            load_kwargs_with_config = load_kwargs.copy()
            load_kwargs_with_config['config'] = checkpoint_config
            
            # Store excluded init params to set after model loading
            post_load_settings = {}
            for param in excluded_init_params:
                if param in kwargs:
                    post_load_settings[param] = kwargs[param]
            
            # Get appropriate model class using transformers internal mappings
            primary_model_class, fallback_model_class = cls._get_model_class_for_loading(checkpoint_config, logger)
            
            try:
                moe_model = primary_model_class.from_pretrained(
                    pretrained_model_name_or_path,
                    *model_args,
                    **load_kwargs_with_config
                )
            except (ValueError, AttributeError, TypeError) as e:
                logger.warning(f"  ⚠️  {primary_model_class.__name__} failed ({type(e).__name__}), falling back to {fallback_model_class.__name__}...")
                moe_model = fallback_model_class.from_pretrained(
                    pretrained_model_name_or_path,
                    *model_args,
                    **load_kwargs_with_config
                )
            
            model_class_name = type(moe_model).__name__
            logger.info(f"  ✅ Base MoE model loaded in {time.time() - moe_load_start:.2f}s")
            logger.info(f"     Model class: {model_class_name}")
            
            # CRITICAL: Patch forward() methods to filter framework-only arguments
            # This ensures compatibility with PEFT and other wrappers
            logger.info("  🔧 Patching forward() methods to filter framework-only arguments...")
            cls._patch_forward_for_framework_args(moe_model)
            
            # Apply post-load settings (parameters that should be set after model initialization)
            if post_load_settings:
                logger.info("  🔧 Applying post-load settings...")
                if "use_cache" in post_load_settings:
                    if hasattr(moe_model, 'config'):
                        moe_model.config.use_cache = post_load_settings["use_cache"]
                    logger.info(f"     - use_cache: {post_load_settings['use_cache']}")
                if "gradient_checkpointing" in post_load_settings and post_load_settings["gradient_checkpointing"]:
                    if hasattr(moe_model, 'gradient_checkpointing_enable'):
                        if not moe_model.supports_gradient_checkpointing:
                            moe_model.supports_gradient_checkpointing = True
                        moe_model.gradient_checkpointing_enable()
                    logger.info("     - gradient_checkpointing: enabled")
                if "attn_implementation" in post_load_settings:
                    if hasattr(moe_model, 'config'):
                        moe_model.config.attn_implementation = post_load_settings["attn_implementation"]
                    logger.info(f"     - attn_implementation: {post_load_settings['attn_implementation']}")
            
            # Vision config 검증 (multimodal 모델의 경우)
            has_vision = hasattr(spectra_config, 'vision_config') and spectra_config.vision_config is not None
            if has_vision:
                has_vision_in_model = hasattr(moe_model.config, 'vision_config') or \
                                      hasattr(moe_model, 'visual') or \
                                      hasattr(moe_model, 'vision_tower')
                if has_vision_in_model:
                    logger.info("     🖼️  Vision/multimodal components detected")
                else:
                    logger.warning("     ⚠️  SPECTRA config has vision_config but model doesn't support it")
            
            # Apply overrides to spectra_config (use original_config which is SPECTRA config)
            # We need spectra_config for router creation, but we loaded the model with native config
            if force_upcycle and spectra_overrides:
                logger.info(f"  🔧 Applying config overrides: {spectra_overrides}")
                # Use original_config (SPECTRA config) for router creation
                router_config = original_config.text_config if hasattr(original_config, 'text_config') else original_config
                for k, v in spectra_overrides.items():
                    if hasattr(router_config, k):
                        setattr(router_config, k, v)
                    elif isinstance(router_config, dict):
                        router_config[k] = v

            # CRITICAL: Vision-Language 모델인 경우, 모델의 config에 vision_config가 유지되도록 설정
            # 모델이 로드된 후에도 vision_config를 사용할 수 있어야 함
            if has_vision and hasattr(checkpoint_config, 'vision_config') and checkpoint_config.vision_config is not None:
                # 모델의 config에 vision_config가 없으면 추가
                if not hasattr(moe_model.config, 'vision_config') or moe_model.config.vision_config is None:
                    if hasattr(moe_model, 'config'):
                        # checkpoint_config의 vision_config를 모델 config에 복사
                        if hasattr(checkpoint_config.vision_config, 'to_dict'):
                            vision_config_dict = checkpoint_config.vision_config.to_dict()
                        else:
                            vision_config_dict = checkpoint_config.vision_config
                        # 모델 config에 vision_config 속성 추가
                        setattr(moe_model.config, 'vision_config', checkpoint_config.vision_config)
                        logger.info("  ✅ Added vision_config to model.config for Vision-Language model")
            
            # Router 교체 수행
            if force_upcycle:
                # CRITICAL: 모든 rank에서 동일한 모듈 구조를 보장하기 위해
                # 모듈 교체 전에 동기화
                try:
                    if dist.is_available() and dist.is_initialized():
                        dist.barrier()
                        logger.info("  🔄 Synchronized all ranks before router swap")
                except:
                    pass
                
                logger.info("  🔀 Swapping routers → SPECTRA Router...")
                swap_start = time.time()
                
                # Use router_config_for_swap (SPECTRA config) for router creation
                # The model itself was loaded with its native config, but router needs SPECTRA config
                moe_model = cls._swap_router_to_spectra(moe_model, router_config_for_swap)
                
                logger.info(f"  ✅ Router swap completed in {time.time() - swap_start:.2f}s")
                
                # CRITICAL: 모든 rank에서 동일한 모듈 구조를 보장하기 위해
                # 모듈 교체 후에 동기화
                try:
                    if dist.is_available() and dist.is_initialized():
                        dist.barrier()
                        logger.info("  🔄 Synchronized all ranks after router swap")
                except:
                    pass
                # Note: 모델은 이미 device_map에 따라 배치되어 있으므로 추가 dispatch 불필요
            else:
                logger.info("  ⏭️  Skipping router swap (force_upcycle=False)")
                
                # Still move to device if needed
                if original_device_map is not None and original_device_map != "cpu":
                    logger.info(f"  📍 Moving model to device_map={original_device_map}...")
                    try:
                        if original_device_map == "auto":
                            from accelerate import dispatch_model, infer_auto_device_map
                            device_map = infer_auto_device_map(
                                moe_model,
                                max_memory=kwargs.get("max_memory"),
                                dtype=original_torch_dtype,
                            )
                            dispatch_model(moe_model, device_map=device_map)
                        else:
                            moe_model = moe_model.to(original_device_map)
                    except Exception as e:
                        logger.warning(f"  ⚠️  Device placement failed: {e}")
            
            logger.info(f"✅ Total time: {time.time() - start_time:.2f}s")
            logging.set_verbosity_warning()
            return moe_model
            
        else:  # checkpoint_type == "dense"
            # Dense model checkpoint → SPECTRA로 upcycling
            logger.info("🟢 Loading Dense model for upcycling")
            
            # Step 1: SPECTRA config에 defer_moe_init=True 설정
            # 이렇게 하면 MoE 초기화를 연기하고 MLP만 생성함
            spectra_config.text_config.defer_moe_init = True
            
            load_kwargs = {
                "config": spectra_config,  # SPECTRA config 사용 (but defer_moe_init=True)
                "cache_dir": cache_dir,
                "ignore_mismatched_sizes": True,  # MLP keys만 매칭
                "force_download": force_download,
                "local_files_only": local_files_only,
                "token": token,
                "revision": revision,
                "use_safetensors": use_safetensors,
                "weights_only": weights_only,
                "trust_remote_code": kwargs.get("trust_remote_code", True),
            }
            
            # Weight 로딩 최적화 파라미터
            for param in ["device_map", "torch_dtype", "low_cpu_mem_usage", "offload_state_dict", "max_memory"]:
                if param in kwargs:
                    load_kwargs[param] = kwargs[param]
            
            for k, v in kwargs.items():
                if k not in load_kwargs and k not in ["config_kwargs", "force_upcycle"]:
                    load_kwargs[k] = v
            
            # Step 2: Load actual Dense Model (preserving architecture)
            logger.info("  📥 Loading base model using AutoModelForCausalLM / Vision2Seq...")
            # Remove config from load_kwargs to avoid conflict if we want auto-detection
            load_kwargs.pop("config", None)
            
            # Sanitize kwargs for strict model inits (like Idefics3)
            # These args shouldn't be passed to __init__ via from_pretrained if they aren't in signature
            sanitized_load_kwargs = load_kwargs.copy()
            for unsafe_arg in ["gradient_checkpointing", "use_cache", "attn_implementation"]:
                sanitized_load_kwargs.pop(unsafe_arg, None)
            
            # Robustly load the original model
            original_model = None
            try:
                # Try CausalLM first (Most common)
                original_model = AutoModelForCausalLM.from_pretrained(
                    pretrained_model_name_or_path,
                    *model_args,
                    **sanitized_load_kwargs
                )
            except ValueError as e:
                # If config not recognized (e.g. Idefics3Config for CausalLM), try Vision2Seq
                if "Unrecognized configuration class" in str(e):
                    try:
                        logger.info("  ⚠️ CausalLM failed, trying AutoModelForVision2Seq for VLM...")
                        original_model = AutoModelForVision2Seq.from_pretrained(
                           pretrained_model_name_or_path,
                           *model_args,
                           **sanitized_load_kwargs
                        )
                    except Exception as e2:
                        logger.warning(f"  ⚠️ Vision2Seq also failed: {e2}")
                
                # If still None, try generic AutoModel
                if original_model is None:
                    logger.info("  Fallback to generic AutoModel...")
                    original_model = AutoModel.from_pretrained(
                        pretrained_model_name_or_path,
                        *model_args,
                        **sanitized_load_kwargs
                    )
            
            # Instantiate SPECTRA Shell
            logger.info("  🔧 Instantiating SPECTRA Shell...")
            # Ensure config matches loaded model
            if hasattr(original_model, "config"):
                # Copy relevant attributes to spectra_config.text_config
                for k in ["hidden_size", "vocab_size", "num_hidden_layers", "num_attention_heads", "intermediate_size"]:
                    if hasattr(original_model.config, k):
                        setattr(spectra_config.text_config, k, getattr(original_model.config, k))
            
            spectra_model = cls(spectra_config) # SPECTRAForConditionalGeneration
            
            # Transplant Components
            logger.info("  🔄 Transplanting components from Base Model to SPECTRA Shell...")
            
            # 1. Locate source components (Enhanced for VLMs like Idefics3)
            source_model = None
            candidates = ["model", "transformer", "language_model", "text_model"]
            
            # Check top level first
            if hasattr(original_model, "layers"): 
                source_model = original_model
            
            # DFS-like search for layers if not found
            if source_model is None:
                for c in candidates:
                    if hasattr(original_model, c):
                        sub = getattr(original_model, c)
                        if hasattr(sub, "layers") or hasattr(sub, "h"):
                            source_model = sub
                            break
                        # One level deeper (e.g. model.text_model)
                        for c2 in candidates:
                            if hasattr(sub, c2):
                                sub2 = getattr(sub, c2)
                                if hasattr(sub2, "layers") or hasattr(sub2, "h"):
                                    source_model = sub2
                                    break
                        if source_model: break
            
            if source_model is None:
                source_model = original_model # Last resort fallback
            
            source_layers = None
            if hasattr(source_model, "layers"): source_layers = source_model.layers
            elif hasattr(source_model, "h"): source_layers = source_model.h
            
            # 2. Locate target components (SPECTRA)
            if hasattr(spectra_model, 'language_model'):
                target_model = spectra_model.language_model 
            elif hasattr(spectra_model, 'model'):
                target_model = spectra_model.model
            else:
                target_model = spectra_model
            
            # 3. Transplant Layers
            if source_layers is not None:
                if len(source_layers) != len(target_model.layers):
                    logger.warning(f"  ⚠️ Layer count mismatch: Source {len(source_layers)} vs Target {len(target_model.layers)}")
                    # Re-init target layers to match source count
                    target_model.layers = nn.ModuleList([SPECTRADecoderLayer(spectra_config.text_config, i, target_model.global_router) for i in range(len(source_layers))])
                
                # Copy internals strictly (Transplant) to preserve SPECTRA Wrapper logic
                for idx, (src, tgt) in enumerate(zip(source_layers, target_model.layers)):
                    # Attention
                    if hasattr(src, "self_attn"): tgt.self_attn = src.self_attn
                    elif hasattr(src, "attn"): tgt.self_attn = src.attn
                    elif hasattr(src, "attention"): tgt.self_attn = src.attention
                    
                    # MLP
                    if hasattr(src, "mlp"): tgt.mlp = src.mlp
                    elif hasattr(src, "feed_forward"): tgt.mlp = src.feed_forward
                    
                    # Norms
                    if hasattr(src, "input_layernorm"): tgt.input_layernorm = src.input_layernorm
                    elif hasattr(src, "ln_1"): tgt.input_layernorm = src.ln_1
                    elif hasattr(src, "norm1"): tgt.input_layernorm = src.norm1
                    
                    if hasattr(src, "post_attention_layernorm"): tgt.post_attention_layernorm = src.post_attention_layernorm
                    elif hasattr(src, "ln_2"): tgt.post_attention_layernorm = src.ln_2
                    elif hasattr(src, "norm2"): tgt.post_attention_layernorm = src.norm2
                
                # Verify Transplant Success
                if isinstance(target_model.layers[0].self_attn, nn.Identity):
                    raise ValueError(f"❌ Transplant Failed! Could not find attention module in source layer type: {type(source_layers[0])}. Checked: self_attn, attn, attention")
                
                logger.info(f"     ✅ Transplanted internals of {len(source_layers)} layers into SPECTRA Exoskeleton")
            else:
                raise ValueError(f"❌ Could not find source layers to transplant from {type(original_model)}")

            # 4. Transplant Embedding & Norm & Head
            if hasattr(source_model, "embed_tokens"): target_model.embed_tokens = source_model.embed_tokens
            if hasattr(source_model, "norm"): target_model.norm = source_model.norm
            
            # Transplant Rotary Embedding (Critical for Attention dimensions)
            if hasattr(source_model, "rotary_emb"):
                target_model.rotary_emb = source_model.rotary_emb
                logger.info(f"     ✅ Transplanted rotary_emb from {type(source_model)} to fix RoPE compatibility")
            
            if hasattr(original_model, "lm_head"): spectra_model.lm_head = original_model.lm_head

            logger.info(f"  ✅ Base model loaded & transplanted in {time.time() - start_time:.2f}s")
            
            base_model = spectra_model

            # Step 3: Upcycling (MLP → MoE)
            if force_upcycle:
                logger.info("  🔄 Upcycling MLP → MoE...")
                upcycle_start = time.time()
                
                if hasattr(base_model, 'model') and hasattr(base_model.model, 'layers'):
                    base_model.model = base_model._upcycle(base_model.model)
                elif hasattr(base_model, 'language_model') and hasattr(base_model.language_model, 'layers'): # SPECTRA structure
                    base_model.language_model = base_model._upcycle(base_model.language_model)
                else:
                    logger.warning("⚠️  Model structure not recognized for upcycling")
                
                logger.info(f"  ✅ Upcycling completed in {time.time() - upcycle_start:.2f}s")
            else:
                logger.info("  ⏭️  Skipping upcycling (force_upcycle=False)")
            
            logger.info(f"✅ Total load + upcycle time: {time.time() - start_time:.2f}s")
            
            # CRITICAL: Verify that base_model is a SPECTRAPreTrainedModel instance
            if not isinstance(base_model, SPECTRAPreTrainedModel):
                logger.error(f"  ❌ Expected SPECTRAPreTrainedModel instance, but got {type(base_model).__name__}")
                logger.error("     This indicates that cls(spectra_config) did not return a SPECTRA model.")
                logger.error("     This may happen if SPECTRAForConditionalGeneration.__init__ failed.")
                raise TypeError(
                    f"Expected SPECTRAPreTrainedModel instance from cls(spectra_config), "
                    f"but got {type(base_model).__name__}. "
                    f"This is likely a bug in SPECTRAForConditionalGeneration.__init__."
                )
            
            logging.set_verbosity_warning()
            return base_model

    @staticmethod
    def _patch_forward_for_framework_args(model, visited=None):
        """
        Patch forward() methods to filter out framework-only arguments (curriculum_seqlen, etc.)
        This ensures compatibility with PEFT and other wrappers that may call forward() directly.
        
        Args:
            model: The model to patch
            visited: Set of already visited model IDs to prevent infinite recursion
        """
        if visited is None:
            visited = set()
        
        # Use id() to track visited models and prevent circular references
        model_id = id(model)
        if model_id in visited:
            return model
        visited.add(model_id)
        
        framework_only_args = {
            'curriculum_seqlen',  # Trainer curriculum learning
            'num_items_in_batch',  # Trainer batch info
            'num_tokens',  # Trainer token counting
            'num_items',  # Trainer item counting
        }
        
        # Check if already patched (has _spectra_patched_forward attribute)
        if hasattr(model, '_spectra_patched_forward'):
            # Already patched, just update visited set for nested models
            for attr_name in ['language_model', 'model', 'base_model']:
                if hasattr(model, attr_name):
                    nested_model = getattr(model, attr_name)
                    if hasattr(nested_model, 'forward'):
                        SPECTRAPreTrainedModel._patch_forward_for_framework_args(nested_model, visited)
            return model
        
        # Get original forward method
        original_forward = model.forward
        
        # Create wrapped forward that filters framework args
        def patched_forward(*args, **kwargs):
            filtered_kwargs = {k: v for k, v in kwargs.items() if k not in framework_only_args}
            return original_forward(*args, **filtered_kwargs)
        
        # Replace forward method
        model.forward = patched_forward
        # Mark as patched to prevent re-patching
        model._spectra_patched_forward = True
        
        # Also patch nested models (language_model, model, etc.) - but only if they're different objects
        for attr_name in ['language_model', 'model', 'base_model']:
            if hasattr(model, attr_name):
                nested_model = getattr(model, attr_name)
                # Only patch if it's a different object and has forward method
                if nested_model is not model and hasattr(nested_model, 'forward'):
                    SPECTRAPreTrainedModel._patch_forward_for_framework_args(nested_model, visited)
        
        return model

    @classmethod
    @torch.no_grad()
    def _swap_router_to_spectra(cls, moe_model, spectra_config):
        """
        [SPECTRA Router Swapping]
        기존 MoE 모델의 router를 SPECTRA router로 교체합니다.
        - DeepSeek, Mixtral, Qwen 등의 local router → SPECTRA global router
        - Experts는 그대로 유지
        
        CRITICAL: DeepSpeed ZeRO-3 호환성을 위해 모든 rank에서 동일한 순서로 실행되어야 함
        """
        logger = logging.get_logger('transformers')
        
        # CRITICAL: 모든 rank에서 동일한 모듈 구조를 보장하기 위해
        # distributed 환경에서 동기화
        try:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                # 모든 rank가 동일한 지점에서 대기
                dist.barrier()
        except:
            pass
        
        # Extract model structure (다양한 모델 아키텍처 지원)
        layers = None
        model_container = None
        
        # Case 1: model.layers (Llama, Mistral, Gemma 등)
        if hasattr(moe_model, 'model') and hasattr(moe_model.model, 'layers'):
            layers = moe_model.model.layers
            model_container = moe_model.model
            logger.info(f"  📂 Model structure: model.layers (found {len(layers)} layers)")
        
        # Case 2: language_model.layers (VLM 모델들)
        elif hasattr(moe_model, 'language_model') and hasattr(moe_model.language_model, 'layers'):
            layers = moe_model.language_model.layers
            model_container = moe_model.language_model
            logger.info(f"  📂 Model structure: language_model.layers (found {len(layers)} layers)")
        
        # Case 3: transformer.h (GPT-2 style)
        elif hasattr(moe_model, 'transformer') and hasattr(moe_model.transformer, 'h'):
            layers = moe_model.transformer.h
            model_container = moe_model.transformer
            logger.info(f"  📂 Model structure: transformer.h (found {len(layers)} layers)")
        
        # Case 4: layers (직접 접근)
        elif hasattr(moe_model, 'layers'):
            layers = moe_model.layers
            model_container = moe_model
            logger.info(f"  📂 Model structure: layers (found {len(layers)} layers)")
        
        # Case 5: 실패
        else:
            logger.warning("⚠️  Cannot find layers to swap router")
            logger.warning(f"   Model attributes: {list(vars(moe_model).keys())[:10]}")
            return moe_model
        
        # Create SPECTRA global router
        logger.info("  🔧 Creating SPECTRA global router...")
        router_config = spectra_config.text_config if hasattr(spectra_config, 'text_config') else spectra_config
        
        # CRITICAL: 기존 global_router가 있다면 제거 (중복 파라미터 방지)
        if hasattr(model_container, 'global_router') and model_container.global_router is not None:
            logger.info("  ⚠️  Found existing global_router, will replace it to avoid parameter duplication")
            old_router = model_container.global_router
            # 기존 router의 파라미터를 모델에서 제거
            for name, param in list(model_container.named_parameters()):
                if 'global_router' in name:
                    # 파라미터를 모델에서 분리
                    param.detach_()
            del old_router
        
        global_router = SPECTRARouter(router_config)
        
        # Move router to same device and dtype as model
        try:
            # Get model device and dtype (first parameter)
            model_param = next(moe_model.parameters())
            model_device = model_param.device
            model_dtype = model_param.dtype
            
            # CRITICAL: meta device인 경우 cuda:0로 강제 이동 (device_map="auto" 로딩 직후일 수 있음)
            if model_device.type == 'meta':
                logger.warning("  ⚠️  Model device is 'meta', forcing router to cuda:0")
                model_device = torch.device('cuda:0')
            
            # 동기화 후 이동 (에러 전파 방지 및 안정성 확보)
            if model_device.type == 'cuda':
                torch.cuda.synchronize()
            
            global_router = global_router.to(device=model_device, dtype=model_dtype)
            
            if model_device.type == 'cuda':
                torch.cuda.synchronize()
            logger.info(f"  📍 Router moved to device: {model_device}, dtype: {model_dtype}")
        except StopIteration:
            logger.warning("  ⚠️  Could not determine model device/dtype, router stays on default")
        
        model_container.global_router = global_router
        
        # "ONE global router" 요청에 따라 모든 레이어에서 동일한 router 인스턴스를 공유하며 사용합니다.
        
        swapped_count = 0
        rank = 0
        if torch.distributed.is_initialized():
            rank = torch.distributed.get_rank()

        for layer_idx, layer in enumerate(tqdm(layers, desc="🔀 Swapping routers", leave=True, ncols=100, disable=(rank != 0))):
            # Check if this layer has MoE
            moe_attr = None
            # Common names for the MoE/MLP block that contains routing logic
            possible_moe_names = ['block_sparse_moe', 'mlp', 'moe', 'router']
            
            for attr in possible_moe_names:
                if hasattr(layer, attr):
                    mod = getattr(layer, attr)
                    # Robust check: does it look like an MoE module?
                    # It should have a gate/router OR an experts/expert list OR num_experts > 1
                    if any(hasattr(mod, k) for k in ['gate', 'router', 'experts', 'expert', 'n_experts', 'num_experts']):
                        moe_attr = attr
                        break
            
            if moe_attr is None:
                continue
            
            moe_module = getattr(layer, moe_attr)
            
            # Extract experts with flexible naming
            experts = getattr(moe_module, 'experts', getattr(moe_module, 'expert', None))
            shared_experts = getattr(moe_module, 'shared_experts', getattr(moe_module, 'shared_expert', None))
            has_shared = shared_experts is not None
            
            # If no explicit experts attribute, check if the module itself is monolithic
            is_monolithic = False
            if experts is None:
                if hasattr(moe_module, 'num_experts') or hasattr(moe_module, 'n_experts'):
                    # The module itself handles experts internally (monolithic)
                    experts = moe_module
                    is_monolithic = True
            
            if experts is None:
                continue
            
            # CRITICAL: For monolithic MoE modules (e.g., Kimi-VL, Qwen), 
            # we need to patch the forward() method to bypass gate/router calls
            # Otherwise, the original gate will still be called inside the expert module
            if is_monolithic and hasattr(moe_module, 'forward'):
                original_forward = moe_module.forward
                
                # Create a patched forward that bypasses gate/router
                def patched_forward(hidden_states, *args, **kwargs):
                    # For monolithic MoE modules, we need to bypass the gate/router
                    # and directly call the expert computation
                    # This is a simplified version that just passes through
                    # SPECTRAMoE will handle the routing
                    if hasattr(moe_module, 'experts'):
                        # If it has experts, call the first expert (SPECTRAMoE will handle routing)
                        if isinstance(moe_module.experts, (nn.ModuleList, list)):
                            return moe_module.experts[0](hidden_states, *args, **kwargs)
                        else:
                            return moe_module.experts(hidden_states, *args, **kwargs)
                    else:
                        # Fallback: just return hidden_states (SPECTRAMoE will handle it)
                        return hidden_states
                
                moe_module.forward = patched_forward
                if layer_idx == 0:
                    logger.info(f"     🔧 Patched forward() to bypass gate/router in layer.{layer_idx}.{moe_attr} (monolithic MoE)")
            
            # Also remove gate/router attributes to prevent accidental calls
            if hasattr(moe_module, 'gate'):
                delattr(moe_module, 'gate')
                if layer_idx == 0:
                    logger.info(f"     🔧 Removed gate attribute from layer.{layer_idx}.{moe_attr}")
            elif hasattr(moe_module, 'router'):
                delattr(moe_module, 'router')
                if layer_idx == 0:
                    logger.info(f"     🔧 Removed router attribute from layer.{layer_idx}.{moe_attr}")
            
            # ✅ "Exoskeleton" Wrapping (Vertical Recurrence)
            # return_tuple=False, upcycle=False를 사용하여 RAM 절약 및 호환성 유지
            new_moe = SPECTRAMoE(
                config=spectra_config, 
                experts=experts,
                router=global_router,
                shared_experts=shared_experts,
                return_tuple=False, # Standard models expect just a tensor
                upcycle=False      # CRITICAL: Prevent cloning existing experts
            )
            
            # Move to same device/dtype as original to avoid CPU allocation
            try:
                p = next(experts.parameters())
                new_moe = new_moe.to(device=p.device, dtype=p.dtype)
            except:
                pass
                
            # Replace the old MoE module with our exoskeleton
            setattr(layer, moe_attr, new_moe)
            swapped_count += 1
            
            # Help GC
            if hasattr(moe_module, 'experts'): moe_module.experts = None
            del moe_module
            
            if swapped_count % 4 == 0:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            if layer_idx == 0:
                logger.info(f"     ✅ Wrapped layer.{moe_attr} in SPECTRAMoE Exoskeleton")

            
            # Shared experts 처리 경고
            if layer_idx == 0 and not has_shared:
                logger.info(f"     ℹ️  Note: Shared experts will remain None (model architecture doesn't use them)")
        
        logger.info(f"  ✅ Swapped routers in {swapped_count} layers")
        
        # Update model config
        # CRITICAL: Vision-Language 모델인 경우, 원본 모델의 vision_config를 유지해야 함
        if hasattr(moe_model, 'config') and hasattr(moe_model.config, 'vision_config') and moe_model.config.vision_config is not None:
            # 원본 모델의 vision_config를 spectra_config에 복사
            if not hasattr(spectra_config, 'vision_config') or spectra_config.vision_config is None:
                spectra_config.vision_config = moe_model.config.vision_config
                logger.info("  ✅ Preserved vision_config from original model")
        
        moe_model.config = spectra_config
        if hasattr(moe_model, 'model'):
            moe_model.model.config = spectra_config
        elif hasattr(moe_model, 'language_model'):
            moe_model.language_model.config = spectra_config
        
        # 3. Swap Projector
        moe_model = cls._swap_projector_to_spectra(moe_model, spectra_config)
        
        # CRITICAL: Patch forward() methods to filter framework-only arguments
        # This ensures compatibility with PEFT and other wrappers
        # DISABLE PATCH: It seems to break method binding or bypass forward logic in some cases
        # resulting in "Model did not return a loss". The standard forward handles kwargs via **lm_kwargs.
        # cls._patch_forward_for_framework_args(moe_model)
            
        return moe_model

    @classmethod
    def _discover_projector(cls, model, vision_hidden_size, text_hidden_size):
        """
        Structurally identify the multimodal projector by scanning for any Linear/MLP 
        bridging the vision hidden dimension to the text hidden dimension.
        """
        logger = logging.get_logger('transformers')
        best_candidate = None
        best_name = None
        
        # 1. Direct search in children
        for name, module in model.named_modules():
            # Filter for modules that might be the projector (usually near the top level or 'visual' block)
            # We look for something that has a Linear layer or is a Linear layer fitting our dimensions.
            if isinstance(module, nn.Linear):
                if module.in_features == vision_hidden_size and module.out_features == text_hidden_size:
                    # Found a direct match
                    return name, module
            
            # Check for bottleneck/merger structures (e.g. Qwen2-VL PatchMerger) 
            # by identifying the parameter shapes.
            params = list(module.parameters(recurse=False))
            for p in params:
                if p.dim() == 2:
                    if (p.shape[0] == text_hidden_size and p.shape[1] == vision_hidden_size) or \
                       (p.shape[1] == text_hidden_size and p.shape[0] == vision_hidden_size):
                        # Potential candidate
                        if best_candidate is None or len(name.split('.')) < len(best_name.split('.')):
                            best_candidate = module
                            best_name = name
                            
        return best_name, best_candidate

    @classmethod
    def _swap_projector_to_spectra(cls, model, spectra_config):
        """
        Dynamically identify and wrap the multimodal projector in a SPECTRA exoskeleton.
        Uses structural discovery (shape analysis) instead of hardcoded names.
        """
        logger = logging.get_logger('transformers')
        
        vision_hidden_size = getattr(spectra_config.vision_config, "hidden_size", None)
        text_hidden_size = getattr(spectra_config.text_config, "hidden_size", None)
        
        if vision_hidden_size is None or text_hidden_size is None:
            logger.warning("⚠️ Missing hidden size configs. Falling back to name-based discovery.")
            # Fallback logic if configs are incomplete
            projector_names = ["multi_modal_projector", "mm_projector", "projector", "visual_projection"]
            for name in projector_names:
                if hasattr(model, name):
                    base_projector = getattr(model, name)
                    if not isinstance(base_projector, SPECTRAMultiModalProjector):
                        setattr(model, name, SPECTRAMultiModalProjector(spectra_config, projector=base_projector))
                        return model
            return model

        # Structural discovery
        name, projector = cls._discover_projector(model, vision_hidden_size, text_hidden_size)
        
        if projector is not None and not isinstance(projector, SPECTRAMultiModalProjector):
            logger.info(f"🔄 Structurally identified projector at '{name}'. Wrapping in SPECTRA exoskeleton.")
            
            # Find the parent and replace the attribute
            if '.' in name:
                parent_name, attr_name = name.rsplit('.', 1)
                parent = model.get_submodule(parent_name)
            else:
                parent = model
                attr_name = name
                
            setattr(parent, attr_name, SPECTRAMultiModalProjector(spectra_config, projector=projector))
        else:
            logger.warning("⚠️ No multimodal projector identified structurally.")
            
        return model

    @torch.no_grad()
    def _upcycle(self, model):
        """
        [SPECTRA Generic Upcycling]
        Wraps the existing MLP (or Attention) in a SPECTRAMoE exoskeleton.
        This follows the "integrated exoskeleton" architecture where we don't 
        require specialized SPECTRA layers.
        """
        logger = logging.get_logger('transformers')
        
        # Robustly discover layers
        layers = None
        if hasattr(model, 'layers'):
            layers = model.layers
        elif hasattr(model, 'language_model') and hasattr(model.language_model, 'layers'):
            layers = model.language_model.layers
        elif hasattr(model, 'model') and hasattr(model.model, 'layers'):
            layers = model.model.layers
        elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):
            layers = model.transformer.h
            
        if layers is None:
            logger.warning(f"⚠️ Could not find layers in {type(model).__name__} for upcycling")
            return model
            
        # [Strategy Dispatch]
        # If the model is ALREADY an MoE (has experts/router), we perform Router Replacement.
        # If the model is Dense (standard MLP), we perform Sparse Upcycling (Exoskeleton).
        if len(layers) > 0:
            first_layer = layers[0]
            # Common MoE attribute names
            for attr in ['block_sparse_moe', 'mlp', 'moe', 'experts']:
                if hasattr(first_layer, attr):
                    mod = getattr(first_layer, attr)
                    # Check for MoE characteristics (experts list, router, gate)
                    # And ensure it's not already our SPECTRA MoE
                    if (hasattr(mod, 'experts') or hasattr(mod, 'expert') or hasattr(mod, 'router') or hasattr(mod, 'gate')) \
                       and not isinstance(mod, SPECTRAMoE):
                        logger.info(f"  🔄 Detected existing MoE architecture in layer.{attr}. Switching to Router Replacement strategy.")
                        # Delegate to router swapper
                        # We use self.__class__ because _swap_router_to_spectra is a classmethod
                        return self.__class__._swap_router_to_spectra(model, self.config)
                    # If we found the main MLP/MoE module but it's dense, we stop checking other attributes
                    if hasattr(mod, 'act_fn') or hasattr(mod, 'down_proj'): 
                        break

        # Robustly discover global_router
        global_router = getattr(self, 'global_router', None)
        if global_router is None:
            # Check inner models
            candidates = ['model', 'language_model', 'transformer', 'text_model']
            for c in candidates:
                if hasattr(self, c):
                    sub = getattr(self, c)
                    if hasattr(sub, 'global_router'):
                        global_router = sub.global_router
                        break
            
            # Check the target model being upcycled
            if global_router is None and hasattr(model, 'global_router'):
                global_router = model.global_router
            elif global_router is None and hasattr(model, 'language_model') and hasattr(model.language_model, 'global_router'):
                global_router = model.language_model.global_router
                
        if global_router is None:
             raise AttributeError(f"Could not find global_router in {type(self).__name__} or inner models")

        processing = tqdm(
            enumerate(layers),
            total=len(layers),
            desc=f"🔄 Upcycling with Exoskeleton",
            leave=True,
            ncols=100
        )

        upcycled_count = 0
        skipped_count = 0

        for layer_idx, decoder_layer in processing:
            # Use decoder_layer's global_router if available (preferred), otherwise fall back to discovered global_router
            layer_router = getattr(decoder_layer, 'global_router', None) or global_router
            
            # 1. MLP Upcycling (Standard MoE transformation)
            if hasattr(decoder_layer, 'mlp') and decoder_layer.mlp is not None and not isinstance(decoder_layer.mlp, SPECTRAMoE):
                base_mlp = decoder_layer.mlp
                
                # Wrap existing MLP in SPECTRAMoE
                n_shared = getattr(self.config, "n_shared_experts", 0)
                if n_shared > 1:
                    # Multiple shared experts
                    shared_experts = nn.ModuleList([
                        copy.deepcopy(base_mlp) for _ in range(n_shared)
                    ])
                elif n_shared == 1:
                    shared_experts = copy.deepcopy(base_mlp)
                else:
                    shared_experts = None
                
                # CRITICAL FIX for ZeRO-3 + Gradient Checkpointing:
                # Shared Router parameters cause "TBackward0 returned invalid gradient" errors 
                # because ZeRO-3's partitioning conflicts with re-entrant backward passes of the same shared 
                # parameter instance across multiple checkpointed layers.
                # CRITICAL: Use single global router shared across all layers
                decoder_layer.mlp = SPECTRAMoE(
                    self.config, 
                    experts=base_mlp, 
                    router=layer_router,
                    shared_experts=shared_experts,
                    return_tuple=True,
                    upcycle=True # TRANSFORM dense to MoE
                )
                upcycled_count += 1
                
                # Help memory
                if upcycled_count % 4 == 0:
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                processing.set_description(f"🔄 L{layer_idx} wrapped")
            else:
                skipped_count += 1
                processing.set_description(f"⏭️  Skip L{layer_idx}")
                
            # 2. Attention Upcycling (Optional, if configured)
            if getattr(self.config, "route_attention", False) and \
               hasattr(decoder_layer, 'self_attn') and decoder_layer.self_attn is not None and \
               not isinstance(decoder_layer.self_attn, SPECTRAMoE):
                base_attn = decoder_layer.self_attn
                # CRITICAL: Use single global router shared across all layers
                decoder_layer.self_attn = SPECTRAMoE(
                    self.config,
                    experts=base_attn,
                    router=layer_router,
                    return_tuple=True,
                    upcycle=True # TRANSFORM dense to MoE
                )
                upcycled_count += 1

        processing.set_description("✅ Upcycling complete")
        processing.close()
        
        logger.info(f"✅ Upcycling summary: {upcycled_count} layers upcycled, {skipped_count} skipped")
        
        # CRITICAL: 모든 rank에서 동일한 모듈 구조를 보장하기 위해
        # distributed 환경에서 동기화
        try:
            import torch.distributed as dist
            if dist.is_available() and dist.is_initialized():
                # 모든 rank가 upcycling을 완료할 때까지 대기
                dist.barrier()
        except:
            pass
        
        return model

    def get_parameter_groups(self):
        """
        Returns a list of parameter groups for the optimizer, which allows to apply different
        learning rates to different parts of the model. This is particularly useful for MoE models
        where components like routers and experts can benefit from different learning schedules.
        """
        
        router_params = []
        expert_params = []
        shared_expert_params = []
        attention_params = []
        other_params = []
        
        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue
            
            if 'gate.weight' in name or 'router' in name:
                router_params.append(param)
            elif 'shared_experts' in name:
                shared_expert_params.append(param)
            elif 'experts' in name:
                expert_params.append(param)
            elif 'self_attn' in name:
                attention_params.append(param)
            else:
                other_params.append(param)
        
        # In a training script, you can assign different learning rates to these groups.
        # For example:
        # optimizer_grouped_parameters = [
        #     {'params': model.get_parameter_groups()['router'], 'lr': 1e-4},
        #     {'params': model.get_parameter_groups()['expert'], 'lr': 5e-5},
        #     ...
        # ]
        return {
            'router': router_params,
            'expert': expert_params,
            'shared_expert': shared_expert_params,
            'attention': attention_params,
            'other': other_params,
        }

SPECTRA_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            Two formats are allowed:
            - a [`~cache_utils.Cache`] instance, see our
            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
            cache format.

            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
            legacy cache format will be returned.

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""

@add_start_docstrings(
    "The bare SPECTRAText Model outputting raw hidden-states without any specific head on top.",
    SPECTRA_START_DOCSTRING,
)
class SPECTRATextModel(SPECTRAPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`SPECTRATextDecoderLayer`]

    Args:
        config: SPECTRATextConfig
    """
    config: SPECTRATextConfig

    def __init__(self, config: SPECTRATextConfig, **kwargs):
        super().__init__(config)
        
        # Robustly resolve text config without relying on class identity across module boundaries
        if getattr(config, "model_type", None) == "spectra_text" or not hasattr(config, "text_config"):
            self.config = config
        else:
            self.config = config.text_config
        config = self.config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        # Expose a tensor-parallel plan for vLLM on the base text model
        if not hasattr(self, "_tp_plan") or self._tp_plan is None:
            default_tp_plan = {
                "layers.*.self_attn.q_proj": "colwise",
                "layers.*.self_attn.k_proj": "colwise",
                "layers.*.self_attn.v_proj": "colwise",
                "layers.*.self_attn.o_proj": "rowwise",
                "layers.*.moe.experts.*.gate_proj": "colwise",
                "layers.*.moe.experts.*.up_proj": "colwise",
                "layers.*.moe.experts.*.down_proj": "rowwise",
            }
            # allow overriding via config if provided
            self._tp_plan = getattr(config, "base_model_tp_plan", default_tp_plan)

        # Embedding layer
        # Safety check: if pad_token_id is out of bounds (common in extended vocabs), ignore it to prevent assert failure
        padding_idx_to_use = self.padding_idx
        if padding_idx_to_use is not None and padding_idx_to_use >= config.vocab_size:
            padding_idx_to_use = None
            
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx_to_use)
        
        # Router selection based on router_impl config
        router_impl = getattr(config, "router_impl", "spectra")
        if router_impl == "spectra":
            self.global_router = SPECTRARouter(config)
        elif router_impl == "switch_top1":
            self.global_router = SwitchRouterAdapter(config, top_k=1)
        elif router_impl == "switch_top2":
            self.global_router = SwitchRouterAdapter(config, top_k=2)
        else:
            raise ValueError(f"Unknown router_impl: {router_impl}. Supported: 'spectra', 'switch_top1', 'switch_top2'")
        self.layers = nn.ModuleList(
            [SPECTRADecoderLayer(config, layer_idx, self.global_router, **kwargs) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = SPECTRARMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = SPECTRARotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas
        # when we want to create a local RoPE layer. Config defaults should hold values for global RoPE
        config = copy.deepcopy(config)
        config.rope_theta = config.rope_local_base_freq
        config.rope_scaling = config.rope_scaling if config.rope_scaling is not None else {"rope_type":  "default"}
        self.rotary_emb_local = SPECTRARotaryEmbedding(config=config)
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    def _gradient_checkpointing_func(self, user_function, *args, **kwargs):
        """
        Override to completely disable gradient checkpointing for SPECTRADecoderLayer.
        
        CRITICAL: SPECTRADecoderLayer contains SPECTRAMoE which uses expert choice routing.
        Expert choice routing can produce different tensor shapes in forward vs backward pass,
        causing "Recomputed values have different metadata" errors with use_reentrant=False.
        
        Solution: Completely disable checkpointing for SPECTRADecoderLayer calls.
        """
        # Check if the function is SPECTRADecoderLayer.__call__
        # Also check if any argument is a SPECTRADecoderLayer instance
        # CRITICAL: Bypass checkpointing completely - Expert choice routing produces
        # different tensor shapes in forward vs backward, causing shape mismatch errors.
        return user_function(*args, **kwargs)

    @classmethod
    def from_config(cls, config=None, **kwargs):
        # Directly instantiate without going through _from_config
        # to avoid nested deepspeed.zero.Init contexts which cause meta tensor errors
        if config is None:
            config = kwargs.pop("config", None)
        return cls(config, **kwargs)

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> SPECTRACausalLMOutputWithPast:
        # PROBE: Raise error to confirm execution
        import os
        raise RuntimeError(f"PROBE: SPECTRATextModel.forward REACHED! Rank={os.getenv('RANK')}")
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        # NEFTune implementation
        if self.training:
            neftune_noise_alpha = getattr(self.config, "neftune_noise_alpha", 0.0)
            if neftune_noise_alpha > 0.0:
                dims = torch.tensor(inputs_embeds.size(1) * inputs_embeds.size(2), device=inputs_embeds.device)
                mag_norm = neftune_noise_alpha / torch.sqrt(dims)
                inputs_embeds = inputs_embeds + torch.zeros_like(inputs_embeds).uniform_(-mag_norm, mag_norm)


        if use_cache and past_key_values is None and not self.training:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens,
                past_seen_tokens + inputs_embeds.shape[1],
                device=inputs_embeds.device,
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)
        
        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        # embed positions
        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings_global = self.rotary_emb(hidden_states, position_ids)
        position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        all_router_logits = []  # 모든 MoE 레이어의 router_logits를 수집
        global_routing_hn = None
        
        # 각 layer의 loss를 누적하기 위한 리스트
        all_speciality_losses = []
        all_cosine_similarities = []
        all_contrastive_losses = []

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
        
            # CRITICAL FIX: Completely disable gradient checkpointing for all SPECTRADecoderLayer
            # Expert choice routing can produce different tensor shapes in forward vs backward,
            # causing "Recomputed values have different metadata" errors with use_reentrant=False.
            # Even if _gradient_checkpointing_func is overridden, transformers may still apply
            # checkpointing internally, so we bypass it completely by never calling _gradient_checkpointing_func
            # for SPECTRADecoderLayer.
            # 
            # Solution: Always call decoder_layer directly without checkpointing.
            # This ensures SPECTRAMoE forward is never recomputed during backward pass.
            layer_outputs = decoder_layer(
                hidden_states,
                position_embeddings_global=position_embeddings_global,
                position_embeddings_local=position_embeddings_local,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                global_routing_hn=global_routing_hn,
                **flash_attn_kwargs,
            )
            hidden_states = layer_outputs[0]
            routing_result = layer_outputs[-1]
            if routing_result is not None:
                router_logits = routing_result[0]
                if router_logits is not None:
                    all_router_logits.append(router_logits)  # 리스트에 추가
                global_routing_hn = routing_result[1]
                
                # 각 layer의 값을 리스트에 저장 (덮어쓰지 않음)
                # Unpack new 8-tuple: (router_logits, hn, speciality_loss, cosine_similarities, ortho_loss, entropy_loss, routing_uncertainty, contrastive_loss)
                layer_speciality_loss = routing_result[2]
                layer_cosine_similarities = routing_result[3]
                layer_ortho_loss = routing_result[4]
                layer_entropy_loss = routing_result[5]
                layer_routing_uncertainty = routing_result[6]
                layer_contrastive_loss = routing_result[7]
                
                if layer_speciality_loss is not None:
                    all_speciality_losses.append(layer_speciality_loss)
                if layer_cosine_similarities is not None:
                    all_cosine_similarities.append(layer_cosine_similarities)
                if layer_routing_uncertainty is not None:
                    if not hasattr(self, 'all_routing_uncertainties'):
                        self.all_routing_uncertainties = []
                    self.all_routing_uncertainties.append(layer_routing_uncertainty)
                if layer_entropy_loss is not None:
                    if not hasattr(self, 'all_entropy_losses'):
                        self.all_entropy_losses = []
                    self.all_entropy_losses.append(layer_entropy_loss)
                if layer_ortho_loss is not None:
                    if not hasattr(self, 'all_ortho_losses'):
                        self.all_ortho_losses = []
                    self.all_ortho_losses.append(layer_ortho_loss)
                if layer_contrastive_loss is not None:
                    all_contrastive_losses.append(layer_contrastive_loss)


            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        # router_logits를 튜플로 변환 (비어있으면 None)
        router_logits_tuple = tuple(all_router_logits) if all_router_logits else None
        
        # 모든 layer의 loss를 집계 (gradient 유지)
        speciality_loss = None
        cosine_similarities = None
        
        if all_speciality_losses:
            # speciality_loss는 평균 (스칼라 값들의 평균) - gradient 유지
            # CRITICAL FIX: Ensure all losses are scalars before stacking
            # Gradient checkpointing can cause shape mismatches if tensors are passed instead of scalars
            scalar_losses = []
            for loss in all_speciality_losses:
                if loss is not None:
                    if torch.is_tensor(loss):
                        # If tensor, convert to scalar by taking mean or sum
                        if loss.numel() > 1:
                            loss = loss.mean()  # Convert multi-element tensor to scalar
                        elif loss.numel() == 0:
                            continue  # Skip empty tensors
                        # loss is now a scalar tensor
                    scalar_losses.append(loss)
            
            if scalar_losses:
                stacked = torch.stack(scalar_losses)
            speciality_loss = stacked.mean()
            # gradient 명시적으로 유지
            if self.training:
                speciality_loss = speciality_loss.requires_grad_(True)
            else:
                speciality_loss = None
        
        if all_cosine_similarities:
            # cosine_similarities는 평균 (텐서들의 평균) - gradient 유지
            # CRITICAL FIX: Handle shape mismatches from gradient checkpointing
            # Filter out None values and ensure all are tensors
            valid_similarities = [cs for cs in all_cosine_similarities if cs is not None and torch.is_tensor(cs)]
            
            if valid_similarities:
                # Check if all have same shape
                first_shape = valid_similarities[0].shape
                same_shape = all(cs.shape == first_shape for cs in valid_similarities)
                
                if same_shape:
                    try:
                        stacked = torch.stack(valid_similarities)
                        cosine_similarities = stacked.mean(dim=0)
                        # gradient 명시적으로 유지
                        if self.training:
                            cosine_similarities = cosine_similarities.requires_grad_(True)
                    except RuntimeError as e:
                        # Fallback: convert each to scalar and then stack
                        means = [cs.mean() if cs.numel() > 0 else torch.tensor(0.0, device=valid_similarities[0].device, dtype=valid_similarities[0].dtype, requires_grad=True) 
                                for cs in valid_similarities]
                        if means:
                            cosine_similarities = torch.stack(means).mean()
                            if self.training:
                                cosine_similarities = cosine_similarities.requires_grad_(True)
                else:
                    # Different shapes: convert each to scalar and then stack
                    means = [cs.mean() if cs.numel() > 0 else torch.tensor(0.0, device=valid_similarities[0].device, dtype=valid_similarities[0].dtype, requires_grad=True) 
                            for cs in valid_similarities]
                    if means:
                        cosine_similarities = torch.stack(means).mean()
                        if self.training:
                            cosine_similarities = cosine_similarities.requires_grad_(True)
            else:
                cosine_similarities = None
        
        # routing_uncertainty 집계
        routing_uncertainty = None
        if hasattr(self, 'all_routing_uncertainties') and self.all_routing_uncertainties:
            # routing_uncertainty는 평균 (텐서들의 평균)
            try:
                stacked = torch.stack(self.all_routing_uncertainties)
                routing_uncertainty = stacked.mean(dim=0)
                if self.training:
                    routing_uncertainty = routing_uncertainty.requires_grad_(True)
            except RuntimeError:
                # Shape이 다른 경우 각각 평균을 내고 다시 평균
                means = [ru.mean() if torch.is_tensor(ru) and ru.numel() > 0 else torch.tensor(0.0, device=self.all_routing_uncertainties[0].device) 
                        for ru in self.all_routing_uncertainties if ru is not None]
                if means:
                    routing_uncertainty = torch.stack(means).mean()
                    if self.training:
                        routing_uncertainty = routing_uncertainty.requires_grad_(True)
            # 다음 forward를 위해 초기화
            self.all_routing_uncertainties = []
        
        # entropy_loss 집계 (CV 감소를 위한 gradient 있는 loss)
        entropy_loss = None
        if hasattr(self, 'all_entropy_losses') and self.all_entropy_losses:
            stacked = torch.stack(self.all_entropy_losses)
            entropy_loss = stacked.mean()
            if self.training:
                entropy_loss = entropy_loss.requires_grad_(True)
            # 다음 forward를 위해 초기화
            self.all_entropy_losses = []
        
        # ortho_loss 집계 (전문가들의 가중치 직교성 Loss)
        ortho_loss = None
        if hasattr(self, 'all_ortho_losses') and self.all_ortho_losses:
            stacked = torch.stack(self.all_ortho_losses)
            ortho_loss = stacked.mean()
            if self.training:
                ortho_loss = ortho_loss.requires_grad_(True)
            # 다음 forward를 위해 초기화
            self.all_ortho_losses = []
        
        # contrastive_loss 집계 (전문가 간의 대비 손실)
        contrastive_loss = None
        if all_contrastive_losses:
            stacked = torch.stack(all_contrastive_losses)
            contrastive_loss = stacked.mean()
            if self.training:
                contrastive_loss = contrastive_loss.requires_grad_(True)

        return SPECTRAModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
            router_logits=router_logits_tuple,
            speciality_loss=speciality_loss,
            cosine_similarities=cosine_similarities,
            ortho_loss=ortho_loss,
            routing_uncertainty=routing_uncertainty,
            entropy_loss=entropy_loss,
            contrastive_loss=contrastive_loss,
        )


@auto_docstring
class SPECTRAForCausalLM(SPECTRAPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}
    config: SPECTRAConfig
    base_model_prefix = "language_model"
    
    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)

    def __init__(self, config: SPECTRAConfig, **kwargs):
        super().__init__(config)
        self.model = SPECTRATextModel(config.text_config, **kwargs)
        # Ensure config refers to resolved text config from the submodule
        self.config = self.model.config
        self.vocab_size = self.config.vocab_size
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    def __call__(self, *args, **kwargs):
        """
        Override __call__ to filter out framework-only arguments before any forward call.
        This ensures that even if PEFT or other wrappers call forward() directly,
        curriculum_seqlen and other framework args are filtered out.
        """
        # CRITICAL: Filter out framework-only arguments at the top level
        # This catches calls from PEFT, DeepSpeed, and other wrappers
        framework_only_args = {
            'curriculum_seqlen',  # Trainer curriculum learning
            'num_items_in_batch',  # Trainer batch info
            'num_tokens',  # Trainer token counting
            'num_items',  # Trainer item counting
        }
        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in framework_only_args}
        return super().__call__(*args, **filtered_kwargs)

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **loss_kwargs,
    ) -> CausalLMOutputWithPast:
        r"""
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

            logits_to_keep (`int` or `torch.Tensor`, *optional*):
                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
                This is useful when using packed tensor format (single dimension for batch and sequence length).

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, SPECTRAForCausalLM

        >>> model = SPECTRAForCausalLM.from_pretrained("HuggingFaceTB/SmolVLM-256M-Instruct")
        >>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolVLM-256M-Instruct")

        >>> prompt = "What is your favorite condiment?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "What is your favorite condiment?"
        ```"""

        if self.training and self.config.attn_implementation != "eager":
            logger.warning_once(
                "It is strongly recommended to train SPECTRA models with the `eager` attention implementation "
                f"instead of `{self.config.attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`."
            )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        
        # CRITICAL: 화이트리스트 방식 - 모델의 forward() 시그니처에 명시된 파라미터만 통과
        # Trainer/프레임워크가 전달하는 모든 인자를 필터링하여 모델이 받을 수 있는 인자만 전달
        filtered_loss_kwargs = {}
        if loss_kwargs:
            try:
                import inspect
                model_sig = inspect.signature(self.model.forward)
                model_params = set(model_sig.parameters.keys())
                
                # 화이트리스트: 모델의 forward() 시그니처에 명시적으로 정의된 파라미터만 통과
                for k, v in loss_kwargs.items():
                    if k in model_params:
                        filtered_loss_kwargs[k] = v
                    # 나머지는 모두 필터링 (모델이 받지 않음)
            except Exception:
                # inspect 실패 시 모든 인자 필터링 (안전한 fallback)
                filtered_loss_kwargs = {}
        
        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            **filtered_loss_kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping

        loss = None
        aux_loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)
            
            # Speciality loss: Output orthogonality (encourages diverse expert outputs)
            # [수정] Router가 이미 Adaptive Weight를 적용했으므로, 여기서는 1.0을 사용 (이중 스케일링 방지)
            # CRITICAL FIX: Ensure all losses are scalars before adding to loss
            speciality_loss_coef = getattr(self.model.config, "speciality_loss_coef", 1.0)  # 0.02 -> 1.0
            if outputs.speciality_loss is not None and speciality_loss_coef > 0:
                spec_loss = outputs.speciality_loss
                # Convert to scalar if tensor
                if torch.is_tensor(spec_loss) and spec_loss.numel() > 1:
                    spec_loss = spec_loss.mean()
                elif torch.is_tensor(spec_loss) and spec_loss.numel() == 0:
                    spec_loss = torch.tensor(0.0, device=loss.device, dtype=loss.dtype, requires_grad=True)
                loss = loss + spec_loss * speciality_loss_coef
            
            # Expression projector loss: Ensure expression_logits contributes to loss for gradient flow
            # cosine_similarities (domain_orthogonality)를 loss에 추가하여 expression_projector가 학습되도록 함
            cosine_similarities_loss_coef = getattr(self.model.config, "cosine_similarities_loss_coef", 0.001)
            if outputs.cosine_similarities is not None and cosine_similarities_loss_coef > 0:
                # cosine_similarities는 [batch, seq, num_experts] 형태의 텐서 또는 스칼라
                if torch.is_tensor(outputs.cosine_similarities) and outputs.cosine_similarities.numel() > 0:
                    # 텐서인 경우 mean squared value를 최소화하여 expression diversity를 유지
                    expr_loss = torch.mean(outputs.cosine_similarities ** 2) * cosine_similarities_loss_coef
                    # Ensure scalar
                    if expr_loss.numel() > 1:
                        expr_loss = expr_loss.mean()
                    loss = loss + expr_loss
                elif isinstance(outputs.cosine_similarities, (int, float)):
                    # 스칼라인 경우 직접 사용
                    expr_loss = torch.tensor(outputs.cosine_similarities * cosine_similarities_loss_coef, device=loss.device, dtype=loss.dtype, requires_grad=True)
                    loss = loss + expr_loss
            
            # [Sharpening] Entropy Minimization: "한 놈만 패라" (확실한 전문가 선택)
            # router_entropy_coef는 양수로 사용 (entropy를 낮추는 방향)
            router_entropy_coef = getattr(self.model.config, "router_entropy_coef", 0.1)
            if outputs.entropy_loss is not None and router_entropy_coef > 0:
                ent_loss = outputs.entropy_loss
                # Convert to scalar if tensor
                if torch.is_tensor(ent_loss) and ent_loss.numel() > 1:
                    ent_loss = ent_loss.mean()
                elif torch.is_tensor(ent_loss) and ent_loss.numel() == 0:
                    ent_loss = torch.tensor(0.0, device=loss.device, dtype=loss.dtype, requires_grad=True)
                loss = loss + ent_loss * router_entropy_coef
            
            # [Optional] Ortho Loss: 보험으로 약하게 유지 (학습 초반 가이드)
            # Sinkhorn + Sharpening만으로도 분리가 되지만, 초반 헤매지 않도록 도움
            ortho_loss_coef = getattr(self.model.config, "ortho_loss_coef", 0.01)  # 0.05 -> 0.01 (약하게)
            if outputs.ortho_loss is not None and ortho_loss_coef > 0:
                ortho_loss_val = outputs.ortho_loss
                # Convert to scalar if tensor
                if torch.is_tensor(ortho_loss_val) and ortho_loss_val.numel() > 1:
                    ortho_loss_val = ortho_loss_val.mean()
                elif torch.is_tensor(ortho_loss_val) and ortho_loss_val.numel() == 0:
                    ortho_loss_val = torch.tensor(0.0, device=loss.device, dtype=loss.dtype, requires_grad=True)
                loss = loss + ortho_loss_val * ortho_loss_coef

        try:
            import torch.distributed as dist
            is_main_proc = (not dist.is_available()) or (not dist.is_initialized()) or dist.get_rank() == 0
        except Exception:
            is_main_proc = True


        return SPECTRACausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            aux_loss=aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            ortho_loss=outputs.ortho_loss,
            cosine_similarities=outputs.cosine_similarities,
            entropy_loss=outputs.entropy_loss,
        )


class SPECTRAMultiModalProjector(nn.Module):
    def __init__(self, config: SPECTRAConfig, projector: Optional[nn.Module] = None, **kwargs):
        super().__init__()
        self.config = config

        # Wrapped projector (Integrated Exoskeleton)
        self.projector = projector
        
        # Scratch build components (only used if no projector is provided)
        if self.projector is None:
            self.mm_input_projection_weight = nn.Parameter(
                torch.zeros(config.vision_config.hidden_size, config.text_config.hidden_size)
            )

        self.mm_soft_emb_norm = SPECTRARMSNorm(
            config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps
        )

        self.patches_per_image = int(config.vision_config.image_size // config.vision_config.patch_size)
        self.tokens_per_side = int(config.mm_tokens_per_image**0.5)
        self.kernel_size = self.patches_per_image // self.tokens_per_side
        self.avg_pool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=self.kernel_size)

    def forward(self, vision_outputs: torch.Tensor, *args, **kwargs):
        # 1. Use wrapped projector if available (Exoskeleton mode)
        if self.projector is not None:
            # Pass all arguments through for complex projectors (e.g. Qwen2-VL PatchMerger)
            return self.projector(vision_outputs, *args, **kwargs)
            
        # 2. Fallback / Scratch logic
        batch_size, _, seq_length = vision_outputs.shape
        reshaped_vision_outputs = vision_outputs.transpose(1, 2)
        reshaped_vision_outputs = reshaped_vision_outputs.reshape(
            batch_size, seq_length, self.patches_per_image, self.patches_per_image
        )
        reshaped_vision_outputs = reshaped_vision_outputs.contiguous()

        pooled_vision_outputs = self.avg_pool(reshaped_vision_outputs)
        pooled_vision_outputs = pooled_vision_outputs.flatten(2)
        pooled_vision_outputs = pooled_vision_outputs.transpose(1, 2)

        normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)

        projected_vision_outputs = torch.matmul(normed_vision_outputs, self.mm_input_projection_weight)
        return projected_vision_outputs.type_as(vision_outputs)


def token_type_ids_mask_function(
    token_type_ids: Optional[torch.Tensor],
    image_group_ids: Optional[torch.Tensor],
    tokens_per_image: int,
) -> Optional[Callable]:
    """
    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,
    not start and end indices.
    """
    # Do not return an additional mask in this case
    if token_type_ids is None:
        return None

    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:
        # If it's 1 for both query and key/value, we are in an image block
        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length
        # Since vmap doesn't support `if statement` we workaround it with `torch.where`
        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)
        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]
        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)

        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]
        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)

        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)
        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx

        # This is bidirectional attention whenever we are dealing with image tokens
        return is_image_block & same_image_block

    return inner_mask


@auto_docstring(
    custom_intro="""
    The Base SPECTRA model which consists of a vision backbone and a language model withou language modeling head.,
    """
)
class SPECTRAModel(SPECTRAPreTrainedModel):
    config: SPECTRAConfig
    _checkpoint_conversion_mapping = {"language_model.model": "language_model"}
    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch
    accepts_loss_kwargs = False

    def __init__(self, config: SPECTRAConfig, vision_tower=None):
        r"""
        Args:
            config ([`SPECTRAConfig`]):
                Model configuration class with all the parameters of the model.
            vision_tower ([`nn.Module`], *optional*):
                The vision tower to use. If not provided, it will be initialized from `config.vision_config`.
        """
        super().__init__(config)
        
        # CRITICAL REFACTOR: Wrapper Pattern ("Exoskeleton")
        # We initialize the FULL base VLM model (including LM Head) and swap its text backbone.
        
        # Prevent Recursion & Enforce Base Loading:
        # The AutoFactory uses config.model_type to decide which model class to load.
        # If model_type is 'spectra', it loads SPECTRAModel again -> Recursion.
        # We MUST change model_type to the base model's type.
        
        base_config = copy.deepcopy(config)
        
        # GENERIC BACKBONE DETECTION
        # 1. Determine base model type
        base_model_type = "idefics3" # Default fallback for compatibility
        
        # Check for clues in vision config
        if hasattr(base_config, "vision_config") and base_config.vision_config:
                v_type = getattr(base_config.vision_config, "model_type", "")
                if v_type == "siglip":
                    base_model_type = "idefics3"
                elif v_type == "clip":
                    base_model_type = "llava"
                elif v_type == "qwen2":
                    base_model_type = "qwen2_vl"

        # Check override
        if hasattr(base_config, "base_model_type") and base_config.base_model_type:
            base_model_type = base_config.base_model_type

        logger.info(f"SPECTRAModel: Detected base backbone type: '{base_model_type}'")

        # 2. Configure for AutoModel - CRITICAL RECURSION FIX
        # Fix: Create a FRESH config object of the target type (e.g. Idefics3Config) by serializing to dict.
        
        base_config_dict = base_config.to_dict()
        # Remove model_type from dict because it is passed as positional arg to for_model
        if "model_type" in base_config_dict:
            del base_config_dict["model_type"]
        base_config_dict["architectures"] = None
        
        # Re-create config as a generic AutoConfig (or specific type if inferred)
        # This breaks the link to SPECTRAConfig class
        base_config = AutoConfig.for_model(base_model_type, **base_config_dict)
        
        # 3. Load Base Model (Body)
        # We use AutoModel to load the Body (BaseModelOutput) instead of Vision2Seq (which might load Head).
        # This ensures SPECTRAForConditionalGeneration is the only Head.
        try:
             self.model = AutoModel.from_config(base_config)
        except ValueError:
             # If AutoModel fails (e.g. VLM not mapped to AutoModel), try Vision2Seq but STRICTLY.
             # No catch-all fallback.
             self.model = AutoModelForVision2Seq.from_config(base_config)


        # Now SWAP the text model with our MoE-fied SPECTRATextModel
        # Access path depends on model structure (ForCausalLM vs Model)
        
        target_model = self.model
        if hasattr(self.model, "model"):
            target_model = self.model.model
            
        moe_text_model = SPECTRATextModel.from_config(config=config.text_config, trust_remote_code=True)
        
        if hasattr(target_model, "text_model"):
            target_model.text_model = moe_text_model
        elif hasattr(target_model, "language_model"):
            target_model.language_model = moe_text_model
        else:
            # Maybe self.model IS the text model? No, we used Vision2Seq.
            raise ValueError(f"Could not find text_model or language_model in {type(target_model)}")

        # Expose components (properties)
        self.vocab_size = config.text_config.vocab_size
        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1
        self.post_init()

    @property
    def vision_tower(self):
        # Access vision model from wrapped model
        base = self.model.model if hasattr(self.model, "model") else self.model
        return base.vision_model if hasattr(base, 'vision_model') else base.vision_tower

    @property
    def language_model(self):
        base = self.model.model if hasattr(self.model, "model") else self.model
        return base.text_model if hasattr(base, 'text_model') else base.language_model

    @property
    def multi_modal_projector(self):
        base = self.model.model if hasattr(self.model, "model") else self.model
        return base.connector if hasattr(base, 'connector') else base.multi_modal_projector
    
    # Delegate LM Head access if needed
    @property
    def lm_head(self):
        return self.model.lm_head if hasattr(self.model, "lm_head") else None

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        # Update swapped model
        base = self.model.model if hasattr(self.model, "model") else self.model
        if hasattr(base, "text_model"):
            base.text_model = decoder
        else:
            base.language_model = decoder

    def get_decoder(self):
        return self.language_model

    # get_image_features REMOVED/MINIMIZED: Base model handles this.
    # If explicit call is needed, just delegate. The base model (Idefics3) architecture
    # handles vision feature extraction.
    
    def get_image_features(self, pixel_values: torch.Tensor):
        base = self.model.model if hasattr(self.model, "model") else self.model
        if hasattr(base, "get_image_features"):
            return base.get_image_features(pixel_values)
        # If base doesn't expose it, we assume we don't need to call it manually
        # as forward() handles it. Returning None or raising warning if called incorrectly.
        raise NotImplementedError("SPECTRAModel delegates vision. Use forward() or access base model directly.")

    def get_placeholder_mask(
        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor
    ):
        """
        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is
        equal to the length of multimodal features. If the lengths are different, an error is raised.
        """
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        n_image_features = image_features.shape[0] * image_features.shape[1]
        if inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
            )
        return special_image_mask

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **lm_kwargs,
    ) -> Union[tuple, SPECTRAModelOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss.
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # CRITICAL: Trainer/프레임워크 전용 인자를 명시적으로 제거 (최우선 필터링)
        # 이들은 모델이 받지 않으므로 먼저 제거
        framework_only_args = {
            'curriculum_seqlen',  # Trainer curriculum learning
            'num_items_in_batch',  # Trainer batch info
            'num_tokens',  # Trainer token counting
            'num_items',  # Trainer item counting
        }
        if lm_kwargs:
            lm_kwargs = {k: v for k, v in lm_kwargs.items() if k not in framework_only_args}

        # DELEGATION STRATEGY: 
        # Instead of manual fusion (which is incompatible with multi-crop backbones like Idefics3),
        # we delegate the forward pass to the base VLM model.
        # Since we have swapped the base model's text backbone with our SPECTRATextModel (MoE),
        # this delegation will correctly handle multimodal input AND include MoE layers.
        
        # PROBE: Raise error to confirm execution
        raise RuntimeError(f"PROBE: SPECTRAModel.forward REACHED! Rank={os.getenv('RANK')}")

        outputs = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            token_type_ids=token_type_ids,
            cache_position=cache_position,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **lm_kwargs,
        )

        if not return_dict:
            return outputs

        # Wrap in SPECTRAModelOutputWithPast to satisfy the Head's expectations.
        # We attempt to propagate MoE metrics if they are present in the delegated output.
        return SPECTRAModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values if hasattr(outputs, "past_key_values") else None,
            hidden_states=outputs.hidden_states if hasattr(outputs, "hidden_states") else None,
            attentions=outputs.attentions if hasattr(outputs, "attentions") else None,
            image_hidden_states=getattr(outputs, "image_hidden_states", None),
            # Propagate MoE metrics from the text backbone
            router_logits=getattr(outputs, "router_logits", None),
            speciality_loss=getattr(outputs, "speciality_loss", None),
            cosine_similarities=getattr(outputs, "cosine_similarities", None),
            contrastive_loss=getattr(outputs, "contrastive_loss", None),
        )


@add_start_docstrings(
    """The SPECTRA model which consists of a vision backbone and a language model.""",
    SPECTRA_START_DOCSTRING,
)
class SPECTRAForConditionalGeneration(SPECTRAPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {
        "^language_model.model": "model.language_model",
        "^vision_tower": "model.vision_tower",
        "^multi_modal_projector": "model.multi_modal_projector",
        "^language_model.lm_head": "lm_head",
    }
    _tied_weights_keys = ["lm_head.weight"]
    
    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)
  
    def __init__(self, config: SPECTRAConfig, vision_tower=None, **kwargs):
        super().__init__(config)
        self.model = SPECTRAModel(config, vision_tower=vision_tower)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_image_features(self, pixel_values):
        return self.model.get_image_features(pixel_values)

    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **lm_kwargs,
    ) -> Union[Tuple, SPECTRACausalLMOutputWithPast]:
        
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Calls SPECTRAModel (which delegates to Idefics3Model Body)
        # Expects receiving BaseModelOutputWithPast or something similar (hidden states)
        outputs = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
            **lm_kwargs,
        )

        hidden_states = outputs[0]
        # Only compute necessary logits
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        
        if self.config.text_config.final_logit_softcapping is not None:
             logits = logits / self.config.text_config.final_logit_softcapping
             logits = torch.tanh(logits)
             logits = logits * self.config.text_config.final_logit_softcapping

        loss = None
        if labels is not None:
            logits = logits.float()
            shift_logits = logits[..., :-1, :]
            shift_labels = labels[..., 1:]
            
            # Since attention_mask might be 4D (flash attn) or 2D
            # We assume standard CausalLM logic
            loss_fct = nn.CrossEntropyLoss()
            flat_logits = shift_logits.reshape(-1, self.config.text_config.vocab_size)
            flat_labels = shift_labels.reshape(-1).to(shift_logits.device)
            loss = loss_fct(flat_logits, flat_labels)

        # Re-add auxiliary losses from MoE router if present in outputs
        # outputs might be Idefics3Model output, so it might not have these custom keys directly
        # But our SPECTRATextModel returns them. Idefics3Model propagates them?
        # Idefics3Model output class is unlikely to have `router_logits` field unless dynamic.
        # But if it returns standard tuple, we lose them.
        # If Idefics3Model returns dict (BaseModelOutputWithPast), extra keys might be lost or in `hidden_states`?
        # Actually transformers ModelOutput allows random kwargs? No.
        
        # NOTE: Idefics3Model wraps TextModel.
        # If TextModel returns extra keys, they might be dropped by Idefics3Model forwarding unless it just passes `**kwargs`.
        # Taking a risk here: The log said keys WERE returned: "last_hidden_state, router_logits..."
        # So Idefics3Model IS propagating them! Great.
        
        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return SPECTRACausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values if hasattr(outputs, "past_key_values") else None,
            hidden_states=outputs.hidden_states if hasattr(outputs, "hidden_states") else None,
            attentions=outputs.attentions if hasattr(outputs, "attentions") else None,
            image_hidden_states=outputs.image_hidden_states if hasattr(outputs, "image_hidden_states") else None,
            router_logits=getattr(outputs, "router_logits", None),
            speciality_loss=getattr(outputs, "speciality_loss", None),
            cosine_similarities=getattr(outputs, "cosine_similarities", None),
            contrastive_loss=getattr(outputs, "contrastive_loss", None),
        )

    # Make modules available through conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def vision_tower(self):
        return self.model.vision_tower

    @property
    def multi_modal_projector(self):
        return self.model.multi_modal_projector

    def __call__(self, *args, **kwargs):
        """
        Override __call__ to filter out framework-only arguments before any forward call.
        This ensures that even if PEFT or other wrappers call forward() directly,
        curriculum_seqlen and other framework args are filtered out.
        """
        # CRITICAL: Filter out framework-only arguments at the top level
        # This catches calls from PEFT, DeepSpeed, and other wrappers
        framework_only_args = {
            'curriculum_seqlen',  # Trainer curriculum learning
            'num_items_in_batch',  # Trainer batch info
            'num_tokens',  # Trainer token counting
            'num_items',  # Trainer item counting
        }
        filtered_kwargs = {k: v for k, v in kwargs.items() if k not in framework_only_args}
        return super().__call__(*args, **filtered_kwargs)


    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        pixel_values=None,
        attention_mask=None,
        token_type_ids=None,
        use_cache=True,
        logits_to_keep=None,
        labels=None,
        **kwargs,
    ):
        # Overwritten -- custom `position_ids` and `pixel_values` handling
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
            logits_to_keep=logits_to_keep,
            token_type_ids=token_type_ids,
            **kwargs,
        )

        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore
        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always
        if cache_position[0] == 0:
            model_inputs["pixel_values"] = pixel_values

        return model_inputs
    
    @staticmethod
    def create_masks_for_generate(
        config: PretrainedConfig,
        input_embeds: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        cache_position: torch.Tensor,
        past_key_values: Optional[Cache],
        position_ids: Optional[torch.Tensor],
        token_type_ids: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> dict:
        # Prepare mask arguments
        mask_kwargs = {
            "config": config.get_text_config(),
            "input_embeds": input_embeds,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Add the token type ids mask for generate as well
        if token_type_ids is not None and input_embeds.shape[1] != 1:
            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

            # First find where a new image block starts: 1 if image and previous not image
            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
            is_image = (token_type_ids == 1).to(cache_position.device)
            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))
            mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image
            )

        return create_masks_for_generate(**mask_kwargs)


class SPECTRARouterTrainingMonitor:
    """
    PyTorch 학습 루프에서 라우터가 '실제로' 학습되는지 지속 모니터링하는 콜백.
    - on_batch_start: step 스냅샷(파라미터 값) 저장
    - on_after_backward: grad norm/분포/엔트로피/EMA/보조 로스 로깅
    - on_step_end: 파라미터 업데이트량(delta) 확인
    사용자는 학습 루프에서 각 타이밍에 해당 메서드를 호출하면 됩니다.
    """
    def __init__(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        log_every: int = 100,
        log_fn: Optional[Callable[[str], None]] = None,
    ):
        self.model = model
        self.optimizer = optimizer
        self.log_every = max(int(log_every), 1)
        self.log_fn = log_fn if log_fn is not None else (lambda msg: logger.info(msg))
        self._step = 0
        self._pre_step_snapshots: dict[str, dict[str, torch.Tensor]] = {}

    def _iter_router_modules(self):
        for name, module in self.model.named_modules():
            if getattr(module, "_is_spectra_router", False) or isinstance(module, SPECTRARouter):
                yield name, module

    @staticmethod
    def _check_requires_grad(module: nn.Module) -> bool:
        params = list(module.parameters(recurse=True))
        return len(params) > 0 and all(p.requires_grad for p in params)

    def _check_in_optimizer(self, module: nn.Module) -> bool:
        if self.optimizer is None:
            return False
        target_ids = {id(p) for p in module.parameters(recurse=True)}
        if not target_ids:
            return False
        opt_ids = set()
        for group in self.optimizer.param_groups:
            for p in group.get("params", []):
                opt_ids.add(id(p))
        return target_ids.issubset(opt_ids)

    @staticmethod
    def _grad_norms(module: nn.Module) -> dict[str, float]:
        norms: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if p.grad is not None:
                # 작은 수치 노이즈는 0으로 취급하지 않음
                norms[n] = float(p.grad.detach().norm().item())
        return norms

    @staticmethod
    def _snapshot_params(module: nn.Module) -> dict[str, torch.Tensor]:
        return {n: p.detach().clone() for n, p in module.named_parameters(recurse=True)}

    @staticmethod
    def _param_deltas(before: dict[str, torch.Tensor], module: nn.Module) -> dict[str, float]:
        deltas: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if n in before:
                deltas[n] = float((before[n] - p.detach()).abs().sum().item())
        return deltas

    @staticmethod
    def _router_usage_and_entropy(router_logits: Optional[Union[torch.Tensor, Tuple[torch.Tensor, ...]]]) -> tuple[Optional[List[float]], Optional[float]]:
        if router_logits is None:
            return None, None
        if isinstance(router_logits, tuple):
            if not router_logits:
                return None, None
            probs = torch.cat([t for t in router_logits if t is not None and t.numel() > 0], dim=0)
        else:
            probs = router_logits
        if probs.numel() == 0:
            return None, None
        # 본 코드 경로에서는 router_logits가 '확률'로 전달되는 경우가 많음
        p = probs.clamp_min(1e-12)
        num_experts = p.size(-1)
        top1 = p.argmax(dim=-1)
        usage = torch.bincount(top1, minlength=num_experts).float()
        usage = usage / usage.sum().clamp_min(1.0)
        entropy = float((-(p * p.log()).sum(dim=-1)).mean().item())
        return usage.tolist(), entropy

    def on_batch_start(self):
        self._step += 1
        self._pre_step_snapshots.clear()
        for name, module in self._iter_router_modules():
            self._pre_step_snapshots[name] = self._snapshot_params(module)

    def on_after_backward(
        self,
        outputs: Optional[Union[SPECTRACausalLMOutputWithPast, SPECTRAModelOutputWithPast]] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        if self._step % self.log_every != 0:
            return

        # 라우터 모듈별 상태/grad
        lines = []
        for name, module in self._iter_router_modules():
            req = self._check_requires_grad(module)
            inopt = self._check_in_optimizer(module)
            norms = self._grad_norms(module)
            any_grad = any(v > 0.0 for v in norms.values())
            gsum = sum(norms.values()) if norms else 0.0
            gmax = max(norms.values()) if norms else 0.0
            lines.append(f"[router:{name}] requires_grad={req} in_optimizer={inopt} any_grad={any_grad} grad_sum={gsum:.6f} grad_max={gmax:.6f}")

        # 사용 분포/엔트로피
        usage, entropy = (None, None)
        if outputs is not None and hasattr(outputs, "router_logits"):
            usage, entropy = self._router_usage_and_entropy(outputs.router_logits)
            if usage is not None:
                lines.append(f"[routing] usage(top1_ratio)={','.join(f'{u:.3f}' for u in usage)}")
            if entropy is not None:
                lines.append(f"[routing] entropy={entropy:.6f}")

        # EMA (가능한 경우)
        try:
            # ForCausalLM -> .model.global_router, TextModel -> .global_router
            global_router = None
            if hasattr(self.model, "model") and hasattr(self.model.model, "global_router"):
                global_router = self.model.model.global_router
            elif hasattr(self.model, "global_router"):
                global_router = self.model.global_router
            if global_router is not None and hasattr(global_router, "expert_load_ema"):
                ema = global_router.expert_load_ema.detach().float()
                s = float(ema.sum().item())
                ema_norm = (ema / s) if s > 0 else ema
                lines.append(f"[ema] expert_load_ema_norm={','.join(f'{float(x):.3f}' for x in ema_norm.tolist())}")
        except Exception:
            pass

        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))

        # 선택적으로 보조 로스도 로깅(계산 비용 낮음)
        try:
            if outputs is not None and hasattr(outputs, "router_logits") and outputs.router_logits is not None:
                if hasattr(self.model, "model") and hasattr(self.model.model, "config"):
                    cfg = self.model.model.config
                elif hasattr(self.model, "config"):
                    cfg = self.model.config
                else:
                    cfg = None
                if cfg is not None:
                    aux = load_balancing_loss_func(
                        outputs.router_logits,
                        cfg.n_routed_experts,
                        getattr(cfg, "num_experts_per_tok", 2),
                        attention_mask,
                        router_z_loss_coef=getattr(cfg, "router_z_loss_coef", None),
                        router_entropy_coef=getattr(cfg, "router_entropy_coef", None),
                        usage_uniformity_coef=getattr(cfg, "usage_uniformity_coef", None),
                    ).detach().float().item()
                    self.log_fn(f"[step {self._step}] aux_lb_loss={aux:.6f}")
        except Exception:
            pass

    def on_step_end(self):
        if self._step % self.log_every != 0:
            return
        lines = []
        for name, module in self._iter_router_modules():
            before = self._pre_step_snapshots.get(name, {})
            deltas = self._param_deltas(before, module)
            delta_sum = sum(deltas.values()) if deltas else 0.0
            lines.append(f"[router:{name}] param_delta_sum={delta_sum:.6f}")
        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))
        self._pre_step_snapshots.clear()


__all__ = [
    "SPECTRAPreTrainedModel",
    "SPECTRATextModel",
    "SPECTRAForCausalLM",
    "SPECTRAForConditionalGeneration",
    "SPECTRAModel",
    "SPECTRARouterTrainingMonitor",
]
