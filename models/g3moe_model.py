#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨
#           This file was automatically generated from src/transformers/models/G3MoE/modular_G3MoE.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_G3MoE.py file directly. One of our CI enforces this.
#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨
# coding=utf-8
# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import copy
import os
import inspect
import math
from functools import partial
from collections.abc import Callable
from dataclasses import dataclass
from typing import List, Optional, Tuple, Union, Type

from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
# Add dynamo import for torch.compile compatibility
import torch._dynamo

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, HybridCache, StaticCache, DynamicCache
from transformers.generation.utils import GenerationMixin
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask
from transformers.processing_utils import Unpack
from transformers.utils import logging
from transformers.utils.doc import (
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
    add_start_docstrings,
)
from transformers.utils.generic import (
    ModelOutput,
    can_return_tuple,
)
from transformers.utils import auto_docstring
from transformers.utils.import_utils import (
    is_torchdynamo_compiling,
    is_torch_flex_attn_available,
    is_flash_attn_2_available
)
from transformers.modeling_utils import (
    restore_default_dtype,
    SpecificPreTrainedModelType,
)
from transformers.configuration_utils import PretrainedConfig
from transformers import logging
from transformers.utils.deprecation import deprecate_kwarg
from transformers import AutoModel, AutoConfig, AutoModelForCausalLM
from transformers.modeling_utils import set_initialized_submodules
from .g3moe_config import G3MoEConfig, G3MoETextConfig

if is_torch_flex_attn_available():
    from torch.nn.attention.flex_attention import BlockMask
    from transformers.integrations.flex_attention import make_flex_block_causal_mask

    
logger = logging.get_logger(__name__)
_CONFIG_FOR_DOC = "G3MoEConfig"


def calculate_ortho_loss_for_experts(expert_weights: List[torch.Tensor]) -> torch.Tensor:
    """
    Calculates the orthogonalization loss for a set of expert weights from a single MoE layer.
    This loss encourages functional diversity among experts by penalizing similarity
    in their weight spaces. The loss is the squared Frobenius norm
    of (VV' - I) where V is the matrix of normalized expert weights.
    """
    if not expert_weights:
        return torch.tensor(0.0, device=expert_weights[0].device)

    flattened_weights = [w.view(-1) for w in expert_weights]
    V = torch.stack(flattened_weights)

    # Normalize rows to be unit vectors, preventing weights from collapsing to zero
    V = F.normalize(V, p=2, dim=1)
    
    # Gram matrix: V @ V.T
    gram_matrix = torch.matmul(V, V.t())
    
    # Target: identity matrix
    identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
    
    # Loss: squared Frobenius norm of (VV' - I)
    ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
    return ortho_loss


def _orthogonal_constraint_loss(
    num_experts: int, 
    gate_logits: torch.Tensor
) -> torch.Tensor:
        """라우터 출력값들의 직교성 제약 손실"""
        # router_outputs: [batch*seq, num_experts]
        
        # 각 토큰별로 expert 방향이 직교하도록
        normalized_outputs = F.normalize(gate_logits, dim=-1)
        
        # Gram matrix: [num_experts, num_experts]
        gram_matrix = torch.matmul(normalized_outputs.T, normalized_outputs)
        
        # Target: identity matrix
        identity = torch.eye(num_experts, device=gate_logits.device, dtype=gate_logits.dtype)
        
        # Loss: squared Frobenius norm of (GG' - I)
        constraint_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
        return constraint_loss

def load_balancing_loss_func(
    gate_logits: torch.Tensor,
    num_experts: int,
    top_k: int = 2,
    attention_mask: Optional[torch.Tensor] = None,
    router_z_loss_coef: Optional[float] = None,
    router_entropy_coef: Optional[float] = None,
    usage_uniformity_coef: Optional[float] = None,
) -> torch.Tensor:
    r"""
    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.
    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss
    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between
    experts is too unbalanced.
    Args:
        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):
            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of
            shape [batch_size X sequence_length, num_experts].
        attention_mask (`torch.Tensor`, None):
            The attention_mask used in forward function
            shape [batch_size X sequence_length] if not None.
        num_experts (`int`, *optional*):
            Number of experts
        router_z_loss_coef (`float`, *optional*):
            Coefficient for the z-loss term in the load balancing loss.
    Returns:
        The auxiliary loss.
    """
    if gate_logits is None or not isinstance(gate_logits, tuple):
        return torch.tensor(0.0)

    if isinstance(gate_logits, tuple):
        # Add a check for empty tuple
        if not gate_logits:
            return torch.tensor(0.0)
        compute_device = gate_logits[0].device
        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)
    else:
        # handle tensor input
        concatenated_gate_logits = gate_logits

    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)

    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)

    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)

    if attention_mask is None:
        # Compute the percentage of tokens routed to each experts
        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.mean(routing_weights, dim=0)
    else:
        batch_size, sequence_length = attention_mask.shape
        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)

        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask
        expert_attention_mask = (
            attention_mask[None, :, :, None, None]
            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))
            .reshape(-1, top_k, num_experts)
            .to(compute_device)
        )

        # Compute the percentage of tokens routed to each experts
        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(
            expert_attention_mask, dim=0
        )

        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert
        router_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))
            .reshape(-1, num_experts)
            .to(compute_device)
        )

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(
            router_per_expert_attention_mask, dim=0
        )

    # Core Switch-style load balancing loss
    aux_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0)) * num_experts

    # Router z-loss (Switch Transformer) to prevent overconfident routers
    if router_z_loss_coef is not None and router_z_loss_coef > 0:
        log_z = torch.logsumexp(concatenated_gate_logits, dim=-1)
        z_loss = torch.square(log_z).mean()
        aux_loss = aux_loss + router_z_loss_coef * z_loss

    # Entropy regularization to avoid routing collapse (maximize entropy)
    if router_entropy_coef is not None and router_entropy_coef > 0:
        token_entropy = -(routing_weights * torch.log(routing_weights.clamp_min(1e-12))).sum(dim=-1)
        # Normalize by log(num_experts) for scale invariance across different expert counts
        normalized_entropy = token_entropy / math.log(max(num_experts, 2))
        entropy_reg = -normalized_entropy.mean()
        aux_loss = aux_loss + router_entropy_coef * entropy_reg

    # Usage uniformity (optional): encourage average routing probability per expert to be near-uniform
    if usage_uniformity_coef is not None and usage_uniformity_coef > 0:
        # router_prob_per_expert already accounts for attention mask when provided
        target = torch.full_like(router_prob_per_expert, 1.0 / float(num_experts))
        usage_uniformity_loss = torch.mean(torch.square(router_prob_per_expert - target))
        aux_loss = aux_loss + usage_uniformity_coef * usage_uniformity_loss

    return aux_loss


# Copied from Phi-3.5-MoE
def _get_unpad_data(attention_mask):
    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)
    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
    max_seqlen_in_batch = seqlens_in_batch.max().item()
    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))
    return (
        indices,
        cu_seqlens,
        max_seqlen_in_batch,
    )
    
@dataclass
class G3MoEModelOutputWithPast(BaseModelOutputWithPast):
    """
    Base class for G3MoE outputs, with hidden states and attentions.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.
    """

    image_hidden_states: Optional[torch.FloatTensor] = None
    aux_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[torch.FloatTensor] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    expression_loss: Optional[torch.FloatTensor] = None


@dataclass
class G3MoECausalLMOutputWithPast(ModelOutput):
    """
    Base class for G3MoE causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    image_hidden_states: Optional[torch.FloatTensor] = None
    # this is moe specific
    aux_loss: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[torch.FloatTensor] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    expression_loss: Optional[torch.FloatTensor] = None
    # hn_context 제거 - 차원 문제로 인해 사용하지 않음


class G3MoETextScaledWordEmbedding(nn.Embedding):
    """
    This module overrides nn.Embeddings' forward by multiplying with embeddings scale.
    """

    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        self.register_buffer("embed_scale", torch.tensor(embed_scale), persistent=False)

    def forward(self, input_ids: torch.Tensor):
        return super().forward(input_ids) * self.embed_scale


class G3MoEMLP(nn.Module):
    def __init__(self, config: G3MoETextConfig, intermediate_size: Optional[int]=None, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        # Mark for fast init path
        setattr(self.gate_proj, "_is_g3moe_gate_layer", True)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_activation]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class mp(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx, 
        scores: torch.Tensor, 
        multiplier: torch.Tensor, 
        selected_experts: torch.Tensor,
        masked_gates: torch.Tensor,
        mask_for_one: torch.Tensor,
    ):
        ctx.save_for_backward(multiplier, selected_experts, masked_gates)
        return multiplier * mask_for_one
        
    @staticmethod
    def backward(
        ctx, 
        grad_at_output: torch.Tensor, 
    ):
        multiplier, selected_experts, masked_gates = ctx.saved_tensors
        
        grad_at_output = grad_at_output * multiplier
        
        grad_at_scores_expaned = masked_gates * grad_at_output.mul(-1)
        grad_at_scores_expaned.scatter_add_(
            dim=-1,
            index=selected_experts,
            src=grad_at_output,
        )
        
        return (
            grad_at_scores_expaned, 
            None, 
            None, 
            None, 
            None, 
        )


class ExpressionProjector(nn.Module):
    """
    Expression projector for expert specialization with pre-computed orthogonal matrix
    Encourages each expert to learn unique expression patterns through orthogonal projection
    """

    def __init__(self, input_dim, output_dim, num_experts, method='precomputed'):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_experts = num_experts
        self.method = method
        
        if method == 'precomputed':
            # Use nn.Linear for standard weight initialization
            self.linear_projection = nn.Linear(input_dim, output_dim, bias=False)
            # Initialize with orthogonal-like weights using proper initialization
        else:
            # Base projection matrix for training-time orthogonalization
            self.projection_matrix = nn.Parameter(torch.randn(input_dim, output_dim) * 0.1)
            
        # Newton-Schulz iteration parameters (for training only)
        self.ns_steps = 5
        self.ns_coeffs = (3.4445, -4.7750, 2.0315)  # (a, b, c)
        
        # Orthogonal constraint strength
        self.ortho_strength = 0.1
        

    def _initialize_orthogonal_matrix(self, input_dim, output_dim):
        """Initialize a random orthogonal matrix"""
        # BF16/FP8/FP4에서는 SVD/QR이 불안정하므로 단순한 random matrix 사용
        # 실제로는 "quasi-orthogonal" matrix로 충분함
        scale = (2.0 / (input_dim + output_dim)) ** 0.5
        return torch.randn(input_dim, output_dim) * scale
    
    def _initialize_linear_weights(self):
        """Initialize linear projection weights properly"""
        # Use PyTorch's built-in orthogonal initialization
        # This handles dtype and device compatibility automatically
        torch.nn.init.orthogonal_(self.linear_projection.weight)
        
        # Scale down the weights for better stability
        with torch.no_grad():
            self.linear_projection.weight.data *= 0.1
        
    def newton_schulz_orthogonalize(self, G, steps=None):
        """Newton-Schulz iteration for orthogonalization"""
        if steps is None:
            steps = self.ns_steps
            
        a, b, c = self.ns_coeffs
        X = G
        
        # Ensure spectral norm is at most 1
        X = X / (X.norm() + 1e-7)
        
        # Perform NS iterations
        for _ in range(steps):
            A = X @ X.T
            B = b * A + c * A @ A
            X = a * X + B @ X
            
        return X
    
    def qr_orthogonalize(self, G):
        """QR decomposition for orthogonalization"""
        Q, R = torch.linalg.qr(G, mode='reduced')
        return Q
    
    def svd_orthogonalize(self, G):
        """SVD-based orthogonalization"""
        U, S, V = torch.linalg.svd(G, full_matrices=False)
        return U @ V.T
    
    def forward(self, x):
        """
        Forward pass with orthogonal projection
        x: [batch_size, input_dim]
        """
        if self.method == 'precomputed':
            # Fast inference: use linear projection
            orthogonal_logits = self.linear_projection(x)
            # L2 normalization for unit vectors
            orthogonal_logits = F.normalize(orthogonal_logits, p=2, dim=-1)
            return orthogonal_logits
            
        elif self.method == 'newton_schulz' and self.training:
            # Training only: Newton-Schulz iteration for orthogonalization
            P_ortho = self.newton_schulz_orthogonalize(self.projection_matrix)
            orthogonal_logits = torch.matmul(x, P_ortho)
            orthogonal_logits = F.normalize(orthogonal_logits, p=2, dim=-1)
            return orthogonal_logits
            
        elif self.method == 'qr' and self.training:
            # Training only: QR decomposition
            P_ortho = self.qr_orthogonalize(self.projection_matrix)
            orthogonal_logits = torch.matmul(x, P_ortho)
            orthogonal_logits = F.normalize(orthogonal_logits, p=2, dim=-1)
            return orthogonal_logits
            
        elif self.method == 'svd' and self.training:
            # Training only: SVD-based orthogonalization
            P_ortho = self.svd_orthogonalize(self.projection_matrix)
            orthogonal_logits = torch.matmul(x, P_ortho)
            orthogonal_logits = F.normalize(orthogonal_logits, p=2, dim=-1)
            return orthogonal_logits
            
        else:  # 'linear' or inference fallback
            # Simple linear projection (fastest)
            orthogonal_logits = torch.matmul(x, self.projection_matrix)
            orthogonal_logits = F.normalize(orthogonal_logits, p=2, dim=-1)
            return orthogonal_logits
    
    def orthogonal_loss(self):
        """Compute orthogonal constraint loss"""
        if self.method == 'precomputed':
            # For precomputed, use the linear projection weights
            gram_matrix = torch.matmul(self.linear_projection.weight, self.linear_projection.weight.T)
            identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
            ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
            return self.ortho_strength * ortho_loss
            
        elif self.method == 'linear':
            return torch.tensor(0.0, device=self.projection_matrix.device)
            
        else:
            # For other methods, use projection matrix
            gram_matrix = torch.matmul(self.projection_matrix, self.projection_matrix.T)
            identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
            ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
            return self.ortho_strength * ortho_loss


def sparsemixer(scores, top_k, jitter_eps, training):
    assert top_k == 2

    ################ first expert ################
    with torch.no_grad():
        # compute mask for sparsity
        mask_logits_threshold, max_ind = scores.max(dim=-1, keepdim=True)
        factor = scores.abs().clamp(min=mask_logits_threshold.abs()).clamp(min=1e-10)  # 수치적 안정성 개선
        mask_logits_threshold = (
            (mask_logits_threshold - scores) / factor
        ) > (2 * jitter_eps)

    # apply mask 
    masked_gates = scores.masked_fill(mask_logits_threshold, float('-inf'))
    if training:
        selected_experts = (
            masked_gates - torch.empty_like(masked_gates, memory_format=torch.legacy_contiguous_format).exponential_().log()
        ).max(dim=-1)[1].unsqueeze(-1) # gumbel sampling, more robust than than the multinomial method
    else:
        selected_experts = max_ind
        
    # compute scores for gradients
    masked_gates = torch.softmax(masked_gates, dim=-1)
    
    # Ensure selected_experts indices are within bounds
    num_experts = masked_gates.size(-1)
    selected_experts = torch.clamp(selected_experts, 0, num_experts - 1)
    
    multiplier_o = masked_gates.gather(dim=-1, index=selected_experts)
    
    if training:
        # compute midpoint mask 
        max_scores, max_ind = masked_gates.max(dim=-1, keepdim=True)
        mask_for_one = torch.logical_or(
            selected_experts == max_ind,
            torch.rand_like(max_scores) > 0.75 # Heun's third-order method: f(x) - f(0) = .25 f'(x) + .75 f'(x/3.)
        ) 
        # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5
        mask_for_one = torch.add(0.3333, mask_for_one, alpha=0.6667).type_as(masked_gates)

        multiplier = mp.apply(
            scores, 
            multiplier_o, 
            selected_experts, 
            masked_gates, 
            mask_for_one,
        )
    else:
        multiplier = multiplier_o

    # masked out first expert 
    masked_scores = torch.scatter(
        scores,
        -1,
        selected_experts,
        float('-inf'),
    )
    with torch.no_grad():
        # compute mask for sparsity
        mask_logits_threshold, max_ind = masked_scores.max(dim=-1, keepdim=True)
        factor = scores.abs().clamp(min=mask_logits_threshold.abs()).clamp(min=1e-8)  # 수치적 안정성 개선
        mask_logits_threshold = (
            (mask_logits_threshold - scores) / factor
        ) > (2 * jitter_eps)

    # apply mask 
    masked_gates_top2 = masked_scores.masked_fill(mask_logits_threshold, float('-inf'))
    if training:
        selected_experts_top2 = (
            masked_gates_top2 - torch.empty_like(masked_gates_top2, memory_format=torch.legacy_contiguous_format).exponential_().log()
        ).max(dim=-1)[1].unsqueeze(-1) # gumbel sampling, more robust than than the multinomial method
    else:
        selected_experts_top2 = max_ind
    # compute scores for gradients
    masked_gates_top2 = torch.softmax(masked_gates_top2, dim=-1)
    
    # Ensure selected_experts_top2 indices are within bounds
    selected_experts_top2 = torch.clamp(selected_experts_top2, 0, num_experts - 1)
    
    multiplier_top2_o = masked_gates_top2.gather(dim=-1, index=selected_experts_top2)
    
    if training: 
        # compute midpoint mask 
        max_scores, max_ind = masked_gates_top2.max(dim=-1, keepdim=True)
        mask_for_one_top2 = torch.logical_or(
            selected_experts_top2 == max_ind,
            torch.rand_like(max_scores).uniform_() > 0.75 # Heun's third-order method: f(x) - f(0) = .25 f'(x) + .75 f'(x/3.)
        ) 
        # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5
        mask_for_one_top2 = torch.add(0.3333, mask_for_one_top2, alpha=0.6667).type_as(masked_gates_top2)

        multiplier_top2 = mp.apply(
            scores, 
            multiplier_top2_o, 
            selected_experts_top2, 
            masked_gates_top2, 
            mask_for_one_top2,
        )
    else:
        multiplier_top2 = multiplier_top2_o
    
    multiplier = torch.cat((multiplier, multiplier_top2), dim=-1)
    selected_experts = torch.cat((selected_experts, selected_experts_top2), dim=-1)
    
    return (
        multiplier, 
        selected_experts,
    )


class G3MoERouter(nn.Module):
    def __init__(self, config: G3MoETextConfig, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.router_dim = config.router_dim
        self.balancing_strength = getattr(config, "balancing_strength", 0.01)
        self.ema_alpha = getattr(config, "ema_alpha", 0.99)
        self.register_buffer("expert_load_ema", torch.zeros(self.num_experts), persistent=True)

        self.load_balancer = nn.GRU(
            input_size=self.hidden_size,
            hidden_size=self.num_experts * self.router_dim,
            num_layers=1,
            bias=False,
            batch_first=True,
        )
        
        # Global expression projector: hidden_size → router_dim
        # Precomputed method for efficient inference
        self.expression_projector = ExpressionProjector(
            self.hidden_size, 
            self.router_dim, 
            self.num_experts, 
            method='precomputed'
        )

    def forward(self, x, hn, top_k=2, jitter_eps=0.01, training=True):
        # GRU를 통한 전역 라우팅 (hn 활용)
        routing_logits, hn = self.load_balancer(x, hn)
        input_shape = routing_logits.shape[:-1]
        hidden_shape = (*input_shape, -1, self.router_dim)

        # Enhanced expression projection for expert specialization
        expression_logits = self.expression_projector(x)
        expression_logits = expression_logits.view(hidden_shape)
        
        routing_logits = routing_logits.view(hidden_shape)
        routing_logits = F.normalize(routing_logits, dim=-1)
        
        # Enhanced Gram matrix calculation with hn context
        # routing_logits: [batch*seq, num_experts, router_dim]
        # Reshape to [batch, seq, num_experts, router_dim] for bmm
        batch_size, seq_len = input_shape
        routing_logits_reshaped = routing_logits.view(batch_size, seq_len, self.num_experts, self.router_dim)
        
        # Compute Gram matrix for each sequence position
        gram = torch.matmul(routing_logits_reshaped, routing_logits_reshaped.transpose(-2, -1))
        # gram: [batch, seq, num_experts, num_experts]
        
        routing_i = torch.eye(self.num_experts, device=routing_logits.device)
        
        # Speciality penalty: encourage orthogonal expert representations
        speciality_penalty = torch.mean((F.normalize(gram - routing_i.unsqueeze(0).unsqueeze(0), dim=-1) ** 2).sum(dim=(-2,-1)))
        
        # Cosine similarity between expression and routing logits
        cosine_similarities = 1.0 - F.cosine_similarity(expression_logits, routing_logits, dim=-1)
        
        # routing_logits 대신 cosine_similarities를 기반으로 한 도메인 스코어 반환
        # 이렇게 하면 각 expert의 표현 정보가 더 잘 반영됨
        # speciality_penalty: [batch, seq] -> [batch, seq, 1, 1] for broadcasting
        domain_scores = cosine_similarities * (1.0 + speciality_penalty.unsqueeze(-1).unsqueeze(-1))
        
        # Sparsemixer를 통한 최종 expert 선택 및 가중치 계산
        # domain_scores를 [batch*seq, num_experts] 형태로 변환
        batch_size, seq_len = input_shape
        domain_scores_flat = domain_scores.view(batch_size * seq_len, self.num_experts)
        
        multiplier, selected_experts = sparsemixer(
            domain_scores_flat, 
            top_k=top_k, 
            jitter_eps=jitter_eps, 
            training=training
        )

        # Compute expression loss for the projection matrix
        expression_loss = self.expression_projector.orthogonal_loss()

        # ---- Adaptive filter logic for load balancing (applied during training) ----
        if self.training:
            with torch.no_grad():
                total_load = self.expert_load_ema.sum()
                if total_load > 0:
                    # Normalize EMA load to get balancing scores
                    load_balancing_scores = self.expert_load_ema / total_load
                else:
                    load_balancing_scores = torch.zeros_like(self.expert_load_ema)
                
                # Penalize experts with high load by adjusting multiplier
                # The strength of the penalty is controlled by balancing_strength
                adjustment = load_balancing_scores * self.balancing_strength * self.num_experts
                # multiplier에 load balancing 적용
                multiplier = multiplier - adjustment[selected_experts]

                # Count how many tokens were routed to each expert in this batch
                current_load = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).sum(dim=[0, 1]).float()
                # Update EMA of expert loads
                self.expert_load_ema.mul_(self.ema_alpha).add_(current_load, alpha=1.0 - self.ema_alpha)

        return multiplier, selected_experts, expression_logits, hn, speciality_penalty, cosine_similarities, expression_loss

iterations = 0
class G3MoEGRINMoE(nn.Module):
    """Hybrid Router: 하나의 linear layer에서 sigmoid로 expert 선택, sparsemixer로 가중치 계산"""

    def __init__(self, config, global_router, **kwargs):
        super().__init__()
        config = config
        self.hidden_dim = config.hidden_size
        self.ffn_dim = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.top_k = config.num_experts_per_tok
        self.router_dim = config.router_dim
        global iterations
        iterations += 1
        self.iter = iterations
        self.router = global_router
        # self.router = nn.Linear(config.hidden_size, config.n_routed_experts, bias=False)
        self.experts = nn.ModuleList([G3MoEMLP(config) for _ in range(self.num_experts)])
        self.shared_experts = G3MoEMLP(config=config, intermediate_size=config.intermediate_size * config.n_shared_experts)

        self.router_jitter_noise = getattr(config, 'router_jitter_noise', 0.01)
        self.input_jitter_noise = getattr(config, 'input_jitter_noise', 0.0)   

        setattr(self.router, "_is_g3moe_router", True)
        setattr(self.router.expression_projector, "_is_g3moe_expression_projector", True)

        # Adaptive filter parameters for load balancing
        
        # Enhanced Expert Utilization
        self.register_buffer("expert_specialization_ema", torch.zeros(self.num_experts, self.hidden_dim), persistent=True)
        self.routing_temperature = nn.Parameter(torch.ones(1))
        self.specialization_strength = getattr(config, "specialization_strength", 0.01)
        
        # shared_experts freeze 여부 (기본값은 True로 설정)
        self.freeze_shared_experts = getattr(config, 'freeze_shared_experts', True)
        if self.freeze_shared_experts:
            self._freeze_shared_experts()
    
        # Orthogonal projector는 이제 global router에서 처리됨
        self.ortho_strength = getattr(config, 'ortho_strength', 1.0)

    
    def _freeze_shared_experts(self):
        """shared_experts의 파라미터들을 freeze"""
        for param in self.shared_experts.parameters():
            param.requires_grad = False
        logger.debug(f"Shared experts frozen for layer {self.iter}")
    
    def _unfreeze_shared_experts(self):
        """shared_experts의 파라미터들을 unfreeze"""
        for param in self.shared_experts.parameters():
            param.requires_grad = True
        logger.debug(f"Shared experts unfrozen for layer {self.iter}")
    
    def freeze_shared_experts_manual(self):
        """수동으로 shared_experts freeze"""
        self._freeze_shared_experts()
        self.freeze_shared_experts = True
    
    def unfreeze_shared_experts_manual(self):
        """수동으로 shared_experts unfreeze"""
        self._unfreeze_shared_experts()
        self.freeze_shared_experts = False

    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def forward(self, hidden_states: torch.Tensor, global_routing_logits: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        residual = hidden_states
        final_hidden_states, routing_info = self._sparse_routing(hidden_states, global_routing_logits)
        router_logits, hn, speciality_loss, cosine_similarities, expression_loss = routing_info
        with torch.no_grad():
            # print( f'residual in rank {torch.distributed.get_rank()}', residual.shape, residual.dtype)
            pretriained_residual = self.shared_experts(residual)
        final_hidden_states = final_hidden_states + pretriained_residual * 1.0
        if self.training:
            final_hidden_states = final_hidden_states.requires_grad_(True)
            if router_logits is not None:
                router_logits = router_logits.requires_grad_(True)
        return final_hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, expression_loss)    
    
    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def _sparse_routing(self, hidden_states: torch.Tensor, global_routing_logits: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        batch_size, sequence_length, hidden_dim = hidden_states.shape
        if self.training and self.input_jitter_noise > 0:
            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise)
        
        # Global router에서 전체 라우팅 처리 (GRU + expression projection + sparsemixer)
        router_output = self.router(
            hidden_states, 
            global_routing_logits,
            top_k=self.top_k,
            jitter_eps=self.router_jitter_noise,
            training=self.training
        )
        routing_weights, selected_experts, expression_logits, hn, speciality_loss, cosine_similarities, expression_loss = router_output

        # multiplier와 selected_experts는 이미 global router에서 sparsemixer를 통해 계산됨
        assert routing_weights.isnan().sum() == 0, f"{self.iter} layer routing_weights is nan Line: 826"

        final_hidden_states = torch.zeros(
            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device
        )

        # One hot encode the selected experts to create an expert mask
        # this will be used to easily index which expert is going to be sollicitated
        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)

        # Loop over all available experts in the model and perform the computation on each expert
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            idx, top_x = torch.where(expert_mask[expert_idx])

            # Use torch.where to handle empty tensor case in a compile-friendly way
            has_tokens = top_x.numel() > 0
            if has_tokens:
                # in torch it is faster to index using lists than torch tensors
                top_x_list = top_x.tolist()
                idx_list = idx.tolist()

                # Index the correct hidden states and compute the expert hidden state for
                # the current expert. We need to make sure to multiply the output hidden
                # states by `routing_weights` on the corresponding tokens (top-1 and top-2)
                hidden_states_flat = hidden_states.view(batch_size * sequence_length, hidden_dim)
                current_state = hidden_states_flat[top_x_list]
                current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list].unsqueeze(-1)

                # However `index_add_` only support torch tensors for indexing so we'll use
                # the `top_x` tensor here.
                final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
                
                # --- Update Specialization EMA ---
                if self.training:
                    with torch.no_grad():
                        # Flatten hidden_states to 2D for proper indexing
                        hidden_states_flat = hidden_states.view(-1, hidden_states.size(-1))
                        current_mean_hidden = hidden_states_flat[top_x_list].mean(dim=0)
                        self.expert_specialization_ema[expert_idx].mul_(self.router.ema_alpha).add_(current_mean_hidden, alpha=1.0 - self.router.ema_alpha)
                # --- End Update Specialization EMA ---

        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)
        return final_hidden_states, (routing_weights, hn, speciality_loss, cosine_similarities, expression_loss)


class G3MoERMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6, **kwargs):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.zeros(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        # Llama does x.to(float16) * w whilst G3MoE is (x * w).to(float16)
        # See https://github.com/huggingface/transformers/pull/29402
        output = output * (1.0 + self.weight.float())
        return output.type_as(x)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.eps}"


class G3MoERotaryEmbedding(nn.Module):
    def __init__(self, config: G3MoETextConfig, device=None, **kwargs):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    softcap: Optional[float] = None,
    **kwargs,
) -> Tuple[torch.Tensor, torch.Tensor]:
    if scaling is None:
        scaling = module.head_dim**-0.5

    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

    if softcap is not None:
        attn_weights = attn_weights / softcap
        attn_weights = torch.tanh(attn_weights)
        attn_weights = attn_weights * softcap
    if attention_mask is not None:  # no matter the length, we just slice it
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    # upcast attention to fp32
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.bfloat16).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output, attn_weights


class G3MoEAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: G3MoETextConfig, layer_idx: int, **kwargs):
        super().__init__()
        self.is_sliding = bool((layer_idx + 1) % config.sliding_window_pattern)
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = config.query_pre_attn_scalar**-0.5
        self.attention_dropout = self.config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.attn_logit_softcapping = self.config.attn_logit_softcapping
        self.sliding_window = config.sliding_window if self.is_sliding else None

        self.q_norm = G3MoERMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)
        self.k_norm = G3MoERMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False, # Added default value
        use_cache: bool = False, # Added default value
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)
        
        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states   = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        
        query_states = self.q_norm(query_states)
        key_states   = self.k_norm(key_states)

        cos, sin = None, None
        if position_embeddings is not None:
            cos, sin = position_embeddings
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
        # else: NoPE, 그대로 사용

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {
                "sin": sin,
                "cos": cos,
                "cache_position": cache_position,
                "sliding_window": self.sliding_window,
            }
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
            
            # Here we need to slice as we use a static cache by default, but FA2 does not support it
            # if attention_mask is not None and self.config.attn_implementation == "flash_attention_2":
            #     if hasattr(past_key_value, "get_seq_length"):
            #         seq_len = past_key_value.get_seq_length()
            #     else:
            #         seq_len = key_states.shape[-1]
            #     key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]
        
        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=self.attention_dropout if self.training else 0.0,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            output_attentions=output_attentions,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights, past_key_value


class G3MoEDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: G3MoETextConfig, layer_idx: int, global_router: G3MoERouter, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx
        self.attention_type = config.layer_types[layer_idx]
        self.self_attn = G3MoEAttention(config=config, layer_idx=layer_idx, **kwargs)
        self.mlp = G3MoEMLP(config=config) # this layer is for loading pretrained base G3MoE model weights
        self.is_dense_replacement = layer_idx >= config.first_k_dense_replace
        if self.is_dense_replacement:
            self.moe = G3MoEGRINMoE(config=config, global_router=global_router)
            # self.moe = G3MoESparseGRINBlock(config=config)
        else:
            self.moe = G3MoEMLP(config=config)
        self.input_layernorm = G3MoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = G3MoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.pre_feedforward_layernorm = G3MoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_feedforward_layernorm = G3MoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.is_sliding = self.self_attn.is_sliding
        self.sliding_window = config.sliding_window
        self.use_nope = (hasattr(config, 'no_rope_layers') and bool(config.no_rope_layers[self.layer_idx]))

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings_global: torch.Tensor,
        position_embeddings_local: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        global_routing_hn: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:

        residual = hidden_states
        
        hidden_states = self.input_layernorm(hidden_states)

        # 하이브리드 rope-nope positional embedding 적용
        if self.use_nope:
            # NoPE: position embedding 없이 self-attn
            position_embeddings = None
        else:
            # 기존 방식: RoPE
            position_embeddings = position_embeddings_local if self.self_attn.is_sliding else position_embeddings_global
        hidden_states, self_attn_weights, past_key_value = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = residual + hidden_states
        
        residual = hidden_states
        hidden_states = self.pre_feedforward_layernorm(hidden_states)
        if self.layer_idx >= self.config.first_k_dense_replace:
            hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, expression_loss) = self.moe(hidden_states, global_routing_hn)
        else:
            with torch.no_grad():
                hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, expression_loss) = self.moe(hidden_states), (None,)*5
        hidden_states = self.post_feedforward_layernorm(hidden_states)
        if self.training:
            hidden_states = hidden_states.requires_grad_(True)
            if router_logits is not None:
                router_logits = router_logits.requires_grad_(True)
        hidden_states = residual + hidden_states
        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)
        outputs += ((router_logits, hn, speciality_loss, cosine_similarities, expression_loss),)
        return outputs


G3MoE_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`G3MoEConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""

@auto_docstring
class G3MoEPreTrainedModel(PreTrainedModel):
    config: G3MoEConfig
    base_model_prefix = ""
    supports_gradient_checkpointing = True
    _no_split_modules = [
        "G3MoEDecoderLayer",
        "SiglipVisionEmbeddings",
        "SiglipEncoderLayer",
        "SiglipMultiheadAttentionPoolingHead",
    ]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True
    
    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": G3MoEDecoderLayer,
        "attentions": G3MoEAttention
    }

    def _initialize_moe_router_and_temperature(self) -> None:
        """Initialize only MoE router weights and routing temperature if they were not loaded from a checkpoint.

        - Router linear weights: Xavier uniform for stable logits
        - Routing temperature: ones (softplus(1) ~ 1.313) keeps scale reasonable
        """
        with torch.no_grad():
            for module in self.modules():
                # Initialize router linears ONLY if not already initialized/loaded
                router = getattr(module, "router", None)
                if isinstance(router, nn.Linear):
                    already_init = getattr(router, "_is_hf_initialized", False)
                    if not already_init:
                        nn.init.xavier_uniform_(router.weight)
                        if router.bias is not None:
                            router.bias.zero_()
                # Initialize routing temperature ONLY if looks uninitialized/bad
                routing_temp = getattr(module, "routing_temperature", None)
                if isinstance(routing_temp, nn.Parameter):
                    if not torch.isfinite(routing_temp).all() or routing_temp.abs().sum() == 0:
                        routing_temp.data.fill_(1.0)

    def _init_weights(self, module):
        # important: this ported version of Gemma2 isn't meant for training from scratch - only
        # inference and fine-tuning - so the proper init weights code has been removed

        # Only initialize router linears explicitly; skip expert MLPs and other dense layers during fine-tuning
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):
            if getattr(module, "_is_g3moe_router", False):
                logging.get_logger('transformers').debug(f"Initializing router layer with Xavier uniform: {module}")
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    module.bias.data.zero_()
            else:
                # Do not touch non-router linears here to avoid re-initializing experts or base MLPs
                pass
        elif isinstance(module, ExpressionProjector):
            logging.get_logger('transformers').debug(f"Initializing expression projector layer with Xavier uniform: {module}")
            if module.method == "precomputed":
                original_dtype = module.linear_projection.weight.dtype
                module.to(torch.float32)
                nn.init.orthogonal_(module.linear_projection.weight)
                if module.linear_projection.bias is not None:
                    module.linear_projection.bias.data.zero_()
                module.to(original_dtype)
            else:
                nn.init.xavier_uniform_(module.projection_matrix.weight)
                if module.projection_matrix.bias is not None:
                    module.projection_matrix.bias.data.zero_()
        elif isinstance(module, G3MoEMultiModalProjector):
            nn.init.xavier_uniform_(module.mm_input_projection_weight)
        elif isinstance(module, nn.GRU):
            for name, param in module.named_parameters():
                if 'weight_ih' in name:
                    nn.init.xavier_uniform_(param.data)
                elif 'weight_hh' in name:
                    nn.init.xavier_uniform_(param.data)
                elif 'weight_hr' in name:
                    nn.init.xavier_uniform_(param.data)
                elif 'bias' in name:
                    param.data.fill_(0)
        else:
            super()._init_weights(module)

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: Type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        ffn_checkpoint_for_moe_conversion: Optional[Union[str, os.PathLike, dict]] = None,
        **kwargs,
    ) -> SpecificPreTrainedModelType:

        # config 처리 (G2MoEConfig 인스턴스 확보)
        if config is None:
            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs.get("config_kwargs", {}))
        if not isinstance(config, G3MoEConfig):
            config = G3MoEConfig(**config.to_dict())
            if config.attn_implementation == None:
                if is_flash_attn_2_available():
                    config.attn_implementation = "flash_attention_2"
                elif is_torch_flex_attn_available():
                    config.attn_implementation = "flex_attention"
                elif cls.training:
                    config.attn_implementation = "eager"
                else:
                    config.attn_implementation = "sdpa"
            print(f"Forced attn implementation: {config.attn_implementation}")

        logging.get_logger('transformers').debug("Loading G3MoE model skeleton using super().from_pretrained...")
        logging.set_verbosity_error()
        import time
        debug_time = time.time()
        base_model = super().from_pretrained(
            pretrained_model_name_or_path,
            *model_args,
            config=config,
            cache_dir=cache_dir,
            # Critical: only init truly missing keys to avoid full-graph init slowdown
            ignore_mismatched_sizes=True,
            force_download=force_download,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
            use_safetensors=use_safetensors,
            weights_only=weights_only,
            **{k: v for k, v in kwargs.items()}
        )
        print(f"G3MoE model skeleton loaded in {time.time() - debug_time} seconds")
        logging.set_verbosity_warning()
        logging.get_logger('transformers').debug("G3MoE model skeleton loaded.")

        if hasattr(base_model, 'model'):
            if hasattr(base_model.model, 'layers') and hasattr(base_model.model.layers, 'moe'):
                logging.get_logger('transformers').debug("G3MoE Pretrained model loaded.")
                return base_model
            logging.get_logger('transformers').debug("Initializing MoE experts with MLP weights...")
            if hasattr(base_model.model, 'layers'):
                base_model.model = base_model._upcycle(base_model.model)
            elif hasattr(base_model.model.language_model, 'layers'):
                base_model.model.language_model = base_model._upcycle(base_model.model.language_model)
            logging.get_logger('transformers').debug("MoE experts initialization completed.")
        elif hasattr(base_model, "language_model"):
            if hasattr(base_model.language_model, 'layers') and hasattr(base_model.language_model.layers, 'moe'):
                logging.get_logger('transformers').debug("G3MoE Pretrained model loaded.")
                return base_model
            logging.get_logger('transformers').debug("Initializing MoE experts with MLP weights...")
            base_model.language_model = base_model._upcycle(base_model.language_model)
            logging.get_logger('transformers').debug("MoE experts initialization completed.")
        else:
            logging.get_logger('transformers').info("Model does not have expected structure. MoE experts not initialized from MLP weights.")
        logging.set_verbosity_warning() 
        return base_model

    @torch.no_grad()
    def _upcycle(self, model):
        processing = tqdm(
            enumerate(model.layers),
            total=len(model.layers),
            desc=f"Copying MLP weights to {self.__class__.__name__} MoE experts: start",
            leave=False)

        for layer_idx, decoder_layer in processing:
            if hasattr(decoder_layer.moe, 'experts') or hasattr(decoder_layer.moe, 'shared_experts'):
                if hasattr(decoder_layer.moe, 'shared_experts'):
                    processing.set_description(f"Copying mlp {layer_idx} → shared experts")
                    decoder_layer.moe.shared_experts.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                    decoder_layer.moe.shared_experts.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                    decoder_layer.moe.shared_experts.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

                for expert_idx, expert in enumerate(decoder_layer.moe.experts):
                    if expert_idx % 2 == 0:
                        processing.set_description(f"Copying mlp {layer_idx} → expert {expert_idx}")
                    expert.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                    expert.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                    expert.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

            elif hasattr(decoder_layer.moe, 'gate_proj'):
                processing.set_description(f"Copying mlp {layer_idx} → dense MoE")
                decoder_layer.moe.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                decoder_layer.moe.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                decoder_layer.moe.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)
            else:
                raise Exception("MoE model has no MLP or shared MLP")
            del decoder_layer.mlp
        processing.set_description("Copy finished")
        return model

    def get_parameter_groups(self):
        """
        Returns a list of parameter groups for the optimizer, which allows to apply different
        learning rates to different parts of the model. This is particularly useful for MoE models
        where components like routers and experts can benefit from different learning schedules.
        """
        
        router_params = []
        expert_params = []
        shared_expert_params = []
        attention_params = []
        other_params = []

        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue

            if 'gate.weight' in name or 'router' in name:
                router_params.append(param)
            elif 'shared_experts' in name:
                shared_expert_params.append(param)
            elif 'experts' in name:
                expert_params.append(param)
            elif 'self_attn' in name:
                attention_params.append(param)
            else:
                other_params.append(param)
        
        # In a training script, you can assign different learning rates to these groups.
        # For example:
        # optimizer_grouped_parameters = [
        #     {'params': model.get_parameter_groups()['router'], 'lr': 1e-4},
        #     {'params': model.get_parameter_groups()['expert'], 'lr': 5e-5},
        #     ...
        # ]
        return {
            'router': router_params,
            'expert': expert_params,
            'shared_expert': shared_expert_params,
            'attention': attention_params,
            'other': other_params,
        }

G3MoE_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            Two formats are allowed:
            - a [`~cache_utils.Cache`] instance, see our
            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
            cache format.

            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
            legacy cache format will be returned.

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""

@add_start_docstrings(
    "The bare G3MoEText Model outputting raw hidden-states without any specific head on top.",
    G3MoE_START_DOCSTRING,
)
class G3MoETextModel(G3MoEPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`G3MoETextDecoderLayer`]

    Args:
        config: G3MoETextConfig
    """
    config: G3MoETextConfig

    def __init__(self, config: G3MoETextConfig, **kwargs):
        super().__init__(config)
        # Robustly resolve text config without relying on class identity across module boundaries
        if getattr(config, "model_type", None) == "g3moe_text" or not hasattr(config, "text_config"):
            self.config = config
        else:
            self.config = config.text_config
        config = self.config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        # Expose a tensor-parallel plan for vLLM on the base text model
        if not hasattr(self, "_tp_plan") or self._tp_plan is None:
            default_tp_plan = {
                "layers.*.self_attn.q_proj": "colwise",
                "layers.*.self_attn.k_proj": "colwise",
                "layers.*.self_attn.v_proj": "colwise",
                "layers.*.self_attn.o_proj": "rowwise",
                "layers.*.moe.experts.*.gate_proj": "colwise",
                "layers.*.moe.experts.*.up_proj": "colwise",
                "layers.*.moe.experts.*.down_proj": "rowwise",
            }
            # allow overriding via config if provided
            self._tp_plan = getattr(config, "base_model_tp_plan", default_tp_plan)

        # G3MoE downcasts the below to bfloat16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402
        self.embed_tokens = G3MoETextScaledWordEmbedding(
            config.vocab_size, config.hidden_size, self.padding_idx, embed_scale=self.config.hidden_size**0.5
        )
        self.global_router = G3MoERouter(config)
        self.layers = nn.ModuleList(
            [G3MoEDecoderLayer(config, layer_idx, self.global_router, **kwargs) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = G3MoERMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = G3MoERotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        self.router_aux_loss_coef = config.router_aux_loss_coef
        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas
        # when we want to create a local RoPE layer. Config defaults should hold values for global RoPE
        config = copy.deepcopy(config)
        config.rope_theta = config.rope_local_base_freq
        config.rope_scaling = config.rope_scaling if config.rope_scaling is not None else {"rope_type":  "default"}
        self.rotary_emb_local = G3MoERotaryEmbedding(config=config)
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    @classmethod
    def from_config(cls, **kwargs):
        return cls._from_config(**kwargs)

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> G3MoECausalLMOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None and not self.training:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens,
                past_seen_tokens + inputs_embeds.shape[1],
                device=inputs_embeds.device,
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)
        
        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        # embed positions
        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings_global = self.rotary_emb(hidden_states, position_ids)
        position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        global_routing_hn = None

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
        
            layer_outputs = decoder_layer(
                hidden_states,
                position_embeddings_global=position_embeddings_global,
                position_embeddings_local=position_embeddings_local,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                global_routing_hn=global_routing_hn,
                **flash_attn_kwargs,
            )
            hidden_states = layer_outputs[0]
            routing_result = layer_outputs[-1]
            if routing_result is not None:
                router_logits = routing_result[0]
                global_routing_hn = routing_result[1]
                speciality_loss = routing_result[2]
                cosine_similarities = routing_result[3]
                expression_loss = routing_result[4]

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        return G3MoEModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
            router_logits=router_logits,
            speciality_loss=speciality_loss,
            cosine_similarities=cosine_similarities,
            expression_loss=expression_loss,
        )


@auto_docstring
class G3MoEForCausalLM(G3MoEPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}
    config: G3MoETextConfig
    base_model_prefix = "language_model"

    def __init__(self, config: G3MoETextConfig, **kwargs):
        super().__init__(config)
        self.model = G3MoETextModel(config, **kwargs)
        # Ensure config refers to resolved text config from the submodule
        self.config = self.model.config
        self.vocab_size = self.config.vocab_size
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **loss_kwargs,
    ) -> CausalLMOutputWithPast:
        r"""
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

            logits_to_keep (`int` or `torch.Tensor`, *optional*):
                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
                This is useful when using packed tensor format (single dimension for batch and sequence length).

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, G3MoEForCausalLM

        >>> model = G3MoEForCausalLM.from_pretrained("google/gemma-2-9b")
        >>> tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b")

        >>> prompt = "What is your favorite condiment?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "What is your favorite condiment?"
        ```"""

        if self.training and self.config.attn_implementation != "eager":
            logger.warning_once(
                "It is strongly recommended to train G3MoE models with the `eager` attention implementation "
                f"instead of `{self.config.attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`."
            )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            **loss_kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping

        loss = None
        aux_loss = None
        ortho_loss = None
        expression_loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)
            # Speciality loss 추가 (스칼라 값으로 변환)
            if outputs.speciality_loss is not None:
                # speciality_loss는 이미 스칼라이므로 그대로 사용
                loss += outputs.speciality_loss * 0.01  # 가중치 적용
            
            # Cosine similarities loss (평균값으로 변환)
            if outputs.cosine_similarities is not None:
                cosine_loss = outputs.cosine_similarities.mean()
                loss += cosine_loss * 0.005  # 가중치 적용
            
            # HN context loss는 제거 (이미 speciality_loss에 포함됨)
            if outputs.expression_loss is not None:
                expression_loss = outputs.expression_loss.mean()
                loss += expression_loss * 0.005  # 가중치 적용
                
            # Orthogonalization loss for expert weights to encourage functional diversity
            if self.training and self.config.text_config.ortho_loss_coef > 0:
                ortho_loss = torch.tensor(0.0, device=loss.device, dtype=loss.dtype)
                num_moe_layers = 0
                
                for layer in self.model.layers:
                    if layer.is_dense_replacement and hasattr(layer.moe, "experts"):
                        num_moe_layers += 1
                        expert_weights = [expert.down_proj.weight for expert in layer.moe.experts]
                        ortho_loss += calculate_ortho_loss_for_experts(expert_weights)

                if num_moe_layers > 0:
                    ortho_loss = ortho_loss / num_moe_layers  # Average over layers
                    loss += self.config.text_config.ortho_loss_coef * ortho_loss
                    ortho_loss += _orthogonal_constraint_loss(
                        self.model.config.n_routed_experts, 
                        outputs.router_logits
                    )

            if outputs.router_logits is not None:
                # Add router z-loss to prevent router from being too confident
                aux_loss = load_balancing_loss_func(
                    outputs.router_logits,
                    self.model.config.n_routed_experts,
                    self.model.config.num_experts_per_tok,
                    attention_mask,
                    router_z_loss_coef=self.model.config.router_z_loss_coef,
                    router_entropy_coef=getattr(self.model.config, "router_entropy_coef", 0.0),
                    usage_uniformity_coef=getattr(self.model.config, "usage_uniformity_coef", 0.0),
                )
                loss += self.model.config.router_aux_loss_coef * aux_loss

        try:
            import torch.distributed as dist
            is_main_proc = (not dist.is_available()) or (not dist.is_initialized()) or dist.get_rank() == 0
        except Exception:
            is_main_proc = True


        return G3MoECausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            aux_loss=outputs.aux_loss,
            router_logits=outputs.router_logits,
            ortho_loss=ortho_loss,
            speciality_loss=outputs.speciality_loss,
            cosine_similarities=outputs.cosine_similarities,
            # hn_context 제거 - 차원 문제로 인해 사용하지 않음
        )


class G3MoEMultiModalProjector(nn.Module):
    def __init__(self, config: G3MoEConfig, **kwargs):
        super().__init__()

        self.mm_input_projection_weight = nn.Parameter(
            torch.zeros(config.vision_config.hidden_size, config.text_config.hidden_size)
        )

        self.mm_soft_emb_norm = G3MoERMSNorm(
            config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps
        )

        self.patches_per_image = int(config.vision_config.image_size // config.vision_config.patch_size)
        self.tokens_per_side = int(config.mm_tokens_per_image**0.5)
        self.kernel_size = self.patches_per_image // self.tokens_per_side
        self.avg_pool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=self.kernel_size)

    def forward(self, vision_outputs: torch.Tensor):
        batch_size, _, seq_length = vision_outputs.shape

        reshaped_vision_outputs = vision_outputs.transpose(1, 2)
        reshaped_vision_outputs = reshaped_vision_outputs.reshape(
            batch_size, seq_length, self.patches_per_image, self.patches_per_image
        )
        reshaped_vision_outputs = reshaped_vision_outputs.contiguous()

        pooled_vision_outputs = self.avg_pool(reshaped_vision_outputs)
        pooled_vision_outputs = pooled_vision_outputs.flatten(2)
        pooled_vision_outputs = pooled_vision_outputs.transpose(1, 2)

        normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)

        projected_vision_outputs = torch.matmul(normed_vision_outputs, self.mm_input_projection_weight)
        return projected_vision_outputs.type_as(vision_outputs)


def token_type_ids_mask_function(
    token_type_ids: Optional[torch.Tensor],
    image_group_ids: Optional[torch.Tensor],
    tokens_per_image: int,
) -> Optional[Callable]:
    """
    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,
    not start and end indices.
    """
    # Do not return an additional mask in this case
    if token_type_ids is None:
        return None

    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:
        # If it's 1 for both query and key/value, we are in an image block
        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length
        # Since vmap doesn't support `if statement` we workaround it with `torch.where`
        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)
        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]
        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)

        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]
        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)

        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)
        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx

        # This is bidirectional attention whenever we are dealing with image tokens
        return is_image_block & same_image_block

    return inner_mask


@auto_docstring(
    custom_intro="""
    The Base G3MoE model which consists of a vision backbone and a language model withou language modeling head.,
    """
)
class G3MoEModel(G3MoEPreTrainedModel):
    _checkpoint_conversion_mapping = {"language_model.model": "language_model"}
    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch
    accepts_loss_kwargs = False

    def __init__(self, config: G3MoEConfig):
        super().__init__(config)
        self.vision_tower = AutoModel.from_config(config=config.vision_config)
        self.multi_modal_projector = G3MoEMultiModalProjector(config=config)
        self.vocab_size = config.text_config.vocab_size

        language_model = AutoModel.from_config(config=config.text_config)
        self.language_model = language_model
        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1
        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.language_model = decoder

    def get_decoder(self):
        return self.language_model

    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:
        """
        Projects the last hidden state from the vision model into language model space.

        Args:
            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)
               The tensors corresponding to the input images.
        Returns:
            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
        """
        vision_outputs = self.vision_tower(pixel_values=pixel_values).last_hidden_state
        image_features = self.multi_modal_projector(vision_outputs)
        return image_features

    def get_placeholder_mask(
        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor
    ):
        """
        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is
        equal to the length of multimodal features. If the lengths are different, an error is raised.
        """
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        n_image_features = image_features.shape[0] * image_features.shape[1]
        if inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
            )
        return special_image_mask

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **lm_kwargs,
    ) -> Union[tuple, G3MoEModelOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, G3MoEForConditionalGeneration

        >>> model = G3MoEForConditionalGeneration.from_pretrained("google/gemma32-3b-mix-224")
        >>> processor = AutoProcessor.from_pretrained("google/gemma32-3b-mix-224")

        >>> prompt = "Where is the cat standing?"
        >>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> inputs = processor(images=image, text=prompt,  return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(**inputs,)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Where is the cat standing?\nsnow"
        ```"""
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Replace image id with PAD if the image token if OOV, to avoid index-errors
        if input_ids is not None and self.config.image_token_id >= self.vocab_size:
            special_image_mask = input_ids == self.config.image_token_id
            llm_input_ids = input_ids.clone()
            llm_input_ids[special_image_mask] = 0
        else:
            llm_input_ids = input_ids

        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(llm_input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # Merge text and images
        if pixel_values is not None:
            image_features = self.get_image_features(pixel_values)
            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)
            special_image_mask = self.get_placeholder_mask(
                input_ids, inputs_embeds=inputs_embeds, image_features=image_features
            )
            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config.get_text_config(),
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            if token_type_ids is not None and inputs_embeds.shape[1] != 1:
                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

                # First find where a new image block starts: 1 if image and previous not image
                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
                is_image = (token_type_ids == 1).to(cache_position.device)
                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
                image_group_ids = torch.where(
                    is_image, image_group_ids, torch.full_like(token_type_ids, -1, device=is_image.device)
                )
                mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image
                )

            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        outputs = self.language_model(
            attention_mask=causal_mask_mapping,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **lm_kwargs,
        )

        return G3MoEModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values if use_cache else None,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=image_features if pixel_values is not None else None,
            aux_loss=outputs.aux_loss,
            router_logits=outputs.router_logits,
            ortho_loss=outputs.ortho_loss,
            speciality_loss=outputs.speciality_loss,
        )


@add_start_docstrings(
    """The G3MoE model which consists of a vision backbone and a language model.""",
    G3MoE_START_DOCSTRING,
)
class G3MoEForConditionalGeneration(G3MoEPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {
        "^language_model.model": "model.language_model",
        "^vision_tower": "model.vision_tower",
        "^multi_modal_projector": "model.multi_modal_projector",
        "^language_model.lm_head": "lm_head",
    }
    _tied_weights_keys = ["lm_head.weight"]  
  
    def __init__(self, config: G3MoEConfig, **kwargs):
        super().__init__(config)
        self.model = G3MoEModel(config)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_image_features(self, pixel_values):
        return self.model.get_image_features(pixel_values)

    # Make modules available through conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def vision_tower(self):
        return self.model.vision_tower

    @property
    def multi_modal_projector(self):
        return self.model.multi_modal_projector

    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **lm_kwargs,
    ) -> Union[Tuple, G3MoECausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, G3MoEForConditionalGeneration

        >>> model = G3MoEForConditionalGeneration.from_pretrained("google/gemma-3-4b-it")
        >>> processor = AutoProcessor.from_pretrained("google/gemma-3-4b-it")

        >>> messages = [
        ...     {
        ...         "role": "system",
        ...         "content": [
        ...             {"type": "text", "text": "You are a helpful assistant."}
        ...         ]
        ...     },
        ...     {
        ...         "role": "user", "content": [
        ...             {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"},
        ...             {"type": "text", "text": "Where is the cat standing?"},
        ...         ]
        ...     },
        ... ]

        >>> inputs = processor.apply_chat_template(
        ...     messages,
        ...     tokenizer=True,
        ...     return_dict=True,
        ...     return_tensors="pt",
        ...     add_generation_prompt=True
        ... )
        >>> # Generate
        >>> generate_ids = model.generate(**inputs)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "user\nYou are a helpful assistant.\n\n\n\n\n\nWhere is the cat standing?\nmodel\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to"
        ```
        """

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs: G3MoEModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            token_type_ids=token_type_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
            **lm_kwargs,
        )
        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping
            
        loss = None
        aux_loss = None
        ortho_loss = None
        expression_loss = None
        if labels is not None:
            # Upcast to float if we need to compute the loss to avoid potential precision issues
            logits = logits.float()
            shift_logits = logits[..., :-1, :]
            shift_labels = labels[..., 1:]
            if attention_mask is not None:
                # we use the input attention mask to shift the logits and labels, because it is 2D.
                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft
                shift_attention_mask = attention_mask[:, -shift_logits.shape[1] :].to(logits.device)
                shift_logits = shift_logits[shift_attention_mask.to(logits.device) != 0].contiguous()
                shift_labels = shift_labels[shift_attention_mask.to(shift_labels.device) != 0].contiguous()
            else:
                shift_logits = shift_logits.contiguous()
                shift_labels = shift_labels.contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()

            flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)
            flat_labels = shift_labels.view(-1).to(shift_logits.device)
            loss = loss_fct(flat_logits, flat_labels)
                            # Speciality loss 추가 (스칼라 값으로 변환)
            if outputs.speciality_loss is not None:
                # speciality_loss는 이미 스칼라이므로 그대로 사용
                loss += outputs.speciality_loss * 0.01  # 가중치 적용
            
            # Cosine similarities loss (평균값으로 변환)
            if outputs.cosine_similarities is not None:
                cosine_loss = outputs.cosine_similarities.mean()
                loss += cosine_loss * 0.005  # 가중치 적용
            
            if outputs.expression_loss is not None:
                expression_loss = outputs.expression_loss.mean()
                loss += expression_loss * 0.005  # 가중치 적용
                
            # Orthogonalization loss for expert weights to encourage functional diversity
            if self.training and self.config.text_config.ortho_loss_coef > 0:
                ortho_loss = torch.tensor(0.0, device=loss.device, dtype=loss.dtype)
                num_moe_layers = 0
                for layer in self.language_model.layers:
                    if layer.is_dense_replacement and hasattr(layer.moe, "experts"):
                        num_moe_layers += 1
                        expert_weights = [expert.down_proj.weight for expert in layer.moe.experts]
                        ortho_loss += calculate_ortho_loss_for_experts(expert_weights)

                if num_moe_layers > 0:
                    ortho_loss = ortho_loss / num_moe_layers  # Average over layers
                    loss += self.config.text_config.ortho_loss_coef * ortho_loss
                    ortho_loss += _orthogonal_constraint_loss(
                        self.config.text_config.num_experts_per_tok,
                        outputs.router_logits
                    )

            if outputs.router_logits is not None:
                # Add router z-loss to prevent router from being too confident
                aux_loss = load_balancing_loss_func(
                    outputs.router_logits,
                    self.config.text_config.n_routed_experts,
                    self.config.text_config.num_experts_per_tok,
                    attention_mask,
                    router_z_loss_coef=self.config.text_config.router_z_loss_coef,
                    router_entropy_coef=getattr(self.config.text_config, "router_entropy_coef", 0.0),
                    usage_uniformity_coef=getattr(self.config.text_config, "usage_uniformity_coef", 0.0),
                )
                loss += self.config.text_config.router_aux_loss_coef * aux_loss

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return G3MoECausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=outputs.image_hidden_states,
            aux_loss=outputs.aux_loss,
            ortho_loss=ortho_loss,
            router_logits=outputs.router_logits,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        pixel_values=None,
        attention_mask=None,
        token_type_ids=None,
        use_cache=True,
        logits_to_keep=None,
        labels=None,
        **kwargs,
    ):
        # Overwritten -- custom `position_ids` and `pixel_values` handling
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
            logits_to_keep=logits_to_keep,
            token_type_ids=token_type_ids,
            **kwargs,
        )

        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore
        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always
        if cache_position[0] == 0:
            model_inputs["pixel_values"] = pixel_values

        return model_inputs
    
    @staticmethod
    def create_masks_for_generate(
        config: PretrainedConfig,
        input_embeds: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        cache_position: torch.Tensor,
        past_key_values: Optional[Cache],
        position_ids: Optional[torch.Tensor],
        token_type_ids: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> dict:
        # Prepare mask arguments
        mask_kwargs = {
            "config": config.get_text_config(),
            "input_embeds": input_embeds,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Add the token type ids mask for generate as well
        if token_type_ids is not None and input_embeds.shape[1] != 1:
            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

            # First find where a new image block starts: 1 if image and previous not image
            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
            is_image = (token_type_ids == 1).to(cache_position.device)
            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))
            mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image
            )

        return create_masks_for_generate(**mask_kwargs)


__all__ = [
    "G3MoEPreTrainedModel",
    "G3MoETextModel",
    "G3MoEForCausalLM",
    "G3MoEForConditionalGeneration",
    "G3MoEModel",
]
