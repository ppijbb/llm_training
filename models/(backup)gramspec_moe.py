#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/GramSpecMoE/modular_GramSpecMoE.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_GramSpecMoE.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import copy
import os
import inspect
import math
from functools import partial
from collections.abc import Callable
from dataclasses import dataclass
from typing import List, Optional, Tuple, Union, Type

from tqdm.auto import tqdm

import torch

import torch.nn.init as torch_init
_original_orthogonal = torch_init.orthogonal_

def safe_orthogonal_(tensor, gain=1):
    """CPUì—ì„œ BFloat16/Float16ì— ëŒ€í•´ orthogonal_ ì´ˆê¸°í™” ì‹œ ë°œìƒí•˜ëŠ” geqrf_cpu ë¯¸ì§€ì› ì˜¤ë¥˜ë¥¼ ë°©ì§€"""
    if tensor.is_meta:
        return tensor
    if tensor.device.type == 'cpu' and tensor.dtype in [torch.bfloat16, torch.float16]:
        with torch.no_grad():
            t = tensor.to(torch.float32)
            _original_orthogonal(t, gain=gain)
            tensor.copy_(t.to(tensor.dtype))
    else:
        _original_orthogonal(tensor, gain=gain)
    return tensor

# Global monkey patch
torch_init.orthogonal_ = safe_orthogonal_
import torch.nn as nn
import torch.nn.functional as F
# Add dynamo import for torch.compile compatibility
import torch._dynamo

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, HybridCache, StaticCache, DynamicCache
from transformers.generation.utils import GenerationMixin
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask
from transformers.processing_utils import Unpack
from transformers.utils import logging
from transformers.utils.doc import (
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
    add_start_docstrings,
)
from transformers.utils.generic import (
    ModelOutput,
    can_return_tuple,
)
from transformers.utils import auto_docstring
from transformers.utils.import_utils import (
    is_torchdynamo_compiling,
    is_torch_flex_attn_available,
    is_flash_attn_2_available
)
from transformers.modeling_utils import (
    restore_default_dtype,
    SpecificPreTrainedModelType,
)
from transformers.configuration_utils import PretrainedConfig
from transformers import logging
from transformers.utils.deprecation import deprecate_kwarg
from transformers import AutoModel, AutoConfig, AutoModelForCausalLM
from .gramspec_moe_config import GramSpecMoEConfig, GramSpecMoETextConfig

if is_torch_flex_attn_available():
    from torch.nn.attention.flex_attention import BlockMask
    from transformers.integrations.flex_attention import make_flex_block_causal_mask

    
logger = logging.get_logger(__name__)
_CONFIG_FOR_DOC = "GramSpecMoEConfig"


def calculate_ortho_loss_for_experts(expert_weights: List[torch.Tensor]) -> torch.Tensor:
    """
    Calculates the orthogonalization loss for a set of expert weights from a single MoE layer.
    This loss encourages functional diversity among experts by penalizing similarity
    in their weight spaces. The loss is the squared Frobenius norm
    of (VV' - I) where V is the matrix of normalized expert weights.
    """
    if not expert_weights:
        # Return zero loss on the same device as the model (default to cuda:0 or cpu)
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        return torch.tensor(0.0, device=device)

    flattened_weights = [w.view(-1) for w in expert_weights]
    V = torch.stack(flattened_weights)

    # Normalize rows to be unit vectors, preventing weights from collapsing to zero
    # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    V = V / (V.norm(p=2, dim=1, keepdim=True) + 1e-6)
    
    # Gram matrix: V @ V.T
    gram_matrix = torch.matmul(V, V.t())
    
    # Target: identity matrix
    identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
    
    # Loss: squared Frobenius norm of (VV' - I)
    ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
    return ortho_loss


def _orthogonal_constraint_loss(
    num_experts: int, 
    gate_logits: torch.Tensor
) -> torch.Tensor:
        """ë¼ìš°í„° ì¶œë ¥ê°’ë“¤ì˜ ì§êµì„± ì œì•½ ì†ì‹¤"""
        # router_outputs: [batch*seq, num_experts]
        
        # ê° í† í°ë³„ë¡œ expert ë°©í–¥ì´ ì§êµí•˜ë„ë¡
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        normalized_outputs = gate_logits / (gate_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # Gram matrix: [num_experts, num_experts]
        gram_matrix = torch.matmul(normalized_outputs.T, normalized_outputs)
        
        # Target: identity matrix
        identity = torch.eye(num_experts, device=gate_logits.device, dtype=gate_logits.dtype)
        
        # Loss: squared Frobenius norm of (GG' - I)
        constraint_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
        return constraint_loss


class ContrastiveRouterLoss(nn.Module):
    """
    Encourages experts to process tokens from distinct semantic spaces (Hidden States).
    Uses Soft Probabilities for differentiability.
    """
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.epsilon = 1e-6

    def forward(self, hidden_states, routing_weights):
        """
        hidden_states: [batch, seq, hidden_dim]
        routing_weights: [batch, seq, num_experts] (Softmax probabilities)
        """
        # Flatten: [batch, seq, hidden] -> [N, hidden], [batch*seq, experts] -> [N, experts]
        flattened_states = hidden_states.reshape(-1, hidden_states.size(-1))
        flattened_weights = routing_weights.reshape(-1, routing_weights.size(-1))
        
        # Ensure dtype consistency for matmul
        flattened_weights = flattened_weights.to(dtype=flattened_states.dtype)
        
        # Calculate weighted centroids for each expert
        # [experts, N] @ [N, hidden] -> [experts, hidden]
        expert_centroids = torch.matmul(flattened_weights.t(), flattened_states)
        
        # Normalize by total weight assigned to expert
        expert_weight_sums = flattened_weights.sum(dim=0) + self.epsilon
        expert_centroids = expert_centroids / expert_weight_sums.unsqueeze(-1)
        
        # Cosine similarity between centroids
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        normalized_centroids = expert_centroids / (expert_centroids.norm(p=2, dim=1, keepdim=True) + 1e-6)
        similarity = torch.matmul(normalized_centroids, normalized_centroids.t())
        
        # Minimize off-diagonal similarity (contrastive)
        num_experts = similarity.size(0)
        mask = torch.eye(num_experts, device=similarity.device).bool()
        contrastive_loss = similarity[~mask].mean()
        
        return contrastive_loss

def load_balancing_loss_func(
    gate_logits: torch.Tensor,
    num_experts: int,
    top_k: int = 2,
    attention_mask: Optional[torch.Tensor] = None,
    router_z_loss_coef: Optional[float] = None,
    router_entropy_coef: Optional[float] = None,
    usage_uniformity_coef: Optional[float] = None,
) -> torch.Tensor:
    r"""
    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.
    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss
    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between
    experts is too unbalanced.
    Args:
        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):
            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of
            shape [batch_size X sequence_length, num_experts].
        attention_mask (`torch.Tensor`, None):
            The attention_mask used in forward function
            shape [batch_size X sequence_length] if not None.
        num_experts (`int`, *optional*):
            Number of experts
        router_z_loss_coef (`float`, *optional*):
            Coefficient for the z-loss term in the load balancing loss.
    Returns:
        The auxiliary loss.
    """
    if gate_logits is None:
        return torch.tensor(0.0)

    if isinstance(gate_logits, tuple):
        # Add a check for empty tuple
        if not gate_logits:
            return torch.tensor(0.0)
        compute_device = gate_logits[0].device
        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)
    else:
        # handle tensor input (single tensor from global router)
        concatenated_gate_logits = gate_logits
        compute_device = concatenated_gate_logits.device

    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)

    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)

    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)

    if attention_mask is None:
        # Compute the percentage of tokens routed to each experts
        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.mean(routing_weights, dim=0)
    else:
        batch_size, sequence_length = attention_mask.shape
        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)

        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask
        expert_attention_mask = (
            attention_mask[None, :, :, None, None]
            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))
            .reshape(-1, top_k, num_experts)
            .to(compute_device)
        )

        # Compute the percentage of tokens routed to each experts
        # Sum over batch*seq and top_k dimensions to get [num_experts]
        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=(0, 1)) / torch.sum(
            expert_attention_mask, dim=(0, 1)
        )

        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert
        router_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))
            .reshape(-1, num_experts)
            .to(compute_device)
        )

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(
            router_per_expert_attention_mask, dim=0
        )

    # Core Switch-style load balancing loss
    aux_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0)) * num_experts

    # Router z-loss (Switch Transformer) to prevent overconfident routers
    if router_z_loss_coef is not None and router_z_loss_coef > 0:
        log_z = torch.logsumexp(concatenated_gate_logits, dim=-1)
        z_loss = torch.square(log_z).mean()
        aux_loss = aux_loss + router_z_loss_coef * z_loss

    # Entropy regularization to avoid routing collapse (maximize entropy)
    if router_entropy_coef is not None and router_entropy_coef > 0:
        token_entropy = -(routing_weights * torch.log(routing_weights.clamp_min(1e-12))).sum(dim=-1)
        # Normalize by log(num_experts) for scale invariance across different expert counts
        normalized_entropy = token_entropy / math.log(max(num_experts, 2))
        entropy_reg = -normalized_entropy.mean()
        aux_loss = aux_loss + router_entropy_coef * entropy_reg

    # Usage uniformity (optional): encourage average routing probability per expert to be near-uniform
    if usage_uniformity_coef is not None and usage_uniformity_coef > 0:
        # router_prob_per_expert already accounts for attention mask when provided
        target = torch.full_like(router_prob_per_expert, 1.0 / float(num_experts))
        usage_uniformity_loss = torch.mean(torch.square(router_prob_per_expert - target))
        aux_loss = aux_loss + usage_uniformity_coef * usage_uniformity_loss

    return aux_loss


def gramspec_lb_loss(
    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
    num_experts: int,
    lb_l2_coef: float = 1.0,
    lb_cv_coef: float = 0.5,
    lb_entropy_floor_coef: float = 0.0,
    top_k: int = 2,
    lb_topk_l2_coef: float = 0.0,
    lb_topk_cv_coef: float = 0.0,
    attention_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    GramSpec-aligned low-overhead load balancing loss.
    Uses per-expert statistics only (O(E)) to minimize compute overhead.
    No shape changes or fallback zeros/nan - skips invalid terms.

    Args:
        gate_logits: Routing logits, either tensor [N, E] or tuple of tensors
        num_experts: Number of experts E
        lb_l2_coef: Weight for L2 uniformity loss
        lb_cv_coef: Weight for CV minimization
        lb_entropy_floor_coef: Weight for entropy floor (optional)
        top_k: Number of experts selected per token
        lb_topk_l2_coef: Weight for top-k token count uniformity loss
        lb_topk_cv_coef: Weight for top-k token count CV minimization
        attention_mask: Attention mask for valid tokens

    Returns:
        Scalar loss tensor (sum of weighted terms)
    """
    # Handle tuple input (concatenate if needed)
    if isinstance(gate_logits, tuple):
        if not gate_logits:  # Empty tuple - skip entirely
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        tensors = [gl for gl in gate_logits if gl is not None and gl.numel() > 0]
        if not tensors:
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        gate_logits = torch.cat(tensors, dim=0)

    if gate_logits is None or gate_logits.numel() == 0:
        return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)

    # Get routing weights using existing softmax path
    routing_weights = torch.nn.functional.softmax(gate_logits, dim=-1)
    device, dtype = routing_weights.device, routing_weights.dtype

    # Compute per-expert mean probabilities
    routing_per_expert_attention_mask = None
    if attention_mask is not None:
        # Use existing masking logic from load_balancing_loss_func
        batch_size, seq_length = attention_mask.shape
        num_hidden_layers = gate_logits.shape[0] // (batch_size * seq_length)

        routing_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, seq_length, num_experts))
            .reshape(-1, num_experts)
            .to(device)
        )

        # Mean per expert (masked)
        p_bar = torch.sum(routing_weights * routing_per_expert_attention_mask, dim=0) / torch.sum(
            routing_per_expert_attention_mask, dim=0
        )
    else:
        # Simple mean across batch dimension
        p_bar = routing_weights.mean(dim=0)  # [E]

    # Uniform target distribution
    u = torch.full_like(p_bar, 1.0 / float(num_experts))

    # Numerical stability
    eps = torch.finfo(dtype).eps

    total_loss = torch.zeros((), dtype=dtype, device=device)

    # L2 uniformity loss
    if lb_l2_coef > 0:
        l2_loss = torch.sum((p_bar - u) ** 2)
        total_loss = total_loss + lb_l2_coef * l2_loss

    # CV minimization (approximate)
    if lb_cv_coef > 0:
        mean_p = p_bar.mean()
        var_p = p_bar.var(unbiased=False)  # Population variance
        cv_loss = var_p / (mean_p + eps)
        total_loss = total_loss + lb_cv_coef * cv_loss

    # Entropy floor (optional)
    if lb_entropy_floor_coef > 0:
        # Token-level entropy floor
        token_entropy = -torch.sum(routing_weights * torch.log(routing_weights + eps), dim=-1)
        entropy_floor_loss = -token_entropy.mean()  # Minimize negative entropy = maximize entropy
        total_loss = total_loss + lb_entropy_floor_coef * entropy_floor_loss

    # Top-k token count based losses (works on discrete expert selection)
    if (lb_topk_l2_coef > 0 or lb_topk_cv_coef > 0) and top_k > 0:
        k = min(top_k, num_experts)
        # top-k indices based on routing probabilities (monotonic with logits)
        topk_probs, topk_indices = torch.topk(routing_weights, k=k, dim=-1)

        if attention_mask is not None and routing_per_expert_attention_mask is not None:
            token_mask = routing_per_expert_attention_mask.any(dim=-1)
            if token_mask.any():
                topk_indices = topk_indices[token_mask]
                topk_probs = topk_probs[token_mask]
            else:
                topk_indices = topk_indices[:0]
                topk_probs = topk_probs[:0]

        flat_indices = topk_indices.reshape(-1)
        flat_probs = topk_probs.reshape(-1)

        counts = torch.zeros(num_experts, dtype=dtype, device=device)
        if flat_indices.numel() > 0:
            ones = torch.ones_like(flat_indices, dtype=dtype, device=device)
            counts.scatter_add_(0, flat_indices, ones)

            weighted_counts = torch.zeros_like(counts)
            weighted_counts.scatter_add_(0, flat_indices, flat_probs.to(dtype))

            total_counts = counts.sum()
            if total_counts > 0 and lb_topk_l2_coef > 0:
                usage_distribution = counts / total_counts
                l2_topk_loss = torch.sum((usage_distribution - u) ** 2)
                total_loss = total_loss + lb_topk_l2_coef * l2_topk_loss

            mean_weight = weighted_counts.mean()
            var_weight = weighted_counts.var(unbiased=False) if weighted_counts.numel() > 0 else torch.tensor(0.0, device=device, dtype=dtype)
            if lb_topk_cv_coef > 0 and mean_weight > 0:
                topk_cv_loss = var_weight / (mean_weight + eps)
                total_loss = total_loss + lb_topk_cv_coef * topk_cv_loss

    return total_loss


# Copied from Phi-3.5-MoE
def _get_unpad_data(attention_mask):
    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)
    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
    max_seqlen_in_batch = seqlens_in_batch.max().item()
    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))
    return (
        indices,
        cu_seqlens,
        max_seqlen_in_batch,
    )
    
@dataclass
class GramSpecMoEModelOutputWithPast(BaseModelOutputWithPast):
    """
    Base class for GramSpecMoE outputs, with hidden states and attentions.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.
    """

    image_hidden_states: Optional[torch.FloatTensor] = None
    aux_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None
    expression_reg_loss: Optional[torch.FloatTensor] = None
    routing_uncertainty: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    load_balancing_loss: Optional[torch.FloatTensor] = None
    sinkhorn_loss: Optional[torch.FloatTensor] = None


@dataclass
class GramSpecMoECausalLMOutputWithPast(ModelOutput):
    """
    Base class for GramSpecMoE causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    image_hidden_states: Optional[torch.FloatTensor] = None
    # this is moe specific
    aux_loss: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None
    expression_reg_loss: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    load_balancing_loss: Optional[torch.FloatTensor] = None
    sinkhorn_loss: Optional[torch.FloatTensor] = None
    # hn_context ì œê±° - ì°¨ì› ë¬¸ì œë¡œ ì¸í•´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ


class GramSpecMoETextScaledWordEmbedding(nn.Embedding):
    """
    This module overrides nn.Embeddings' forward by multiplying with embeddings scale.
    """

    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        self.register_buffer("embed_scale", torch.tensor(embed_scale), persistent=False)

    def forward(self, input_ids: torch.Tensor):
        return super().forward(input_ids) * self.embed_scale


class GramSpecMoEMLP(nn.Module):
    def __init__(self, config: GramSpecMoETextConfig, intermediate_size: Optional[int]=None, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        # Mark for fast init path
        setattr(self.gate_proj, "_is_gramspec_moe_gate_layer", True)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_activation]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class mp(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx, 
        scores: torch.Tensor, 
        multiplier: torch.Tensor, 
        selected_experts: torch.Tensor,
        masked_gates: torch.Tensor,
        mask_for_one: torch.Tensor,
    ):
        ctx.save_for_backward(multiplier, selected_experts, masked_gates)
        return multiplier * mask_for_one
        
    @staticmethod
    def backward(
        ctx, 
        grad_at_output: torch.Tensor, 
    ):
        multiplier, selected_experts, masked_gates = ctx.saved_tensors
        
        grad_at_output = grad_at_output * multiplier
        
        grad_at_scores_expaned = masked_gates * grad_at_output.mul(-1)
        grad_at_scores_expaned.scatter_add_(
            dim=-1,
            index=selected_experts,
            src=grad_at_output,
        )
        
        return (
            grad_at_scores_expaned, 
            None, 
            None, 
            None, 
            None, 
        )


class ExpressionProjector(nn.Module):
    """
    Expression projector for expert specialization with pre-computed orthogonal matrix
    Encourages each expert to learn unique expression patterns through orthogonal projection
    """

    def __init__(self, input_dim, output_dim, num_experts, method='precomputed'):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_experts = num_experts
        self.method = method
        
        if method == 'precomputed':
            # Use nn.Linear for standard weight initialization
            self.linear_projection = nn.Linear(input_dim, output_dim, bias=False)
            # Initialize with orthogonal-like weights using proper initialization
        else:
            # Base projection matrix for training-time orthogonalization
            self.projection_matrix = nn.Parameter(torch.randn(input_dim, output_dim) * 0.1)
            
        # Newton-Schulz iteration parameters (for training only)
        self.ns_steps = 5
        self.ns_coeffs = (3.4445, -4.7750, 2.0315)  # (a, b, c)
        
        # Orthogonal constraint strength
        self.ortho_strength = 0.1
        

    def _initialize_orthogonal_matrix(self, input_dim, output_dim):
        """Initialize a random orthogonal matrix"""
        # BF16/FP8/FP4ì—ì„œëŠ” SVD/QRì´ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ ë‹¨ìˆœí•œ random matrix ì‚¬ìš©
        # ì‹¤ì œë¡œëŠ” "quasi-orthogonal" matrixë¡œ ì¶©ë¶„í•¨
        scale = (2.0 / (input_dim + output_dim)) ** 0.5
        return torch.randn(input_dim, output_dim) * scale
    
    def _initialize_linear_weights(self):
        """Initialize linear projection weights properly"""
        # Use PyTorch's built-in orthogonal initialization
        # This handles dtype and device compatibility automatically
        torch.safe_orthogonal_(self.linear_projection.weight)
        
        # Scale down the weights for better stability
        with torch.no_grad():
            self.linear_projection.weight.data *= 0.1
        
    def newton_schulz_orthogonalize(self, G, steps=None):
        """Newton-Schulz iteration for orthogonalization"""
        if steps is None:
            steps = self.ns_steps
            
        a, b, c = self.ns_coeffs
        X = G
        
        # Ensure spectral norm is at most 1
        X = X / (X.norm() + 1e-7)
        
        # Perform NS iterations
        for _ in range(steps):
            A = X @ X.T
            B = b * A + c * A @ A
            X = a * X + B @ X
            
        return X
    
    def qr_orthogonalize(self, G):
        """QR decomposition for orthogonalization"""
        Q, R = torch.linalg.qr(G, mode='reduced')
        return Q
    
    def svd_orthogonalize(self, G):
        """SVD-based orthogonalization"""
        U, S, V = torch.linalg.svd(G, full_matrices=False)
        return U @ V.T
    
    def forward(self, x):
        """
        Forward pass with orthogonal projection
        x: [batch_size, input_dim]
        """
        if self.method == 'precomputed':
            # Fast inference: use linear projection
            # Ensure linear_projection parameters are trainable
            if hasattr(self.linear_projection, 'weight'):
                if not self.linear_projection.weight.requires_grad:
                    self.linear_projection.weight.requires_grad = True
            if hasattr(self.linear_projection, 'bias') and self.linear_projection.bias is not None:
                if not self.linear_projection.bias.requires_grad:
                    self.linear_projection.bias.requires_grad = True
            
            orthogonal_logits = self.linear_projection(x)
            # L2 normalization for unit vectors with safe epsilon to prevent NaN (0/0)
            # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            orthogonal_logits = orthogonal_logits / (orthogonal_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
            
            # Ensure gradient is retained during training
            if self.training:
                orthogonal_logits = orthogonal_logits#.requires_grad_(True)
            
            return orthogonal_logits
            
        elif self.method == 'newton_schulz' and self.training:
            # Training only: Newton-Schulz iteration for orthogonalization
            P_ortho = self.newton_schulz_orthogonalize(self.projection_matrix)
            orthogonal_logits = torch.matmul(x, P_ortho)
            # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            orthogonal_logits = orthogonal_logits / (orthogonal_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
            return orthogonal_logits
            
        elif self.method == 'qr' and self.training:
            # Training only: QR decomposition
            P_ortho = self.qr_orthogonalize(self.projection_matrix)
            orthogonal_logits = torch.matmul(x, P_ortho)
            # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            orthogonal_logits = orthogonal_logits / (orthogonal_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
            return orthogonal_logits
            
        elif self.method == 'svd' and self.training:
            # Training only: SVD-based orthogonalization
            P_ortho = self.svd_orthogonalize(self.projection_matrix)
            orthogonal_logits = torch.matmul(x, P_ortho)
            # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            orthogonal_logits = orthogonal_logits / (orthogonal_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
            return orthogonal_logits
            
        else:  # 'linear' or inference fallback
            # Simple linear projection (fastest)
            orthogonal_logits = torch.matmul(x, self.projection_matrix)
            # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            orthogonal_logits = orthogonal_logits / (orthogonal_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
            return orthogonal_logits
    
    def orthogonal_loss(self):
        """Compute orthogonal constraint loss"""
        if self.method == 'precomputed':
            # For precomputed, use the linear projection weights
            gram_matrix = torch.matmul(self.linear_projection.weight, self.linear_projection.weight.T)
            identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
            ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
            return self.ortho_strength * ortho_loss
            
        elif self.method == 'linear':
            return torch.tensor(0.0, device=self.projection_matrix.device)
            
        else:
            # For other methods, use projection matrix
            gram_matrix = torch.matmul(self.projection_matrix, self.projection_matrix.T)
            identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
            ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
            return self.ortho_strength * ortho_loss

# ì°¸ê³ ìš© sparse mixer2. ì‚¬ìš©ë˜ì§€ëŠ” ì•ŠìŒ
def sparsemixer(scores, top_k, jitter_eps, training):
    assert top_k == 2

    ################ first expert ################
    with torch.no_grad():
        # compute mask for sparsity
        mask_logits_threshold, max_ind = scores.max(dim=-1, keepdim=True)
        factor = scores.abs().clamp(min=mask_logits_threshold.abs()).clamp(min=1e-10)  # ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê°œì„ 
        mask_logits_threshold = (
            (mask_logits_threshold - scores) / factor
        ) > (2 * jitter_eps)

    # apply mask 
    masked_gates = scores.masked_fill(mask_logits_threshold, float('-inf'))
    if training:
        selected_experts = (
            masked_gates - torch.empty_like(masked_gates, memory_format=torch.legacy_contiguous_format).exponential_().log()
        ).max(dim=-1)[1].unsqueeze(-1) # gumbel sampling, more robust than than the multinomial method
    else:
        selected_experts = max_ind
        
    # compute scores for gradients
    masked_gates = torch.softmax(masked_gates, dim=-1)
    
    # Ensure selected_experts indices are within bounds
    num_experts = masked_gates.size(-1)
    selected_experts = torch.clamp(selected_experts, 0, num_experts - 1)
    
    multiplier_o = masked_gates.gather(dim=-1, index=selected_experts)
    
    if training:
        # compute midpoint mask 
        max_scores, max_ind = masked_gates.max(dim=-1, keepdim=True)
        mask_for_one = torch.logical_or(
            selected_experts == max_ind,
            torch.rand_like(max_scores) > 0.75 # Heun's third-order method: f(x) - f(0) = .25 f'(x) + .75 f'(x/3.)
        ) 
        # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5
        mask_for_one = torch.add(0.3333, mask_for_one, alpha=0.6667).type_as(masked_gates)

        multiplier = mp.apply(
            scores, 
            multiplier_o, 
            selected_experts, 
            masked_gates, 
            mask_for_one,
        )
    else:
        multiplier = multiplier_o

    # masked out first expert 
    masked_scores = torch.scatter(
        scores,
        -1,
        selected_experts,
        float('-inf'),
    )
    with torch.no_grad():
        # compute mask for sparsity
        mask_logits_threshold, max_ind = masked_scores.max(dim=-1, keepdim=True)
        factor = scores.abs().clamp(min=mask_logits_threshold.abs()).clamp(min=1e-8)  # ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê°œì„ 
        mask_logits_threshold = (
            (mask_logits_threshold - scores) / factor
        ) > (2 * jitter_eps)

    # apply mask 
    masked_gates_top2 = masked_scores.masked_fill(mask_logits_threshold, float('-inf'))
    if training:
        selected_experts_top2 = (
            masked_gates_top2 - torch.empty_like(masked_gates_top2, memory_format=torch.legacy_contiguous_format).exponential_().log()
        ).max(dim=-1)[1].unsqueeze(-1) # gumbel sampling, more robust than than the multinomial method
    else:
        selected_experts_top2 = max_ind
    # compute scores for gradients
    masked_gates_top2 = torch.softmax(masked_gates_top2, dim=-1)
    
    # Ensure selected_experts_top2 indices are within bounds
    selected_experts_top2 = torch.clamp(selected_experts_top2, 0, num_experts - 1)
    
    multiplier_top2_o = masked_gates_top2.gather(dim=-1, index=selected_experts_top2)
    
    if training: 
        # compute midpoint mask 
        max_scores, max_ind = masked_gates_top2.max(dim=-1, keepdim=True)
        mask_for_one_top2 = torch.logical_or(
            selected_experts_top2 == max_ind,
            torch.rand_like(max_scores).uniform_() > 0.75 # Heun's third-order method: f(x) - f(0) = .25 f'(x) + .75 f'(x/3.)
        ) 
        # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5
        mask_for_one_top2 = torch.add(0.3333, mask_for_one_top2, alpha=0.6667).type_as(masked_gates_top2)

        multiplier_top2 = mp.apply(
            scores, 
            multiplier_top2_o, 
            selected_experts_top2, 
            masked_gates_top2, 
            mask_for_one_top2,
        )
    else:
        multiplier_top2 = multiplier_top2_o
    
    multiplier = torch.cat((multiplier, multiplier_top2), dim=-1)
    selected_experts = torch.cat((selected_experts, selected_experts_top2), dim=-1)
    
    return (
        multiplier, 
        selected_experts,
    )


class ExpressionProjector(nn.Module):
    """
    Expression Projector: ê° ì „ë¬¸ê°€ë³„ ê³ ìœ í•œ 128ì°¨ì› ë²¡í„° ê³µê°„ì„ ìƒì„±
    ì°¨ì› í™•ìž¥: 128 -> 128 * num_experts (ê° ì „ë¬¸ê°€ê°€ ìžì‹ ë§Œì˜ ê³µê°„ì„ ê°€ì§)
    """
    def __init__(self, hidden_size: int, output_dim: int, num_experts: int, method: str = 'precomputed', config=None):
        super().__init__()
        self.hidden_size = hidden_size
        self.output_dim = output_dim  # num_experts * router_dim
        self.num_experts = num_experts
        # [ì¤‘ìš”] router_dim ê³„ì‚° ë°©ì‹ í™•ì¸
        self.router_dim = output_dim // num_experts
        self.method = method
        
        self.projection = nn.Linear(hidden_size, output_dim, bias=False)
        # Configì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ìœ ì§€)
        self.ortho_strength = getattr(config, "expression_ortho_strength", 0.1) if config else 0.1
        self.init_scale = getattr(config, "expression_init_scale", 0.1) if config else 0.1
        
        # [ìˆ˜ì • 1] ì´ˆê¸°í™” ë³€ê²½: Orthogonal -> Xavier
        # ì´ìœ : ì²˜ìŒë¶€í„° ì§êµí•˜ë©´ Lossê°€ 0ì´ë¼ í•™ìŠµì„ ì•ˆ í•¨. 
        # ëžœë¤(Xavier)ìœ¼ë¡œ ì‹œìž‘í•´ì•¼ Lossê°€ ëœ¨ê³ , ê·¸ê±¸ ì¤„ì´ë ¤ê³  ë…¸ë ¥í•¨.
        self._initialize_weights()

    def _initialize_weights(self):
        # Xavier Uniformìœ¼ë¡œ ëžœë¤í•˜ê²Œ ì„žì–´ì„œ ì‹œìž‘
        nn.init.xavier_uniform_(self.projection.weight)
        # ì´ˆê¸° í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•´ í¬ê¸°ë§Œ ì‚´ì§ ì¤„ìž„ (configì—ì„œ ì œì–´ ê°€ëŠ¥)
        with torch.no_grad():
            self.projection.weight.data *= self.init_scale
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [batch, seq, hidden_size]
        Returns: [batch, seq, num_experts * router_dim]
        """
        return self.projection(x)
    
    def orthogonal_loss(self) -> torch.Tensor:
        """
        Orthogonality loss: projection weightì˜ ì§êµì„± ì œì•½
        """
        W = self.projection.weight  # [output_dim, hidden_size]
        
        # Reshape to [num_experts, router_dim, hidden_size]
        W_reshaped = W.view(self.num_experts, self.router_dim, self.hidden_size)
        
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ (+ 1e-6)
        W_normalized = W_reshaped / (W_reshaped.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # Compute Gram matrix for each expert: [num_experts, router_dim, router_dim]
        gram_matrices = torch.matmul(W_normalized, W_normalized.transpose(-2, -1))
        
        # Target: identity matrix for each expert
        identity = torch.eye(self.router_dim, device=W.device, dtype=W.dtype).unsqueeze(0).expand(self.num_experts, -1, -1)
        
        # [ìˆ˜ì •] Loss ê°•ë„ ì¡°ì ˆ (ë„ˆë¬´ í¬ë©´ ì•ˆ ë˜ë¯€ë¡œ í‰ê· )
        ortho_loss = torch.mean(torch.pow(torch.norm(gram_matrices - identity, p='fro', dim=(-2, -1)), 2))
        return self.ortho_strength * ortho_loss


class ManualGRUCell(nn.Module):
    """
    LoRA-friendly GRU cell implemented with Linear layers only.
    Gate weights are explicitly named for PEFT targeting.
    """

    def __init__(self, input_size: int, hidden_size: int):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        self.weight_ih_gates = nn.Linear(input_size, hidden_size * 2)
        self.weight_hh_gates = nn.Linear(hidden_size, hidden_size * 2)

        self.weight_ih_cand = nn.Linear(input_size, hidden_size)
        self.weight_hh_cand = nn.Linear(hidden_size, hidden_size)

        safe_orthogonal_(self.weight_hh_gates.weight)
        safe_orthogonal_(self.weight_hh_cand.weight)
        nn.init.xavier_uniform_(self.weight_ih_gates.weight)
        nn.init.xavier_uniform_(self.weight_ih_cand.weight)

    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:
        gates_x = self.weight_ih_gates(x)
        gates_h = self.weight_hh_gates(h)
        gates = gates_x + gates_h

        r_gate, z_gate = gates.chunk(2, dim=1)
        r_gate = torch.sigmoid(r_gate)
        z_gate = torch.sigmoid(z_gate)

        cand_h = self.weight_hh_cand(h * r_gate)
        cand_x = self.weight_ih_cand(x)
        n = torch.tanh(cand_x + cand_h)

        next_h = (1 - z_gate) * n + z_gate * h
        return next_h


class DualPotentialLinearSolver(nn.Module):
    """
    Sinkhorn-inspired dual potential optimizer using the manual GRU cell.
    Iteratively refines logits to satisfy row/column balancing targets.
    """

    def __init__(self, num_experts: int, n_iterations: int = 3):
        super().__init__()
        self.num_experts = num_experts
        self.n_iterations = n_iterations

        dim = num_experts
        self.gru_cell = ManualGRUCell(input_size=dim * 2, hidden_size=dim * 2)
        self.delta_proj = nn.Linear(dim * 2, dim * 2)

        nn.init.zeros_(self.delta_proj.weight)
        nn.init.zeros_(self.delta_proj.bias)

    def forward(self, logits: torch.Tensor, training: bool = True) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            logits: [..., num_experts] raw logits to balance
            training: when True, return residual constraint loss
        Returns:
            balanced_logits: same shape as logits
            balance_loss: scalar residual (row/col constraint violation)
        """
        input_shape = logits.shape
        logits_2d = logits.view(-1, self.num_experts)
        batch_tokens = logits_2d.shape[0]
        device = logits_2d.device
        dtype = logits_2d.dtype

        h = torch.zeros(batch_tokens, self.num_experts * 2, device=device, dtype=dtype)
        target_row = torch.zeros(batch_tokens, 1, device=device, dtype=dtype)
        target_col = torch.full((1, self.num_experts), -math.log(self.num_experts), device=device, dtype=dtype)

        current_logits = logits_2d
        final_row_error = None
        final_col_error = None

        for _ in range(self.n_iterations):
            curr_row_sum = torch.logsumexp(current_logits, dim=-1, keepdim=True)
            curr_col_sum = torch.logsumexp(current_logits, dim=0, keepdim=True)

            row_error = curr_row_sum - target_row
            col_error = curr_col_sum - target_col

            final_row_error = row_error
            final_col_error = col_error

            error_input = torch.cat(
                [
                    row_error.expand(-1, self.num_experts),
                    col_error.expand(batch_tokens, -1),
                ],
                dim=-1,
            )

            h = self.gru_cell(error_input, h)
            delta = self.delta_proj(h)
            delta_u, delta_v = delta.chunk(2, dim=-1)
            current_logits = current_logits + delta_u + delta_v

        balance_loss = torch.tensor(0.0, device=device, dtype=dtype)
        if training and final_row_error is not None:
            balance_loss = (final_row_error.pow(2).mean()) + (final_col_error.pow(2).mean())

        return current_logits.view(input_shape), balance_loss


class SinkhornGRUBalancer(nn.Module):
    """
    Legacy code: this one must replaced by DualPotentialLinearSolver
    Residual balancer: learns a bias correction (target = Sinkhorn(base) - base).
    Returns corrected logits and imitation loss.
    """

    def __init__(self, num_experts: int, n_iterations: int = 3, temperature: float = 0.1):
        super().__init__()
        self.num_experts = num_experts
        self.n_iterations = n_iterations
        self.temperature = temperature

        # Features: [current_load, target_load, logit_mean, logit_std] -> 4E
        self.balancing_gru = nn.GRU(
            input_size=num_experts * 4,
            hidden_size=num_experts * 4,
            num_layers=1,
            batch_first=True,
        )
        self.bias_head = nn.Linear(num_experts * 4, num_experts)
        nn.init.zeros_(self.bias_head.weight)
        nn.init.zeros_(self.bias_head.bias)

    def _run_math_sinkhorn(self, logits: torch.Tensor) -> torch.Tensor:
        # Teacher: produce balanced logits (detached)
        epsilon = self.temperature
        Q = (logits.detach()) / epsilon
        for _ in range(self.n_iterations):
            Q = Q - torch.logsumexp(Q, dim=-1, keepdim=True)
            log_col_sum = torch.logsumexp(Q, dim=0, keepdim=True)
            log_mean_sum = torch.logsumexp(Q, dim=(0, 1), keepdim=True) - math.log(self.num_experts)
            Q = Q - (log_col_sum - log_mean_sum)
        return Q  # balanced logits (in log-space)

    def _run_gru_predictor(self, logits: torch.Tensor) -> torch.Tensor:
        probs = F.softmax(logits.detach(), dim=-1)
        current_load = probs.mean(dim=0)  # [E]
        target_load = torch.full_like(current_load, 1.0 / self.num_experts)
        logit_mean = logits.mean(dim=0)
        logit_std = logits.std(dim=0)

        features = torch.cat([current_load, target_load, logit_mean, logit_std], dim=-1)
        features = features.unsqueeze(0).unsqueeze(0)  # [1,1,4E]
        self.balancing_gru.flatten_parameters()
        _, hn = self.balancing_gru(features)
        predicted_bias = self.bias_head(hn.squeeze(0))
        return predicted_bias

    def forward(self, base_logits: torch.Tensor, training: bool = True):
        # Student predicts correction
        predicted_bias = self._run_gru_predictor(base_logits)
        imitation_loss = base_logits.new_zeros(())

        if training:
            # Teacher target
            balanced_logits_target = self._run_math_sinkhorn(base_logits)
            target_bias = balanced_logits_target - base_logits.detach()
            imitation_loss = F.mse_loss(predicted_bias, target_bias)
            final_logits = base_logits + predicted_bias
        else:
            final_logits = base_logits + predicted_bias

        return final_logits, imitation_loss


class GramSpecMoERouter(nn.Module):
    def __init__(self, config: GramSpecMoETextConfig, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.router_dim = config.router_dim
        
        # [ìˆ˜ì • 1: ì°¨ì› í™•ìž¥] ì•ˆê²½ ëŒë ¤ì“°ê¸° ë°©ì§€ (128 -> 128 * N)
        # ê° ì „ë¬¸ê°€ê°€ ë…ë¦½ì ì¸ ë²¡í„° ê³µê°„ì„ ê°€ì ¸ì•¼ Orthogonalityê°€ ì„±ë¦½í•¨
        self.expression_projector = ExpressionProjector(
            self.hidden_size, 
            self.num_experts * self.router_dim, 
            self.num_experts, 
            method='precomputed',
            config=config
        )
        
        # [ìœ ì§€] GRU Load Balancer (ë¬¸ë§¥ íŒŒì•…ìš©)
        # Configì—ì„œ GRU layer ìˆ˜ ì œì–´ ê°€ëŠ¥
        gru_num_layers = getattr(config, "router_gru_num_layers", 1)
        self.load_balancer = nn.GRU(
            input_size=self.hidden_size,
            hidden_size=self.num_experts * self.router_dim,
            num_layers=gru_num_layers,
            bias=False,
            batch_first=True,
        )
        
        # [ìˆ˜ì •] í•˜ë“œì½”ë”© *10.0 ì‚­ì œ -> í•™ìŠµ ê°€ëŠ¥í•œ Temperature íŒŒë¼ë¯¸í„° ì¶”ê°€
        # ì´ˆê¸°ê°’ì€ configì—ì„œ ì œì–´ ê°€ëŠ¥ (ê¸°ë³¸ê°’: log(10) = 2.302...)
        logit_scale_init = getattr(config, "router_logit_scale_init", math.log(10))
        self.logit_scale = nn.Parameter(torch.ones([]) * logit_scale_init)
        
        # Logit scaleì˜ ìµœëŒ€ê°’ ì œí•œ (configì—ì„œ ì œì–´ ê°€ëŠ¥)
        self.logit_scale_max = getattr(config, "router_logit_scale_max", 100.0)
        
        # LayerNorm epsilon (configì—ì„œ ì œì–´ ê°€ëŠ¥)
        self.layernorm_eps = getattr(config, "router_layernorm_eps", 1e-5)
        
        # EMA coefficient (for expert specialization EMA in _sparse_routing)
        self.ema_alpha = getattr(config, "ema_alpha", 0.99)
        
        # Callbackìš© ì†ì„± ì´ˆê¸°í™”
        self.last_selected_experts = None
        self.last_routing_weights = None
        self.last_num_experts = None
        self.last_speciality_loss = None
        self.last_cosine_similarities = None
        self.last_expression_reg_loss = None
        self.last_ortho_loss = None
        # Neural Sinkhorn solver built from Linear-only GRU for PEFT-friendly routing
        solver_iters = getattr(config, "sinkhorn_iter", 3)
        self.sinkhorn_solver = DualPotentialLinearSolver(
            num_experts=self.num_experts,
            n_iterations=solver_iters,
        )
        # Alias for downstream PEFT helper code expecting logit_balancer attr
        self.logit_balancer = self.sinkhorn_solver

    def compute_token_expert_similarity(self, token_states: torch.Tensor, expert_weights: torch.Tensor) -> torch.Tensor:
        """
        Compute token-wise cosine similarity between tokens and expert projection weights.
        Returns [N, E] similarity matrix where N is number of tokens, E is number of experts.
        
        Args:
            token_states: [N, hidden_size] flattened token states
            expert_weights: [num_experts * router_dim, hidden_size] projection weights
        Returns:
            sim_matrix: [N, E] cosine similarity matrix
        """
        # Reshape expert weights: [E * R, H] -> [E, R, H]
        W = expert_weights.view(self.num_experts, self.router_dim, self.hidden_size)
        # Normalize along hidden dim
        W_norm = W / W.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-6)

        # Normalize tokens
        token_norm = token_states / token_states.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-6)  # [N, H]

        # Cosine sim per router_dim then average: einsum for clarity
        # token_norm: [N, H], W_norm: [E, R, H] -> [N, E, R]
        sim = torch.einsum("nh,erh->ner", token_norm, W_norm)
        sim_matrix = sim.mean(dim=-1)  # [N, E]

        return sim_matrix

    def build_sinkhorn_cost(self, logits: torch.Tensor, sim_matrix: torch.Tensor, alpha: float = 0.5) -> torch.Tensor:
        """
        Build Sinkhorn cost matrix with orthogonality penalty.
        C = -logits + alpha * (1 - sim_matrix)
        
        Args:
            logits: [N, E] routing logits
            sim_matrix: [N, E] token-expert similarity matrix
            alpha: penalty strength
        Returns:
            cost: [N, E] cost matrix
        """
        sim_clamped = sim_matrix.clamp(-1.0, 1.0)  # cosine range safety
        cost = -logits + alpha * (1.0 - sim_clamped)
        return cost

    def sinkhorn_algorithm(self, cost: torch.Tensor, epsilon: float, iterations: int) -> torch.Tensor:
        """
        Log-Space Sinkhorn: BF16 ì•ˆì •ì„±ì„ ìœ„í•´ ë¡œê·¸ ê³µê°„ì—ì„œ ì—°ì‚°
        Cost matrixë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬ (C = -logits + alpha * (1 - sim_matrix))
        
        Args:
            cost: [N, E] cost matrix (lower cost = better match)
            epsilon: temperature parameter
            iterations: number of Sinkhorn iterations
        Returns:
            Q: [N, E] doubly-stochastic matrix
        """
        # Log-domain initialization: exp(-C/epsilon)
        log_Q = -cost / epsilon

        for _ in range(iterations):
            # Row Norm: ëª¨ë“  í† í°ì€ 1ì˜ í™•ë¥ ì„ ê°€ì ¸ì•¼ í•¨
            log_Q = log_Q - torch.logsumexp(log_Q, dim=-1, keepdim=True)
            
            # Column Norm: ëª¨ë“  ì „ë¬¸ê°€ëŠ” í‰ê· ì ìœ¼ë¡œ (1/N)ì˜ ë¡œë“œë¥¼ ê°€ì ¸ì•¼ í•¨
            # ë°°ì¹˜ ë‚´ ì´í•©(Probability Mass) ë³´ì¡´ì„ ìœ„í•´ Mean-Centering ë°©ì‹ ì‚¬ìš©
            log_col_sum = torch.logsumexp(log_Q, dim=0, keepdim=True)
            log_mean_sum = torch.logsumexp(log_Q, dim=(0, 1), keepdim=True) - math.log(self.num_experts)
            log_Q = log_Q - (log_col_sum - log_mean_sum)

        return torch.exp(log_Q)

    @torch._dynamo.disable
    def forward(self, x, hn, top_k=2, jitter_eps=0.01):
        batch_size, seq_len, _ = x.shape
        
        routing_output, hn = self.load_balancer(x, hn)
        expression_output_raw = self.expression_projector(x)
        # Spec norm regularization (pre-normalization): keep magnitude near 1
        expression_reg_loss = (expression_output_raw.norm(p=2, dim=-1) - 1.0).abs().mean() * 0.01

        routing_vec = routing_output.view(batch_size, seq_len, self.num_experts, self.router_dim)
        expression_vec = expression_output_raw.view(batch_size, seq_len, self.num_experts, self.router_dim)
        
        routing_vec = routing_vec / (routing_vec.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        expression_vec = expression_vec / (expression_vec.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        domain_orthogonality = (routing_vec * expression_vec).sum(dim=-1)
        norm_logits = F.layer_norm(domain_orthogonality, (self.num_experts,), eps=self.layernorm_eps)
        balanced_logits, sinkhorn_loss = self.logit_balancer(norm_logits, training=self.training)
        
        # Probabilistic routing (softmax; sigmoids removed)
        routing_probs = F.softmax(balanced_logits, dim=-1)
        if self.training and jitter_eps > 0:
            routing_probs = routing_probs + torch.rand_like(routing_probs) * jitter_eps
            routing_probs = routing_probs / (routing_probs.sum(dim=-1, keepdim=True) + 1e-6)
        routing_probs_full = routing_probs
        top_k_probs, selected_experts = torch.topk(routing_probs, top_k, dim=-1)
        multiplier = top_k_probs / (top_k_probs.sum(dim=-1, keepdim=True) + 1e-6)

        ortho_loss = self.expression_projector.orthogonal_loss()
        zeros = torch.tensor(0.0, device=x.device, requires_grad=True)
        distill_loss = sinkhorn_loss
        progressive_loss = zeros
        
        # Store last_xxx attributes for callback
        if self.training:
            with torch.no_grad():
                self.last_selected_experts = selected_experts.detach()
                self.last_routing_weights = routing_probs_full.detach() if routing_probs_full is not None else None
                self.last_num_experts = self.num_experts
                self.last_speciality_loss = ortho_loss.detach() if ortho_loss is not None else None
                self.last_cosine_similarities = domain_orthogonality.mean().detach() if domain_orthogonality is not None else None
                self.last_expression_reg_loss = expression_reg_loss.detach()
                self.last_ortho_loss = ortho_loss.detach() if ortho_loss is not None else None

        return (
            multiplier,                      # routing_weights (top-k probs)
            selected_experts,                # selected expert indices
            None,                            # expression_logits (unused)
            hn,                              # GRU hidden state
            ortho_loss,                      # speciality_loss
            domain_orthogonality.mean() if domain_orthogonality is not None else zeros,  # cosine_similarities
            None,                            # contrastive_loss (unused)
            routing_probs_full,              # routing_probs_full / router_logits
            expression_reg_loss,             # expression_reg_loss
            distill_loss,                    # routing_uncertainty slot repurposed for distill_loss
            None,                            # entropy_loss (unused)
            progressive_loss,                # load_balancing_loss slot repurposed for progressive_loss
            sinkhorn_loss,                   # sinkhorn_loss (residual from solver)
            ortho_loss,                      # ortho_loss (compat/logging)
        )

iterations = 0
class GramSpecMoEGRINMoE(nn.Module):
    """Hybrid Router: í•˜ë‚˜ì˜ linear layerì—ì„œ sigmoidë¡œ expert ì„ íƒ, sparsemixerë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""

    def __init__(self, config, global_router, **kwargs):
        super().__init__()
        config = config
        self.hidden_dim = config.hidden_size
        self.ffn_dim = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.top_k = config.num_experts_per_tok
        self.router_dim = config.router_dim
        global iterations
        iterations += 1
        self.iter = iterations
        self.router = global_router
        # self.router = nn.Linear(config.hidden_size, config.n_routed_experts, bias=False)
        self.experts = nn.ModuleList([GramSpecMoEMLP(config) for _ in range(self.num_experts)])
        self.shared_experts = GramSpecMoEMLP(config=config, intermediate_size=config.intermediate_size * config.n_shared_experts)

        self.router_jitter_noise = getattr(config, 'router_jitter_noise', 0.01)
        self.input_jitter_noise = getattr(config, 'input_jitter_noise', 0.0)   

        setattr(self.router, "_is_gramspec_moe_router", True)

        # Adaptive filter parameters for load balancing
        
        # Enhanced Expert Utilization
        self.register_buffer("expert_specialization_ema", torch.zeros(self.num_experts, self.hidden_dim), persistent=True)
        self.register_buffer("expert_strength_ema", torch.zeros(self.num_experts), persistent=True) # New: Expert strength tracking
        self.routing_temperature = nn.Parameter(torch.ones(1))
        self.specialization_strength = getattr(config, "specialization_strength", 0.01)
        
        # Routing parameters
        self.enable_uncertainty_broadcast = getattr(config, "enable_uncertainty_broadcast", True)
        self.uncertainty_threshold = getattr(config, "uncertainty_threshold", 0.7)
        
        # shared_experts freeze ì—¬ë¶€ (ê¸°ë³¸ê°’ì€ Trueë¡œ ì„¤ì •)
        self.freeze_shared_experts = getattr(config, 'freeze_shared_experts', True)
        if self.freeze_shared_experts:
            self._freeze_shared_experts()
    
        # Orthogonal projectorëŠ” ì´ì œ global routerì—ì„œ ì²˜ë¦¬ë¨
        self.ortho_strength = getattr(config, 'ortho_strength', 1.0)

    
    def _freeze_shared_experts(self):
        """shared_expertsì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ freeze"""
        for param in self.shared_experts.parameters():
            param.requires_grad = False
        logger.debug(f"Shared experts frozen for layer {self.iter}")
    
    def _unfreeze_shared_experts(self):
        """shared_expertsì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ unfreeze"""
        for param in self.shared_experts.parameters():
            param.requires_grad = True
        logger.debug(f"Shared experts unfrozen for layer {self.iter}")
    
    def freeze_shared_experts_manual(self):
        """ìˆ˜ë™ìœ¼ë¡œ shared_experts freeze"""
        self._freeze_shared_experts()
        self.freeze_shared_experts = True
    
    def unfreeze_shared_experts_manual(self):
        """ìˆ˜ë™ìœ¼ë¡œ shared_experts unfreeze"""
        self._unfreeze_shared_experts()
        self.freeze_shared_experts = False

    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def forward(self, hidden_states: torch.Tensor, global_routing_logits: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        residual = hidden_states
        final_hidden_states, routing_info = self._sparse_routing(hidden_states, global_routing_logits)
        router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss = routing_info
        with torch.no_grad():
            # print( f'residual in rank {torch.distributed.get_rank()}', residual.shape, residual.dtype)
            pretriained_residual = self.shared_experts(residual)
        
        # final_hidden_statesë¥¼ pretrained_residualì˜ í¬ê¸°ì— ë§žì¶° normalize
        # pretrained_residualì˜ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ final_hidden_statesë¥¼ scale
        pretrained_norm = torch.norm(pretriained_residual, dim=-1, keepdim=True)
        final_norm = torch.norm(final_hidden_states, dim=-1, keepdim=True)
        # ì•ˆì „ìž¥ì¹˜: 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        scale_factor = torch.where(
            final_norm > 1e-8,
            pretrained_norm / (final_norm + 1e-8),
            torch.ones_like(pretrained_norm)
        )
        final_hidden_states_normalized = final_hidden_states * scale_factor
        final_hidden_states = final_hidden_states_normalized + pretriained_residual * 1.0
        if self.training:
            final_hidden_states = final_hidden_states.requires_grad_(True)
            if router_logits is not None:
                router_logits = router_logits.requires_grad_(True)
            # Lossì˜ gradientë„ ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            if speciality_loss is not None and torch.is_tensor(speciality_loss):
                speciality_loss = speciality_loss.requires_grad_(True)
            if cosine_similarities is not None and torch.is_tensor(cosine_similarities):
                cosine_similarities = cosine_similarities.requires_grad_(True)
            if contrastive_loss is not None and torch.is_tensor(contrastive_loss):
                contrastive_loss = contrastive_loss.requires_grad_(True)
            if expression_reg_loss is not None and torch.is_tensor(expression_reg_loss):
                expression_reg_loss = expression_reg_loss.requires_grad_(True)
            if routing_uncertainty is not None and torch.is_tensor(routing_uncertainty):
                routing_uncertainty = routing_uncertainty.requires_grad_(True)
            if entropy_loss is not None and torch.is_tensor(entropy_loss):
                entropy_loss = entropy_loss.requires_grad_(True)
            if load_balancing_loss is not None and torch.is_tensor(load_balancing_loss):
                load_balancing_loss = load_balancing_loss.requires_grad_(True)
            if sinkhorn_loss is not None and torch.is_tensor(sinkhorn_loss):
                sinkhorn_loss = sinkhorn_loss.requires_grad_(True)
            if ortho_loss is not None and torch.is_tensor(ortho_loss):
                ortho_loss = ortho_loss.requires_grad_(True)
        return final_hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss)    
    
    def compute_pairwise_expert_similarity(self, expert_outputs: torch.Tensor, expert_mask: torch.Tensor) -> torch.Tensor:
        """
        Compute pairwise cosine similarity between expert outputs for the same inputs.
        Ideally, we want experts to be orthogonal (diverse).
        Returns scalar average similarity.
        """
        # This is tricky because experts process different tokens.
        # We can only compare experts if they processed the same tokens (or we force them to).
        # Alternatively, we can compare their weights, but here we want output similarity.
        # If we want to enforce orthogonality of EXPERT FUNCTIONS, we can look at their outputs
        # for a shared set of inputs, or their weight matrices.
        # Given the prompt "PES... ê° ì „ë¬¸ê°€ì˜ ì¶œë ¥ì— ëŒ€í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°", it implies output similarity.
        # However, usually experts are sparse.
        # If we are in a setting where we can capture outputs for all experts on some tokens (e.g. broadcasting),
        # we can use that.
        # Or we can use the 'expert_specialization_ema' which tracks average input/output patterns.
        
        # Let's implementation based on expert_specialization_ema (input patterns) or check if we have outputs.
        # The plan says "expert_outputs" as input.
        # But _sparse_routing only computes selected experts.
        
        # If we look at the plan: "3.1 PES... ê° ì „ë¬¸ê°€ì˜ ì¶œë ¥ì— ëŒ€í•´... ì „ë¬¸ê°€ ìŒ ê°„ì˜ í‰ê·  ìœ ì‚¬ë„ ë°˜í™˜"
        # This might be best computed on the 'expert_specialization_ema' or similar aggregate.
        # Or maybe it refers to the 'cosine_similarities' we already compute?
        # But 'cosine_similarities' in current code is between expression and routing logits.
        
        # Let's assume we calculate it based on the 'expert_specialization_ema' which represents the 
        # "specialization direction" of each expert.
        
        if self.expert_specialization_ema is not None:
             # Normalize
            normalized_specs = F.normalize(self.expert_specialization_ema, dim=-1)
            # Similarity matrix
            sim_matrix = torch.matmul(normalized_specs, normalized_specs.t())
            # We want to minimize off-diagonal elements
            mask = torch.eye(self.num_experts, device=sim_matrix.device).bool()
            off_diagonal = sim_matrix[~mask]
            return off_diagonal.mean()
            
        return torch.tensor(0.0, device=self.router.load_balancer.weight_hh_l0.device)

    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def _sparse_routing(self, hidden_states: torch.Tensor, global_routing_logits: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        batch_size, sequence_length, hidden_dim = hidden_states.shape
        if self.training and self.input_jitter_noise > 0:
            # Inplace ì—°ì‚° ëŒ€ì‹  ìƒˆë¡œìš´ í…ì„œ ìƒì„± (gradient checkpointing í˜¸í™˜)
            jitter = torch.empty_like(hidden_states).uniform_(1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise)
            hidden_states = hidden_states * jitter
        
        # Global routerì—ì„œ ì „ì²´ ë¼ìš°íŒ… ì²˜ë¦¬ (GRU + expression projection + sparsemixer)
        router_output = self.router(
            hidden_states, 
            global_routing_logits,
            top_k=self.top_k,
            jitter_eps=self.router_jitter_noise
        )
        # Unpack including full probabilities, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss
        routing_weights, selected_experts, expression_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, routing_probs_full, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss = router_output

        # Ensure 2D for downstream one_hot/permutations
        selected_experts = selected_experts.view(batch_size * sequence_length, -1)
        routing_weights = routing_weights.view(batch_size * sequence_length, -1)
        if routing_probs_full is not None:
            routing_probs_full = routing_probs_full.view(batch_size * sequence_length, self.num_experts)

        # multiplierì™€ selected_expertsëŠ” ì´ë¯¸ global routerì—ì„œ sparsemixerë¥¼ í†µí•´ ê³„ì‚°ë¨
        assert routing_weights.isnan().sum() == 0, f"{self.iter} layer routing_weights is nan Line: 826"

        # Build expert mask (N = batch*seq)
        flat_selected = selected_experts  # [N, K]
        expert_mask = F.one_hot(flat_selected, num_classes=self.num_experts).sum(dim=1)  # [N, E]

        uncertain_mask = None
        if self.training and self.enable_uncertainty_broadcast and routing_probs_full is not None:
            probs = routing_probs_full + 1e-8
            entropy = -torch.sum(probs * torch.log(probs), dim=-1)  # [N]
            max_entropy = math.log(self.num_experts)
            normalized_entropy = entropy / max_entropy
            uncertain_mask = (normalized_entropy > self.uncertainty_threshold).unsqueeze(-1)  # [N,1]
            expert_mask = (expert_mask > 0) | uncertain_mask
        else:
            expert_mask = expert_mask > 0

        # Sparse weights map [N, E]
        weights_sparse = torch.zeros_like(expert_mask, dtype=routing_weights.dtype)
        weights_sparse.scatter_add_(1, flat_selected, routing_weights)
        if uncertain_mask is not None:
            weights_sparse = torch.where(uncertain_mask, torch.ones_like(weights_sparse), weights_sparse)

        expert_mask_flat = expert_mask.transpose(0, 1).bool()  # [E, N]

        final_hidden_states = torch.zeros(
            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device
        )
        hidden_states_flat = hidden_states.view(batch_size * sequence_length, hidden_dim)

        # Loop over all available experts in the model and perform the computation on each expert
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            top_x = torch.nonzero(expert_mask_flat[expert_idx], as_tuple=True)[0]

            if top_x.numel() > 0:
                current_state = hidden_states_flat[top_x]
                current_weights = weights_sparse[top_x, expert_idx]
                expert_out = expert_layer(current_state) * current_weights.unsqueeze(-1)

                final_hidden_states.index_add_(0, top_x, expert_out.to(hidden_states.dtype))
                
                # --- Update Specialization EMA ---
                if self.training:
                    with torch.no_grad():
                        current_mean_hidden = hidden_states_flat[top_x].mean(dim=0)
                        self.expert_specialization_ema[expert_idx].mul_(self.router.ema_alpha).add_(current_mean_hidden, alpha=1.0 - self.router.ema_alpha)
                # --- End Update Specialization EMA ---

        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)
        
        # Calculate speciality_loss from expert weights (orthogonality of expert weight matrices)
        if self.training:
            # Collect expert weights for orthogonality loss
            expert_weights_list = []
            for expert_idx in range(self.num_experts):
                expert_layer = self.experts[expert_idx]
                # Get the first linear layer weight (gate_proj or equivalent)
                # Most MoE experts have gate_proj, up_proj, down_proj
                if hasattr(expert_layer, 'gate_proj'):
                    expert_weights_list.append(expert_layer.gate_proj.weight)
                elif hasattr(expert_layer, 'fc1'):
                    expert_weights_list.append(expert_layer.fc1.weight)
                elif hasattr(expert_layer, 'w1'):
                    expert_weights_list.append(expert_layer.w1.weight)
                elif len(list(expert_layer.children())) > 0:
                    # Get first child module's weight
                    first_child = next(expert_layer.children())
                    if hasattr(first_child, 'weight'):
                        expert_weights_list.append(first_child.weight)
            
            # Calculate orthogonality loss from expert weights
            if expert_weights_list:
                expert_weight_ortho_loss = calculate_ortho_loss_for_experts(expert_weights_list)
                # Combine with router's speciality_loss (Spectral Vector orthogonality)
                if speciality_loss is not None and torch.is_tensor(speciality_loss):
                    # Weighted combination: expert weights ortho + spectral vector ortho
                    speciality_loss = (speciality_loss * 0.5 + expert_weight_ortho_loss * 0.5)#.requires_grad_(True)
                else:
                    speciality_loss = expert_weight_ortho_loss.requires_grad_(True)
            elif speciality_loss is None or not torch.is_tensor(speciality_loss):
                # Fallback if no expert weights found
                speciality_loss = torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype)
        
        # ì½œë°±ì„ ìœ„í•´ ë¼ìš°íŒ… ì •ë³´ë¥¼ ëª¨ë“ˆì— ì €ìž¥
        if self.training:
            with torch.no_grad():
                self.last_selected_experts = selected_experts.detach()
                self.last_routing_weights = routing_weights.detach()
                self.last_num_experts = self.num_experts
        
        return final_hidden_states, (routing_probs_full, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss)


class GramSpecMoERMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6, **kwargs):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.zeros(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        # Llama does x.to(float16) * w whilst GramSpecMoE is (x * w).to(float16)
        # See https://github.com/huggingface/transformers/pull/29402
        output = output * (1.0 + self.weight.float())
        return output.type_as(x)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.eps}"


class GramSpecMoERotaryEmbedding(nn.Module):
    def __init__(self, config: GramSpecMoETextConfig, device=None, **kwargs):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    softcap: Optional[float] = None,
    **kwargs,
) -> Tuple[torch.Tensor, torch.Tensor]:
    if scaling is None:
        scaling = module.head_dim**-0.5

    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

    if softcap is not None:
        attn_weights = attn_weights / softcap
        attn_weights = torch.tanh(attn_weights)
        attn_weights = attn_weights * softcap
    if attention_mask is not None:  # no matter the length, we just slice it
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    # upcast attention to fp32
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.bfloat16).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output, attn_weights


class GramSpecMoEAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: GramSpecMoETextConfig, layer_idx: int, **kwargs):
        super().__init__()
        self.is_sliding = bool((layer_idx + 1) % config.sliding_window_pattern)
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = config.query_pre_attn_scalar**-0.5
        self.attention_dropout = self.config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.attn_logit_softcapping = self.config.attn_logit_softcapping
        self.sliding_window = config.sliding_window if self.is_sliding else None

        self.q_norm = GramSpecMoERMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)
        self.k_norm = GramSpecMoERMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False, # Added default value
        use_cache: bool = False, # Added default value
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)
        
        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states   = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        
        query_states = self.q_norm(query_states)
        key_states   = self.k_norm(key_states)

        cos, sin = None, None
        if position_embeddings is not None:
            cos, sin = position_embeddings
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
        # else: NoPE, ê·¸ëŒ€ë¡œ ì‚¬ìš©

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {
                "sin": sin,
                "cos": cos,
                "cache_position": cache_position,
                "sliding_window": self.sliding_window,
            }
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
            
            # Here we need to slice as we use a static cache by default, but FA2 does not support it
            # if attention_mask is not None and self.config.attn_implementation == "flash_attention_2":
            #     if hasattr(past_key_value, "get_seq_length"):
            #         seq_len = past_key_value.get_seq_length()
            #     else:
            #         seq_len = key_states.shape[-1]
            #     key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]
        
        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=self.attention_dropout if self.training else 0.0,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            output_attentions=output_attentions,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights, past_key_value


class GramSpecMoEDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: GramSpecMoETextConfig, layer_idx: int, global_router: GramSpecMoERouter, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx
        self.attention_type = config.layer_types[layer_idx]
        self.self_attn = GramSpecMoEAttention(config=config, layer_idx=layer_idx, **kwargs)
        self.mlp = GramSpecMoEMLP(config=config) # this layer is for loading pretrained base GramSpecMoE model weights
        self.is_dense_replacement = layer_idx >= config.first_k_dense_replace
        if self.is_dense_replacement:
            self.moe = GramSpecMoEGRINMoE(config=config, global_router=global_router)
            # self.moe = GramSpecMoESparseGRINBlock(config=config)
        else:
            self.moe = GramSpecMoEMLP(config=config)
        self.input_layernorm = GramSpecMoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = GramSpecMoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.pre_feedforward_layernorm = GramSpecMoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_feedforward_layernorm = GramSpecMoERMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.is_sliding = self.self_attn.is_sliding
        self.sliding_window = config.sliding_window
        self.use_nope = (hasattr(config, 'no_rope_layers') and bool(config.no_rope_layers[self.layer_idx]))

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings_global: torch.Tensor,
        position_embeddings_local: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        global_routing_hn: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:

        residual = hidden_states
        
        hidden_states = self.input_layernorm(hidden_states)

        # í•˜ì´ë¸Œë¦¬ë“œ rope-nope positional embedding ì ìš©
        if self.use_nope:
            # NoPE: position embedding ì—†ì´ self-attn
            position_embeddings = None
        else:
            # ê¸°ì¡´ ë°©ì‹: RoPE
            position_embeddings = position_embeddings_local if self.self_attn.is_sliding else position_embeddings_global
        hidden_states, self_attn_weights, past_key_value = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = residual + hidden_states
        
        residual = hidden_states
        hidden_states = self.pre_feedforward_layernorm(hidden_states)
        if self.layer_idx >= self.config.first_k_dense_replace:
            hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss) = self.moe(hidden_states, global_routing_hn)
        else:
            with torch.no_grad():
                hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss) = self.moe(hidden_states), (None,)*11
        hidden_states = self.post_feedforward_layernorm(hidden_states)
        if self.training:
            hidden_states = hidden_states.requires_grad_(True)
            if router_logits is not None:
                router_logits = router_logits.requires_grad_(True)
            # Lossì˜ gradientë„ ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            if speciality_loss is not None and torch.is_tensor(speciality_loss):
                speciality_loss = speciality_loss.requires_grad_(True)
            if cosine_similarities is not None and torch.is_tensor(cosine_similarities):
                cosine_similarities = cosine_similarities.requires_grad_(True)
            if contrastive_loss is not None and torch.is_tensor(contrastive_loss):
                contrastive_loss = contrastive_loss.requires_grad_(True)
            if expression_reg_loss is not None and torch.is_tensor(expression_reg_loss):
                expression_reg_loss = expression_reg_loss.requires_grad_(True)
            if routing_uncertainty is not None and torch.is_tensor(routing_uncertainty):
                routing_uncertainty = routing_uncertainty.requires_grad_(True)
            if entropy_loss is not None and torch.is_tensor(entropy_loss):
                entropy_loss = entropy_loss.requires_grad_(True)
            if load_balancing_loss is not None and torch.is_tensor(load_balancing_loss):
                load_balancing_loss = load_balancing_loss.requires_grad_(True)
            if sinkhorn_loss is not None and torch.is_tensor(sinkhorn_loss):
                sinkhorn_loss = sinkhorn_loss.requires_grad_(True)
            if ortho_loss is not None and torch.is_tensor(ortho_loss):
                ortho_loss = ortho_loss.requires_grad_(True)
        hidden_states = residual + hidden_states
        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)
        outputs += ((router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss),)
        return outputs


GramSpecMoE_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`GramSpecMoEConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""

@auto_docstring
class GramSpecMoEPreTrainedModel(PreTrainedModel):
    config: GramSpecMoEConfig
    base_model_prefix = ""
    supports_gradient_checkpointing = True
    _no_split_modules = [
        "GramSpecMoEDecoderLayer",
        "SiglipVisionEmbeddings",
        "SiglipEncoderLayer",
        "SiglipMultiheadAttentionPoolingHead",
    ]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True
    
    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": GramSpecMoEDecoderLayer,
        "attentions": GramSpecMoEAttention
    }

    def _initialize_moe_router_and_temperature(self) -> None:
        """Initialize only MoE router weights and routing temperature if they were not loaded from a checkpoint.

        - Router linear weights: Xavier uniform for stable logits
        - Routing temperature: ones (softplus(1) ~ 1.313) keeps scale reasonable
        """
        with torch.no_grad():
            for module in self.modules():
                # Initialize router linears ONLY if not already initialized/loaded
                router = getattr(module, "router", None)
                if isinstance(router, nn.Linear):
                    already_init = getattr(router, "_is_hf_initialized", False)
                    if not already_init:
                        nn.init.xavier_uniform_(router.weight)
                        if router.bias is not None:
                            router.bias.zero_()
                # Initialize routing temperature ONLY if looks uninitialized/bad
                routing_temp = getattr(module, "routing_temperature", None)
                if isinstance(routing_temp, nn.Parameter):
                    if not torch.isfinite(routing_temp).all() or routing_temp.abs().sum() == 0:
                        routing_temp.data.fill_(1.0)

    def _init_weights(self, module):
        # important: this ported version of Gemma2 isn't meant for training from scratch - only
        # inference and fine-tuning - so the proper init weights code has been removed

        # Only initialize router linears explicitly; skip expert MLPs and other dense layers during fine-tuning
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):
            if getattr(module, "_is_gramspec_moe_router", False):
                logging.get_logger('transformers').debug(f"Initializing router layer with Xavier uniform: {module}")
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    module.bias.data.zero_()
            else:
                # Do not touch non-router linears here to avoid re-initializing experts or base MLPs
                pass
        elif isinstance(module, ExpressionProjector):
            logging.get_logger('transformers').debug(f"Initializing expression projector layer with Xavier uniform: {module}")
            if module.method == "precomputed":
                original_dtype = module.projection.weight.dtype
                module.to(torch.float32)
                safe_orthogonal_(module.projection.weight)
                if module.projection.bias is not None:
                    module.projection.bias.data.zero_()
                module.to(original_dtype)
            else:
                nn.init.xavier_uniform_(module.projection.weight)
                if module.projection.bias is not None:
                    module.projection.bias.data.zero_()
        elif isinstance(module, GramSpecMoEMultiModalProjector):
            nn.init.xavier_uniform_(module.mm_input_projection_weight)
        elif isinstance(module, nn.GRU):
            for name, param in module.named_parameters():
                if 'weight_ih' in name:
                    nn.init.xavier_uniform_(param.data)
                elif 'weight_hh' in name:
                    nn.init.xavier_uniform_(param.data)
                elif 'weight_hr' in name:
                    nn.init.xavier_uniform_(param.data)
                elif 'bias' in name:
                    param.data.fill_(0)
        else:
            super()._init_weights(module)

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: Type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        ffn_checkpoint_for_moe_conversion: Optional[Union[str, os.PathLike, dict]] = None,
        **kwargs,
    ) -> SpecificPreTrainedModelType:

        # config ì²˜ë¦¬ (G2MoEConfig ì¸ìŠ¤í„´ìŠ¤ í™•ë³´)
        if config is None:
            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs.get("config_kwargs", {}))
        if not isinstance(config, GramSpecMoEConfig):
            config = GramSpecMoEConfig(**config.to_dict())
            if config.text_config.attn_implementation == None:
                if is_flash_attn_2_available():
                    config.text_config.attn_implementation = "flash_attention_2"
                elif is_torch_flex_attn_available():
                    config.text_config.attn_implementation = "flex_attention"
                elif cls.training:
                    config.text_config.attn_implementation = "eager"
                else:
                    config.text_config.attn_implementation = "sdpa"
            print(f"Forced attn implementation: {config.text_config.attn_implementation}")

        logging.get_logger('transformers').debug("Loading GramSpecMoE model skeleton using super().from_pretrained...")
        logging.set_verbosity_error()
        import time
        debug_time = time.time()
        base_model = super().from_pretrained(
            pretrained_model_name_or_path,
            *model_args,
            config=config,
            cache_dir=cache_dir,
            # Critical: only init truly missing keys to avoid full-graph init slowdown
            ignore_mismatched_sizes=True,
            force_download=force_download,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
            use_safetensors=use_safetensors,
            weights_only=weights_only,
            **{k: v for k, v in kwargs.items()}
        )
        print(f"GramSpecMoE model skeleton loaded in {time.time() - debug_time} seconds")
        logging.set_verbosity_warning()
        logging.get_logger('transformers').debug("GramSpecMoE model skeleton loaded.")

        if hasattr(base_model, 'model'):
            if hasattr(base_model.model, 'layers') and hasattr(base_model.model.layers, 'moe'):
                logging.get_logger('transformers').debug("GramSpecMoE Pretrained model loaded.")
                return base_model
            logging.get_logger('transformers').debug("Initializing MoE experts with MLP weights...")
            if hasattr(base_model.model, 'layers'):
                base_model.model = base_model._upcycle(base_model.model)
            elif hasattr(base_model.model.language_model, 'layers'):
                base_model.model.language_model = base_model._upcycle(base_model.model.language_model)
            logging.get_logger('transformers').debug("MoE experts initialization completed.")
        elif hasattr(base_model, "language_model"):
            if hasattr(base_model.language_model, 'layers') and hasattr(base_model.language_model.layers, 'moe'):
                logging.get_logger('transformers').debug("GramSpecMoE Pretrained model loaded.")
                return base_model
            logging.get_logger('transformers').debug("Initializing MoE experts with MLP weights...")
            base_model.language_model = base_model._upcycle(base_model.language_model)
            logging.get_logger('transformers').debug("MoE experts initialization completed.")
        else:
            logging.get_logger('transformers').info("Model does not have expected structure. MoE experts not initialized from MLP weights.")
        logging.set_verbosity_warning() 
        return base_model

    @torch.no_grad()
    def _upcycle(self, model):
        processing = tqdm(
            enumerate(model.layers),
            total=len(model.layers),
            desc=f"Copying MLP weights to {self.__class__.__name__} MoE experts: start",
            leave=False)

        for layer_idx, decoder_layer in processing:
            if hasattr(decoder_layer.moe, 'experts') or hasattr(decoder_layer.moe, 'shared_experts'):
                if hasattr(decoder_layer.moe, 'shared_experts'):
                    processing.set_description(f"Copying mlp {layer_idx} â†’ shared experts")
                    decoder_layer.moe.shared_experts.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                    decoder_layer.moe.shared_experts.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                    decoder_layer.moe.shared_experts.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

                for expert_idx, expert in enumerate(decoder_layer.moe.experts):
                    if expert_idx % 2 == 0:
                        processing.set_description(f"Copying mlp {layer_idx} â†’ expert {expert_idx}")
                    expert.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                    expert.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                    expert.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

            elif hasattr(decoder_layer.moe, 'gate_proj'):
                processing.set_description(f"Copying mlp {layer_idx} â†’ dense MoE")
                decoder_layer.moe.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                decoder_layer.moe.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                decoder_layer.moe.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)
            else:
                raise Exception("MoE model has no MLP or shared MLP")
            del decoder_layer.mlp
        processing.set_description("Copy finished")
        return model

    def get_parameter_groups(self):
        """
        Returns a list of parameter groups for the optimizer, which allows to apply different
        learning rates to different parts of the model. This is particularly useful for MoE models
        where components like routers and experts can benefit from different learning schedules.
        """
        
        router_params = []
        expert_params = []
        shared_expert_params = []
        attention_params = []
        other_params = []

        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue

            if 'gate.weight' in name or 'router' in name:
                router_params.append(param)
            elif 'shared_experts' in name:
                shared_expert_params.append(param)
            elif 'experts' in name:
                expert_params.append(param)
            elif 'self_attn' in name:
                attention_params.append(param)
            else:
                other_params.append(param)
        
        # In a training script, you can assign different learning rates to these groups.
        # For example:
        # optimizer_grouped_parameters = [
        #     {'params': model.get_parameter_groups()['router'], 'lr': 1e-4},
        #     {'params': model.get_parameter_groups()['expert'], 'lr': 5e-5},
        #     ...
        # ]
        return {
            'router': router_params,
            'expert': expert_params,
            'shared_expert': shared_expert_params,
            'attention': attention_params,
            'other': other_params,
        }

GramSpecMoE_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            Two formats are allowed:
            - a [`~cache_utils.Cache`] instance, see our
            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
            cache format.

            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
            legacy cache format will be returned.

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""

@add_start_docstrings(
    "The bare GramSpecMoEText Model outputting raw hidden-states without any specific head on top.",
    GramSpecMoE_START_DOCSTRING,
)
class GramSpecMoETextModel(GramSpecMoEPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`GramSpecMoETextDecoderLayer`]

    Args:
        config: GramSpecMoETextConfig
    """
    config: GramSpecMoETextConfig

    def __init__(self, config: GramSpecMoETextConfig, **kwargs):
        super().__init__(config)
        # Robustly resolve text config without relying on class identity across module boundaries
        if getattr(config, "model_type", None) == "gramspec_moe_text" or not hasattr(config, "text_config"):
            self.config = config
        else:
            self.config = config.text_config
        config = self.config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        # Expose a tensor-parallel plan for vLLM on the base text model
        if not hasattr(self, "_tp_plan") or self._tp_plan is None:
            default_tp_plan = {
                "layers.*.self_attn.q_proj": "colwise",
                "layers.*.self_attn.k_proj": "colwise",
                "layers.*.self_attn.v_proj": "colwise",
                "layers.*.self_attn.o_proj": "rowwise",
                "layers.*.moe.experts.*.gate_proj": "colwise",
                "layers.*.moe.experts.*.up_proj": "colwise",
                "layers.*.moe.experts.*.down_proj": "rowwise",
            }
            # allow overriding via config if provided
            self._tp_plan = getattr(config, "base_model_tp_plan", default_tp_plan)

        # GramSpecMoE downcasts the below to bfloat16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402
        self.embed_tokens = GramSpecMoETextScaledWordEmbedding(
            config.vocab_size, config.hidden_size, self.padding_idx, embed_scale=self.config.hidden_size**0.5
        )
        self.global_router = GramSpecMoERouter(config)
        self.layers = nn.ModuleList(
            [GramSpecMoEDecoderLayer(config, layer_idx, self.global_router, **kwargs) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = GramSpecMoERMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = GramSpecMoERotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        self.router_aux_loss_coef = config.router_aux_loss_coef
        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas
        # when we want to create a local RoPE layer. Config defaults should hold values for global RoPE
        config = copy.deepcopy(config)
        config.rope_theta = config.rope_local_base_freq
        config.rope_scaling = config.rope_scaling if config.rope_scaling is not None else {"rope_type":  "default"}
        self.rotary_emb_local = GramSpecMoERotaryEmbedding(config=config)
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    @classmethod
    def from_config(cls, **kwargs):
        return cls._from_config(**kwargs)

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> GramSpecMoECausalLMOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        # NEFTune implementation
        if self.training:
            neftune_noise_alpha = getattr(self.config, "neftune_noise_alpha", 0.0)
            if neftune_noise_alpha > 0.0:
                dims = torch.tensor(inputs_embeds.size(1) * inputs_embeds.size(2), device=inputs_embeds.device)
                mag_norm = neftune_noise_alpha / torch.sqrt(dims)
                inputs_embeds = inputs_embeds + torch.zeros_like(inputs_embeds).uniform_(-mag_norm, mag_norm)


        if use_cache and past_key_values is None and not self.training:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens,
                past_seen_tokens + inputs_embeds.shape[1],
                device=inputs_embeds.device,
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)
        
        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        # embed positions
        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings_global = self.rotary_emb(hidden_states, position_ids)
        position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        all_router_logits = []  # ëª¨ë“  MoE ë ˆì´ì–´ì˜ router_logitsë¥¼ ìˆ˜ì§‘
        global_routing_hn = None
        
        # ê° layerì˜ lossë¥¼ ëˆ„ì í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        all_speciality_losses = []
        all_cosine_similarities = []
        all_contrastive_losses = []

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
        
            layer_outputs = decoder_layer(
                hidden_states,
                position_embeddings_global=position_embeddings_global,
                position_embeddings_local=position_embeddings_local,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                global_routing_hn=global_routing_hn,
                **flash_attn_kwargs,
            )
            hidden_states = layer_outputs[0]
            routing_result = layer_outputs[-1]
            if routing_result is not None:
                router_logits = routing_result[0]
                if router_logits is not None:
                    all_router_logits.append(router_logits)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                global_routing_hn = routing_result[1]
                
                # ê° layerì˜ ê°’ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ìž¥ (ë®ì–´ì“°ì§€ ì•ŠìŒ)
                layer_speciality_loss = routing_result[2]
                layer_cosine_similarities = routing_result[3]
                layer_contrastive_loss = routing_result[4]
                layer_expression_reg_loss = routing_result[5] if len(routing_result) > 5 else None
                layer_routing_uncertainty = routing_result[6] if len(routing_result) > 6 else None
                layer_entropy_loss = routing_result[7] if len(routing_result) > 7 else None
                layer_load_balancing_loss = routing_result[8] if len(routing_result) > 8 else None
                layer_sinkhorn_loss = routing_result[9] if len(routing_result) > 9 else None
                layer_ortho_loss = routing_result[10] if len(routing_result) > 10 else None
                
                if layer_speciality_loss is not None:
                    all_speciality_losses.append(layer_speciality_loss)
                if layer_cosine_similarities is not None:
                    all_cosine_similarities.append(layer_cosine_similarities)
                if layer_contrastive_loss is not None:
                    all_contrastive_losses.append(layer_contrastive_loss)
                if layer_expression_reg_loss is not None:
                    if not hasattr(self, 'all_expression_reg_losses'):
                        self.all_expression_reg_losses = []
                    self.all_expression_reg_losses.append(layer_expression_reg_loss)
                if layer_routing_uncertainty is not None:
                    if not hasattr(self, 'all_routing_uncertainties'):
                        self.all_routing_uncertainties = []
                    self.all_routing_uncertainties.append(layer_routing_uncertainty)
                if layer_entropy_loss is not None:
                    if not hasattr(self, 'all_entropy_losses'):
                        self.all_entropy_losses = []
                    self.all_entropy_losses.append(layer_entropy_loss)
                if layer_load_balancing_loss is not None:
                    if not hasattr(self, 'all_load_balancing_losses'):
                        self.all_load_balancing_losses = []
                    self.all_load_balancing_losses.append(layer_load_balancing_loss)
                if layer_sinkhorn_loss is not None:
                    if not hasattr(self, 'all_sinkhorn_losses'):
                        self.all_sinkhorn_losses = []
                    self.all_sinkhorn_losses.append(layer_sinkhorn_loss)
                if layer_ortho_loss is not None:
                    if not hasattr(self, 'all_ortho_losses'):
                        self.all_ortho_losses = []
                    self.all_ortho_losses.append(layer_ortho_loss)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        # router_logitsë¥¼ íŠœí”Œë¡œ ë³€í™˜ (ë¹„ì–´ìžˆìœ¼ë©´ None)
        router_logits_tuple = tuple(all_router_logits) if all_router_logits else None
        
        # ëª¨ë“  layerì˜ lossë¥¼ ì§‘ê³„ (gradient ìœ ì§€)
        speciality_loss = None
        cosine_similarities = None
        contrastive_loss = None
        expression_reg_loss = None
        
        # expression_reg_loss ì§‘ê³„
        if hasattr(self, 'all_expression_reg_losses') and self.all_expression_reg_losses:
            stacked = torch.stack(self.all_expression_reg_losses)
            expression_reg_loss = stacked.mean()
            if self.training:
                expression_reg_loss = expression_reg_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_expression_reg_losses = []
        
        if all_speciality_losses:
            # speciality_lossëŠ” í‰ê·  (ìŠ¤ì¹¼ë¼ ê°’ë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            stacked = torch.stack(all_speciality_losses)
            speciality_loss = stacked.mean()
            
            # [ì‚­ì œ] ì•„ëž˜ ì½”ë“œê°€ ìžˆìœ¼ë©´ Gradientê°€ ëŠì–´ì§€ê±°ë‚˜ ìž¬ì„¤ì •ë  ìˆ˜ ìžˆìŒ
            # Routerì—ì„œ ì´ë¯¸ ê³„ì‚° ê·¸ëž˜í”„ë¥¼ ë‹¬ê³  ë„˜ì–´ì˜¤ê¸° ë•Œë¬¸
            # if self.training:
            #     speciality_loss = speciality_loss.requires_grad_(True)
        
        if all_cosine_similarities:
            # cosine_similaritiesëŠ” í‰ê·  (í…ì„œë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            # ëª¨ë“  í…ì„œê°€ ë™ì¼í•œ shapeì¸ì§€ í™•ì¸
            try:
                stacked = torch.stack(all_cosine_similarities)
                cosine_similarities = stacked.mean(dim=0)
                # gradient ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
                if self.training:
                    cosine_similarities = cosine_similarities.requires_grad_(True)
            except RuntimeError as e:
                # Shapeì´ ë‹¤ë¥¸ ê²½ìš° ê°ê° í‰ê· ì„ ë‚´ê³  ë‹¤ì‹œ í‰ê· 
                if "size" in str(e).lower() or "shape" in str(e).lower():
                    # ê° í…ì„œì˜ í‰ê· ì„ êµ¬í•œ í›„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
                    means = [cs.mean() if torch.is_tensor(cs) and cs.numel() > 0 else torch.tensor(0.0, device=all_cosine_similarities[0].device, requires_grad=True) 
                            for cs in all_cosine_similarities if cs is not None]
                    if means:
                        cosine_similarities = torch.stack(means).mean()
                        if self.training:
                            cosine_similarities = cosine_similarities.requires_grad_(True)
                else:
                    raise
        
        if all_contrastive_losses:
            # contrastive_lossëŠ” í‰ê·  (ìŠ¤ì¹¼ë¼ ê°’ë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            stacked = torch.stack(all_contrastive_losses)
            contrastive_loss = stacked.mean()
            # gradient ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            if self.training:
                contrastive_loss = contrastive_loss.requires_grad_(True)
        
        # routing_uncertainty ì§‘ê³„
        routing_uncertainty = None
        if hasattr(self, 'all_routing_uncertainties') and self.all_routing_uncertainties:
            # routing_uncertaintyëŠ” í‰ê·  (í…ì„œë“¤ì˜ í‰ê· )
            try:
                stacked = torch.stack(self.all_routing_uncertainties)
                routing_uncertainty = stacked.mean(dim=0)
                if self.training:
                    routing_uncertainty = routing_uncertainty.requires_grad_(True)
            except RuntimeError:
                # Shapeì´ ë‹¤ë¥¸ ê²½ìš° ê°ê° í‰ê· ì„ ë‚´ê³  ë‹¤ì‹œ í‰ê· 
                means = [ru.mean() if torch.is_tensor(ru) and ru.numel() > 0 else torch.tensor(0.0, device=self.all_routing_uncertainties[0].device) 
                        for ru in self.all_routing_uncertainties if ru is not None]
                if means:
                    routing_uncertainty = torch.stack(means).mean()
                    if self.training:
                        routing_uncertainty = routing_uncertainty.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_routing_uncertainties = []
        
        # entropy_loss ì§‘ê³„ (CV ê°ì†Œë¥¼ ìœ„í•œ gradient ìžˆëŠ” loss)
        entropy_loss = None
        if hasattr(self, 'all_entropy_losses') and self.all_entropy_losses:
            stacked = torch.stack(self.all_entropy_losses)
            entropy_loss = stacked.mean()
            if self.training:
                entropy_loss = entropy_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_entropy_losses = []
        
        # load_balancing_loss ì§‘ê³„ (CV ê°ì†Œë¥¼ ìœ„í•œ gradient ìžˆëŠ” loss)
        load_balancing_loss = None
        if hasattr(self, 'all_load_balancing_losses') and self.all_load_balancing_losses:
            stacked = torch.stack(self.all_load_balancing_losses)
            load_balancing_loss = stacked.mean()
            if self.training:
                load_balancing_loss = load_balancing_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_load_balancing_losses = []
        
        # sinkhorn_loss ì§‘ê³„ (SpecHorn-G: GRUê°€ Sinkhornì„ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” loss)
        sinkhorn_loss = None
        if hasattr(self, 'all_sinkhorn_losses') and self.all_sinkhorn_losses:
            stacked = torch.stack(self.all_sinkhorn_losses)
            sinkhorn_loss = stacked.mean()
            if self.training:
                sinkhorn_loss = sinkhorn_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_sinkhorn_losses = []
        
        # ortho_loss ì§‘ê³„ (ì „ë¬¸ê°€ë“¤ì˜ ê°€ì¤‘ì¹˜ ì§êµì„± Loss)
        ortho_loss = None
        if hasattr(self, 'all_ortho_losses') and self.all_ortho_losses:
            stacked = torch.stack(self.all_ortho_losses)
            ortho_loss = stacked.mean()
            if self.training:
                ortho_loss = ortho_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_ortho_losses = []

        return GramSpecMoEModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
            router_logits=router_logits_tuple,
            speciality_loss=speciality_loss,
            cosine_similarities=cosine_similarities,
            contrastive_loss=contrastive_loss,
            expression_reg_loss=expression_reg_loss,
            routing_uncertainty=routing_uncertainty,
            entropy_loss=entropy_loss,
            load_balancing_loss=load_balancing_loss,
            sinkhorn_loss=sinkhorn_loss,
            ortho_loss=ortho_loss,
        )


@auto_docstring
class GramSpecMoEForCausalLM(GramSpecMoEPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}
    config: GramSpecMoEConfig
    base_model_prefix = "language_model"
    
    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)

    def __init__(self, config: GramSpecMoEConfig, **kwargs):
        super().__init__(config)
        self.model = GramSpecMoETextModel(config.text_config, **kwargs)
        # Ensure config refers to resolved text config from the submodule
        self.config = self.model.config
        self.vocab_size = self.config.vocab_size
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **loss_kwargs,
    ) -> CausalLMOutputWithPast:
        r"""
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

            logits_to_keep (`int` or `torch.Tensor`, *optional*):
                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
                This is useful when using packed tensor format (single dimension for batch and sequence length).

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, GramSpecMoEForCausalLM

        >>> model = GramSpecMoEForCausalLM.from_pretrained("google/gemma-2-9b")
        >>> tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b")

        >>> prompt = "What is your favorite condiment?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "What is your favorite condiment?"
        ```"""

        if self.training and self.config.attn_implementation != "eager":
            logger.warning_once(
                "It is strongly recommended to train GramSpecMoE models with the `eager` attention implementation "
                f"instead of `{self.config.attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`."
            )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            **loss_kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping

        loss = None
        aux_loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)
            
            # Speciality loss: Output orthogonality (encourages diverse expert outputs)
            # [ìˆ˜ì •] Loss ì—°ê²° í›„ ê°€ì¤‘ì¹˜ ì¤‘ìš”: 0.05~0.1ë¡œ ì„¤ì •í•˜ì—¬ ì§êµì„± ìœ ì§€ ì••ë ¥ í™•ë³´
            speciality_loss_coef = getattr(self.model.config, "speciality_loss_coef", 0.05)
            if outputs.speciality_loss is not None and speciality_loss_coef > 0:
                loss += outputs.speciality_loss * speciality_loss_coef
            
            # Contrastive loss: Input clustering (encourages experts to process distinct token types)
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš©
            contrastive_loss_coef = getattr(self.model.config, "contrastive_loss_coef", 1.0)  # 0.01 -> 1.0
            if outputs.contrastive_loss is not None and contrastive_loss_coef > 0:
                loss += outputs.contrastive_loss * contrastive_loss_coef
            
            # Expression projector regularization loss: Direct connection to expression_logits for gradient flow
            # This ensures expression_projector parameters receive gradients
            expression_reg_loss_coef = getattr(self.model.config, "expression_reg_loss_coef", 1.0)
            if outputs.expression_reg_loss is not None and expression_reg_loss_coef > 0:
                loss += outputs.expression_reg_loss * expression_reg_loss_coef
            
            # Expression projector loss: Ensure expression_logits contributes to loss for gradient flow
            # cosine_similarities (domain_orthogonality)ë¥¼ lossì— ì¶”ê°€í•˜ì—¬ expression_projectorê°€ í•™ìŠµë˜ë„ë¡ í•¨
            cosine_similarities_loss_coef = getattr(self.model.config, "cosine_similarities_loss_coef", 0.001)
            if outputs.cosine_similarities is not None and cosine_similarities_loss_coef > 0:
                # cosine_similaritiesëŠ” [batch, seq, num_experts] í˜•íƒœì˜ í…ì„œ ë˜ëŠ” ìŠ¤ì¹¼ë¼
                if torch.is_tensor(outputs.cosine_similarities) and outputs.cosine_similarities.numel() > 0:
                    # í…ì„œì¸ ê²½ìš° mean squared valueë¥¼ ìµœì†Œí™”í•˜ì—¬ expression diversityë¥¼ ìœ ì§€
                    expr_loss = torch.mean(outputs.cosine_similarities ** 2) * cosine_similarities_loss_coef
                    loss += expr_loss
                elif isinstance(outputs.cosine_similarities, (int, float)):
                    # ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° ì§ì ‘ ì‚¬ìš©
                    expr_loss = outputs.cosine_similarities * cosine_similarities_loss_coef
                    loss += expr_loss
            
            # Disable aux_loss (Switch-style LB) to avoid conflict with gramspec_lb_loss
            # if outputs.router_logits is not None:
            #     aux_loss = load_balancing_loss_func(...)
            #     loss += self.model.config.router_aux_loss_coef * aux_loss
            
            # ======================================================================================
            # [ì‹ ê·œ] Entropy Loss: ë¶„í¬ í‰íƒ„í™”ë¥¼ ìœ„í•œ gradient ìžˆëŠ” loss (CV ê°ì†Œ)
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš©
            # ======================================================================================
            router_entropy_coef = getattr(self.model.config, "router_entropy_coef", 1.0)  # 0.01 -> 1.0
            if outputs.entropy_loss is not None and router_entropy_coef > 0:
                loss += outputs.entropy_loss * router_entropy_coef
            
            # ======================================================================================
            # [ì‹ ê·œ] Load Balancing Loss: CV ì§ì ‘ ìµœì†Œí™”ë¥¼ ìœ„í•œ gradient ìžˆëŠ” loss
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš© (ê°€ìž¥ ì¤‘ìš”!)
            # ======================================================================================
            usage_uniformity_coef = getattr(self.model.config, "usage_uniformity_coef", 1.0)  # 0.01 -> 1.0
            if outputs.load_balancing_loss is not None and usage_uniformity_coef > 0:
                loss += outputs.load_balancing_loss * usage_uniformity_coef
            
            # ======================================================================================
            # [SpecHorn-G] Sinkhorn Distillation Loss: GRUê°€ Sinkhornì„ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” loss
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš©
            # ======================================================================================
            sinkhorn_distillation_coef = getattr(self.model.config, "sinkhorn_distillation_coef", 1.0)  # 0.1 -> 1.0
            if outputs.sinkhorn_loss is not None and sinkhorn_distillation_coef > 0:
                loss += outputs.sinkhorn_loss * sinkhorn_distillation_coef
            
            # ======================================================================================
            # [í•„ìˆ˜] Ortho Loss: ì „ë¬¸ê°€ë“¤ì˜ ê°€ì¤‘ì¹˜ ì§êµì„± Loss (ì „ë¬¸ê°€ë“¤ì˜ 'ë³¸ì§ˆ(Weight)'ì„ ì°¢ì–´ë†“ëŠ” ê°€ìž¥ ì¤‘ìš”í•œ Loss)
            # ======================================================================================
            ortho_loss_coef = getattr(self.model.config, "ortho_loss_coef", 0.05)
            if outputs.ortho_loss is not None and ortho_loss_coef > 0:
                loss += outputs.ortho_loss * ortho_loss_coef

            # Add bias magnitude loss for load balancing
            # Collect bias magnitude from all routers in the model
            if self.training:
                gslb_coef = getattr(self.model.config, "gslb_coef", 0.0)
                lb_bias_coef = getattr(self.model.config, "lb_bias_coef", 1.0)
                if gslb_coef > 0:
                    from models.gramspec_moe_model import GramSpecMoERouter
                    total_bias_magnitude = torch.tensor(0.0, device=loss.device, dtype=loss.dtype)
                    router_count = 0
                    
                    # Collect bias from all routers in the model using named_modules
                    for name, module in self.model.named_modules():
                        if isinstance(module, GramSpecMoERouter) and hasattr(module, 'expert_bias'):
                            # Calculate bias magnitude (L2 norm squared)
                            bias_magnitude = torch.norm(module.expert_bias, p=2) ** 2
                            total_bias_magnitude = total_bias_magnitude + bias_magnitude
                            router_count += 1
                    
                    if router_count > 0:
                        # Average bias magnitude across all routers
                        avg_bias_magnitude = total_bias_magnitude / router_count
                        bias_loss = gslb_coef * lb_bias_coef * avg_bias_magnitude
                        loss += bias_loss

        try:
            import torch.distributed as dist
            is_main_proc = (not dist.is_available()) or (not dist.is_initialized()) or dist.get_rank() == 0
        except Exception:
            is_main_proc = True


        return GramSpecMoECausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            aux_loss=aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            ortho_loss=outputs.ortho_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
            expression_reg_loss=outputs.expression_reg_loss,
            entropy_loss=outputs.entropy_loss,
            load_balancing_loss=outputs.load_balancing_loss,
            sinkhorn_loss=outputs.sinkhorn_loss
        )


class GramSpecMoEMultiModalProjector(nn.Module):
    def __init__(self, config: GramSpecMoEConfig, **kwargs):
        super().__init__()

        self.mm_input_projection_weight = nn.Parameter(
            torch.zeros(config.vision_config.hidden_size, config.text_config.hidden_size)
        )

        self.mm_soft_emb_norm = GramSpecMoERMSNorm(
            config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps
        )

        self.patches_per_image = int(config.vision_config.image_size // config.vision_config.patch_size)
        self.tokens_per_side = int(config.mm_tokens_per_image**0.5)
        self.kernel_size = self.patches_per_image // self.tokens_per_side
        self.avg_pool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=self.kernel_size)

    def forward(self, vision_outputs: torch.Tensor):
        batch_size, _, seq_length = vision_outputs.shape

        reshaped_vision_outputs = vision_outputs.transpose(1, 2)
        reshaped_vision_outputs = reshaped_vision_outputs.reshape(
            batch_size, seq_length, self.patches_per_image, self.patches_per_image
        )
        reshaped_vision_outputs = reshaped_vision_outputs.contiguous()

        pooled_vision_outputs = self.avg_pool(reshaped_vision_outputs)
        pooled_vision_outputs = pooled_vision_outputs.flatten(2)
        pooled_vision_outputs = pooled_vision_outputs.transpose(1, 2)

        normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)

        projected_vision_outputs = torch.matmul(normed_vision_outputs, self.mm_input_projection_weight)
        return projected_vision_outputs.type_as(vision_outputs)


def token_type_ids_mask_function(
    token_type_ids: Optional[torch.Tensor],
    image_group_ids: Optional[torch.Tensor],
    tokens_per_image: int,
) -> Optional[Callable]:
    """
    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,
    not start and end indices.
    """
    # Do not return an additional mask in this case
    if token_type_ids is None:
        return None

    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:
        # If it's 1 for both query and key/value, we are in an image block
        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length
        # Since vmap doesn't support `if statement` we workaround it with `torch.where`
        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)
        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]
        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)

        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]
        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)

        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)
        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx

        # This is bidirectional attention whenever we are dealing with image tokens
        return is_image_block & same_image_block

    return inner_mask


@auto_docstring(
    custom_intro="""
    The Base GramSpecMoE model which consists of a vision backbone and a language model withou language modeling head.,
    """
)
class GramSpecMoEModel(GramSpecMoEPreTrainedModel):
    config: GramSpecMoEConfig
    _checkpoint_conversion_mapping = {"language_model.model": "language_model"}
    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch
    accepts_loss_kwargs = False

    def __init__(self, config: GramSpecMoEConfig):
        super().__init__(config)
        self.vision_tower = AutoModel.from_config(config=config.vision_config, trust_remote_code=True)
        self.language_model = GramSpecMoETextModel.from_config(config=config.text_config, trust_remote_code=True)
        self.multi_modal_projector = GramSpecMoEMultiModalProjector(config=config)
        self.vocab_size = config.text_config.vocab_size
        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1
        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.language_model = decoder

    def get_decoder(self):
        return self.language_model

    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:
        """
        Projects the last hidden state from the vision model into language model space.

        Args:
            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)
               The tensors corresponding to the input images.
        Returns:
            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
        """
        vision_outputs = self.vision_tower(pixel_values=pixel_values).last_hidden_state
        image_features = self.multi_modal_projector(vision_outputs)
        return image_features

    def get_placeholder_mask(
        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor
    ):
        """
        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is
        equal to the length of multimodal features. If the lengths are different, an error is raised.
        """
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        n_image_features = image_features.shape[0] * image_features.shape[1]
        if inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
            )
        return special_image_mask

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **lm_kwargs,
    ) -> Union[tuple, GramSpecMoEModelOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, GramSpecMoEForConditionalGeneration

        >>> model = GramSpecMoEForConditionalGeneration.from_pretrained("google/gemma32-3b-mix-224")
        >>> processor = AutoProcessor.from_pretrained("google/gemma32-3b-mix-224")

        >>> prompt = "Where is the cat standing?"
        >>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> inputs = processor(images=image, text=prompt,  return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(**inputs,)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Where is the cat standing?\nsnow"
        ```"""
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Replace image id with PAD if the image token if OOV, to avoid index-errors
        if input_ids is not None and self.config.image_token_id >= self.vocab_size:
            special_image_mask = input_ids == self.config.image_token_id
            llm_input_ids = input_ids.clone()
            llm_input_ids[special_image_mask] = 0
        else:
            llm_input_ids = input_ids

        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(llm_input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # Merge text and images
        if pixel_values is not None:
            image_features = self.get_image_features(pixel_values)
            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)
            special_image_mask = self.get_placeholder_mask(
                input_ids, inputs_embeds=inputs_embeds, image_features=image_features
            )
            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config.get_text_config(),
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            if token_type_ids is not None and inputs_embeds.shape[1] != 1:
                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

                # First find where a new image block starts: 1 if image and previous not image
                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
                is_image = (token_type_ids == 1).to(cache_position.device)
                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
                image_group_ids = torch.where(
                    is_image, image_group_ids, torch.full_like(token_type_ids, -1, device=is_image.device)
                )
                mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image
                )

            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        outputs = self.language_model(
            attention_mask=causal_mask_mapping,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **lm_kwargs,
        )

        return GramSpecMoEModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values if use_cache else None,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=image_features if pixel_values is not None else None,
            aux_loss=outputs.aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
        )


@add_start_docstrings(
    """The GramSpecMoE model which consists of a vision backbone and a language model.""",
    GramSpecMoE_START_DOCSTRING,
)
class GramSpecMoEForConditionalGeneration(GramSpecMoEPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {
        "^language_model.model": "model.language_model",
        "^vision_tower": "model.vision_tower",
        "^multi_modal_projector": "model.multi_modal_projector",
        "^language_model.lm_head": "lm_head",
    }
    _tied_weights_keys = ["lm_head.weight"]
    
    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)
  
    def __init__(self, config: GramSpecMoEConfig, **kwargs):
        super().__init__(config)
        self.model = GramSpecMoEModel(config)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_image_features(self, pixel_values):
        return self.model.get_image_features(pixel_values)

    # Make modules available through conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def vision_tower(self):
        return self.model.vision_tower

    @property
    def multi_modal_projector(self):
        return self.model.multi_modal_projector

    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **lm_kwargs,
    ) -> Union[Tuple, GramSpecMoECausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, GramSpecMoEForConditionalGeneration

        >>> model = GramSpecMoEForConditionalGeneration.from_pretrained("google/gemma-3-4b-it")
        >>> processor = AutoProcessor.from_pretrained("google/gemma-3-4b-it")

        >>> messages = [
        ...     {
        ...         "role": "system",
        ...         "content": [
        ...             {"type": "text", "text": "You are a helpful assistant."}
        ...         ]
        ...     },
        ...     {
        ...         "role": "user", "content": [
        ...             {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"},
        ...             {"type": "text", "text": "Where is the cat standing?"},
        ...         ]
        ...     },
        ... ]

        >>> inputs = processor.apply_chat_template(
        ...     messages,
        ...     tokenizer=True,
        ...     return_dict=True,
        ...     return_tensors="pt",
        ...     add_generation_prompt=True
        ... )
        >>> # Generate
        >>> generate_ids = model.generate(**inputs)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "user\nYou are a helpful assistant.\n\n\n\n\n\nWhere is the cat standing?\nmodel\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to"
        ```
        """

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs: GramSpecMoEModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            token_type_ids=token_type_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
            **lm_kwargs,
        )
        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping
            
        loss = None
        aux_loss = None
        if labels is not None:
            # Upcast to float if we need to compute the loss to avoid potential precision issues
            logits = logits.float()
            shift_logits = logits[..., :-1, :]
            shift_labels = labels[..., 1:]
            if attention_mask is not None:
                # we use the input attention mask to shift the logits and labels, because it is 2D.
                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft
                shift_attention_mask = attention_mask[:, -shift_logits.shape[1] :].to(logits.device)
                shift_logits = shift_logits[shift_attention_mask.to(logits.device) != 0].contiguous()
                shift_labels = shift_labels[shift_attention_mask.to(shift_labels.device) != 0].contiguous()
            else:
                shift_logits = shift_logits.contiguous()
                shift_labels = shift_labels.contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()

            flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)
            flat_labels = shift_labels.view(-1).to(shift_logits.device)
            task_loss = loss_fct(flat_logits, flat_labels)

            # ===== Track 1: Task loss multiplicative penalty (balance) =====
            penalty_factor = 1.0
            all_logits = None
            if outputs.router_logits is not None:
                if isinstance(outputs.router_logits, tuple):
                    valid_logits = [l for l in outputs.router_logits if l is not None and l.numel() > 0]
                    if valid_logits:
                        all_logits = torch.cat(valid_logits, dim=0)
                elif torch.is_tensor(outputs.router_logits) and outputs.router_logits.numel() > 0:
                    all_logits = outputs.router_logits

            if all_logits is not None and all_logits.numel() > 0:
                probs = torch.softmax(all_logits, dim=-1)
                expert_load = probs.mean(dim=0)
                target_load = 1.0 / self.config.text_config.n_routed_experts
                load_mean = expert_load.mean() + 1e-6
                cv_metric = expert_load.std() / load_mean
                max_vio_metric = (expert_load.max() / target_load) - 1.0
                max_vio_metric = torch.clamp(max_vio_metric, min=0.0)
                strength = getattr(self.config.text_config, "balance_penalty_strength", 2.0)
                penalty_factor = 1.0 + cv_metric * strength + max_vio_metric * strength * 0.5

            loss = task_loss * penalty_factor

            # ===== Track 2: Structural losses (additive) =====
            # Ortho loss (projection weights)
            ortho_loss_coef = getattr(self.model.config, "ortho_loss_coef", 0.1)
            if outputs.ortho_loss is not None and ortho_loss_coef > 0:
                loss = loss + outputs.ortho_loss * ortho_loss_coef

            # Speciality loss (router-level orthogonality)
            speciality_loss_coef = getattr(self.model.config, "speciality_loss_coef", 0.1)
            if outputs.speciality_loss is not None and speciality_loss_coef > 0:
                loss = loss + outputs.speciality_loss * speciality_loss_coef

            # Expression regularization
            expr_reg_coef = getattr(self.model.config, "expression_reg_loss_coef", 1.0)
            if outputs.expression_reg_loss is not None and expr_reg_coef > 0:
                loss = loss + outputs.expression_reg_loss * expr_reg_coef

            # Sinkhorn residual / distillation loss
            sinkhorn_coef = getattr(self.model.config, "sinkhorn_distillation_coef", 0.0)
            if outputs.sinkhorn_loss is not None and sinkhorn_coef > 0:
                loss = loss + outputs.sinkhorn_loss * sinkhorn_coef

            # Routing entropy (encourage spread) and load balance (cv)
            if outputs.router_logits is not None:
                probs = outputs.router_logits
                if probs.dim() > 2:
                    probs = probs.view(-1, probs.size(-1))
                probs = probs + 1e-8
                expert_load = probs.mean(dim=0)
                load_mean = expert_load.mean() + 1e-6
                cv_metric = expert_load.std() / load_mean
                router_entropy = -(probs * torch.log(probs)).sum(dim=-1).mean()

                entropy_coef = getattr(self.model.config, "router_entropy_coef", 0.0)
                usage_coef = getattr(self.model.config, "usage_uniformity_coef", 0.0)
                if usage_coef > 0:
                    loss = loss + usage_coef * cv_metric
                if entropy_coef > 0:
                    loss = loss + entropy_coef * router_entropy

            # Contrastive (if available)
            contrastive_coef = getattr(self.model.config, "contrastive_loss_coef", 0.0)
            if outputs.contrastive_loss is not None and contrastive_coef > 0:
                loss = loss + outputs.contrastive_loss * contrastive_coef

            # Cosine similarities (optional small penalty)
            cos_coef = getattr(self.model.config, "cosine_similarities_loss_coef", 0.0)
            if cos_coef > 0 and outputs.cosine_similarities is not None:
                if torch.is_tensor(outputs.cosine_similarities) and outputs.cosine_similarities.numel() > 0:
                    loss = loss + cos_coef * torch.mean(outputs.cosine_similarities ** 2)
                elif isinstance(outputs.cosine_similarities, (int, float)):
                    loss = loss + cos_coef * outputs.cosine_similarities

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return GramSpecMoECausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=outputs.image_hidden_states,
            aux_loss=aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
            expression_reg_loss=outputs.expression_reg_loss,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        pixel_values=None,
        attention_mask=None,
        token_type_ids=None,
        use_cache=True,
        logits_to_keep=None,
        labels=None,
        **kwargs,
    ):
        # Overwritten -- custom `position_ids` and `pixel_values` handling
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
            logits_to_keep=logits_to_keep,
            token_type_ids=token_type_ids,
            **kwargs,
        )

        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore
        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always
        if cache_position[0] == 0:
            model_inputs["pixel_values"] = pixel_values

        return model_inputs
    
    @staticmethod
    def create_masks_for_generate(
        config: PretrainedConfig,
        input_embeds: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        cache_position: torch.Tensor,
        past_key_values: Optional[Cache],
        position_ids: Optional[torch.Tensor],
        token_type_ids: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> dict:
        # Prepare mask arguments
        mask_kwargs = {
            "config": config.get_text_config(),
            "input_embeds": input_embeds,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Add the token type ids mask for generate as well
        if token_type_ids is not None and input_embeds.shape[1] != 1:
            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

            # First find where a new image block starts: 1 if image and previous not image
            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
            is_image = (token_type_ids == 1).to(cache_position.device)
            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))
            mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image
            )

        return create_masks_for_generate(**mask_kwargs)


class GramSpecMoERouterTrainingMonitor:
    """
    PyTorch í•™ìŠµ ë£¨í”„ì—ì„œ ë¼ìš°í„°ê°€ 'ì‹¤ì œë¡œ' í•™ìŠµë˜ëŠ”ì§€ ì§€ì† ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±.
    - on_batch_start: step ìŠ¤ëƒ…ìƒ·(íŒŒë¼ë¯¸í„° ê°’) ì €ìž¥
    - on_after_backward: grad norm/ë¶„í¬/ì—”íŠ¸ë¡œí”¼/EMA/ë³´ì¡° ë¡œìŠ¤ ë¡œê¹…
    - on_step_end: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ëŸ‰(delta) í™•ì¸
    ì‚¬ìš©ìžëŠ” í•™ìŠµ ë£¨í”„ì—ì„œ ê° íƒ€ì´ë°ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
    """
    def __init__(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        log_every: int = 100,
        log_fn: Optional[Callable[[str], None]] = None,
    ):
        self.model = model
        self.optimizer = optimizer
        self.log_every = max(int(log_every), 1)
        self.log_fn = log_fn if log_fn is not None else (lambda msg: logger.info(msg))
        self._step = 0
        self._pre_step_snapshots: dict[str, dict[str, torch.Tensor]] = {}

    def _iter_router_modules(self):
        for name, module in self.model.named_modules():
            if getattr(module, "_is_gramspec_moe_router", False) or isinstance(module, GramSpecMoERouter):
                yield name, module

    @staticmethod
    def _check_requires_grad(module: nn.Module) -> bool:
        params = list(module.parameters(recurse=True))
        return len(params) > 0 and all(p.requires_grad for p in params)

    def _check_in_optimizer(self, module: nn.Module) -> bool:
        if self.optimizer is None:
            return False
        target_ids = {id(p) for p in module.parameters(recurse=True)}
        if not target_ids:
            return False
        opt_ids = set()
        for group in self.optimizer.param_groups:
            for p in group.get("params", []):
                opt_ids.add(id(p))
        return target_ids.issubset(opt_ids)

    @staticmethod
    def _grad_norms(module: nn.Module) -> dict[str, float]:
        norms: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if p.grad is not None:
                # ìž‘ì€ ìˆ˜ì¹˜ ë…¸ì´ì¦ˆëŠ” 0ìœ¼ë¡œ ì·¨ê¸‰í•˜ì§€ ì•ŠìŒ
                norms[n] = float(p.grad.detach().norm().item())
        return norms

    @staticmethod
    def _snapshot_params(module: nn.Module) -> dict[str, torch.Tensor]:
        return {n: p.detach().clone() for n, p in module.named_parameters(recurse=True)}

    @staticmethod
    def _param_deltas(before: dict[str, torch.Tensor], module: nn.Module) -> dict[str, float]:
        deltas: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if n in before:
                deltas[n] = float((before[n] - p.detach()).abs().sum().item())
        return deltas

    @staticmethod
    def _router_usage_and_entropy(router_logits: Optional[Union[torch.Tensor, Tuple[torch.Tensor, ...]]]) -> tuple[Optional[List[float]], Optional[float]]:
        if router_logits is None:
            return None, None
        if isinstance(router_logits, tuple):
            if not router_logits:
                return None, None
            probs = torch.cat([t for t in router_logits if t is not None and t.numel() > 0], dim=0)
        else:
            probs = router_logits
        if probs.numel() == 0:
            return None, None
        # ë³¸ ì½”ë“œ ê²½ë¡œì—ì„œëŠ” router_logitsê°€ 'í™•ë¥ 'ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš°ê°€ ë§ŽìŒ
        p = probs.clamp_min(1e-12)
        num_experts = p.size(-1)
        top1 = p.argmax(dim=-1)
        usage = torch.bincount(top1, minlength=num_experts).float()
        usage = usage / usage.sum().clamp_min(1.0)
        entropy = float((-(p * p.log()).sum(dim=-1)).mean().item())
        return usage.tolist(), entropy

    def on_batch_start(self):
        self._step += 1
        self._pre_step_snapshots.clear()
        for name, module in self._iter_router_modules():
            self._pre_step_snapshots[name] = self._snapshot_params(module)

    def on_after_backward(
        self,
        outputs: Optional[Union[GramSpecMoECausalLMOutputWithPast, GramSpecMoEModelOutputWithPast]] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        if self._step % self.log_every != 0:
            return

        # ë¼ìš°í„° ëª¨ë“ˆë³„ ìƒíƒœ/grad
        lines = []
        for name, module in self._iter_router_modules():
            req = self._check_requires_grad(module)
            inopt = self._check_in_optimizer(module)
            norms = self._grad_norms(module)
            any_grad = any(v > 0.0 for v in norms.values())
            gsum = sum(norms.values()) if norms else 0.0
            gmax = max(norms.values()) if norms else 0.0
            lines.append(f"[router:{name}] requires_grad={req} in_optimizer={inopt} any_grad={any_grad} grad_sum={gsum:.6f} grad_max={gmax:.6f}")

        # ì‚¬ìš© ë¶„í¬/ì—”íŠ¸ë¡œí”¼
        usage, entropy = (None, None)
        if outputs is not None and hasattr(outputs, "router_logits"):
            usage, entropy = self._router_usage_and_entropy(outputs.router_logits)
            if usage is not None:
                lines.append(f"[routing] usage(top1_ratio)={','.join(f'{u:.3f}' for u in usage)}")
            if entropy is not None:
                lines.append(f"[routing] entropy={entropy:.6f}")

        # EMA (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            # ForCausalLM -> .model.global_router, TextModel -> .global_router
            global_router = None
            if hasattr(self.model, "model") and hasattr(self.model.model, "global_router"):
                global_router = self.model.model.global_router
            elif hasattr(self.model, "global_router"):
                global_router = self.model.global_router
            if global_router is not None and hasattr(global_router, "expert_load_ema"):
                ema = global_router.expert_load_ema.detach().float()
                s = float(ema.sum().item())
                ema_norm = (ema / s) if s > 0 else ema
                lines.append(f"[ema] expert_load_ema_norm={','.join(f'{float(x):.3f}' for x in ema_norm.tolist())}")
        except Exception:
            pass

        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))

        # ì„ íƒì ìœ¼ë¡œ ë³´ì¡° ë¡œìŠ¤ë„ ë¡œê¹…(ê³„ì‚° ë¹„ìš© ë‚®ìŒ)
        try:
            if outputs is not None and hasattr(outputs, "router_logits") and outputs.router_logits is not None:
                if hasattr(self.model, "model") and hasattr(self.model.model, "config"):
                    cfg = self.model.model.config
                elif hasattr(self.model, "config"):
                    cfg = self.model.config
                else:
                    cfg = None
                if cfg is not None:
                    aux = load_balancing_loss_func(
                        outputs.router_logits,
                        cfg.n_routed_experts,
                        getattr(cfg, "num_experts_per_tok", 2),
                        attention_mask,
                        router_z_loss_coef=getattr(cfg, "router_z_loss_coef", None),
                        router_entropy_coef=getattr(cfg, "router_entropy_coef", None),
                        usage_uniformity_coef=getattr(cfg, "usage_uniformity_coef", None),
                    ).detach().float().item()
                    self.log_fn(f"[step {self._step}] aux_lb_loss={aux:.6f}")
        except Exception:
            pass

    def on_step_end(self):
        if self._step % self.log_every != 0:
            return
        lines = []
        for name, module in self._iter_router_modules():
            before = self._pre_step_snapshots.get(name, {})
            deltas = self._param_deltas(before, module)
            delta_sum = sum(deltas.values()) if deltas else 0.0
            lines.append(f"[router:{name}] param_delta_sum={delta_sum:.6f}")
        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))
        self._pre_step_snapshots.clear()


__all__ = [
    "GramSpecMoEPreTrainedModel",
    "GramSpecMoETextModel",
    "GramSpecMoEForCausalLM",
    "GramSpecMoEForConditionalGeneration",
    "GramSpecMoEModel",
    "GramSpecMoERouterTrainingMonitor",
]
