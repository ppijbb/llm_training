#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/SPECTRA/modular_SPECTRA.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_SPECTRA.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import copy
import os
import inspect
import math
from functools import partial
from collections.abc import Callable
from dataclasses import dataclass
from typing import List, Optional, Tuple, Union, Type

from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
# Add dynamo import for torch.compile compatibility
import torch._dynamo

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, HybridCache, StaticCache, DynamicCache
from transformers.generation.utils import GenerationMixin
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask
from transformers.processing_utils import Unpack
from transformers.utils import logging
from transformers.utils.doc import (
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
    add_start_docstrings,
)
from transformers.utils.generic import (
    ModelOutput,
    can_return_tuple,
)
from transformers.utils import auto_docstring
from transformers.utils.import_utils import (
    is_torchdynamo_compiling,
    is_torch_flex_attn_available,
    is_flash_attn_2_available
)
from transformers.modeling_utils import (
    restore_default_dtype,
    SpecificPreTrainedModelType,
)
from transformers.configuration_utils import PretrainedConfig
from transformers import logging
from transformers.utils.deprecation import deprecate_kwarg
from transformers import AutoModel, AutoConfig, AutoModelForCausalLM
from .spectra_config import SPECTRAConfig, SPECTRATextConfig

if is_torch_flex_attn_available():
    from torch.nn.attention.flex_attention import BlockMask
    from transformers.integrations.flex_attention import make_flex_block_causal_mask

    
logger = logging.get_logger(__name__)
_CONFIG_FOR_DOC = "SPECTRAConfig"

# [ZeRO-3 Safety Patch] Monkey-patch SigLIP initialization to avoid fan-in calculation on partitioned tensors
# This patches the root cause (variance_scaling_) used by ALL SigLIP init functions:
# - lecun_normal_ (for linear layers)
# - default_flax_embed_init (for embeddings)
# - Any future SigLIP initialization that uses variance_scaling_
try:
    import transformers.models.siglip.modeling_siglip as siglip_mod
    if hasattr(siglip_mod, "variance_scaling_"):
        original_variance_scaling_ = siglip_mod.variance_scaling_
        def safe_variance_scaling_(tensor, scale=1.0, mode="fan_in", distribution="truncated_normal"):
            """ZeRO-3 safe version that skips initialization for partitioned tensors."""
            if tensor is None or tensor.dim() < 2:
                # Skip initialization for 0D/1D partitioned tensors during ZeRO-3
                # DeepSpeed will properly initialize these after gathering
                return
            return original_variance_scaling_(tensor, scale, mode, distribution)
        siglip_mod.variance_scaling_ = safe_variance_scaling_
        logger.info("âœ… Applied comprehensive ZeRO-3 safety patch to SigLIP variance_scaling_")
    else:
        logger.warning("âš ï¸ Could not find variance_scaling_ in SigLIP module - patch not applied")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to apply SigLIP ZeRO-3 patch: {e}")


def calculate_ortho_loss_for_experts(expert_weights: List[torch.Tensor]) -> torch.Tensor:
    """
    Calculates the orthogonalization loss for a set of expert weights from a single MoE layer.
    This loss encourages functional diversity among experts by penalizing similarity
    in their weight spaces. The loss is the squared Frobenius norm
    of (VV' - I) where V is the matrix of normalized expert weights.
    """
    if not expert_weights:
        return torch.tensor(0.0, device=expert_weights[0].device)

    flattened_weights = [w.view(-1) for w in expert_weights]
    V = torch.stack(flattened_weights)

    # Normalize rows to be unit vectors, preventing weights from collapsing to zero
    # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    V = V / (V.norm(p=2, dim=1, keepdim=True) + 1e-6)
    
    # Gram matrix: V @ V.T
    gram_matrix = torch.matmul(V, V.t())
    
    # Target: identity matrix
    identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)
    
    # Loss: squared Frobenius norm of (VV' - I)
    ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
    return ortho_loss


def _orthogonal_constraint_loss(
    num_experts: int, 
    gate_logits: torch.Tensor
) -> torch.Tensor:
        """ë¼ìš°í„° ì¶œë ¥ê°’ë“¤ì˜ ì§êµì„± ì œì•½ ì†ì‹¤"""
        # router_outputs: [batch*seq, num_experts]
        
        # ê° í† í°ë³„ë¡œ expert ë°©í–¥ì´ ì§êµí•˜ë„ë¡
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        normalized_outputs = gate_logits / (gate_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # Gram matrix: [num_experts, num_experts]
        gram_matrix = torch.matmul(normalized_outputs.T, normalized_outputs)
        
        # Target: identity matrix
        identity = torch.eye(num_experts, device=gate_logits.device, dtype=gate_logits.dtype)
        
        # Loss: squared Frobenius norm of (GG' - I)
        constraint_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
        return constraint_loss


class ContrastiveRouterLoss(nn.Module):
    """
    Encourages experts to process tokens from distinct semantic spaces (Hidden States).
    Uses Soft Probabilities for differentiability.
    """
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.epsilon = 1e-6

    def forward(self, hidden_states, routing_weights):
        """
        hidden_states: [batch, seq, hidden_dim]
        routing_weights: [batch, seq, num_experts] (Softmax probabilities)
        """
        # Flatten: [batch, seq, hidden] -> [N, hidden], [batch*seq, experts] -> [N, experts]
        flattened_states = hidden_states.reshape(-1, hidden_states.size(-1))
        flattened_weights = routing_weights.reshape(-1, routing_weights.size(-1))
        
        # Ensure dtype consistency for matmul
        flattened_weights = flattened_weights.to(dtype=flattened_states.dtype)
        
        # Calculate weighted centroids for each expert
        # [experts, N] @ [N, hidden] -> [experts, hidden]
        expert_centroids = torch.matmul(flattened_weights.t(), flattened_states)
        
        # Normalize by total weight assigned to expert
        expert_weight_sums = flattened_weights.sum(dim=0) + self.epsilon
        expert_centroids = expert_centroids / expert_weight_sums.unsqueeze(-1)
        
        # Cosine similarity between centroids
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        normalized_centroids = expert_centroids / (expert_centroids.norm(p=2, dim=1, keepdim=True) + 1e-6)
        similarity = torch.matmul(normalized_centroids, normalized_centroids.t())
        
        # Minimize off-diagonal similarity (contrastive)
        num_experts = similarity.size(0)
        mask = torch.eye(num_experts, device=similarity.device).bool()
        contrastive_loss = similarity[~mask].mean()
        
        return contrastive_loss

def load_balancing_loss_func(
    gate_logits: torch.Tensor,
    num_experts: int,
    top_k: int = 2,
    attention_mask: Optional[torch.Tensor] = None,
    router_z_loss_coef: Optional[float] = None,
    router_entropy_coef: Optional[float] = None,
    usage_uniformity_coef: Optional[float] = None,
) -> torch.Tensor:
    r"""
    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.
    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss
    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between
    experts is too unbalanced.
    Args:
        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):
            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of
            shape [batch_size X sequence_length, num_experts].
        attention_mask (`torch.Tensor`, None):
            The attention_mask used in forward function
            shape [batch_size X sequence_length] if not None.
        num_experts (`int`, *optional*):
            Number of experts
        router_z_loss_coef (`float`, *optional*):
            Coefficient for the z-loss term in the load balancing loss.
    Returns:
        The auxiliary loss.
    """
    if gate_logits is None:
        return torch.tensor(0.0)

    if isinstance(gate_logits, tuple):
        # Add a check for empty tuple
        if not gate_logits:
            return torch.tensor(0.0)
        compute_device = gate_logits[0].device
        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)
    else:
        # handle tensor input (single tensor from global router)
        concatenated_gate_logits = gate_logits
        compute_device = concatenated_gate_logits.device

    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)

    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)

    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)

    if attention_mask is None:
        # Compute the percentage of tokens routed to each experts
        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.mean(routing_weights, dim=0)
    else:
        batch_size, sequence_length = attention_mask.shape
        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)

        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask
        expert_attention_mask = (
            attention_mask[None, :, :, None, None]
            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))
            .reshape(-1, top_k, num_experts)
            .to(compute_device)
        )

        # Compute the percentage of tokens routed to each experts
        # Sum over batch*seq and top_k dimensions to get [num_experts]
        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=(0, 1)) / torch.sum(
            expert_attention_mask, dim=(0, 1)
        )

        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert
        router_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))
            .reshape(-1, num_experts)
            .to(compute_device)
        )

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(
            router_per_expert_attention_mask, dim=0
        )

    # Core Switch-style load balancing loss
    aux_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0)) * num_experts

    # Router z-loss (Switch Transformer) to prevent overconfident routers
    if router_z_loss_coef is not None and router_z_loss_coef > 0:
        log_z = torch.logsumexp(concatenated_gate_logits, dim=-1)
        z_loss = torch.square(log_z).mean()
        aux_loss = aux_loss + router_z_loss_coef * z_loss

    # Entropy regularization to avoid routing collapse (maximize entropy)
    if router_entropy_coef is not None and router_entropy_coef > 0:
        token_entropy = -(routing_weights * torch.log(routing_weights.clamp_min(1e-12))).sum(dim=-1)
        # Normalize by log(num_experts) for scale invariance across different expert counts
        normalized_entropy = token_entropy / math.log(max(num_experts, 2))
        entropy_reg = -normalized_entropy.mean()
        aux_loss = aux_loss + router_entropy_coef * entropy_reg

    # Usage uniformity (optional): encourage average routing probability per expert to be near-uniform
    if usage_uniformity_coef is not None and usage_uniformity_coef > 0:
        # router_prob_per_expert already accounts for attention mask when provided
        target = torch.full_like(router_prob_per_expert, 1.0 / float(num_experts))
        usage_uniformity_loss = torch.mean(torch.square(router_prob_per_expert - target))
        aux_loss = aux_loss + usage_uniformity_coef * usage_uniformity_loss

    return aux_loss


def spectra_lb_loss(
    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
    num_experts: int,
    lb_l2_coef: float = 1.0,
    lb_cv_coef: float = 0.5,
    lb_entropy_floor_coef: float = 0.0,
    top_k: int = 2,
    lb_topk_l2_coef: float = 0.0,
    lb_topk_cv_coef: float = 0.0,
    attention_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    SPECTRA-aligned low-overhead load balancing loss.
    Uses per-expert statistics only (O(E)) to minimize compute overhead.
    No shape changes or fallback zeros/nan - skips invalid terms.

    Args:
        gate_logits: Routing logits, either tensor [N, E] or tuple of tensors
        num_experts: Number of experts E
        lb_l2_coef: Weight for L2 uniformity loss
        lb_cv_coef: Weight for CV minimization
        lb_entropy_floor_coef: Weight for entropy floor (optional)
        top_k: Number of experts selected per token
        lb_topk_l2_coef: Weight for top-k token count uniformity loss
        lb_topk_cv_coef: Weight for top-k token count CV minimization
        attention_mask: Attention mask for valid tokens

    Returns:
        Scalar loss tensor (sum of weighted terms)
    """
    # Handle tuple input (concatenate if needed)
    if isinstance(gate_logits, tuple):
        if not gate_logits:  # Empty tuple - skip entirely
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        tensors = [gl for gl in gate_logits if gl is not None and gl.numel() > 0]
        if not tensors:
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        gate_logits = torch.cat(tensors, dim=0)

    if gate_logits is None or gate_logits.numel() == 0:
        return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)

    # Get routing weights using existing softmax path
    routing_weights = torch.nn.functional.softmax(gate_logits, dim=-1)
    device, dtype = routing_weights.device, routing_weights.dtype

    # Compute per-expert mean probabilities
    routing_per_expert_attention_mask = None
    if attention_mask is not None:
        # Use existing masking logic from load_balancing_loss_func
        batch_size, seq_length = attention_mask.shape
        tokens = routing_weights.shape[0]
        denom = batch_size * seq_length
        num_hidden_layers = max(tokens // denom, 1) if denom > 0 else 1

        routing_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, seq_length, num_experts))
            .reshape(-1, num_experts)
            .to(device)
        )

        # If shapes mismatch (e.g., tokens not divisible), fall back to full mask
        if routing_per_expert_attention_mask.shape[0] != routing_weights.shape[0]:
            routing_per_expert_attention_mask = torch.ones_like(routing_weights, device=device, dtype=dtype)

        # Mean per expert (masked)
        denom_mask = torch.sum(routing_per_expert_attention_mask, dim=0).clamp_min(1.0)
        p_bar = torch.sum(routing_weights * routing_per_expert_attention_mask, dim=0) / denom_mask
    else:
        # Simple mean across batch dimension
        p_bar = routing_weights.mean(dim=0)  # [E]

    # Uniform target distribution
    u = torch.full_like(p_bar, 1.0 / float(num_experts))

    # Numerical stability
    eps = torch.finfo(dtype).eps

    total_loss = torch.zeros((), dtype=dtype, device=device)

    # L2 uniformity loss
    if lb_l2_coef > 0:
        l2_loss = torch.sum((p_bar - u) ** 2)
        total_loss = total_loss + lb_l2_coef * l2_loss

    # CV minimization (approximate)
    if lb_cv_coef > 0:
        mean_p = p_bar.mean()
        var_p = p_bar.var(unbiased=False)  # Population variance
        cv_loss = var_p / (mean_p + eps)
        total_loss = total_loss + lb_cv_coef * cv_loss

    # Entropy floor (optional)
    if lb_entropy_floor_coef > 0:
        # Token-level entropy floor
        token_entropy = -torch.sum(routing_weights * torch.log(routing_weights + eps), dim=-1)
        entropy_floor_loss = -token_entropy.mean()  # Minimize negative entropy = maximize entropy
        total_loss = total_loss + lb_entropy_floor_coef * entropy_floor_loss

    # Top-k token count based losses (works on discrete expert selection)
    if (lb_topk_l2_coef > 0 or lb_topk_cv_coef > 0) and top_k > 0:
        k = min(top_k, num_experts)
        # top-k indices based on routing probabilities (monotonic with logits)
        topk_probs, topk_indices = torch.topk(routing_weights, k=k, dim=-1)

        if attention_mask is not None and routing_per_expert_attention_mask is not None:
            token_mask = routing_per_expert_attention_mask.any(dim=-1)
            if token_mask.any():
                topk_indices = topk_indices[token_mask]
                topk_probs = topk_probs[token_mask]
            else:
                topk_indices = topk_indices[:0]
                topk_probs = topk_probs[:0]

        flat_indices = topk_indices.reshape(-1)
        flat_probs = topk_probs.reshape(-1)

        counts = torch.zeros(num_experts, dtype=dtype, device=device)
        if flat_indices.numel() > 0:
            ones = torch.ones_like(flat_indices, dtype=dtype, device=device)
            counts.scatter_add_(0, flat_indices, ones)

            weighted_counts = torch.zeros_like(counts)
            weighted_counts.scatter_add_(0, flat_indices, flat_probs.to(dtype))

            total_counts = counts.sum()
            if total_counts > 0 and lb_topk_l2_coef > 0:
                usage_distribution = counts / total_counts
                l2_topk_loss = torch.sum((usage_distribution - u) ** 2)
                total_loss = total_loss + lb_topk_l2_coef * l2_topk_loss

            mean_weight = weighted_counts.mean()
            var_weight = weighted_counts.var(unbiased=False) if weighted_counts.numel() > 0 else torch.tensor(0.0, device=device, dtype=dtype)
            if lb_topk_cv_coef > 0 and mean_weight > 0:
                topk_cv_loss = var_weight / (mean_weight + eps)
                total_loss = total_loss + lb_topk_cv_coef * topk_cv_loss

    return total_loss


# Copied from Phi-3.5-MoE
def _get_unpad_data(attention_mask):
    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)
    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
    max_seqlen_in_batch = seqlens_in_batch.max().item()
    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))
    return (
        indices,
        cu_seqlens,
        max_seqlen_in_batch,
    )
    
@dataclass
class SPECTRAModelOutputWithPast(BaseModelOutputWithPast):
    """
    Base class for SPECTRA outputs, with hidden states and attentions.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.
    """

    image_hidden_states: Optional[torch.FloatTensor] = None
    aux_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None
    expression_reg_loss: Optional[torch.FloatTensor] = None
    routing_uncertainty: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    load_balancing_loss: Optional[torch.FloatTensor] = None
    sinkhorn_loss: Optional[torch.FloatTensor] = None
    balance_loss: Optional[torch.FloatTensor] = None


@dataclass
class SPECTRACausalLMOutputWithPast(ModelOutput):
    """
    Base class for SPECTRA causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    image_hidden_states: Optional[torch.FloatTensor] = None
    # this is moe specific
    aux_loss: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None
    expression_reg_loss: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    load_balancing_loss: Optional[torch.FloatTensor] = None
    sinkhorn_loss: Optional[torch.FloatTensor] = None
    # hn_context ì œê±° - ì°¨ì› ë¬¸ì œë¡œ ì¸í•´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ


class SPECTRATextScaledWordEmbedding(nn.Embedding):
    """
    This module overrides nn.Embeddings' forward by multiplying with embeddings scale.
    """

    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        self.register_buffer("embed_scale", torch.tensor(embed_scale), persistent=False)

    def forward(self, input_ids: torch.Tensor):
        return super().forward(input_ids) * self.embed_scale


class SPECTRAMLP(nn.Module):
    def __init__(self, config: SPECTRATextConfig, intermediate_size: Optional[int]=None, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        # Mark for fast init path
        setattr(self.gate_proj, "_is_spectra_gate_layer", True)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_activation]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class mp(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx, 
        scores: torch.Tensor, 
        multiplier: torch.Tensor, 
        selected_experts: torch.Tensor,
        masked_gates: torch.Tensor,
        mask_for_one: torch.Tensor,
    ):
        ctx.save_for_backward(multiplier, selected_experts, masked_gates)
        return multiplier * mask_for_one
        
    @staticmethod
    def backward(
        ctx, 
        grad_at_output: torch.Tensor, 
    ):
        multiplier, selected_experts, masked_gates = ctx.saved_tensors
        
        grad_at_output = grad_at_output * multiplier
        
        grad_at_scores_expaned = masked_gates * grad_at_output.mul(-1)
        grad_at_scores_expaned.scatter_add_(
            dim=-1,
            index=selected_experts,
            src=grad_at_output,
        )
        
        return (
            grad_at_scores_expaned, 
            None, 
            None, 
            None, 
            None, 
            None, 
        )


def enhanced_soft_orthogonality_loss(
    expert_embeddings: torch.Tensor,
    lambda_so: float = 1e-4,
    use_srip: bool = True,
) -> torch.Tensor:
    """
    Enhanced Soft Orthogonality Loss with SRIP variant.
    
    í•µì‹¬ ì°¨ë³„ì :
    1. Frobenius Norm + Spectral Norm ê²°í•©
    2. ì–‘ë°©í–¥ ì–µì œ: cos(e_i, e_j)^2ë¡œ +1ê³¼ -1 ëª¨ë‘ í˜ë„í‹°
    3. Warm-up ì§€ì›
    
    Args:
        expert_embeddings: [num_experts, dim] or [batch, num_experts, dim]
        lambda_so: loss coefficient
        use_srip: use spectral norm variant
    
    Returns:
        loss: scalar orthogonality loss
    """
    if expert_embeddings.dim() == 3:
        # Batch case: average over batch
        # This is important: we want the EXPERTS to be orthogonal on average,
        # but momentarily they can shift. Averaging the REPRESENTATION first
        # stabilizes the gradient.
        expert_embeddings = expert_embeddings.mean(dim=0)  # [E, dim]
    
    # Check for empty or invalid input
    if expert_embeddings.numel() == 0:
        return torch.tensor(0.0, device=expert_embeddings.device, requires_grad=True)

    # Normalize to unit vectors
    E_norm = F.normalize(expert_embeddings, p=2, dim=-1)  # [E, dim]
    
    # Gram matrix (pairwise cosine similarities)
    # G_ij = cos(e_i, e_j)
    G = torch.matmul(E_norm, E_norm.t())  # [E, E]
    
    # Target: Identity matrix
    I = torch.eye(G.size(0), device=G.device, dtype=G.dtype)
    
    # Frobenius norm loss: ||G - I||_F^2
    # This penalizes off-diagonals (both positive and negative correlations)
    # (cos)^2 will be minimized
    frob_loss = torch.pow(torch.norm(G - I, p='fro'), 2)
    
    if use_srip:
        # SRIP: Spectral norm of (G - I), bounds Lipschitz constant
        # Approximate with power iteration for efficiency? 
        # For typical E (e.g., 8-64), exact computation via svd or matrix_norm is feasible/fast enough on GPU.
        # If E is very large (e.g., 256+), might be slow, but usually done once per step.
        diff = G - I
        # Use spectral norm (2-norm)
        # torch.linalg.matrix_norm with ord=2 computes the spectral norm (largest singular value)
        spectral_loss = torch.linalg.matrix_norm(diff, ord=2) ** 2
        loss = 0.7 * frob_loss + 0.3 * spectral_loss
    else:
        loss = frob_loss
    
    return lambda_so * loss


class ExpressionProjector(nn.Module):
    """
    [Verified] Newton-Schulz Orthogonal Projector
    í•™ìŠµ/ì¶”ë¡  ëª¨ë‘ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ê°•ì œë¡œ ì§êµí™”í•˜ì—¬ ë¶•ê´´ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """

    def __init__(self, input_dim, output_dim, num_experts, method="newton_schulz", iterations=3, **kwargs):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_experts = num_experts
        self.method = method
        self.iterations = iterations

        # ë‹¨ì¼ ì„ í˜•ì¸µ + ì •ì§êµ ì´ˆê¸°í™” (ZeRO-3 safe)
        self.projection = nn.Linear(input_dim, output_dim, bias=False)
        try:
            if self.projection.weight.dim() >= 2:
                nn.init.orthogonal_(self.projection.weight)
            else:
                logging.get_logger('transformers').debug("Skipping orthogonal init for 1D partitioned tensor")
        except Exception as e:
            logging.get_logger('transformers').warning(f"Failed orthogonal initialization (ZeRO-3 partition?): {e}")

    def newton_schulz(self, W: torch.Tensor, steps: int = 3) -> torch.Tensor:
        """SVD-free orthogonalization."""
        norms = W.norm(p="fro") + 1e-8
        X = W / norms  # Always normalize for stability

        transpose = X.shape[0] < X.shape[1]
        if transpose:
            X = X.t()

        for _ in range(steps):
            A = torch.matmul(X.t(), X)
            X = 1.5 * X - 0.5 * torch.matmul(X, A)

        if transpose:
            X = X.t()
        return X

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # í•™ìŠµ/ì¶”ë¡  ê³µí†µìœ¼ë¡œ ì§êµí™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©
        W_ortho = self.newton_schulz(self.projection.weight, steps=self.iterations)
        return F.linear(x, W_ortho)

    def orthogonal_loss(self):
        # ê°•ì œ ì§êµí™” ê²½ë¡œì´ë¯€ë¡œ ë³„ë„ ì†ì‹¤ ë¶ˆí•„ìš”
        return torch.tensor(0.0, device=self.projection.weight.device)


class ManualGRUCell(nn.Module):
    """
    GRU Cell with manually managed (non-fused) parameters.
    
    DeepSpeed ZeRO-3 + NVMe Offloading compatible version.
    Splits large weight matrices into chunks to stay under NVme buffer limit (500M elements).
    """
    # Maximum elements per Linear to stay under NVMe buffer limit
    MAX_ELEMENTS_PER_WEIGHT = 400_000_000  # 400M to be safe (buffer is 500M)

    def __init__(self, input_size: int, hidden_size: int):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # Calculate if we need to chunk the hidden-to-hidden weights
        hh_gates_elements = hidden_size * hidden_size * 2
        hh_cand_elements = hidden_size * hidden_size
        
        # For input-to-hidden, check similarly
        ih_gates_elements = input_size * hidden_size * 2
        ih_cand_elements = input_size * hidden_size
        
        # Input-to-hidden gates (typically smaller, likely fits)
        self.weight_ih_gates = nn.Linear(input_size, hidden_size * 2)
        
        # Hidden-to-hidden gates: split if exceeds limit
        if hh_gates_elements > self.MAX_ELEMENTS_PER_WEIGHT:
            # Split into 2 chunks for gates (r, z)
            self.weight_hh_gates_r = nn.Linear(hidden_size, hidden_size)
            self.weight_hh_gates_z = nn.Linear(hidden_size, hidden_size)
            self._use_chunked_hh_gates = True
        else:
            self.weight_hh_gates = nn.Linear(hidden_size, hidden_size * 2)
            self._use_chunked_hh_gates = False

        self.weight_ih_cand = nn.Linear(input_size, hidden_size)
        self.weight_hh_cand = nn.Linear(hidden_size, hidden_size)

        # Initialize weights (skip if ZeRO-3 partitioned - less than 2 dims)
        def _safe_init(init_fn, weight):
            try:
                if weight.dim() >= 2:
                    init_fn(weight)
                else:
                    logging.get_logger('transformers').debug(f"Skipping {init_fn.__name__} for 1D partitioned tensor")
            except Exception as e:
                logging.get_logger('transformers').warning(f"Failed {init_fn.__name__} (ZeRO-3 partition?): {e}")
        
        if self._use_chunked_hh_gates:
            _safe_init(nn.init.orthogonal_, self.weight_hh_gates_r.weight)
            _safe_init(nn.init.orthogonal_, self.weight_hh_gates_z.weight)
        else:
            _safe_init(nn.init.orthogonal_, self.weight_hh_gates.weight)
        _safe_init(nn.init.orthogonal_, self.weight_hh_cand.weight)
        _safe_init(nn.init.xavier_uniform_, self.weight_ih_gates.weight)
        _safe_init(nn.init.xavier_uniform_, self.weight_ih_cand.weight)


    def forward(self, x: torch.Tensor, h: torch.Tensor) -> torch.Tensor:
        gates_x = self.weight_ih_gates(x)
        
        if self._use_chunked_hh_gates:
            # Compute r and z gates separately then concat
            gates_h_r = self.weight_hh_gates_r(h)
            gates_h_z = self.weight_hh_gates_z(h)
            gates_h = torch.cat([gates_h_r, gates_h_z], dim=-1)
        else:
            gates_h = self.weight_hh_gates(h)
        
        gates = gates_x + gates_h

        r_gate, z_gate = gates.chunk(2, dim=-1)
        r_gate = torch.sigmoid(r_gate)
        z_gate = torch.sigmoid(z_gate)

        cand_h = self.weight_hh_cand(h * r_gate)
        cand_x = self.weight_ih_cand(x)
        n = torch.tanh(cand_x + cand_h)

        next_h = (1 - z_gate) * n + z_gate * h
        return next_h


def differentiable_sinkhorn(cost: torch.Tensor, num_experts: int, epsilon: float = 0.05, iterations: int = 3) -> torch.Tensor:
    """
    [Pure Math Sinkhorn - No Learning, Just Computing]
    Differentiable Sinkhorn algorithm for optimal transport.
    í•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ 0ê°œì´ë¯€ë¡œ ë§ê°€ì§€ê³  ì‹¶ì–´ë„ ë§ê°€ì§ˆ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    
    Args:
        cost: [Batch, Experts] cost matrix (lower is better)
        num_experts: number of experts
        epsilon: temperature parameter
        iterations: number of Sinkhorn iterations
    
    Returns:
        Q: [Batch, Experts] doubly stochastic matrix (assignment probabilities)
    """
    batch_tokens = cost.shape[0]
    device = cost.device
    dtype = cost.dtype
    
    # Validate epsilon
    if epsilon <= 0:
        raise ValueError(f"epsilon must be positive, got {epsilon}")
    if iterations <= 0:
        raise ValueError(f"iterations must be positive, got {iterations}")
    
    # Initialize Q: exp(-cost / epsilon)
    # cost_float = cost.float()
    cost_min, _ = cost.min(dim=-1, keepdim=True)
    cost_float = (cost - cost_min).float()  # ì´ì œ ìµœì†Ÿê°’ì€ 0ì´ ë¨ -> exp(0) = 1 (Safe!)
    
    # Clamp cost to prevent exp overflow
    max_cost_ratio = 50.0
    cost_clamped = cost_float / epsilon
    cost_clamped = torch.clamp(cost_clamped, min=-max_cost_ratio, max=max_cost_ratio)
    
    Q = torch.exp(-cost_clamped).to(dtype=dtype)
    
    # Store original shape
    original_shape = Q.shape
    N, E = original_shape[0], original_shape[1]
    target_load = float(N) / float(E)
    
    # Sinkhorn iterations (standard Sinkhorn-Knopp)
    for i in range(iterations):
        # Row normalization: ê° í† í°ì˜ í™•ë¥  í•© = 1
        row_sum = Q.sum(dim=-1, keepdim=True) + 1e-8
        Q = Q / row_sum
        
        # NaN/Inf check
        if torch.isnan(Q).any() or torch.isinf(Q).any():
            raise ValueError(f"NaN/Inf in Sinkhorn iteration {i} after row norm")
        
        # Column normalization: ê° ì „ë¬¸ê°€ì˜ ê¸°ëŒ€ í† í° ìˆ˜ = N/E
        col_sum = Q.sum(dim=0, keepdim=True) + 1e-8
        Q = Q / col_sum * target_load
        
        # NaN/Inf check
        if torch.isnan(Q).any() or torch.isinf(Q).any():
            raise ValueError(f"NaN/Inf in Sinkhorn iteration {i} after col norm")
        
        # Shape consistency check
        assert Q.shape == original_shape, f"Shape changed in iteration {i}: {Q.shape} vs {original_shape}"
    
    # Final row normalization to ensure row sums = 1.0
    row_sum = Q.sum(dim=-1, keepdim=True) + 1e-8
    Q = Q / row_sum
    
    return Q


def log_sinkhorn_stabilized(
    logits: torch.Tensor,
    num_experts: int,
    epsilon: float = 0.05,
    iterations: int = 5,
    adaptive_epsilon: bool = True,
    cv_ema: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Log-Domain Stabilized Sinkhorn Algorithm.
    
    í•µì‹¬: exp(-C/epsilon) ëŒ€ì‹  log-spaceì—ì„œ ì—°ì‚°í•˜ì—¬ underflow/overflow ë°©ì§€.
    
    Args:
        logits: [N, E] routing logits (higher = better)
        num_experts: number of experts
        epsilon: base temperature (adaptiveí•˜ê²Œ ì¡°ì ˆë¨)
        iterations: Sinkhorn iterations
        adaptive_epsilon: CVì— ë”°ë¼ epsilon ì¡°ì ˆ
        cv_ema: CV EMA value for adaptive epsilon
    
    Returns:
        P: [N, E] doubly stochastic matrix
        max_vio: maximum constraint violation (for monitoring)
    """
    N, E = logits.shape
    device, dtype = logits.device, logits.dtype
    
    # Adaptive Epsilon: CVê°€ ë†’ìœ¼ë©´ epsilon ì¦ê°€ (ë” ë¶€ë“œëŸ¬ìš´ í• ë‹¹)
    if adaptive_epsilon and cv_ema is not None:
        cv_val = cv_ema.item() if torch.is_tensor(cv_ema) else cv_ema
        # Max scaling factor: 4.0 (1 + 3.0)
        # If CV explodes (e.g. > 10.0), clamp it.
        epsilon = epsilon * (1.0 + min(cv_val, 3.0))
    
    # Safety clamp for epsilon
    epsilon = max(epsilon, 1e-4) # Avoid division by zero
    
    # Log-space cost matrix: M = logits / epsilon
    # We want to maximize logits <-> minimize cost
    # Cost = -logits
    # K = exp(-Cost/eps) = exp(logits/eps)
    # working in log domain: M = log(K) = logits/eps
    M = logits.float() / epsilon
    
    # Numerical stability: subtract max per row (log-sum-exp trick preparation)
    # Does not change the resulting distribution P
    M_max = M.max(dim=-1, keepdim=True).values
    M = M - M_max
    
    # Initialize dual potentials
    f = torch.zeros(N, 1, device=device, dtype=torch.float32)  # row potential
    g = torch.zeros(1, E, device=device, dtype=torch.float32)  # column potential
    
    # Target marginals
    # Row target: 1 (actually 1/N but we are working with probabilities summing to 1 per row)
    # Wait, Sinkhorn usually projects to DSM where rowsum=1, colsum=N/E?
    # Or rowsum=1/N, colsum=1/E?
    # Standard Attention/Routing: row_sum = 1 (each token goes somewhere)
    # Col sum = N/E (uniform load)
    
    target_row = torch.zeros(N, 1, device=device, dtype=torch.float32) # log(1) = 0
    target_col = torch.log(torch.tensor(N / E, device=device, dtype=torch.float32) + 1e-10) # log(N/E)
    
    for _ in range(iterations):
        # Row normalization in log-space
        # u = 1 ./ (K @ v) => log(u) = -log(K @ exp(log_v))
        # log_sum_exp_row = logsumexp(M + g^T)
        log_sum_exp_row = torch.logsumexp(M + g, dim=-1, keepdim=True)
        # f update: f = log(target_row) - log_sum_exp_row
        # But wait, original M includes f and g implicitly?
        # Usually Sinkhorn updates are:
        # u <- target_r / (K @ v)
        # v <- target_c / (K.T @ u)
        # In log domain:
        # log_u <- log_target_r - logsumexp(M + log_v)
        # log_v <- log_target_c - logsumexp(M.T + log_u)
        
        f = target_row - log_sum_exp_row
        
        # Column normalization in log-space  
        log_sum_exp_col = torch.logsumexp(M + f, dim=0, keepdim=True)
        g = target_col - log_sum_exp_col
    
    # Compute final transport plan P = diag(u) K diag(v)
    # log P = log u + M + log v
    log_P = f + M + g
    
    # Convert back to probability space
    # Since we normalized rows to sum to 1 (target_row=0 => log(1)),
    # P rows should sum to 1.
    P = torch.exp(log_P).to(dtype)
    
    # Final cleanup: ensure row sums are exactly 1
    # (sometimes small errors accumulate)
    P = P / (P.sum(dim=-1, keepdim=True) + 1e-8)
    
    # Compute MaxVio for monitoring
    if iterations > 0:
        with torch.no_grad():
            row_sum = P.sum(dim=-1) # Should be 1.0
            col_sum = P.sum(dim=0)  # Should be N/E
            
            # Use float64 for precision in check
            row_vio = (row_sum.float() - 1.0).abs().max()
            col_vio = (col_sum.float() - (N/E)).abs().max()
            max_vio = torch.max(row_vio, col_vio)
    else:
        max_vio = torch.tensor(0.0, device=device)
    
    return P, max_vio


# [OSR] Neural Solver ì œê±° ì™„ë£Œ - Pure Math Sinkhornë§Œ ì‚¬ìš©
# [OSR] Neural Solver í´ë˜ìŠ¤ ì œê±° ì™„ë£Œ
# NeuralGradientProjectorì™€ DualPotentialLinearSolverëŠ” ì œê±°ë¨
# ì´ì œ differentiable_sinkhorn í•¨ìˆ˜ë§Œ ì‚¬ìš© (í•™ìŠµ íŒŒë¼ë¯¸í„° 0ê°œ)

class SPECTRARouter(nn.Module):
    def __init__(self, config: SPECTRATextConfig, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.router_dim = config.router_dim
        self.layernorm_eps = getattr(config, "router_layernorm_eps", 1e-5)

        # ------------------------------------------------------------------
        # Expert-Choice (Quota) Routing
        # - Keep sparse MoE compute (still runs only top-k experts per token)
        # - But make expert loads stable by enforcing per-expert capacity at the
        #   *selection* stage (token-choice top-k tends to break Sinkhorn balance).
        # ------------------------------------------------------------------
        self.expert_choice_routing = bool(getattr(config, "expert_choice_routing", True))
        # Capacity factor (typical: 1.0~2.0). We also accept `capacity_factor` for convenience.
        self.expert_choice_capacity_factor = float(
            getattr(config, "expert_choice_capacity_factor", getattr(config, "capacity_factor", 1.25))
        )
        
        # ===== ê°œì„ ëœ Loss ê°€ì¤‘ì¹˜ ì„¤ì • =====
        # 1. Speciality: ë„ˆë¬´ ê°•í•˜ì§€ ì•Šê²Œ (ê¸°ì¡´ 0.02 â†’ 0.001)
        self.speciality_strength = getattr(config, "speciality_strength", 0.001)
        
        # 2. Sinkhorn: Teacher ê°•ë„ ì¡°ì ˆ (ê¸°ì¡´ 0.1 â†’ 0.05)
        self.sinkhorn_distillation_coef = getattr(config, "sinkhorn_distillation_coef", 0.05)
        
        # [ì „ëµ 1] Balance Loss ê°€ì¤‘ì¹˜: CVë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ "ê°€ì¤‘ì¹˜ í­íƒ„"
        # GRU Solverì˜ balance_lossì— ê°•í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¤˜ì„œ ë°¸ëŸ°ì‹±ì„ ìµœìš°ì„ ìœ¼ë¡œ ë§Œë“¦
        self.balance_loss_coef = getattr(config, "balance_loss_coef", 2.0)  # ê¸°ë³¸ê°’ 2.0 (1.0 ~ 5.0 ê¶Œì¥)
        
        # 3. Adaptive weighting: CVì— ë”°ë¼ loss ê°€ì¤‘ì¹˜ ìë™ ì¡°ì ˆ
        self.adaptive_loss_scaling = getattr(config, "adaptive_loss_scaling", True)
        
        # 4. Curriculum learning: ì´ˆë°˜ì—ëŠ” load balancing ê°•í•˜ê²Œ, í›„ë°˜ì—ëŠ” speciality ê°•í•˜ê²Œ
        self.curriculum_steps = getattr(config, "curriculum_steps", 10000)
        self.register_buffer("training_step", torch.tensor(0, dtype=torch.long))
        
        # EMA ì„¤ì •
        self.balancing_strength = getattr(config, "balancing_strength", 0.01)
        self.ema_alpha = getattr(config, "ema_alpha", 0.99)
        self.register_buffer("expert_load_ema", torch.zeros(self.num_experts), persistent=True)
        
        # CV ì¶”ì 
        self.register_buffer("cv_ema", torch.tensor(1.0), persistent=True)
        self.cv_ema_alpha = 0.95
        
        # Sinkhorn ì„¤ì •
        self.enable_sinkhorn = getattr(config, "enable_sinkhorn", True)
        self.sinkhorn_epsilon = max(getattr(config, "sinkhorn_epsilon", 0.1), 1e-6)
        self.sinkhorn_iterations = max(getattr(config, "sinkhorn_iterations", 3), 1)
        self.sinkhorn_inference = getattr(config, "sinkhorn_inference", False)
        
        # Global routing GRU cell (depth-shared)
        self.load_balancer = ManualGRUCell(
            input_size=self.hidden_size,
            hidden_size=self.num_experts * self.router_dim,
        )
        
        # ===== Expression Projector (Linear + Ortho Init) =====
        # ë³µì¡í•œ Newton-Schulzë„ ì¼ë‹¨ ëºì‹œë‹¤. OSR ì²™ë ¥ì´ë©´ ì¶©ë¶„í•©ë‹ˆë‹¤.
        self.expression_projector = ExpressionProjector(
            self.hidden_size,
            self.num_experts * self.router_dim,
            self.num_experts,
            method="linear",
            iterations=getattr(config, "spechorn_osr_iter", 3),
        )
        # Orthogonal initialization for better starting point (ZeRO-3 safe)
        if hasattr(self.expression_projector, 'projection'):
            try:
                if self.expression_projector.projection.weight.dim() >= 2:
                    nn.init.orthogonal_(self.expression_projector.projection.weight)
            except Exception as e:
                logging.get_logger('transformers').warning(f"Failed router basis initialization: {e}")
        self.expression_projector.ortho_strength = 0.0  # OSR ì²™ë ¥ì´ ì§êµì„±ì„ ê°•ì œí•˜ë¯€ë¡œ ì¶”ê°€ ì œì•½ ë¶ˆí•„ìš”
        
        # Bias Predictor (GRU ê¸°ë°˜) - Sequential ë¶„ë¦¬ (LoRA ëŒ€ìƒ ì„ í˜•ì¸µ ë…¸ì¶œ)
        self.bias_predictor_hidden_dim = getattr(config, "bias_predictor_hidden_dim", 256)
        self.bias_pred_fc1 = nn.Linear(
            self.num_experts * self.router_dim + self.num_experts, self.bias_predictor_hidden_dim
        )
        self.bias_pred_fc2 = nn.Linear(self.bias_predictor_hidden_dim, self.num_experts)
        
        # Contrastive loss
        self.contrastive_loss = ContrastiveRouterLoss()

        # OSR Hyperparameters
        self.repulsion_weight = getattr(config, "osr_repulsion_weight", 0.5)  # [í•µì‹¬] OSR ì²™ë ¥ ê°€ì¤‘ì¹˜
        
        # SOS-RMoE Configuration
        self.log_sinkhorn_enabled = getattr(config, "log_sinkhorn_enabled", True)
        self.srip_enabled = getattr(config, "srip_enabled", True)
        self.so_warmup_steps = getattr(config, "so_warmup_steps", 100)
        self.so_lambda_max = getattr(config, "so_lambda_max", 5e-2)
        
        # MaxVio tracking
        self.register_buffer("max_vio_ema", torch.tensor(0.0), persistent=True)
        self.max_vio_ema_alpha = 0.95

        # [EQT] P-Control Pending Buffers for CheckpointError prevention
        # In-place updates in forward cause metadata mismatch during recomputation.
        # We accumulate deltas here and apply them in step_update() at forward start.
        self.register_buffer("pending_expert_bias_delta", torch.zeros(self.num_experts), persistent=False)
        self.register_buffer("pending_cv_sum", torch.tensor(0.0), persistent=False)
        self.register_buffer("pending_cv_count", torch.tensor(0.0), persistent=False)
        
        # [ASEOD] Self-Evolving Basis Pending Buffer
        # Buffer shape matches expression_projector output: [num_experts * router_dim, hidden_size]
        # Will be initialized lazily in backward hook if needed
        self.register_buffer("pending_basis_delta", None, persistent=False)
        self.basis_evolution_rate = getattr(config, "basis_evolution_rate", 0.001)
        
        # Bias update rate for P-control
        self.bias_update_rate = getattr(config, "bias_update_rate", 0.01)
        
        # [EQT] Register backward hook for bias update
        # CRITICAL: This runs AFTER backward pass completes, so no CheckpointError
        self.register_full_backward_hook(self._backward_hook_update)


    def _expert_choice_topk_selection(
        self,
        routing_probs_full: torch.Tensor,  # [B, S, E], Sinkhorn probs
        top_k: int,
        capacity_factor: float,
    ) -> Tuple[torch.Tensor, torch.Tensor, int, torch.Tensor]:
        """
        Expert-choice (quota) sparse selection.

        We keep the *scores/probabilities* for all experts (cheap) but choose a sparse top-k
        set under a per-expert capacity constraint:
          capacity â‰ˆ capacity_factor * (N_tokens * top_k) / num_experts

        Returns:
          multiplier: [B, S, top_k] normalized weights (sum=1 over top_k)
          selected_experts: [B, S, top_k] expert indices
          cap: per-expert capacity in tokens
          fallback_mask: [N] mask where quota selection failed and token-choice fallback was used
        """
        if routing_probs_full.dim() != 3:
            raise ValueError(f"routing_probs_full must be [B,S,E], got {routing_probs_full.shape}")

        batch_size, seq_len, num_experts = routing_probs_full.shape
        if num_experts != self.num_experts:
            raise ValueError(f"Expected num_experts={self.num_experts}, got {num_experts}")

        k = int(min(max(int(top_k), 1), num_experts))
        N = int(batch_size * seq_len)
        scores = routing_probs_full.reshape(N, num_experts)

        # Per-expert capacity in terms of *slots* (N tokens * k experts per token).
        # Add slack via capacity_factor. We cap by N*k slots, not N tokens.
        cap = int(math.ceil(float(capacity_factor) * (float(N) * float(k)) / float(num_experts)))
        cap = max(1, min(cap, N * k))

        # Capacity-masked k-round selection (hard constraint):
        # - Each round picks 1 expert per token among experts with remaining capacity
        # - Prevents any expert from exceeding cap (=> CV/MaxVio drift from selection stage is stopped)
        cap_left = torch.full((num_experts,), cap, dtype=torch.long, device=scores.device)
        selected = torch.empty((N, k), dtype=torch.long, device=scores.device)
        selected_vals = torch.empty((N, k), dtype=scores.dtype, device=scores.device)
        fallback_mask = torch.zeros((N,), dtype=torch.bool, device=scores.device)

        neg_inf = torch.tensor(float("-inf"), device=scores.device, dtype=scores.dtype)

        for r in range(k):
            # Start from original scores
            ms = scores

            # Mask experts with no remaining capacity
            avail = cap_left > 0  # [E]
            # CRITICAL FIX: Use tensor operations instead of .item() for deterministic behavior
            # in forward vs backward pass with use_reentrant=False
            # Always compute masked_scores to ensure consistent tensor shapes
            masked_scores = ms.clone()
            masked_scores[:, ~avail] = neg_inf

            # Mask already chosen experts for this token
            if r > 0:
                row_ids = torch.arange(N, device=scores.device).unsqueeze(1).expand(N, r)
                masked_scores[row_ids, selected[:, :r]] = neg_inf

            # CRITICAL FIX: Use tensor operations instead of conditional logic
            # to ensure deterministic behavior in forward vs backward pass
            # Check if any expert is available (tensor operation, not .item())
            avail_any = avail.any()  # Tensor, not scalar
            
            # Check if tokens have any valid scores (tensor operation)
            has_any = torch.isfinite(masked_scores).any(dim=-1)  # [N]
            has_any_all = has_any.all()  # Tensor, not scalar
            
            # Use torch.where to handle both cases deterministically
            # If no experts available, use fallback; otherwise use masked choice
            # CRITICAL: Always compute both paths to ensure consistent tensor shapes
            fallback_scores = scores.clone()
            if r > 0:
                row_ids = torch.arange(N, device=scores.device).unsqueeze(1).expand(N, r)
                fallback_scores[row_ids, selected[:, :r]] = neg_inf
            fallback_choice = fallback_scores.argmax(dim=-1)
            masked_choice = masked_scores.argmax(dim=-1)
            
            # Use tensor operations to combine choices deterministically
            # If no experts available OR token has no valid scores, use fallback
            use_fallback = ~avail_any | ~has_any
            chosen = torch.where(use_fallback, fallback_choice, masked_choice)
            
            # Update fallback_mask using tensor operations
            fallback_mask = fallback_mask | use_fallback
            
            # Get chosen values
            chosen_vals = scores.gather(1, chosen.unsqueeze(1)).squeeze(1)

            selected[:, r] = chosen
            selected_vals[:, r] = chosen_vals

            # Update capacities
            used = torch.bincount(chosen, minlength=num_experts).to(cap_left.dtype)
            cap_left = cap_left - used
            cap_left = torch.clamp(cap_left, min=0)

        # Normalize weights over top-k for stable scaling.
        # CRITICAL FIX: Detach denominator to prevent invalid gradient flow
        # The normalization denominator doesn't need gradients - only the routing scores do
        selected_vals = selected_vals.clamp_min(0.0)
        denom = selected_vals.sum(dim=-1, keepdim=True).clamp_min(1e-8).detach()
        multiplier = (selected_vals / denom).to(dtype=routing_probs_full.dtype)


        multiplier = multiplier.view(batch_size, seq_len, k)
        selected_experts = selected.view(batch_size, seq_len, k)
        return multiplier, selected_experts, cap, fallback_mask

    def step_update(self):
        """
        [EQT] Apply pending P-control updates to state buffers.
        Called at the START of forward() to ensure state is stable for the entire step
        (Forward + Backward + Recompute under gradient checkpointing).
        
        CONSTRAINT: No in-place ops on nn.Parameter. Uses copy_() on buffers only.
        """
        if not self.training:
            return

        with torch.no_grad():
            # 1. Apply Expert Bias Delta (Accumulated from backward hook)
            delta_sum = self.pending_expert_bias_delta.abs().sum()
            if delta_sum > 0:
                # Apply delta to expert_bias buffer (not nn.Parameter)
                self.expert_load_ema.add_(self.pending_expert_bias_delta)
                # Mean centering to prevent drift
                self.expert_load_ema.sub_(self.expert_load_ema.mean())
                # Clear pending delta
                self.pending_expert_bias_delta.zero_()
            
            # 2. Apply CV EMA Update
            if self.pending_cv_count > 0:
                avg_cv = self.pending_cv_sum / self.pending_cv_count
                self.cv_ema.mul_(self.cv_ema_alpha).add_(avg_cv * (1.0 - self.cv_ema_alpha))
                self.pending_cv_sum.zero_()
                self.pending_cv_count.zero_()
            
            # 3. [ASEOD] Apply Self-Evolving Basis Delta (Manifold Projection)
            if self.pending_basis_delta is not None and hasattr(self, 'expression_projector'):
                if hasattr(self.expression_projector, 'projection'):
                    delta_sum = self.pending_basis_delta.abs().sum()
                    if delta_sum > 0:
                        proj_weight = self.expression_projector.projection.weight
                        # Apply delta (additive evolution)
                        proj_weight.data.add_(self.pending_basis_delta)
                        
                        # Re-normalize rows for manifold projection (orthogonality preservation)
                        # Each row is one expert's representation vector
                        E, R = self.num_experts, self.router_dim
                        if proj_weight.shape[0] == E * R:
                            W_reshaped = proj_weight.data.view(E, R, -1)  # [E, R, H]
                            W_flat = W_reshaped.view(E, -1)  # [E, R*H]
                            W_norm = F.normalize(W_flat, dim=1)  # Normalize each expert
                            proj_weight.data.copy_(W_norm.view_as(proj_weight.data))
                        
                        # Clear pending delta
                        self.pending_basis_delta.zero_()


    def _backward_hook_update(self, module, grad_input, grad_output):
        """
        [EQT] Accumulate P-control bias updates into pending buffers.
        Runs AFTER backward pass, so no CheckpointError.
        
        CONSTRAINT: Accumulates to buffers, does NOT mutate nn.Parameter in-place.
        """
        if not self.training:
            return

        with torch.no_grad():
            # 1. Update Expert Bias based on last forward's routing
            if hasattr(self, 'last_routing_probs_full') and self.last_routing_probs_full is not None:
                routing_probs = self.last_routing_probs_full
                last_routing_probs_mean = routing_probs.mean(dim=(0, 1))
                target_load = 1.0 / self.num_experts
                load_error = last_routing_probs_mean - target_load
                
                # Accumulate Bias Delta (sign-based for stability)
                delta = -self.bias_update_rate * load_error.sign()
                self.pending_expert_bias_delta.add_(delta)
                
                # 2. CV Calculation for monitoring
                expert_load = routing_probs.sum(dim=(0, 1))
                total_load = expert_load.sum()
                if total_load > 0:
                    expert_dist = expert_load / total_load
                else:
                    expert_dist = torch.ones_like(expert_load) / self.num_experts
                
                var_p = expert_dist.var(unbiased=False)
                mean_p = expert_dist.mean()
                current_cv = var_p.sqrt() / (mean_p + 1e-6)
                
                # Accumulate CV stats
                self.pending_cv_sum.add_(current_cv)
                self.pending_cv_count.add_(1)
                
                # 3. [ASEOD] Self-Evolving Basis Evolution
                # Evolve expression projector weights based on load + PES error
                if hasattr(self, 'expression_projector') and hasattr(self.expression_projector, 'projection'):
                    proj_weight = self.expression_projector.projection.weight  # [out, in]
                    
                    # Initialize pending buffer if needed
                    if self.pending_basis_delta is None or self.pending_basis_delta.shape != proj_weight.shape:
                        self.pending_basis_delta = torch.zeros_like(proj_weight)
                    
                    # Compute load-weighted direction for each expert
                    # Reshape weight: [num_experts * router_dim, hidden] -> [num_experts, router_dim, hidden]
                    E, R, H = self.num_experts, self.router_dim, self.hidden_size
                    if proj_weight.shape[0] == E * R:
                        W_reshaped = proj_weight.view(E, R, H)  # [E, R, H]
                        
                        # PES error: off-diagonal of Gram matrix
                        W_flat = W_reshaped.view(E, R * H)  # [E, R*H]
                        W_norm = F.normalize(W_flat, dim=1)
                        G = W_norm @ W_norm.T  # [E, E]
                        I = torch.eye(E, device=G.device, dtype=G.dtype)
                        pes_error_per_expert = (G - I).abs().sum(dim=1)  # [E]
                        
                        # Evolution: push apart high-PES experts, balance overloaded
                        # delta = rate * (load_error * noise + pes_error * current_dir)
                        noise = torch.randn_like(W_reshaped) * 0.01  # Small random perturbation
                        
                        # Expand errors to match weight shape
                        load_err_expanded = load_error.view(E, 1, 1).expand_as(W_reshaped)  # [E, R, H]
                        pes_err_expanded = pes_error_per_expert.view(E, 1, 1).expand_as(W_reshaped)  # [E, R, H]
                        
                        evolution_delta = self.basis_evolution_rate * (
                            load_err_expanded * noise +  # Load-driven perturbation
                            pes_err_expanded * W_reshaped * 0.01  # PES-driven push
                        )
                        
                        # Accumulate to pending buffer
                        self.pending_basis_delta.add_(evolution_delta.view_as(proj_weight))



    def compute_adaptive_loss_weights(self, current_cv: float) -> dict:
        """CVì— ë”°ë¼ loss ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆ"""
        # CVê°€ ë†’ì„ìˆ˜ë¡ load balancingì— ì§‘ì¤‘, ë‚®ì„ìˆ˜ë¡ specialityì— ì§‘ì¤‘
        cv_factor = float(torch.clamp(torch.tensor(current_cv), 0.1, 2.0).item())  # floatë¡œ ë³€í™˜
        
        # Curriculum learning: ì´ˆë°˜ì—ëŠ” LB ê°•í•˜ê²Œ
        progress = min(1.0, float(self.training_step) / self.curriculum_steps)
        curriculum_factor = 1.0 - 0.5 * progress  # 1.0 â†’ 0.5
        
        weights = {
            # [ìˆ˜ì •] ê³„ìˆ˜ ëŒ€í­ ìƒí–¥ - CVê°€ ë†’ì„ ë•Œ Lossê°€ 'ìœ ì˜ë¯¸í•œ í¬ê¸°(0.1~1.0)'ê°€ ë˜ì–´ì•¼ ë¼ìš°í„°ê°€ ë°˜ì‘í•©ë‹ˆë‹¤
            # Load balancing: CV ë†’ìœ¼ë©´ ê°•í•˜ê²Œ
            'sinkhorn': 0.5 * cv_factor * curriculum_factor,      # ê¸°ì¡´ 0.05 -> 0.5 (10ë°°)
            'entropy': 0.1 * cv_factor * curriculum_factor,       # ê¸°ì¡´ 0.01 -> 0.1 (10ë°°)
            'lb_direct': 1.0 * cv_factor * curriculum_factor,     # ê¸°ì¡´ 0.01 -> 1.0 (100ë°°! ê°€ì¥ ì¤‘ìš”)
            
            # [ì „ëµ 1] Balance Loss: GRU Solverì˜ constraint violation lossì— ê°•í•œ ê°€ì¤‘ì¹˜
            # CVê°€ ë†’ì„ìˆ˜ë¡ ë” ê°•í•˜ê²Œ ì ìš©í•˜ì—¬ ë°¸ëŸ°ì‹±ì„ ìµœìš°ì„ ìœ¼ë¡œ ë§Œë“¦
            'balance_loss': self.balance_loss_coef * cv_factor * curriculum_factor,
            
            # Specialization: CV ë‚®ìœ¼ë©´ ê°•í•˜ê²Œ (progressì— ë”°ë¼ ì¦ê°€)
            'speciality': 0.0005 * (1.0 / cv_factor) * progress,
            'contrastive': 0.005 * (1.0 / cv_factor) * progress,
            'expression_reg': 0.0001,
        }
        return weights

    def compute_improved_speciality_loss(
        self, 
        routing_logits: torch.Tensor,
        expression_logits: torch.Tensor
    ) -> torch.Tensor:
        """ê°œì„ ëœ Speciality Loss: Soft orthogonality + Diversity"""
        batch_size, seq_len, num_experts, router_dim = routing_logits.shape
        
        # 1. Routing spaceì˜ soft orthogonality (ê¸°ì¡´ë³´ë‹¤ ì•½í•˜ê²Œ)
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        routing_normalized = routing_logits / (routing_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        gram = torch.matmul(
            routing_normalized.view(-1, num_experts, router_dim),
            routing_normalized.view(-1, num_experts, router_dim).transpose(-2, -1)
        )
        identity = torch.eye(num_experts, device=gram.device)
        
        # Off-diagonalë§Œ í˜ë„í‹° (diagonalì€ 1ì´ì–´ë„ ë¨)
        mask = ~torch.eye(num_experts, dtype=torch.bool, device=gram.device)
        off_diag_loss = (gram[:, mask] ** 2).mean()
        
        # 2. Expression spaceì˜ diversity (ì „ì²´ ë°°ì¹˜ì—ì„œ ë‹¤ì–‘ì„±)
        # expression_logitsì˜ shapeì„ routing_logitsì™€ ë§ì¶¤
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        expr_normalized = expression_logits / (expression_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # expression_logitsì˜ shape í™•ì¸ ë° ë³€í™˜
        if expr_normalized.shape == routing_logits.shape:
            # ì´ë¯¸ ì˜¬ë°”ë¥¸ shape: [batch_size, seq_len, num_experts, router_dim]
            expr_flat = expr_normalized.view(-1, num_experts, router_dim)
        else:
            # shapeì´ ë‹¤ë¥´ë©´ ë³€í™˜ ì‹œë„
            # expression_logits: [batch_size, seq_len, num_experts, router_dim] ë˜ëŠ” ë‹¤ë¥¸ í˜•íƒœ
            expr_total_elements = expr_normalized.numel()
            expected_elements = batch_size * seq_len * num_experts * router_dim
            
            if expr_total_elements == expected_elements:
                # ì „ì²´ ìš”ì†Œ ìˆ˜ê°€ ê°™ìœ¼ë©´ reshapeë§Œ í•˜ë©´ ë¨
                expr_flat = expr_normalized.view(batch_size, seq_len, num_experts, router_dim).view(-1, num_experts, router_dim)
            else:
                # ìš”ì†Œ ìˆ˜ê°€ ë‹¤ë¥´ë©´ ë§ˆì§€ë§‰ router_dim ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
                # expr_normalizedë¥¼ [..., router_dim] í˜•íƒœë¡œ ê°€ì •
                if expr_normalized.dim() >= 2 and expr_normalized.size(-1) == router_dim:
                    # ë§ˆì§€ë§‰ ì°¨ì›ì´ router_dimì´ë©´, ì•ë¶€ë¶„ì„ flatten
                    expr_flat_all = expr_normalized.view(-1, router_dim)
                    # í•„ìš”í•œ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
                    needed_count = batch_size * seq_len * num_experts
                    if expr_flat_all.size(0) >= needed_count:
                        expr_flat = expr_flat_all[:needed_count].view(-1, num_experts, router_dim)
                    else:
                        # ë¶€ì¡±í•˜ë©´ ë°˜ë³µ
                        repeat_times = (needed_count + expr_flat_all.size(0) - 1) // expr_flat_all.size(0)
                        expr_flat = expr_flat_all.repeat(repeat_times, 1)[:needed_count].view(-1, num_experts, router_dim)
                else:
                    # shapeì´ ë§ì§€ ì•Šìœ¼ë©´ routing_logitsë§Œ ì‚¬ìš©
                    return off_diag_loss
        
        # Expertë³„ í‰ê·  ë°©í–¥
        expert_directions = expr_flat.mean(dim=0)  # [num_experts, router_dim]
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        expert_directions = expert_directions / (expert_directions.norm(p=2, dim=-1, keepdim=True) + 1e-6)
        
        # Expertê°„ cosine similarityì˜ ë¶„ì‚° ìµœì†Œí™” (ë” uniformí•˜ê²Œ)
        similarity_matrix = torch.matmul(expert_directions, expert_directions.T)
        target_similarity = torch.ones_like(similarity_matrix) * (1.0 / num_experts)
        target_similarity.diagonal().fill_(1.0)
        diversity_loss = F.mse_loss(similarity_matrix, target_similarity)
        
        return off_diag_loss * 0.5 + diversity_loss * 0.5

    def compute_improved_sinkhorn_loss(
        self,
        raw_logits: torch.Tensor,
        balanced_logits: torch.Tensor,
        Q_target: Optional[torch.Tensor]
    ) -> torch.Tensor:
        """ê°œì„ ëœ Sinkhorn Distillation: Soft target + Curriculum"""
        if Q_target is None:
            return torch.tensor(0.0, device=raw_logits.device, requires_grad=True)
        
        # Student distribution (GRUê°€ ì˜ˆì¸¡í•œ ê²ƒ)
        P_student = F.softmax(balanced_logits, dim=-1)
        
        # Soft target: Q_targetì™€ uniformì˜ í˜¼í•©
        uniform = torch.ones_like(Q_target) / self.num_experts
        progress = min(1.0, float(self.training_step) / self.curriculum_steps)
        
        # ì´ˆë°˜: uniform ë¹„ì¤‘ ë†’ìŒ, í›„ë°˜: Sinkhorn ë¹„ì¤‘ ë†’ìŒ
        Q_soft = (1 - progress) * uniform + progress * Q_target
        
        # KL Divergence (ë” ì•ˆì •ì )
        # ì–‘ë°©í–¥ KLì˜ ê°€ì¤‘ í‰ê· 
        kl_forward = F.kl_div(
            P_student.log().clamp(min=-10),
            Q_soft,
            reduction='batchmean',
            log_target=False
        )
        kl_reverse = F.kl_div(
            Q_soft.log().clamp(min=-10),
            P_student,
            reduction='batchmean',
            log_target=False
        )
        
        return 0.7 * kl_forward + 0.3 * kl_reverse

    @torch._dynamo.disable  # Disable torch.compile for gradient checkpointing compatibility
    def sinkhorn_algorithm(self, cost: torch.Tensor, epsilon: float = None, iterations: int = None) -> torch.Tensor:
        """
        Sinkhorn-Knopp algorithm for Optimal Transport.
        Computes a doubly stochastic matrix Q that ensures uniform expert load distribution.
        
        Args:
            cost: [N, E] cost matrix (negative logits, higher score = lower cost)
            epsilon: Temperature parameter (default: self.sinkhorn_epsilon)
            iterations: Number of Sinkhorn iterations (default: self.sinkhorn_iterations)
        
        Returns:
            Q: [N, E] doubly stochastic matrix (assignment probabilities)
        """
        if self.log_sinkhorn_enabled:
            # Use Log-Domain Sinkhorn for stability
            P, max_vio = log_sinkhorn_stabilized(
                logits=cost,  # cost -> logits (negate), higher logits = lower cost
                num_experts=self.num_experts,
                epsilon=epsilon,
                iterations=iterations,
                adaptive_epsilon=self.adaptive_loss_scaling,
                cv_ema=self.cv_ema,
            )
        else:
            # Standard Sinkhorn (legacy) - only if explicitly disabled
            # ... (omitted, assuming log_sinkhorn is preferred)
            # Just fallback to log_sinkhorn for safety
            P, max_vio = log_sinkhorn_stabilized(
                logits=cost,
                num_experts=self.num_experts,
                epsilon=epsilon,
                iterations=iterations,
                adaptive_epsilon=False, # Disable adaptive if legacy mode
                cv_ema=None,
            )
        
        # Update MaxVio EMA
        if self.training:
            with torch.no_grad():
                self.max_vio_ema.mul_(self.max_vio_ema_alpha).add_(
                    max_vio * (1.0 - self.max_vio_ema_alpha)
                )
        
        return P
    
    def compute_routing_uncertainty(self, routing_probs: torch.Tensor) -> torch.Tensor:
        """
        Compute normalized entropy of routing probabilities as uncertainty measure.
        Returns: [batch, seq_len] or [batch * seq_len] depending on input
        """
        # routing_probs: [..., num_experts]
        probs = routing_probs + 1e-8
        
        # Entropy: -sum(p * log(p))
        entropy = -torch.sum(probs * torch.log(probs), dim=-1)
        
        # Normalize by max entropy (log(num_experts))
        max_entropy = torch.log(torch.tensor(self.num_experts, device=probs.device, dtype=probs.dtype))
        normalized_entropy = entropy / max_entropy
        
        return normalized_entropy

    def compute_osr_cost(self, similarity: torch.Tensor, expert_embeddings: torch.Tensor) -> torch.Tensor:
        """
        [OSR Core Logic - Stabilized: Repulsion penalizes |expert-expert cosine| to drive orthogonality]
        Cost = -Similarity + lambda * Repulsion
        Repulsion: í† í°ì´ ì„ í˜¸(|similarity|)í•˜ëŠ” ì „ë¬¸ê°€ë“¤ì´ ì„œë¡œ ìƒê´€(|cos|)ì´ í¬ë©´ í˜ë„í‹°ë¥¼ ì¤Œ.
        (í•µì‹¬) expert-expert così˜ ë¶€í˜¸(ì–‘/ìŒ)ì™€ ë¬´ê´€í•˜ê²Œ |cos|ê°€ í¬ë©´ "ì§êµ(0)"ì—ì„œ ë©€ê¸° ë•Œë¬¸ì— í˜ë„í‹°.
        
        Args:
            similarity: [Batch, Experts] cosine similarity between routing and expression vectors
            expert_embeddings: [Experts, Dim] expert representation vectors
        
        Returns:
            cost: [Batch, Experts] cost matrix (lower is better for Sinkhorn)
        """
        # 1. ì „ë¬¸ê°€ ê°„ ìœ ì‚¬ë„ (Gram Matrix)
        # expert_embeddings: [Experts, Dim]
        # G: [Experts, Experts]
        expert_sim_matrix = torch.matmul(expert_embeddings, expert_embeddings.t())
        
        # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„(ëŒ€ê°ì„ )ëŠ” 0ìœ¼ë¡œ ë§Œë“¦ (ìê¸° ìì‹ ì„ ë°€ì–´ë‚´ë©´ ì•ˆ ë˜ë‹ˆê¹Œ)
        identity = torch.eye(self.num_experts, device=similarity.device, dtype=similarity.dtype)
        expert_sim_matrix = expert_sim_matrix * (1 - identity)
        
        # |cos|ë¥¼ ë²Œì : squareëŠ” ë¶€í˜¸ë¥¼ ì œê±°í•˜ê³  í° ìƒê´€(ì–‘/ìŒ)ì„ ê°•í•˜ê²Œ ë²Œì 
        # -> anti-parallel(-1)ë¡œ "ìŒìˆ˜ ìˆ˜ë ´"í•˜ëŠ” í•´ë„ ë§‰ê³ , ëª©í‘œë¥¼ orthogonal(0)ë¡œ ë‘ 
        repulsion_matrix = torch.square(expert_sim_matrix)
        
        # 2. ì²™ë ¥ ê³„ì‚° (Lateral Inhibition)
        # í† í°-ì „ë¬¸ê°€ ê´€ë ¨ì„±(|similarity|) í¬ê¸°ë¡œ ê°€ì¤‘: ê°•í•˜ê²Œ ê´€ë ¨ëœ ì „ë¬¸ê°€ë“¤ë¼ë¦¬ë§Œ ë” ê°•í•˜ê²Œ ë°€ì–´ëƒ„
        similarity_magnitude = torch.abs(similarity)
        repulsion_score = torch.matmul(similarity_magnitude, repulsion_matrix)
        
        # 3. ìµœì¢… Cost (Sinkhornì€ Costê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ì•„í•¨)
        # Similarityê°€ ë†’ìœ¼ë©´ Cost ë‚®ì¶¤ (-Sim)
        # Repulsionì´ ë†’ìœ¼ë©´ Cost ë†’ì„ (+Rep)
        cost = -similarity + self.repulsion_weight * repulsion_score
        
        return cost

    def predict_expert_bias_from_gru(self, hn: torch.Tensor) -> torch.Tensor:
        """ê°œì„ ëœ Bias ì˜ˆì¸¡: ë” ê°•í•œ ì •ê·œí™”"""
        # last_hn = hn[-1]  # [ìˆ˜ì •] hnì€ [tokens, dim] í˜•íƒœì´ë¯€ë¡œ ì „ì²´ í† í° ì‚¬ìš©
        
        # EMA loadë¥¼ í™•ë¥ ë¡œ ì •ê·œí™”
        ema_total = self.expert_load_ema.sum()
        if ema_total > 0:
            ema_normalized = self.expert_load_ema / ema_total
        else:
            ema_normalized = torch.full_like(self.expert_load_ema, 1.0 / self.num_experts)
        
        # hn: [tokens, dim]
        batch_tokens = hn.size(0)
        
        # Expand EMA to match tokens
        ema_expanded = ema_normalized.unsqueeze(0).expand(batch_tokens, -1)
        combined_input = torch.cat([hn, ema_expanded], dim=-1)
        
        # Bias ì˜ˆì¸¡ (Tanhë¡œ [-1, 1] ì œí•œë¨)
        x = F.relu(self.bias_pred_fc1(combined_input))
        predicted_bias = self.bias_pred_fc2(x)
        mean_bias = predicted_bias.mean(dim=0)
        
        # Zero-mean ê°•ì œ
        mean_bias = mean_bias - mean_bias.mean()
        
        # [ìˆ˜ì •] Bias ì œí•œ ì™„ì „ í•´ì œ - Logit(10.0)ì„ ì´ê²¨ë¨¹ì„ ìˆ˜ ìˆë„ë¡
        # CV ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§: CVê°€ ë†’ì„ìˆ˜ë¡ Biasë¥¼ ë” í¬ê²Œ ì ìš©í•˜ì—¬ Load Balancing ê°•í™”
        current_cv = self.cv_ema.item()
        # cv_scaleì„ CVì— ë¹„ë¡€í•˜ë„ë¡ ì„¤ì • (ìµœì†Œ 1.0, ìƒí•œì„  ì—†ìŒ)
        # CVê°€ 1.0ì¼ ë•Œ 1.0ë°°, CVê°€ 2.0ì¼ ë•Œ 2.0ë°°, CVê°€ 5.0ì¼ ë•Œ 5.0ë°°ê¹Œì§€ ê°€ëŠ¥
        cv_scale = torch.tensor(max(1.0, current_cv), device=mean_bias.device, dtype=mean_bias.dtype)  # ìµœì†Œ 1.0, ìƒí•œì„  ì—†ìŒ
        
        # [ì‚­ì œ] bias_max í´ë¨í•‘ ì œê±° - Biasê°€ Â±1.0ì— ê°‡íˆì§€ ì•Šë„ë¡
        # bias_max = getattr(self.config, "bias_max", 1.0)  # ì‚­ì œ
        # if bias_max > 0:
        #     mean_bias = torch.tanh(mean_bias / bias_max) * bias_max  # ì‚­ì œ
        
        # Biasê°€ Logit(10.0) ìˆ˜ì¤€ê¹Œì§€ ì»¤ì§ˆ ìˆ˜ ìˆë„ë¡ ì œí•œ í•´ì œ
        # mean_biasëŠ” bias_predictor ì¶œë ¥ìœ¼ë¡œ ì´ë¯¸ ì¶©ë¶„íˆ í´ ìˆ˜ ìˆê³ ,
        # cv_scaleì„ ê³±í•˜ë©´ CVê°€ ë†’ì„ ë•Œ -5.0 ~ +5.0 ë²”ìœ„ê¹Œì§€ ê°€ëŠ¥
        return mean_bias * cv_scale  # Biasê°€ ìˆ˜í•™ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ í¬ê¸°ë¥¼ ê°€ì§€ë„ë¡

    @torch._dynamo.disable
    def forward(self, x, hn, top_k=2, jitter_eps=0.01):
        """
        [OSR (Orthogonal Sinkhorn Routing) - Pure Math Version]
        í•™ìŠµí•˜ì§€ ë§ê³ , ìˆ˜í•™ìœ¼ë¡œ íŒ¨ë²„ë¦¬ëŠ” ë°©ì‹.
        """
        batch_size, seq_len, _ = x.shape
        tokens = batch_size * seq_len

        # [EQT] Apply pending P-control updates at step start
        # This ensures state is stable for entire forward + backward + recompute
        if self.training:
            self.step_update()

        # Flatten for processing
        x_flat = x.view(tokens, -1)  # [tokens, hidden]

        if hn is None:
            hn_flat = torch.zeros(
                tokens, self.num_experts * self.router_dim, device=x.device, dtype=x.dtype
            )
        else:
            hn_flat = hn.view(tokens, -1)

        # 1. GRU & Projector (Context Aware)
        # ì–˜ëŠ” ê·¸ëƒ¥ "ìƒí™© íŒŒì•…"ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ë°¸ëŸ°ì‹±ì€ ìˆ˜í•™ì´ í•©ë‹ˆë‹¤.
        routing_output_flat = self.load_balancer(x_flat, hn_flat)  # [tokens, E*R]
        hn_next = routing_output_flat.view_as(hn_flat)
        routing_output = routing_output_flat.view(batch_size, seq_len, self.num_experts, self.router_dim)

        # Expression projection
        proj = self.expression_projector(x_flat)  # [tokens, E*R]
        proj = proj.view(batch_size, seq_len, self.num_experts, self.router_dim)

        # 2. Normalize & Similarity
        routing_vec = F.normalize(routing_output, p=2, dim=-1)  # [B, S, E, R]
        expression_vec = F.normalize(proj, p=2, dim=-1)        # [B, S, E, R]
        
        # Cosine Similarity [B, S, E]
        similarity = (routing_vec * expression_vec).sum(dim=-1)  # [B, S, E]
        domain_orthogonality = similarity  # Alias for compatibility

        # 3. [OSR] Repulsive Cost Calculation
        # ì „ë¬¸ê°€ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ëŒ€í‘œ ë²¡í„° ì¶”ì¶œ
        # ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´ í˜„ì¬ ë°°ì¹˜ì˜ expression_vec í‰ê· ì„ ì‚¬ìš© (Dynamic Orthogonality)
        # [Experts, Dim]
        current_expert_repr = expression_vec.mean(dim=(0, 1))  # [E, R]
        current_expert_repr = F.normalize(current_expert_repr, p=2, dim=-1)
        
        # Expert ê°„ similarity matrix ê³„ì‚° (pairwise_expert_similarityìš©)
        expert_sim_matrix = torch.matmul(current_expert_repr, current_expert_repr.t())  # [E, E]
        
        flat_sim = similarity.view(-1, self.num_experts)  # [B*S, E]
        
        # Cost ê³„ì‚° (ì²™ë ¥ í¬í•¨)
        cost_matrix = self.compute_osr_cost(flat_sim, current_expert_repr)  # [B*S, E]
        
        # Expert similarity matrix ì €ì¥ (callbackì—ì„œ ì‚¬ìš©)
        if self.training:
            with torch.no_grad():
                self.last_expert_sim_matrix = expert_sim_matrix.detach()

        # [NEW] GRU-based Bias Injection
        # GRU state (hn_next)ë¥¼ ì‚¬ìš©í•˜ì—¬ Expert Biasë¥¼ ì˜ˆì¸¡í•˜ê³  Costì— ë°˜ì˜
        if self.training:
             expert_bias = self.predict_expert_bias_from_gru(hn_next) # [Experts]
             # CostëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ. Biasê°€ ë†’ìœ¼ë©´(ì„ í˜¸ë˜ë©´) Costë¥¼ ë‚®ì¶°ì•¼ í•¨.
             # cost = -sim + repulsion
             # new_cost = -sim - bias + repulsion
             cost_matrix = cost_matrix - expert_bias.unsqueeze(0)
        
        # 4. Sinkhorn (Pass-through Gradient) with Dynamic Epsilon
        # Dynamic Epsilon based on CV (ASR Logic)
        if self.training and self.adaptive_loss_scaling:
            # CVê°€ ë†’ìœ¼ë©´ epsilonì„ í‚¤ì›Œì„œ(Temperature Up) ë” Flatí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
            # Base: 0.1, CV=1.0 -> 0.1 * (1 + 1.0) = 0.2
            # ì²™ë ¥ì´ ê°•í•˜ë©´ ì´ë¯¸ Flatí•´ì§€ë¯€ë¡œ, epsilon ì¦ê°€í­ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
            cv_val = self.cv_ema.item()
            epsilon_factor = 1.0 + min(cv_val, 5.0)  # Max factor 6x
            dynamic_epsilon = self.sinkhorn_epsilon * epsilon_factor
        else:
            dynamic_epsilon = self.sinkhorn_epsilon

        
        # Pure Math Sinkhorn - í•™ìŠµ íŒŒë¼ë¯¸í„° 0ê°œ
        Q_flat = self.sinkhorn_algorithm(
            cost_matrix,
            epsilon=dynamic_epsilon,
            iterations=self.sinkhorn_iterations
        )
        
        routing_probs_full = Q_flat.view(batch_size, seq_len, self.num_experts)

        # [EQT] Store routing probs for P-control backward hook
        # CONSTRAINT: Detach to avoid modifying computation graph
        if self.training:
            self.last_routing_probs_full = routing_probs_full.detach()

        # 5. Sparse Selection (Top-k) with optional Expert-Choice quota
        quota_cap = None
        quota_fallback_frac = None
        if self.training and self.expert_choice_routing:
            multiplier, selected_experts, quota_cap, fallback_mask = self._expert_choice_topk_selection(
                routing_probs_full=routing_probs_full,
                top_k=top_k,
                capacity_factor=self.expert_choice_capacity_factor,
            )
            quota_fallback_frac = float(fallback_mask.float().mean().item()) if fallback_mask.numel() > 0 else 0.0
        else:
            top_k_probs, selected_experts = torch.topk(routing_probs_full, min(int(top_k), self.num_experts), dim=-1)
            top_k_probs = top_k_probs.clamp_min(0.0)
            multiplier = top_k_probs / (top_k_probs.sum(dim=-1, keepdim=True) + 1e-8)

        # Expose last routing for monitoring callback (so it never falls back to argmax(routing_probs_full))
        if self.training:
            with torch.no_grad():
                tok = batch_size * seq_len
                k = selected_experts.size(-1)
                self.last_selected_experts = selected_experts.reshape(tok, k).detach()
                self.last_routing_weights = multiplier.reshape(tok, k).detach()
                self.last_num_experts = int(self.num_experts)
                # Quota stats (for debugging/plots)
                self.last_quota_cap = int(quota_cap) if quota_cap is not None else None
                self.last_quota_fallback_frac = float(quota_fallback_frac) if quota_fallback_frac is not None else 0.0
                self.last_expert_choice_enabled = bool(self.training and self.expert_choice_routing)
                # Also log the ingredients so cap is interpretable downstream
                self.last_quota_tokens = int(tok)
                self.last_quota_top_k = int(k)
                self.last_quota_num_experts = int(self.num_experts)
                self.last_quota_capacity_factor = float(self.expert_choice_capacity_factor)

        # 6. Loss ê³„ì‚°
        zero = torch.tensor(0.0, device=x.device, dtype=x.dtype, requires_grad=True)
        speciality_loss = zero
        contrastive_loss = zero
        expression_reg_loss = zero
        routing_uncertainty = zero
        load_balancing_loss = zero
        sinkhorn_loss = zero
        balance_loss = zero  # Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ lossë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        
        # Ortho Loss (Expression Projectorì˜ ì§êµì„±)
        # 1. Projector ìì²´ì˜ ì§êµì„±
        ortho_loss = self.expression_projector.orthogonal_loss()
        
        # 2. [NEW] Enhanced Soft Orthogonality (Representation-level)
        # "ì „ë¬¸ê°€ë“¤ë¼ë¦¬ ë„ˆë¬´ ë‹®ì§€ ë§ˆë¼" (ìŒìˆ˜ ë°œì‚° ë°©ì§€ í¬í•¨)
        if self.training:
            warmup_progress = min(1.0, float(self.training_step) / max(self.so_warmup_steps, 1))
            so_lambda = self.so_lambda_max * warmup_progress
            
            if so_lambda > 0 and self.srip_enabled:
                 # Apply to the computed expert representations (averaged over batch)
                 so_loss = enhanced_soft_orthogonality_loss(
                     current_expert_repr.unsqueeze(0), # Add batch dim for func logic
                     lambda_so=so_lambda,
                     use_srip=self.srip_enabled,
                 )
                 ortho_loss = ortho_loss + so_loss
        
        # [Sharpening] Entropy Minimization: "í•œ ë†ˆë§Œ íŒ¨ë¼" (í™•ì‹¤í•œ ì „ë¬¸ê°€ ì„ íƒ)
        # routing_probs_fullì€ ì´ë¯¸ Sinkhornì„ ê±°ì¹œ í™•ë¥ ì…ë‹ˆë‹¤.
        if self.training:
            # Entropy ê³„ì‚°: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (Sharp = í™•ì‹¤í•œ ì„ íƒ)
            probs = routing_probs_full + 1e-8  # Numerical stability
            entropy = -(probs * torch.log(probs)).sum(dim=-1).mean()  # [B, S, E] -> scalar
            entropy_loss = entropy
        else:
            entropy_loss = zero

        # 7. Update CV EMA (Thermostat Feedback Loop)
        if self.training:
            with torch.no_grad():
                # ë°°ì¹˜ì˜ Expert ì‚¬ìš©ëŸ‰ ê³„ì‚° (Soft Probabilities í•©)
                # routing_probs_full: [Batch, Seq, Experts]
                expert_load = routing_probs_full.sum(dim=(0, 1))  # [Experts]
                
                # Normalize
                total_load = expert_load.sum()
                if total_load > 0:
                    expert_dist = expert_load / total_load
                else:
                    expert_dist = torch.ones_like(expert_load) / self.num_experts
                
                # CV Calculation: std / mean
                # mean is 1/E
                # std = sqrt(mean((x - mean)^2))
                # CV = sqrt(E * sum((p - 1/E)^2))  (simplification for prob distribution)
                # Actual definition: sigma / mu. mu = 1/E.
                # sigma = sqrt(mean(p^2) - mean(p)^2) = sqrt(mean(p^2) - (1/E)^2)
                # CV = sqrt(mean(p^2) - (1/E)^2) / (1/E)
                #    = sqrt(E * sum(p^2) - 1)
                
                # Using standard definition on the counts directly might be more numerically stable if counts are large?
                # Using probabilities is fine.
                
                # Variance of probabilities p_i
                var_p = expert_dist.var(unbiased=False)
                mean_p = expert_dist.mean()
                current_cv = (var_p.sqrt() / (mean_p + 1e-6))
                
                # Update EMA
                self.cv_ema.mul_(self.cv_ema_alpha).add_(current_cv * (1.0 - self.cv_ema_alpha))
                
                # Update Expert Load EMA for Bias Predictor
                self.expert_load_ema.mul_(self.ema_alpha).add_(expert_dist * (1.0 - self.ema_alpha))

        return (
            multiplier,
            selected_experts,
            None,  # expression_logits
            hn_next,
            speciality_loss,
            domain_orthogonality,
            contrastive_loss,
            routing_probs_full,
            expression_reg_loss,
            routing_uncertainty,
            entropy_loss,
            load_balancing_loss,
            sinkhorn_loss,
            ortho_loss,
            balance_loss,
        )


iterations = 0

class SwitchRouterAdapter(nn.Module):
    """
    Adapter to make SwitchRouter compatible with SPECTRAMoE's expected 10-tuple return format.
    
    Wraps SwitchRouter and converts its output to match SPECTRARouter's signature.
    """
    def __init__(self, config: SPECTRATextConfig, top_k: int = 1):
        super().__init__()
        # Lazy import to avoid circular dependency
        from .standard_moe_upcycle import SwitchRouter
        
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.top_k = top_k
        self.router_dim = getattr(config, "router_dim", 128)
        
        # Create SwitchRouter instance
        self.switch_router = SwitchRouter(
            hidden_size=self.hidden_size,
            num_experts=self.num_experts,
            load_balance_loss_coef=getattr(config, "balance_loss_coef", 0.01),
        )
        
        # Register buffer for hidden state (not used but needed for compatibility)
        self.register_buffer("dummy_hn", torch.zeros(1, self.num_experts * self.router_dim))
        
        # Mark as router for compatibility with monitoring callbacks
        setattr(self, "_is_spectra_router", True)
    
    def forward(self, hidden_states: torch.Tensor, global_routing_logits: Optional[torch.Tensor] = None, top_k: Optional[int] = None, jitter_eps: float = 0.01, layer_idx: int = 0):
        """
        Forward pass compatible with SPECTRARouter signature.
        """
        if top_k is None:
            top_k = self.top_k
        
        # Create deterministic generator for this layer/step to ensure recompute consistency
        generator = None
        if self.training and jitter_eps > 0:
            step = getattr(self, "_step", 0)
            seed = (int(step) * 1000) + int(layer_idx)
            generator = torch.Generator(device=hidden_states.device).manual_seed(seed)
        
        # Call SwitchRouter
        switch_output = self.switch_router.forward(
            hidden_states,
            hn=None,
            top_k=top_k,
            jitter_eps=jitter_eps,
            training=self.training,
            generator=generator
        )
        
        multiplier, selected_experts, router_logits, hn, load_balance_loss, router_scores, _ = switch_output
        
        # Create dummy hn_next
        batch_size, seq_len = hidden_states.shape[:2]
        batch_seq_len = batch_size * seq_len
        hn_next = torch.zeros(
            batch_seq_len, 
            self.num_experts * self.router_dim,
            device=hidden_states.device,
            dtype=hidden_states.dtype
        )
        
        zero = torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
        routing_probs_full = router_scores
        
        def _to_scalar(t):
            if t is None:
                return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
            if not torch.is_tensor(t):
                return torch.tensor(float(t), device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
            if t.numel() == 0:
                return torch.tensor(0.0, device=hidden_states.device, dtype=hidden_states.dtype, requires_grad=True)
            if t.dim() > 0:
                return t.mean()
            return t

        # Return 10-tuple matching SPECTRARouter format
        return (
            multiplier,
            selected_experts,
            hn_next,
            routing_probs_full,
            _to_scalar(load_balance_loss),
            _to_scalar(zero),
            _to_scalar(zero),
            _to_scalar(zero),
            _to_scalar(zero),
            _to_scalar(zero),
        )


class SPECTRAMoE(nn.Module):

    """Hybrid Router: í•˜ë‚˜ì˜ linear layerì—ì„œ sigmoidë¡œ expert ì„ íƒ, sparsemixerë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""

    def __init__(self, config, global_router, **kwargs):
        super().__init__()
        config = config
        self.hidden_dim = config.hidden_size
        self.ffn_dim = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.top_k = config.num_experts_per_tok
        self.router_dim = config.router_dim
        global iterations
        iterations += 1
        self.iter = iterations
        self.router = global_router
        # self.router = nn.Linear(config.hidden_size, config.n_routed_experts, bias=False)
        self.experts = nn.ModuleList([SPECTRAMLP(config) for _ in range(self.num_experts)])
        self.shared_experts = SPECTRAMLP(config=config, intermediate_size=config.intermediate_size * config.n_shared_experts)

        self.router_jitter_noise = getattr(config, 'router_jitter_noise', 0.01)
        self.input_jitter_noise = getattr(config, 'input_jitter_noise', 0.0)   

        setattr(self.router, "_is_spectra_router", True)
        setattr(self.router.expression_projector, "_is_spectra_expression_projector", True)

        # Adaptive filter parameters for load balancing
        
        # Enhanced Expert Utilization
        self.register_buffer("expert_specialization_ema", torch.zeros(self.num_experts, self.hidden_dim), persistent=True)
        self.register_buffer("expert_strength_ema", torch.zeros(self.num_experts), persistent=True) # New: Expert strength tracking
        self.routing_temperature = nn.Parameter(torch.ones(1))
        self.specialization_strength = getattr(config, "specialization_strength", 0.01)
        
        # Routing parameters
        self.enable_uncertainty_broadcast = getattr(config, "enable_uncertainty_broadcast", True)
        self.uncertainty_threshold = getattr(config, "uncertainty_threshold", 0.7)
        
        # shared_experts freeze ì—¬ë¶€ (ê¸°ë³¸ê°’ì€ Trueë¡œ ì„¤ì •)
        self.freeze_shared_experts = getattr(config, 'freeze_shared_experts', True)
        if self.freeze_shared_experts:
            self._freeze_shared_experts()
    
        # Orthogonal projectorëŠ” ì´ì œ global routerì—ì„œ ì²˜ë¦¬ë¨
        self.ortho_strength = getattr(config, 'ortho_strength', 1.0)

    
    def _freeze_shared_experts(self):
        """shared_expertsì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ freeze"""
        for param in self.shared_experts.parameters():
            param.requires_grad = False
        logger.debug(f"Shared experts frozen for layer {self.iter}")
    
    def _unfreeze_shared_experts(self):
        """shared_expertsì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ unfreeze"""
        for param in self.shared_experts.parameters():
            param.requires_grad = True
        logger.debug(f"Shared experts unfrozen for layer {self.iter}")
    
    def freeze_shared_experts_manual(self):
        """ìˆ˜ë™ìœ¼ë¡œ shared_experts freeze"""
        self._freeze_shared_experts()
        self.freeze_shared_experts = True
    
    def unfreeze_shared_experts_manual(self):
        """ìˆ˜ë™ìœ¼ë¡œ shared_experts unfreeze"""
        self._unfreeze_shared_experts()
        self.freeze_shared_experts = False

    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def forward(self, hidden_states: torch.Tensor, global_routing_logits: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        residual = hidden_states
        final_hidden_states, routing_info = self._sparse_routing(hidden_states, global_routing_logits)
        router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss = routing_info
        with torch.no_grad():
            # print( f'residual in rank {torch.distributed.get_rank()}', residual.shape, residual.dtype)
            pretriained_residual = self.shared_experts(residual)
        
        # final_hidden_statesë¥¼ pretrained_residualì˜ í¬ê¸°ì— ë§ì¶° normalize
        # pretrained_residualì˜ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ final_hidden_statesë¥¼ scale
        pretrained_norm = torch.norm(pretriained_residual, dim=-1, keepdim=True)
        final_norm = torch.norm(final_hidden_states, dim=-1, keepdim=True)
        # ì•ˆì „ì¥ì¹˜: 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        scale_factor = torch.where(
            final_norm > 1e-8,
            pretrained_norm / (final_norm + 1e-8),
            torch.ones_like(pretrained_norm)
        )
        final_hidden_states_normalized = final_hidden_states * scale_factor
        final_hidden_states = final_hidden_states_normalized + pretriained_residual * 1.0
        if self.training:
            final_hidden_states = final_hidden_states.requires_grad_(True)
            if router_logits is not None:
                router_logits = router_logits.requires_grad_(True)
            # Lossì˜ gradientë„ ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            if speciality_loss is not None and torch.is_tensor(speciality_loss):
                speciality_loss = speciality_loss.requires_grad_(True)
            if cosine_similarities is not None and torch.is_tensor(cosine_similarities):
                cosine_similarities = cosine_similarities.requires_grad_(True)
            if contrastive_loss is not None and torch.is_tensor(contrastive_loss):
                contrastive_loss = contrastive_loss.requires_grad_(True)
            if expression_reg_loss is not None and torch.is_tensor(expression_reg_loss):
                expression_reg_loss = expression_reg_loss.requires_grad_(True)
            if routing_uncertainty is not None and torch.is_tensor(routing_uncertainty):
                routing_uncertainty = routing_uncertainty.requires_grad_(True)
            if entropy_loss is not None and torch.is_tensor(entropy_loss):
                entropy_loss = entropy_loss.requires_grad_(True)
            if load_balancing_loss is not None and torch.is_tensor(load_balancing_loss):
                load_balancing_loss = load_balancing_loss.requires_grad_(True)
            if sinkhorn_loss is not None and torch.is_tensor(sinkhorn_loss):
                sinkhorn_loss = sinkhorn_loss.requires_grad_(True)
            if ortho_loss is not None and torch.is_tensor(ortho_loss):
                ortho_loss = ortho_loss.requires_grad_(True)
            if balance_loss is not None and torch.is_tensor(balance_loss):
                balance_loss = balance_loss.requires_grad_(True)
        return final_hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss)    
    
    def compute_pairwise_expert_similarity(self, expert_outputs: torch.Tensor, expert_mask: torch.Tensor) -> torch.Tensor:
        """
        Compute pairwise cosine similarity between expert outputs for the same inputs.
        Ideally, we want experts to be orthogonal (diverse).
        Returns scalar average similarity.
        """
        # This is tricky because experts process different tokens.
        # We can only compare experts if they processed the same tokens (or we force them to).
        # Alternatively, we can compare their weights, but here we want output similarity.
        # If we want to enforce orthogonality of EXPERT FUNCTIONS, we can look at their outputs
        # for a shared set of inputs, or their weight matrices.
        # Given the prompt "PES... ê° ì „ë¬¸ê°€ì˜ ì¶œë ¥ì— ëŒ€í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°", it implies output similarity.
        # However, usually experts are sparse.
        # If we are in a setting where we can capture outputs for all experts on some tokens (e.g. broadcasting),
        # we can use that.
        # Or we can use the 'expert_specialization_ema' which tracks average input/output patterns.
        
        # Let's implementation based on expert_specialization_ema (input patterns) or check if we have outputs.
        # The plan says "expert_outputs" as input.
        # But _sparse_routing only computes selected experts.
        
        # If we look at the plan: "3.1 PES... ê° ì „ë¬¸ê°€ì˜ ì¶œë ¥ì— ëŒ€í•´... ì „ë¬¸ê°€ ìŒ ê°„ì˜ í‰ê·  ìœ ì‚¬ë„ ë°˜í™˜"
        # This might be best computed on the 'expert_specialization_ema' or similar aggregate.
        # Or maybe it refers to the 'cosine_similarities' we already compute?
        # But 'cosine_similarities' in current code is between expression and routing logits.
        
        # Let's assume we calculate it based on the 'expert_specialization_ema' which represents the 
        # "specialization direction" of each expert.
        
        if self.expert_specialization_ema is not None:
             # Normalize
            normalized_specs = F.normalize(self.expert_specialization_ema, dim=-1)
            # Similarity matrix
            sim_matrix = torch.matmul(normalized_specs, normalized_specs.t())
            # We want to minimize off-diagonal elements
            mask = torch.eye(self.num_experts, device=sim_matrix.device).bool()
            off_diagonal = sim_matrix[~mask]
            return off_diagonal.mean()
            
        return torch.tensor(0.0, device=self.router.load_balancer.weight_hh_l0.device)

    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def _sparse_routing(self, hidden_states: torch.Tensor, global_routing_logits: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        batch_size, sequence_length, hidden_dim = hidden_states.shape
        if self.training and self.input_jitter_noise > 0:
            # Inplace ì—°ì‚° ëŒ€ì‹  ìƒˆë¡œìš´ í…ì„œ ìƒì„± (gradient checkpointing í˜¸í™˜)
            jitter = torch.empty_like(hidden_states).uniform_(1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise)
            hidden_states = hidden_states * jitter
        
        # Global routerì—ì„œ ì „ì²´ ë¼ìš°íŒ… ì²˜ë¦¬ (GRU + expression projection + sparsemixer)
        router_output = self.router(
            hidden_states, 
            global_routing_logits,
            top_k=self.top_k,
            jitter_eps=self.router_jitter_noise
        )
        # Unpack including full probabilities, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss
        routing_weights, selected_experts, expression_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, routing_probs_full, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss = router_output

        # multiplierì™€ selected_expertsëŠ” ì´ë¯¸ global routerì—ì„œ sparsemixerë¥¼ í†µí•´ ê³„ì‚°ë¨
        assert routing_weights.isnan().sum() == 0, f"{self.iter} layer routing_weights is nan Line: 826"

        # Flatten routing outputs for per-token processing
        routing_weights = routing_weights.view(batch_size * sequence_length, -1)  # [tokens, top_k]
        selected_experts = selected_experts.view(batch_size * sequence_length, -1)  # [tokens, top_k]

        final_hidden_states = torch.zeros(
            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device
        )

        # Uncertainty-based broadcasting for uncertain tokens (training only)
        if self.training and self.enable_uncertainty_broadcast and routing_uncertainty is not None:
            # Reshape uncertainty to match token positions: [batch*seq_len]
            uncertainty_flat = routing_uncertainty.view(-1) if routing_uncertainty.dim() > 1 else routing_uncertainty
            # Identify uncertain tokens (high entropy = low confidence)
            uncertain_mask = uncertainty_flat > self.uncertainty_threshold
            # For uncertain tokens, broadcast to all experts
            if uncertain_mask.any():
                one_hot = torch.nn.functional.one_hot(
                    selected_experts, num_classes=self.num_experts
                )  # [tokens, top_k, E]
                expert_mask = one_hot.bool()
                uncertain_token_mask = uncertain_mask.view(-1, 1).expand(-1, self.top_k)  # [tokens, top_k]
                expert_mask[uncertain_token_mask] = True
            else:
                expert_mask = torch.nn.functional.one_hot(
                    selected_experts, num_classes=self.num_experts
                ).bool()
        else:
            # Standard routing: One hot encode the selected experts to create an expert mask
            # this will be used to easily index which expert is going to be sollicitated
            expert_mask = torch.nn.functional.one_hot(
                selected_experts, num_classes=self.num_experts
            ).bool()

        # Reorder to [E, tokens, top_k] for per-expert iteration
        expert_mask = expert_mask.permute(2, 0, 1)

        # Loop over all available experts in the model and perform the computation on each expert
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            token_idx, topk_idx = torch.where(expert_mask[expert_idx])

            # Use torch.where to handle empty tensor case in a compile-friendly way
            has_tokens = token_idx.numel() > 0
            if has_tokens:
                # in torch it is faster to index using lists than torch tensors
                top_x_list = token_idx.tolist()
                idx_list = topk_idx.tolist()

                # Index the correct hidden states and compute the expert hidden state for
                # the current expert. We need to make sure to multiply the output hidden
                # states by `routing_weights` on the corresponding tokens (top-1 and top-2)
                hidden_states_flat = hidden_states.view(batch_size * sequence_length, hidden_dim)
                current_state = hidden_states_flat[top_x_list]
                current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list].unsqueeze(-1)

                # However `index_add_` only support torch tensors for indexing so we'll use
                # the `top_x` tensor here.
                final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(hidden_states.dtype))
                
                # --- Update Specialization EMA ---
                if self.training:
                    with torch.no_grad():
                        # Flatten hidden_states to 2D for proper indexing
                        hidden_states_flat = hidden_states.view(-1, hidden_states.size(-1))
                        current_mean_hidden = hidden_states_flat[top_x_list].mean(dim=0)
                        self.expert_specialization_ema[expert_idx].mul_(self.router.ema_alpha).add_(current_mean_hidden, alpha=1.0 - self.router.ema_alpha)
                # --- End Update Specialization EMA ---

        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)
        
        # ì½œë°±ì„ ìœ„í•´ ë¼ìš°íŒ… ì •ë³´ë¥¼ ëª¨ë“ˆì— ì €ì¥
        if self.training:
            with torch.no_grad():
                self.last_selected_experts = selected_experts.detach()
                self.last_routing_weights = routing_weights.detach()
                self.last_num_experts = self.num_experts
        
        return final_hidden_states, (routing_probs_full, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss)


class SPECTRARMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6, **kwargs):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.zeros(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        # Llama does x.to(float16) * w whilst SPECTRA is (x * w).to(float16)
        # See https://github.com/huggingface/transformers/pull/29402
        output = output * (1.0 + self.weight.float())
        return output.type_as(x)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.eps}"


class SPECTRARotaryEmbedding(nn.Module):
    def __init__(self, config: SPECTRATextConfig, device=None, **kwargs):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    softcap: Optional[float] = None,
    **kwargs,
) -> Tuple[torch.Tensor, torch.Tensor]:
    if scaling is None:
        scaling = module.head_dim**-0.5

    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

    if softcap is not None:
        attn_weights = attn_weights / softcap
        attn_weights = torch.tanh(attn_weights)
        attn_weights = attn_weights * softcap
    if attention_mask is not None:  # no matter the length, we just slice it
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    # upcast attention to fp32
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.bfloat16).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output, attn_weights


class SPECTRAAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: SPECTRATextConfig, layer_idx: int, **kwargs):
        super().__init__()
        self.is_sliding = bool((layer_idx + 1) % config.sliding_window_pattern)
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = config.query_pre_attn_scalar**-0.5
        self.attention_dropout = self.config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.attn_logit_softcapping = self.config.attn_logit_softcapping
        self.sliding_window = config.sliding_window if self.is_sliding else None

        self.q_norm = SPECTRARMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)
        self.k_norm = SPECTRARMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False, # Added default value
        use_cache: bool = False, # Added default value
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)
        
        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states   = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        
        query_states = self.q_norm(query_states)
        key_states   = self.k_norm(key_states)

        cos, sin = None, None
        if position_embeddings is not None:
            cos, sin = position_embeddings
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
        # else: NoPE, ê·¸ëŒ€ë¡œ ì‚¬ìš©

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {
                "sin": sin,
                "cos": cos,
                "cache_position": cache_position,
                "sliding_window": self.sliding_window,
            }
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
            
            # Here we need to slice as we use a static cache by default, but FA2 does not support it
            # if attention_mask is not None and self.config.attn_implementation == "flash_attention_2":
            #     if hasattr(past_key_value, "get_seq_length"):
            #         seq_len = past_key_value.get_seq_length()
            #     else:
            #         seq_len = key_states.shape[-1]
            #     key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]
        
        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=self.attention_dropout if self.training else 0.0,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            output_attentions=output_attentions,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights, past_key_value


class SPECTRADecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: SPECTRATextConfig, layer_idx: int, global_router: SPECTRARouter, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx
        self.attention_type = config.layer_types[layer_idx]
        self.self_attn = SPECTRAAttention(config=config, layer_idx=layer_idx, **kwargs)
        self.mlp = SPECTRAMLP(config=config) # this layer is for loading pretrained base SPECTRA model weights
        self.is_dense_replacement = layer_idx >= config.first_k_dense_replace
        if self.is_dense_replacement:
            self.moe = SPECTRAMoE(config=config, global_router=global_router)
            # self.moe = SPECTRASparseGRINBlock(config=config)
        else:
            self.moe = SPECTRAMLP(config=config)
        self.input_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.pre_feedforward_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_feedforward_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.is_sliding = self.self_attn.is_sliding
        self.sliding_window = config.sliding_window
        self.use_nope = (hasattr(config, 'no_rope_layers') and bool(config.no_rope_layers[self.layer_idx]))

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings_global: torch.Tensor,
        position_embeddings_local: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        global_routing_hn: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:

        residual = hidden_states
        
        hidden_states = self.input_layernorm(hidden_states)

        # í•˜ì´ë¸Œë¦¬ë“œ rope-nope positional embedding ì ìš©
        if self.use_nope:
            # NoPE: position embedding ì—†ì´ self-attn
            position_embeddings = None
        else:
            # ê¸°ì¡´ ë°©ì‹: RoPE
            position_embeddings = position_embeddings_local if self.self_attn.is_sliding else position_embeddings_global
        hidden_states, self_attn_weights, past_key_value = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = residual + hidden_states
        
        residual = hidden_states
        hidden_states = self.pre_feedforward_layernorm(hidden_states)
        if self.layer_idx >= self.config.first_k_dense_replace:
            hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss) = self.moe(hidden_states, global_routing_hn)
        else:
            with torch.no_grad():
                hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss) = self.moe(hidden_states), (None,)*12
        hidden_states = self.post_feedforward_layernorm(hidden_states)
        if self.training:
            hidden_states = hidden_states.requires_grad_(True)
            if router_logits is not None:
                router_logits = router_logits.requires_grad_(True)
            # Lossì˜ gradientë„ ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            if speciality_loss is not None and torch.is_tensor(speciality_loss):
                speciality_loss = speciality_loss.requires_grad_(True)
            if cosine_similarities is not None and torch.is_tensor(cosine_similarities):
                cosine_similarities = cosine_similarities.requires_grad_(True)
            if contrastive_loss is not None and torch.is_tensor(contrastive_loss):
                contrastive_loss = contrastive_loss.requires_grad_(True)
            if expression_reg_loss is not None and torch.is_tensor(expression_reg_loss):
                expression_reg_loss = expression_reg_loss.requires_grad_(True)
            if routing_uncertainty is not None and torch.is_tensor(routing_uncertainty):
                routing_uncertainty = routing_uncertainty.requires_grad_(True)
            if entropy_loss is not None and torch.is_tensor(entropy_loss):
                entropy_loss = entropy_loss.requires_grad_(True)
            if load_balancing_loss is not None and torch.is_tensor(load_balancing_loss):
                load_balancing_loss = load_balancing_loss.requires_grad_(True)
            if sinkhorn_loss is not None and torch.is_tensor(sinkhorn_loss):
                sinkhorn_loss = sinkhorn_loss.requires_grad_(True)
            if ortho_loss is not None and torch.is_tensor(ortho_loss):
                ortho_loss = ortho_loss.requires_grad_(True)
            if balance_loss is not None and torch.is_tensor(balance_loss):
                balance_loss = balance_loss.requires_grad_(True)
        hidden_states = residual + hidden_states
        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)
        outputs += ((router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss),)
        return outputs


spectra_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`SPECTRAConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""

@auto_docstring
class SPECTRAPreTrainedModel(PreTrainedModel):
    config: SPECTRAConfig
    base_model_prefix = ""
    supports_gradient_checkpointing = True
    _no_split_modules = [
        "SPECTRADecoderLayer",
        "SiglipVisionEmbeddings",
        "SiglipEncoderLayer",
        "SiglipMultiheadAttentionPoolingHead",
    ]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True
    
    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": SPECTRADecoderLayer,
        "attentions": SPECTRAAttention
    }

    def _initialize_moe_router_and_temperature(self) -> None:
        """Initialize only MoE router weights and routing temperature if they were not loaded from a checkpoint.

        - Router linear weights: Xavier uniform for stable logits
        - Routing temperature: ones (softplus(1) ~ 1.313) keeps scale reasonable
        """
        with torch.no_grad():
            for module in self.modules():
                # Initialize router linears ONLY if not already initialized/loaded
                router = getattr(module, "router", None)
                if isinstance(router, nn.Linear):
                    already_init = getattr(router, "_is_hf_initialized", False)
                    if not already_init:
                        try:
                            if router.weight.dim() >= 2:
                                nn.init.xavier_uniform_(router.weight)
                        except Exception as e:
                            logging.get_logger('transformers').warning(f"Failed router xavier init: {e}")
                        if router.bias is not None:
                            router.bias.zero_()
                # Initialize routing temperature ONLY if looks uninitialized/bad
                routing_temp = getattr(module, "routing_temperature", None)
                if isinstance(routing_temp, nn.Parameter):
                    if not torch.isfinite(routing_temp).all() or routing_temp.abs().sum() == 0:
                        routing_temp.data.fill_(1.0)

    def _init_weights(self, module):
        # important: this ported version of Gemma2 isn't meant for training from scratch - only
        # inference and fine-tuning - so the proper init weights code has been removed

        # Only initialize router linears explicitly; skip expert MLPs and other dense layers during fine-tuning
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):
            if getattr(module, "_is_spectra_router", False):
                logging.get_logger('transformers').debug(f"Initializing router layer with Xavier uniform: {module}")
                try:
                    if module.weight.dim() >= 2:
                        nn.init.xavier_uniform_(module.weight)
                except Exception as e:
                    logging.get_logger('transformers').warning(f"Failed router xavier init in _init_weights: {e}")
                if module.bias is not None:
                    module.bias.data.zero_()
            else:
                # Do not touch non-router linears here to avoid re-initializing experts or base MLPs
                pass
        elif isinstance(module, ExpressionProjector):
            logging.get_logger('transformers').debug(f"Initializing expression projector layer with Xavier uniform: {module}")
            original_dtype = module.projection.weight.dtype
            module.to(torch.float32)
            try:
                if module.projection.weight.dim() >= 2:
                    nn.init.orthogonal_(module.projection.weight)
            except Exception as e:
                logging.get_logger('transformers').warning(f"Failed projector orthogonal init in _init_weights: {e}")
            module.to(original_dtype)
        elif isinstance(module, SPECTRAMultiModalProjector):
            try:
                if module.mm_input_projection_weight.dim() >= 2:
                    nn.init.xavier_uniform_(module.mm_input_projection_weight)
            except Exception as e:
                logging.get_logger('transformers').warning(f"Failed multimodal projector init: {e}")
        elif isinstance(module, nn.GRU):
            for name, param in module.named_parameters():
                if param.dim() >= 2:
                    try:
                        if 'weight_ih' in name:
                            nn.init.xavier_uniform_(param.data)
                        elif 'weight_hh' in name:
                            nn.init.xavier_uniform_(param.data)
                        elif 'weight_hr' in name:
                            nn.init.xavier_uniform_(param.data)
                    except Exception as e:
                        logging.get_logger('transformers').warning(f"Failed GRU param init: {e}")
                if 'bias' in name:
                    param.data.fill_(0)
        else:
            super()._init_weights(module)

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: Type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        ffn_checkpoint_for_moe_conversion: Optional[Union[str, os.PathLike, dict]] = None,
        **kwargs,
    ) -> SpecificPreTrainedModelType:

        # config ì²˜ë¦¬ (G2MoEConfig ì¸ìŠ¤í„´ìŠ¤ í™•ë³´)
        if config is None:
            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs.get("config_kwargs", {}))
        if not isinstance(config, SPECTRAConfig):
            config = SPECTRAConfig(**config.to_dict())
            if config.text_config.attn_implementation == None:
                if is_flash_attn_2_available():
                    config.text_config.attn_implementation = "flash_attention_2"
                elif is_torch_flex_attn_available():
                    config.text_config.attn_implementation = "flex_attention"
                elif cls.training:
                    config.text_config.attn_implementation = "eager"
                else:
                    config.text_config.attn_implementation = "sdpa"
            print(f"Forced attn implementation: {config.text_config.attn_implementation}")

        logging.get_logger('transformers').debug("Loading SPECTRA model skeleton using super().from_pretrained...")
        logging.set_verbosity_error()
        import time
        debug_time = time.time()
        base_model = super().from_pretrained(
            pretrained_model_name_or_path,
            *model_args,
            config=config,
            cache_dir=cache_dir,
            # Critical: only init truly missing keys to avoid full-graph init slowdown
            ignore_mismatched_sizes=True,
            force_download=force_download,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
            use_safetensors=use_safetensors,
            weights_only=weights_only,
            **{k: v for k, v in kwargs.items()}
        )
        print(f"SPECTRA model skeleton loaded in {time.time() - debug_time} seconds")
        logging.set_verbosity_warning()
        logging.get_logger('transformers').debug("SPECTRA model skeleton loaded.")

        if hasattr(base_model, 'model'):
            if hasattr(base_model.model, 'layers') and hasattr(base_model.model.layers, 'moe'):
                logging.get_logger('transformers').debug("SPECTRA Pretrained model loaded.")
                return base_model
            logging.get_logger('transformers').debug("Initializing MoE experts with MLP weights...")
            if hasattr(base_model.model, 'layers'):
                base_model.model = base_model._upcycle(base_model.model)
            elif hasattr(base_model.model.language_model, 'layers'):
                base_model.model.language_model = base_model._upcycle(base_model.model.language_model)
            logging.get_logger('transformers').debug("MoE experts initialization completed.")
        elif hasattr(base_model, "language_model"):
            if hasattr(base_model.language_model, 'layers') and hasattr(base_model.language_model.layers, 'moe'):
                logging.get_logger('transformers').debug("SPECTRA Pretrained model loaded.")
                return base_model
            logging.get_logger('transformers').debug("Initializing MoE experts with MLP weights...")
            base_model.language_model = base_model._upcycle(base_model.language_model)
            logging.get_logger('transformers').debug("MoE experts initialization completed.")
        else:
            logging.get_logger('transformers').info("Model does not have expected structure. MoE experts not initialized from MLP weights.")
        logging.set_verbosity_warning() 
        return base_model

    @torch.no_grad()
    def _upcycle(self, model):
        processing = tqdm(
            enumerate(model.layers),
            total=len(model.layers),
            desc=f"Copying MLP weights to {self.__class__.__name__} MoE experts: start",
            leave=False)

        for layer_idx, decoder_layer in processing:
            if hasattr(decoder_layer.moe, 'experts') or hasattr(decoder_layer.moe, 'shared_experts'):
                if hasattr(decoder_layer.moe, 'shared_experts'):
                    processing.set_description(f"Copying mlp {layer_idx} â†’ shared experts")
                    # ZeRO-3 safety: skip copy if target is empty (partitioned)
                    if decoder_layer.moe.shared_experts.gate_proj.weight.numel() > 0:
                        if decoder_layer.moe.shared_experts.gate_proj.weight.shape == decoder_layer.mlp.gate_proj.weight.shape:
                            decoder_layer.moe.shared_experts.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                            decoder_layer.moe.shared_experts.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                            decoder_layer.moe.shared_experts.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

                for expert_idx, expert in enumerate(decoder_layer.moe.experts):
                    if expert_idx % 2 == 0:
                        processing.set_description(f"Copying mlp {layer_idx} â†’ expert {expert_idx}")
                    # ZeRO-3 safety: skip copy if target is empty (partitioned)
                    if expert.gate_proj.weight.numel() > 0:
                        if expert.gate_proj.weight.shape == decoder_layer.mlp.gate_proj.weight.shape:
                            expert.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                            expert.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                            expert.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

            elif hasattr(decoder_layer.moe, 'gate_proj'):
                processing.set_description(f"Copying mlp {layer_idx} â†’ dense MoE")
                decoder_layer.moe.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                decoder_layer.moe.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                decoder_layer.moe.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)
            else:
                raise Exception("MoE model has no MLP or shared MLP")
            del decoder_layer.mlp
        processing.set_description("Copy finished")
        return model

    def get_parameter_groups(self):
        """
        Returns a list of parameter groups for the optimizer, which allows to apply different
        learning rates to different parts of the model. This is particularly useful for MoE models
        where components like routers and experts can benefit from different learning schedules.
        """
        
        router_params = []
        expert_params = []
        shared_expert_params = []
        attention_params = []
        other_params = []

        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue

            if 'gate.weight' in name or 'router' in name:
                router_params.append(param)
            elif 'shared_experts' in name:
                shared_expert_params.append(param)
            elif 'experts' in name:
                expert_params.append(param)
            elif 'self_attn' in name:
                attention_params.append(param)
            else:
                other_params.append(param)
        
        # In a training script, you can assign different learning rates to these groups.
        # For example:
        # optimizer_grouped_parameters = [
        #     {'params': model.get_parameter_groups()['router'], 'lr': 1e-4},
        #     {'params': model.get_parameter_groups()['expert'], 'lr': 5e-5},
        #     ...
        # ]
        return {
            'router': router_params,
            'expert': expert_params,
            'shared_expert': shared_expert_params,
            'attention': attention_params,
            'other': other_params,
        }

spectra_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            Two formats are allowed:
            - a [`~cache_utils.Cache`] instance, see our
            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
            cache format.

            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
            legacy cache format will be returned.

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""

@add_start_docstrings(
    "The bare SPECTRAText Model outputting raw hidden-states without any specific head on top.",
    spectra_START_DOCSTRING,
)
class SPECTRATextModel(SPECTRAPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`SPECTRATextDecoderLayer`]

    Args:
        config: SPECTRATextConfig
    """
    config: SPECTRATextConfig

    def __init__(self, config: SPECTRATextConfig, **kwargs):
        super().__init__(config)
        # Robustly resolve text config without relying on class identity across module boundaries
        if getattr(config, "model_type", None) == "spectra_text" or not hasattr(config, "text_config"):
            self.config = config
        else:
            self.config = config.text_config
        config = self.config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        # Expose a tensor-parallel plan for vLLM on the base text model
        if not hasattr(self, "_tp_plan") or self._tp_plan is None:
            default_tp_plan = {
                "layers.*.self_attn.q_proj": "colwise",
                "layers.*.self_attn.k_proj": "colwise",
                "layers.*.self_attn.v_proj": "colwise",
                "layers.*.self_attn.o_proj": "rowwise",
                "layers.*.moe.experts.*.gate_proj": "colwise",
                "layers.*.moe.experts.*.up_proj": "colwise",
                "layers.*.moe.experts.*.down_proj": "rowwise",
            }
            # allow overriding via config if provided
            self._tp_plan = getattr(config, "base_model_tp_plan", default_tp_plan)

        # SPECTRA downcasts the below to bfloat16, causing sqrt(3072)=55.4256 to become 55.5. See https://github.com/huggingface/transformers/pull/29402
        self.embed_tokens = SPECTRATextScaledWordEmbedding(
            config.vocab_size, config.hidden_size, self.padding_idx, embed_scale=self.config.hidden_size**0.5
        )
        self.global_router = SPECTRARouter(config)
        self.layers = nn.ModuleList(
            [SPECTRADecoderLayer(config, layer_idx, self.global_router, **kwargs) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = SPECTRARMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = SPECTRARotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        self.router_aux_loss_coef = config.router_aux_loss_coef
        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas
        # when we want to create a local RoPE layer. Config defaults should hold values for global RoPE
        config = copy.deepcopy(config)
        config.rope_theta = config.rope_local_base_freq
        config.rope_scaling = config.rope_scaling if config.rope_scaling is not None else {"rope_type":  "default"}
        self.rotary_emb_local = SPECTRARotaryEmbedding(config=config)
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    @classmethod
    def from_config(cls, **kwargs):
        return cls._from_config(**kwargs)

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> SPECTRACausalLMOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        # NEFTune implementation
        if self.training:
            neftune_noise_alpha = getattr(self.config, "neftune_noise_alpha", 0.0)
            if neftune_noise_alpha > 0.0:
                dims = torch.tensor(inputs_embeds.size(1) * inputs_embeds.size(2), device=inputs_embeds.device)
                mag_norm = neftune_noise_alpha / torch.sqrt(dims)
                inputs_embeds = inputs_embeds + torch.zeros_like(inputs_embeds).uniform_(-mag_norm, mag_norm)


        if use_cache and past_key_values is None and not self.training:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens,
                past_seen_tokens + inputs_embeds.shape[1],
                device=inputs_embeds.device,
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)
        
        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        # embed positions
        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings_global = self.rotary_emb(hidden_states, position_ids)
        position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        all_router_logits = []  # ëª¨ë“  MoE ë ˆì´ì–´ì˜ router_logitsë¥¼ ìˆ˜ì§‘
        global_routing_hn = None
        
        # ê° layerì˜ lossë¥¼ ëˆ„ì í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        all_speciality_losses = []
        all_cosine_similarities = []
        all_contrastive_losses = []

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)
        
            layer_outputs = decoder_layer(
                hidden_states,
                position_embeddings_global=position_embeddings_global,
                position_embeddings_local=position_embeddings_local,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                global_routing_hn=global_routing_hn,
                **flash_attn_kwargs,
            )
            hidden_states = layer_outputs[0]
            routing_result = layer_outputs[-1]
            if routing_result is not None:
                router_logits = routing_result[0]
                if router_logits is not None:
                    all_router_logits.append(router_logits)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                global_routing_hn = routing_result[1]
                
                # ê° layerì˜ ê°’ì„ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (ë®ì–´ì“°ì§€ ì•ŠìŒ)
                layer_speciality_loss = routing_result[2]
                layer_cosine_similarities = routing_result[3]
                layer_contrastive_loss = routing_result[4]
                layer_expression_reg_loss = routing_result[5] if len(routing_result) > 5 else None
                layer_routing_uncertainty = routing_result[6] if len(routing_result) > 6 else None
                layer_entropy_loss = routing_result[7] if len(routing_result) > 7 else None
                layer_load_balancing_loss = routing_result[8] if len(routing_result) > 8 else None
                layer_sinkhorn_loss = routing_result[9] if len(routing_result) > 9 else None
                layer_ortho_loss = routing_result[10] if len(routing_result) > 10 else None
                layer_balance_loss = routing_result[11] if len(routing_result) > 11 else None
                
                if layer_speciality_loss is not None:
                    all_speciality_losses.append(layer_speciality_loss)
                if layer_cosine_similarities is not None:
                    all_cosine_similarities.append(layer_cosine_similarities)
                if layer_contrastive_loss is not None:
                    all_contrastive_losses.append(layer_contrastive_loss)
                if layer_expression_reg_loss is not None:
                    if not hasattr(self, 'all_expression_reg_losses'):
                        self.all_expression_reg_losses = []
                    self.all_expression_reg_losses.append(layer_expression_reg_loss)
                if layer_routing_uncertainty is not None:
                    if not hasattr(self, 'all_routing_uncertainties'):
                        self.all_routing_uncertainties = []
                    self.all_routing_uncertainties.append(layer_routing_uncertainty)
                if layer_entropy_loss is not None:
                    if not hasattr(self, 'all_entropy_losses'):
                        self.all_entropy_losses = []
                    self.all_entropy_losses.append(layer_entropy_loss)
                if layer_load_balancing_loss is not None:
                    if not hasattr(self, 'all_load_balancing_losses'):
                        self.all_load_balancing_losses = []
                    self.all_load_balancing_losses.append(layer_load_balancing_loss)
                if layer_sinkhorn_loss is not None:
                    if not hasattr(self, 'all_sinkhorn_losses'):
                        self.all_sinkhorn_losses = []
                    self.all_sinkhorn_losses.append(layer_sinkhorn_loss)
                if layer_ortho_loss is not None:
                    if not hasattr(self, 'all_ortho_losses'):
                        self.all_ortho_losses = []
                    self.all_ortho_losses.append(layer_ortho_loss)
                if layer_balance_loss is not None:
                    if not hasattr(self, 'all_balance_losses'):
                        self.all_balance_losses = []
                    self.all_balance_losses.append(layer_balance_loss)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        hidden_states = self.norm(hidden_states)

        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        # router_logitsë¥¼ íŠœí”Œë¡œ ë³€í™˜ (ë¹„ì–´ìˆìœ¼ë©´ None)
        router_logits_tuple = tuple(all_router_logits) if all_router_logits else None
        
        # ëª¨ë“  layerì˜ lossë¥¼ ì§‘ê³„ (gradient ìœ ì§€)
        speciality_loss = None
        cosine_similarities = None
        contrastive_loss = None
        expression_reg_loss = None
        
        # expression_reg_loss ì§‘ê³„
        if hasattr(self, 'all_expression_reg_losses') and self.all_expression_reg_losses:
            stacked = torch.stack(self.all_expression_reg_losses)
            expression_reg_loss = stacked.mean()
            if self.training:
                expression_reg_loss = expression_reg_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_expression_reg_losses = []
        
        if all_speciality_losses:
            # speciality_lossëŠ” í‰ê·  (ìŠ¤ì¹¼ë¼ ê°’ë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            stacked = torch.stack(all_speciality_losses)
            speciality_loss = stacked.mean()
            # gradient ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            if self.training:
                speciality_loss = speciality_loss.requires_grad_(True)
        
        if all_cosine_similarities:
            # cosine_similaritiesëŠ” í‰ê·  (í…ì„œë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            # ëª¨ë“  í…ì„œê°€ ë™ì¼í•œ shapeì¸ì§€ í™•ì¸
            try:
                stacked = torch.stack(all_cosine_similarities)
                cosine_similarities = stacked.mean(dim=0)
                # gradient ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
                if self.training:
                    cosine_similarities = cosine_similarities.requires_grad_(True)
            except RuntimeError as e:
                # Shapeì´ ë‹¤ë¥¸ ê²½ìš° ê°ê° í‰ê· ì„ ë‚´ê³  ë‹¤ì‹œ í‰ê· 
                if "size" in str(e).lower() or "shape" in str(e).lower():
                    # ê° í…ì„œì˜ í‰ê· ì„ êµ¬í•œ í›„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
                    means = [cs.mean() if torch.is_tensor(cs) and cs.numel() > 0 else torch.tensor(0.0, device=all_cosine_similarities[0].device, requires_grad=True) 
                            for cs in all_cosine_similarities if cs is not None]
                    if means:
                        cosine_similarities = torch.stack(means).mean()
                        if self.training:
                            cosine_similarities = cosine_similarities.requires_grad_(True)
                else:
                    raise
        
        if all_contrastive_losses:
            # contrastive_lossëŠ” í‰ê·  (ìŠ¤ì¹¼ë¼ ê°’ë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            stacked = torch.stack(all_contrastive_losses)
            contrastive_loss = stacked.mean()
            # gradient ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            if self.training:
                contrastive_loss = contrastive_loss.requires_grad_(True)
        
        # routing_uncertainty ì§‘ê³„
        routing_uncertainty = None
        if hasattr(self, 'all_routing_uncertainties') and self.all_routing_uncertainties:
            # routing_uncertaintyëŠ” í‰ê·  (í…ì„œë“¤ì˜ í‰ê· )
            try:
                stacked = torch.stack(self.all_routing_uncertainties)
                routing_uncertainty = stacked.mean(dim=0)
                if self.training:
                    routing_uncertainty = routing_uncertainty.requires_grad_(True)
            except RuntimeError:
                # Shapeì´ ë‹¤ë¥¸ ê²½ìš° ê°ê° í‰ê· ì„ ë‚´ê³  ë‹¤ì‹œ í‰ê· 
                means = [ru.mean() if torch.is_tensor(ru) and ru.numel() > 0 else torch.tensor(0.0, device=self.all_routing_uncertainties[0].device) 
                        for ru in self.all_routing_uncertainties if ru is not None]
                if means:
                    routing_uncertainty = torch.stack(means).mean()
                    if self.training:
                        routing_uncertainty = routing_uncertainty.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_routing_uncertainties = []
        
        # entropy_loss ì§‘ê³„ (CV ê°ì†Œë¥¼ ìœ„í•œ gradient ìˆëŠ” loss)
        entropy_loss = None
        if hasattr(self, 'all_entropy_losses') and self.all_entropy_losses:
            stacked = torch.stack(self.all_entropy_losses)
            entropy_loss = stacked.mean()
            if self.training:
                entropy_loss = entropy_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_entropy_losses = []
        
        # load_balancing_loss ì§‘ê³„ (CV ê°ì†Œë¥¼ ìœ„í•œ gradient ìˆëŠ” loss)
        load_balancing_loss = None
        if hasattr(self, 'all_load_balancing_losses') and self.all_load_balancing_losses:
            stacked = torch.stack(self.all_load_balancing_losses)
            load_balancing_loss = stacked.mean()
            if self.training:
                load_balancing_loss = load_balancing_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_load_balancing_losses = []
        
        # sinkhorn_loss ì§‘ê³„ (SpecHorn-G: GRUê°€ Sinkhornì„ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” loss)
        sinkhorn_loss = None
        if hasattr(self, 'all_sinkhorn_losses') and self.all_sinkhorn_losses:
            stacked = torch.stack(self.all_sinkhorn_losses)
            sinkhorn_loss = stacked.mean()
            if self.training:
                sinkhorn_loss = sinkhorn_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_sinkhorn_losses = []
        
        # ortho_loss ì§‘ê³„ (ì „ë¬¸ê°€ë“¤ì˜ ê°€ì¤‘ì¹˜ ì§êµì„± Loss)
        ortho_loss = None
        if hasattr(self, 'all_ortho_losses') and self.all_ortho_losses:
            stacked = torch.stack(self.all_ortho_losses)
            ortho_loss = stacked.mean()
            if self.training:
                ortho_loss = ortho_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_ortho_losses = []
        
        # balance_loss ì§‘ê³„ (GRU Solverì˜ constraint violation loss)
        balance_loss = None
        if hasattr(self, 'all_balance_losses') and self.all_balance_losses:
            stacked = torch.stack(self.all_balance_losses)
            balance_loss = stacked.mean()
            if self.training:
                balance_loss = balance_loss.requires_grad_(True)
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_balance_losses = []

        return SPECTRAModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
            router_logits=router_logits_tuple,
            speciality_loss=speciality_loss,
            cosine_similarities=cosine_similarities,
            contrastive_loss=contrastive_loss,
            expression_reg_loss=expression_reg_loss,
            routing_uncertainty=routing_uncertainty,
            entropy_loss=entropy_loss,
            load_balancing_loss=load_balancing_loss,
            sinkhorn_loss=sinkhorn_loss,
            ortho_loss=ortho_loss,
            balance_loss=balance_loss,
        )


@auto_docstring
class SPECTRAForCausalLM(SPECTRAPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}
    config: SPECTRAConfig
    base_model_prefix = "language_model"
    
    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)

    def __init__(self, config: SPECTRAConfig, **kwargs):
        super().__init__(config)
        self.model = SPECTRATextModel(config.text_config, **kwargs)
        # Ensure config refers to resolved text config from the submodule
        self.config = self.model.config
        self.vocab_size = self.config.vocab_size
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **loss_kwargs,
    ) -> CausalLMOutputWithPast:
        r"""
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

            logits_to_keep (`int` or `torch.Tensor`, *optional*):
                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
                This is useful when using packed tensor format (single dimension for batch and sequence length).

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, SPECTRAForCausalLM

        >>> model = SPECTRAForCausalLM.from_pretrained("google/gemma-2-9b")
        >>> tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b")

        >>> prompt = "What is your favorite condiment?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "What is your favorite condiment?"
        ```"""

        if self.training and self.config.attn_implementation != "eager":
            logger.warning_once(
                "It is strongly recommended to train SPECTRA models with the `eager` attention implementation "
                f"instead of `{self.config.attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`."
            )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            **loss_kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping

        loss = None
        aux_loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)
            
            # Speciality loss: Output orthogonality (encourages diverse expert outputs)
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš© (ì´ì¤‘ ìŠ¤ì¼€ì¼ë§ ë°©ì§€)
            speciality_loss_coef = getattr(self.model.config, "speciality_loss_coef", 1.0)  # 0.02 -> 1.0
            if outputs.speciality_loss is not None and speciality_loss_coef > 0:
                loss += outputs.speciality_loss * speciality_loss_coef
            
            # Contrastive loss: Input clustering (encourages experts to process distinct token types)
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš©
            contrastive_loss_coef = getattr(self.model.config, "contrastive_loss_coef", 1.0)  # 0.01 -> 1.0
            if outputs.contrastive_loss is not None and contrastive_loss_coef > 0:
                loss += outputs.contrastive_loss * contrastive_loss_coef
            
            # Expression projector regularization loss: Direct connection to expression_logits for gradient flow
            # This ensures expression_projector parameters receive gradients
            expression_reg_loss_coef = getattr(self.model.config, "expression_reg_loss_coef", 1.0)
            if outputs.expression_reg_loss is not None and expression_reg_loss_coef > 0:
                loss += outputs.expression_reg_loss * expression_reg_loss_coef
            
            # Expression projector loss: Ensure expression_logits contributes to loss for gradient flow
            # cosine_similarities (domain_orthogonality)ë¥¼ lossì— ì¶”ê°€í•˜ì—¬ expression_projectorê°€ í•™ìŠµë˜ë„ë¡ í•¨
            cosine_similarities_loss_coef = getattr(self.model.config, "cosine_similarities_loss_coef", 0.001)
            if outputs.cosine_similarities is not None and cosine_similarities_loss_coef > 0:
                # cosine_similaritiesëŠ” [batch, seq, num_experts] í˜•íƒœì˜ í…ì„œ ë˜ëŠ” ìŠ¤ì¹¼ë¼
                if torch.is_tensor(outputs.cosine_similarities) and outputs.cosine_similarities.numel() > 0:
                    # í…ì„œì¸ ê²½ìš° mean squared valueë¥¼ ìµœì†Œí™”í•˜ì—¬ expression diversityë¥¼ ìœ ì§€
                    expr_loss = torch.mean(outputs.cosine_similarities ** 2) * cosine_similarities_loss_coef
                    loss += expr_loss
                elif isinstance(outputs.cosine_similarities, (int, float)):
                    # ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° ì§ì ‘ ì‚¬ìš©
                    expr_loss = outputs.cosine_similarities * cosine_similarities_loss_coef
                    loss += expr_loss
            
            # ======================================================================================
            # [Minimalist Loss: Sinkhorn + Sharpening]
            # Sinkhornì€ êµ¬ì¡°ì ìœ¼ë¡œ ì´ë¯¸ ë¶€í•˜ ë¶„ì‚°ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ë³„ë„ loss ë¶ˆí•„ìš”
            # Sharpeningë§Œ entropy minimizationìœ¼ë¡œ ì²˜ë¦¬
            # ======================================================================================
            
            # [Sharpening] Entropy Minimization: "í•œ ë†ˆë§Œ íŒ¨ë¼" (í™•ì‹¤í•œ ì „ë¬¸ê°€ ì„ íƒ)
            # router_entropy_coefëŠ” ì–‘ìˆ˜ë¡œ ì‚¬ìš© (entropyë¥¼ ë‚®ì¶”ëŠ” ë°©í–¥)
            router_entropy_coef = getattr(self.model.config, "router_entropy_coef", 0.1)
            if outputs.entropy_loss is not None and router_entropy_coef > 0:
                loss += outputs.entropy_loss * router_entropy_coef
            
            # [Optional] Ortho Loss: ë³´í—˜ìœ¼ë¡œ ì•½í•˜ê²Œ ìœ ì§€ (í•™ìŠµ ì´ˆë°˜ ê°€ì´ë“œ)
            # Sinkhorn + Sharpeningë§Œìœ¼ë¡œë„ ë¶„ë¦¬ê°€ ë˜ì§€ë§Œ, ì´ˆë°˜ í—¤ë§¤ì§€ ì•Šë„ë¡ ë„ì›€
            ortho_loss_coef = getattr(self.model.config, "ortho_loss_coef", 0.01)  # 0.05 -> 0.01 (ì•½í•˜ê²Œ)
            if outputs.ortho_loss is not None and ortho_loss_coef > 0:
                loss += outputs.ortho_loss * ortho_loss_coef
            
            # ======================================================================================
            # [ì œê±°ëœ Lossë“¤]
            # - gslb_coef: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - router_z_loss_coef: ë¶ˆí•„ìš”
            # - sinkhorn_distillation_coef: Sharpeningì´ ëŒ€ì‹ í•¨
            # - usage_uniformity_coef: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - load_balancing_loss: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - balance_loss: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # ======================================================================================

        try:
            import torch.distributed as dist
            is_main_proc = (not dist.is_available()) or (not dist.is_initialized()) or dist.get_rank() == 0
        except Exception:
            is_main_proc = True


        return SPECTRACausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            aux_loss=aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            ortho_loss=outputs.ortho_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
            expression_reg_loss=outputs.expression_reg_loss,
            entropy_loss=outputs.entropy_loss,
            load_balancing_loss=outputs.load_balancing_loss,
            sinkhorn_loss=outputs.sinkhorn_loss
        )


class SPECTRAMultiModalProjector(nn.Module):
    def __init__(self, config: SPECTRAConfig, **kwargs):
        super().__init__()

        self.mm_input_projection_weight = nn.Parameter(
            torch.zeros(config.vision_config.hidden_size, config.text_config.hidden_size)
        )

        self.mm_soft_emb_norm = SPECTRARMSNorm(
            config.vision_config.hidden_size, eps=config.vision_config.layer_norm_eps
        )

        self.patches_per_image = int(config.vision_config.image_size // config.vision_config.patch_size)
        self.tokens_per_side = int(config.mm_tokens_per_image**0.5)
        self.kernel_size = self.patches_per_image // self.tokens_per_side
        self.avg_pool = nn.AvgPool2d(kernel_size=self.kernel_size, stride=self.kernel_size)

    def forward(self, vision_outputs: torch.Tensor):
        batch_size, _, seq_length = vision_outputs.shape

        reshaped_vision_outputs = vision_outputs.transpose(1, 2)
        reshaped_vision_outputs = reshaped_vision_outputs.reshape(
            batch_size, seq_length, self.patches_per_image, self.patches_per_image
        )
        reshaped_vision_outputs = reshaped_vision_outputs.contiguous()

        pooled_vision_outputs = self.avg_pool(reshaped_vision_outputs)
        pooled_vision_outputs = pooled_vision_outputs.flatten(2)
        pooled_vision_outputs = pooled_vision_outputs.transpose(1, 2)

        normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)

        projected_vision_outputs = torch.matmul(normed_vision_outputs, self.mm_input_projection_weight)
        return projected_vision_outputs.type_as(vision_outputs)


def token_type_ids_mask_function(
    token_type_ids: Optional[torch.Tensor],
    image_group_ids: Optional[torch.Tensor],
    tokens_per_image: int,
) -> Optional[Callable]:
    """
    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,
    not start and end indices.
    """
    # Do not return an additional mask in this case
    if token_type_ids is None:
        return None

    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:
        # If it's 1 for both query and key/value, we are in an image block
        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length
        # Since vmap doesn't support `if statement` we workaround it with `torch.where`
        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)
        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]
        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)

        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]
        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)

        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)
        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx

        # This is bidirectional attention whenever we are dealing with image tokens
        return is_image_block & same_image_block

    return inner_mask


@auto_docstring(
    custom_intro="""
    The Base SPECTRA model which consists of a vision backbone and a language model withou language modeling head.,
    """
)
class SPECTRAModel(SPECTRAPreTrainedModel):
    config: SPECTRAConfig
    _checkpoint_conversion_mapping = {"language_model.model": "language_model"}
    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch
    accepts_loss_kwargs = False

    def __init__(self, config: SPECTRAConfig, vision_tower=None):
        """
        Args:
            config ([`SPECTRAConfig`]):
                Model configuration class with all the parameters of the model.
            vision_tower ([`nn.Module`], *optional*):
                Pre-initialized vision tower. If provided, it will be used instead of
                creating a new one from config. This is CRITICAL for DeepSpeed ZeRO-3
                compatibility - the vision tower should be initialized OUTSIDE the
                ZeRO-3 context to avoid SigLIP initialization errors.
        """
        super().__init__(config)
        if vision_tower is not None:
            self.vision_tower = vision_tower
        else:
            self.vision_tower = AutoModel.from_config(config=config.vision_config, trust_remote_code=True)
        self.language_model = SPECTRATextModel.from_config(config=config.text_config, trust_remote_code=True)
        self.multi_modal_projector = SPECTRAMultiModalProjector(config=config)
        self.vocab_size = config.text_config.vocab_size
        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1
        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.language_model = decoder

    def get_decoder(self):
        return self.language_model

    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:
        """
        Projects the last hidden state from the vision model into language model space.

        Args:
            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)
               The tensors corresponding to the input images.
        Returns:
            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
        """
        vision_outputs = self.vision_tower(pixel_values=pixel_values).last_hidden_state
        image_features = self.multi_modal_projector(vision_outputs)
        return image_features

    def get_placeholder_mask(
        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor
    ):
        """
        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is
        equal to the length of multimodal features. If the lengths are different, an error is raised.
        """
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        n_image_features = image_features.shape[0] * image_features.shape[1]
        if inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
            )
        return special_image_mask

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **lm_kwargs,
    ) -> Union[tuple, SPECTRAModelOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, SPECTRAForConditionalGeneration

        >>> model = SPECTRAForConditionalGeneration.from_pretrained("google/gemma32-3b-mix-224")
        >>> processor = AutoProcessor.from_pretrained("google/gemma32-3b-mix-224")

        >>> prompt = "Where is the cat standing?"
        >>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> inputs = processor(images=image, text=prompt,  return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(**inputs,)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Where is the cat standing?\nsnow"
        ```"""
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Replace image id with PAD if the image token if OOV, to avoid index-errors
        if input_ids is not None and self.config.image_token_id >= self.vocab_size:
            special_image_mask = input_ids == self.config.image_token_id
            llm_input_ids = input_ids.clone()
            llm_input_ids[special_image_mask] = 0
        else:
            llm_input_ids = input_ids

        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(llm_input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # Merge text and images
        if pixel_values is not None:
            image_features = self.get_image_features(pixel_values)
            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)
            special_image_mask = self.get_placeholder_mask(
                input_ids, inputs_embeds=inputs_embeds, image_features=image_features
            )
            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config.get_text_config(),
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            if token_type_ids is not None and inputs_embeds.shape[1] != 1:
                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

                # First find where a new image block starts: 1 if image and previous not image
                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
                is_image = (token_type_ids == 1).to(cache_position.device)
                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
                image_group_ids = torch.where(
                    is_image, image_group_ids, torch.full_like(token_type_ids, -1, device=is_image.device)
                )
                mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image
                )

            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        outputs = self.language_model(
            attention_mask=causal_mask_mapping,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **lm_kwargs,
        )

        return SPECTRAModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values if use_cache else None,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=image_features if pixel_values is not None else None,
            aux_loss=outputs.aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
        )


@add_start_docstrings(
    """The SPECTRA model which consists of a vision backbone and a language model.""",
    spectra_START_DOCSTRING,
)
class SPECTRAForConditionalGeneration(SPECTRAPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {
        "^language_model.model": "model.language_model",
        "^vision_tower": "model.vision_tower",
        "^multi_modal_projector": "model.multi_modal_projector",
        "^language_model.lm_head": "lm_head",
    }
    _tied_weights_keys = ["lm_head.weight"]
    
    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)
  
    def __init__(self, config: SPECTRAConfig, **kwargs):
        super().__init__(config)
        self.model = SPECTRAModel(config)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_image_features(self, pixel_values):
        return self.model.get_image_features(pixel_values)

    # Make modules available through conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def vision_tower(self):
        return self.model.vision_tower

    @property
    def multi_modal_projector(self):
        return self.model.multi_modal_projector

    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **lm_kwargs,
    ) -> Union[Tuple, SPECTRACausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, SPECTRAForConditionalGeneration

        >>> model = SPECTRAForConditionalGeneration.from_pretrained("google/gemma-3-4b-it")
        >>> processor = AutoProcessor.from_pretrained("google/gemma-3-4b-it")

        >>> messages = [
        ...     {
        ...         "role": "system",
        ...         "content": [
        ...             {"type": "text", "text": "You are a helpful assistant."}
        ...         ]
        ...     },
        ...     {
        ...         "role": "user", "content": [
        ...             {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"},
        ...             {"type": "text", "text": "Where is the cat standing?"},
        ...         ]
        ...     },
        ... ]

        >>> inputs = processor.apply_chat_template(
        ...     messages,
        ...     tokenizer=True,
        ...     return_dict=True,
        ...     return_tensors="pt",
        ...     add_generation_prompt=True
        ... )
        >>> # Generate
        >>> generate_ids = model.generate(**inputs)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "user\nYou are a helpful assistant.\n\n\n\n\n\nWhere is the cat standing?\nmodel\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to"
        ```
        """

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs: SPECTRAModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            token_type_ids=token_type_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
            **lm_kwargs,
        )
        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping
            
        loss = None
        aux_loss = None
        if labels is not None:
            # Upcast to float if we need to compute the loss to avoid potential precision issues
            logits = logits.float()
            shift_logits = logits[..., :-1, :]
            shift_labels = labels[..., 1:]
            if attention_mask is not None:
                # we use the input attention mask to shift the logits and labels, because it is 2D.
                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft
                shift_attention_mask = attention_mask[:, -shift_logits.shape[1] :].to(logits.device)
                shift_logits = shift_logits[shift_attention_mask.to(logits.device) != 0].contiguous()
                shift_labels = shift_labels[shift_attention_mask.to(shift_labels.device) != 0].contiguous()
            else:
                shift_logits = shift_logits.contiguous()
                shift_labels = shift_labels.contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()

            flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)
            flat_labels = shift_labels.view(-1).to(shift_logits.device)
            loss = loss_fct(flat_logits, flat_labels)
            
            # Speciality loss: Output orthogonality (encourages diverse expert outputs)
            if outputs.speciality_loss is not None:
                loss += outputs.speciality_loss * 0.02  # ê°€ì¤‘ì¹˜ ì ìš©
            
            # Contrastive loss: Input clustering (encourages experts to process distinct token types)
            if outputs.contrastive_loss is not None:
                loss += outputs.contrastive_loss * 0.01  # ê°€ì¤‘ì¹˜ ì ìš©
            
            # Expression projector regularization loss: Direct connection to expression_logits for gradient flow
            # This ensures expression_projector parameters receive gradients
            if outputs.expression_reg_loss is not None:
                loss += outputs.expression_reg_loss
            
            # Expression projector loss: Ensure expression_logits contributes to loss for gradient flow
            # cosine_similarities (domain_orthogonality)ë¥¼ lossì— ì¶”ê°€í•˜ì—¬ expression_projectorê°€ í•™ìŠµë˜ë„ë¡ í•¨
            if outputs.cosine_similarities is not None:
                # cosine_similaritiesëŠ” [batch, seq, num_experts] í˜•íƒœì˜ í…ì„œ ë˜ëŠ” ìŠ¤ì¹¼ë¼
                if torch.is_tensor(outputs.cosine_similarities) and outputs.cosine_similarities.numel() > 0:
                    # í…ì„œì¸ ê²½ìš° mean squared valueë¥¼ ìµœì†Œí™”í•˜ì—¬ expression diversityë¥¼ ìœ ì§€
                    expr_loss = torch.mean(outputs.cosine_similarities ** 2) * 0.001  # ì‘ì€ ê°€ì¤‘ì¹˜ë¡œ ì¶”ê°€
                    loss += expr_loss
                elif isinstance(outputs.cosine_similarities, (int, float)):
                    # ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° ì§ì ‘ ì‚¬ìš©
                    expr_loss = outputs.cosine_similarities * 0.001
                    loss += expr_loss
            
            # ======================================================================================
            # [Minimalist Loss: Sinkhorn + Sharpening]
            # Sinkhornì€ êµ¬ì¡°ì ìœ¼ë¡œ ì´ë¯¸ ë¶€í•˜ ë¶„ì‚°ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ë³„ë„ loss ë¶ˆí•„ìš”
            # Sharpeningë§Œ entropy minimizationìœ¼ë¡œ ì²˜ë¦¬
            # ======================================================================================
            
            # [Sharpening] Entropy Minimization: "í•œ ë†ˆë§Œ íŒ¨ë¼" (í™•ì‹¤í•œ ì „ë¬¸ê°€ ì„ íƒ)
            # outputs.entropy_lossëŠ” ë¼ìš°í„°ì—ì„œ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ì „ë‹¬ë¨
            router_entropy_coef = getattr(self.config.text_config, "router_entropy_coef", 0.1)
            if outputs.entropy_loss is not None and router_entropy_coef > 0:
                loss += outputs.entropy_loss * router_entropy_coef
            
            # [Optional] Ortho Loss: ë³´í—˜ìœ¼ë¡œ ì•½í•˜ê²Œ ìœ ì§€ (í•™ìŠµ ì´ˆë°˜ ê°€ì´ë“œ)
            ortho_loss_coef = getattr(self.config.text_config, "ortho_loss_coef", 0.01)
            if outputs.ortho_loss is not None and ortho_loss_coef > 0:
                loss += outputs.ortho_loss * ortho_loss_coef
            
            # ======================================================================================
            # [ì œê±°ëœ Lossë“¤]
            # - gslb_coef: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - usage_uniformity_coef: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - aux_loss: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # ======================================================================================

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return SPECTRACausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=outputs.image_hidden_states,
            aux_loss=aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
            expression_reg_loss=outputs.expression_reg_loss,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        pixel_values=None,
        attention_mask=None,
        token_type_ids=None,
        use_cache=True,
        logits_to_keep=None,
        labels=None,
        **kwargs,
    ):
        # Overwritten -- custom `position_ids` and `pixel_values` handling
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
            logits_to_keep=logits_to_keep,
            token_type_ids=token_type_ids,
            **kwargs,
        )

        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore
        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always
        if cache_position[0] == 0:
            model_inputs["pixel_values"] = pixel_values

        return model_inputs
    
    @staticmethod
    def create_masks_for_generate(
        config: PretrainedConfig,
        input_embeds: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        cache_position: torch.Tensor,
        past_key_values: Optional[Cache],
        position_ids: Optional[torch.Tensor],
        token_type_ids: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> dict:
        # Prepare mask arguments
        mask_kwargs = {
            "config": config.get_text_config(),
            "input_embeds": input_embeds,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Add the token type ids mask for generate as well
        if token_type_ids is not None and input_embeds.shape[1] != 1:
            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

            # First find where a new image block starts: 1 if image and previous not image
            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
            is_image = (token_type_ids == 1).to(cache_position.device)
            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))
            mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image
            )

        return create_masks_for_generate(**mask_kwargs)


class SPECTRARouterTrainingMonitor:
    """
    PyTorch í•™ìŠµ ë£¨í”„ì—ì„œ ë¼ìš°í„°ê°€ 'ì‹¤ì œë¡œ' í•™ìŠµë˜ëŠ”ì§€ ì§€ì† ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±.
    - on_batch_start: step ìŠ¤ëƒ…ìƒ·(íŒŒë¼ë¯¸í„° ê°’) ì €ì¥
    - on_after_backward: grad norm/ë¶„í¬/ì—”íŠ¸ë¡œí”¼/EMA/ë³´ì¡° ë¡œìŠ¤ ë¡œê¹…
    - on_step_end: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ëŸ‰(delta) í™•ì¸
    ì‚¬ìš©ìëŠ” í•™ìŠµ ë£¨í”„ì—ì„œ ê° íƒ€ì´ë°ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
    """
    def __init__(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        log_every: int = 100,
        log_fn: Optional[Callable[[str], None]] = None,
    ):
        self.model = model
        self.optimizer = optimizer
        self.log_every = max(int(log_every), 1)
        self.log_fn = log_fn if log_fn is not None else (lambda msg: logger.info(msg))
        self._step = 0
        self._pre_step_snapshots: dict[str, dict[str, torch.Tensor]] = {}

    def _iter_router_modules(self):
        for name, module in self.model.named_modules():
            if getattr(module, "_is_spectra_router", False) or isinstance(module, SPECTRARouter):
                yield name, module

    @staticmethod
    def _check_requires_grad(module: nn.Module) -> bool:
        params = list(module.parameters(recurse=True))
        return len(params) > 0 and all(p.requires_grad for p in params)

    def _check_in_optimizer(self, module: nn.Module) -> bool:
        if self.optimizer is None:
            return False
        target_ids = {id(p) for p in module.parameters(recurse=True)}
        if not target_ids:
            return False
        opt_ids = set()
        for group in self.optimizer.param_groups:
            for p in group.get("params", []):
                opt_ids.add(id(p))
        return target_ids.issubset(opt_ids)

    @staticmethod
    def _grad_norms(module: nn.Module) -> dict[str, float]:
        norms: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if p.grad is not None:
                # ì‘ì€ ìˆ˜ì¹˜ ë…¸ì´ì¦ˆëŠ” 0ìœ¼ë¡œ ì·¨ê¸‰í•˜ì§€ ì•ŠìŒ
                norms[n] = float(p.grad.detach().norm().item())
        return norms

    @staticmethod
    def _snapshot_params(module: nn.Module) -> dict[str, torch.Tensor]:
        return {n: p.detach().clone() for n, p in module.named_parameters(recurse=True)}

    @staticmethod
    def _param_deltas(before: dict[str, torch.Tensor], module: nn.Module) -> dict[str, float]:
        deltas: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if n in before:
                deltas[n] = float((before[n] - p.detach()).abs().sum().item())
        return deltas

    @staticmethod
    def _router_usage_and_entropy(router_logits: Optional[Union[torch.Tensor, Tuple[torch.Tensor, ...]]]) -> tuple[Optional[List[float]], Optional[float]]:
        if router_logits is None:
            return None, None
        if isinstance(router_logits, tuple):
            if not router_logits:
                return None, None
            probs = torch.cat([t for t in router_logits if t is not None and t.numel() > 0], dim=0)
        else:
            probs = router_logits
        if probs.numel() == 0:
            return None, None
        # ë³¸ ì½”ë“œ ê²½ë¡œì—ì„œëŠ” router_logitsê°€ 'í™•ë¥ 'ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
        p = probs.clamp_min(1e-12)
        num_experts = p.size(-1)
        top1 = p.argmax(dim=-1)
        usage = torch.bincount(top1, minlength=num_experts).float()
        usage = usage / usage.sum().clamp_min(1.0)
        entropy = float((-(p * p.log()).sum(dim=-1)).mean().item())
        return usage.tolist(), entropy

    def on_batch_start(self):
        self._step += 1
        self._pre_step_snapshots.clear()
        for name, module in self._iter_router_modules():
            self._pre_step_snapshots[name] = self._snapshot_params(module)

    def on_after_backward(
        self,
        outputs: Optional[Union[SPECTRACausalLMOutputWithPast, SPECTRAModelOutputWithPast]] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        if self._step % self.log_every != 0:
            return

        # ë¼ìš°í„° ëª¨ë“ˆë³„ ìƒíƒœ/grad
        lines = []
        for name, module in self._iter_router_modules():
            req = self._check_requires_grad(module)
            inopt = self._check_in_optimizer(module)
            norms = self._grad_norms(module)
            any_grad = any(v > 0.0 for v in norms.values())
            gsum = sum(norms.values()) if norms else 0.0
            gmax = max(norms.values()) if norms else 0.0
            lines.append(f"[router:{name}] requires_grad={req} in_optimizer={inopt} any_grad={any_grad} grad_sum={gsum:.6f} grad_max={gmax:.6f}")

        # ì‚¬ìš© ë¶„í¬/ì—”íŠ¸ë¡œí”¼
        usage, entropy = (None, None)
        if outputs is not None and hasattr(outputs, "router_logits"):
            usage, entropy = self._router_usage_and_entropy(outputs.router_logits)
            if usage is not None:
                lines.append(f"[routing] usage(top1_ratio)={','.join(f'{u:.3f}' for u in usage)}")
            if entropy is not None:
                lines.append(f"[routing] entropy={entropy:.6f}")

        # EMA (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            # ForCausalLM -> .model.global_router, TextModel -> .global_router
            global_router = None
            if hasattr(self.model, "model") and hasattr(self.model.model, "global_router"):
                global_router = self.model.model.global_router
            elif hasattr(self.model, "global_router"):
                global_router = self.model.global_router
            if global_router is not None and hasattr(global_router, "expert_load_ema"):
                ema = global_router.expert_load_ema.detach().float()
                s = float(ema.sum().item())
                ema_norm = (ema / s) if s > 0 else ema
                lines.append(f"[ema] expert_load_ema_norm={','.join(f'{float(x):.3f}' for x in ema_norm.tolist())}")
        except Exception:
            pass

        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))

        # ì„ íƒì ìœ¼ë¡œ ë³´ì¡° ë¡œìŠ¤ë„ ë¡œê¹…(ê³„ì‚° ë¹„ìš© ë‚®ìŒ)
        try:
            if outputs is not None and hasattr(outputs, "router_logits") and outputs.router_logits is not None:
                if hasattr(self.model, "model") and hasattr(self.model.model, "config"):
                    cfg = self.model.model.config
                elif hasattr(self.model, "config"):
                    cfg = self.model.config
                else:
                    cfg = None
                if cfg is not None:
                    aux = load_balancing_loss_func(
                        outputs.router_logits,
                        cfg.n_routed_experts,
                        getattr(cfg, "num_experts_per_tok", 2),
                        attention_mask,
                        router_z_loss_coef=getattr(cfg, "router_z_loss_coef", None),
                        router_entropy_coef=getattr(cfg, "router_entropy_coef", None),
                        usage_uniformity_coef=getattr(cfg, "usage_uniformity_coef", None),
                    ).detach().float().item()
                    self.log_fn(f"[step {self._step}] aux_lb_loss={aux:.6f}")
        except Exception:
            pass

    def on_step_end(self):
        if self._step % self.log_every != 0:
            return
        lines = []
        for name, module in self._iter_router_modules():
            before = self._pre_step_snapshots.get(name, {})
            deltas = self._param_deltas(before, module)
            delta_sum = sum(deltas.values()) if deltas else 0.0
            lines.append(f"[router:{name}] param_delta_sum={delta_sum:.6f}")
        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))
        self._pre_step_snapshots.clear()


__all__ = [
    "SPECTRAPreTrainedModel",
    "SPECTRATextModel",
    "SPECTRAForCausalLM",
    "SPECTRAForConditionalGeneration",
    "SPECTRAModel",
    "SPECTRARouterTrainingMonitor",
]
