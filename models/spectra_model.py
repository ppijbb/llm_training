#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/SPECTRA/modular_SPECTRA.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_SPECTRA.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import copy
import os
import inspect
import math
import contextlib
from functools import partial
from collections.abc import Callable
from dataclasses import dataclass
from typing import List, Optional, Tuple, Union, Type

from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
# Add dynamo import for torch.compile compatibility
import torch._dynamo
import types
import logging
import deepspeed
logger = logging.getLogger(__name__)

# Siglip ì´ˆê¸°í™” ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•œ íŒ¨ì¹˜
import torch.nn.init as torch_init
_original_orthogonal = torch_init.orthogonal_
_original_xavier_uniform = torch_init.xavier_uniform_
_original_xavier_normal = torch_init.xavier_normal_
_original_kaiming_uniform = torch_init.kaiming_uniform_

def safe_orthogonal_(tensor, gain=1):
    if tensor.is_meta:
        return tensor

    # 1ì°¨ì› í…ì„œ ì²˜ë¦¬: orthogonal_ì€ 2ì°¨ì› ì´ìƒë§Œ ì§€ì›
    if tensor.dim() < 2:
        # 1ì°¨ì› í…ì„œì—ëŠ” ì •ê·œë¶„í¬ ì´ˆê¸°í™” ì‚¬ìš©
        with torch.no_grad():
            # DeepSpeed íŒŒí‹°ì…”ë‹ìœ¼ë¡œ í¬ê¸°ê°€ 0ì¸ í…ì„œ ì²˜ë¦¬
            if tensor.numel() == 0:
                return tensor

            # fan_in ê³„ì‚° (1ì°¨ì› í…ì„œì— ëŒ€í•œ ê·¼ì‚¬)
            fan_in = tensor.size(0)

            # std ê³„ì‚° (LeCun normalê³¼ ìœ ì‚¬í•˜ê²Œ)
            std = gain / math.sqrt(fan_in) if fan_in > 0 else 0.01
            tensor.normal_(0, std)
        return tensor

    # 2ì°¨ì› ì´ìƒ í…ì„œ ì²˜ë¦¬: ê¸°ì¡´ orthogonal ì´ˆê¸°í™” ì‚¬ìš©
    # Fix for "geqrf_cuda" not implemented for 'BFloat16'
    # Always cast to float32 for orthogonal_ init if low precision
    if tensor.dtype in [torch.bfloat16, torch.float16]:
        with torch.no_grad():
            t = tensor.to(torch.float32)
            _original_orthogonal(t, gain=gain)
            tensor.copy_(t.to(tensor.dtype))
    else:
        _original_orthogonal(tensor, gain=gain)
    return tensor

# Global monkey patch to catch all internal and dependency calls (like Transformers init)
torch_init.orthogonal_ = safe_orthogonal_

def safe_xavier_uniform_(tensor, gain=1.0):
    if tensor.is_meta: return tensor
    if tensor.numel() == 0: return tensor
    if tensor.dim() < 2:
        return torch_init.uniform_(tensor, -0.02, 0.02)
    return _original_xavier_uniform(tensor, gain=gain)

def safe_xavier_normal_(tensor, gain=1.0):
    if tensor.is_meta: return tensor
    if tensor.numel() == 0: return tensor
    if tensor.dim() < 2:
        return torch_init.normal_(tensor, std=0.02)
    return _original_xavier_normal(tensor, gain=gain)

def safe_kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):
    if tensor.is_meta: return tensor
    if tensor.numel() == 0: return tensor
    if tensor.dim() < 2:
        return torch_init.uniform_(tensor, -0.02, 0.02)
    return _original_kaiming_uniform(tensor, a=a, mode=mode, nonlinearity=nonlinearity)

torch_init.xavier_uniform_ = safe_xavier_uniform_
torch_init.xavier_normal_ = safe_xavier_normal_
torch_init.kaiming_uniform_ = safe_kaiming_uniform_

# Ensure nn.init also points to patched versions
if hasattr(nn, 'init'):
    nn.init.xavier_uniform_ = safe_xavier_uniform_
    nn.init.xavier_normal_ = safe_xavier_normal_
    nn.init.kaiming_uniform_ = safe_kaiming_uniform_

# Ultimate catch-all: patch the internal fan_in/out calculator
_orig_calc_fan = torch_init._calculate_fan_in_and_fan_out
def safe_calc_fan(tensor):
    if tensor.numel() == 0:
        return 1, 1
    if tensor.dim() < 2:
        return 1, 1
    f_in, f_out = _orig_calc_fan(tensor)
    return max(1, f_in), max(1, f_out)
torch_init._calculate_fan_in_and_fan_out = safe_calc_fan
if hasattr(nn.init, '_calculate_fan_in_and_fan_out'):
    nn.init._calculate_fan_in_and_fan_out = safe_calc_fan
if hasattr(torch.nn.init, '_calculate_fan_in_and_fan_out'):
    torch.nn.init._calculate_fan_in_and_fan_out = safe_calc_fan

def _patch_siglip_initializers():
    """Siglipì˜ ì´ˆê¸°í™” í•¨ìˆ˜ë“¤ì„ ì•ˆì „í•˜ê²Œ íŒ¨ì¹˜"""
    try:
        from transformers.models.siglip import modeling_siglip

        # 1. lecun_normal_ íŒ¨ì¹˜
        if hasattr(modeling_siglip, 'lecun_normal_'):
            original_lecun_normal = modeling_siglip.lecun_normal_

            def _safe_lecun_normal(tensor, a=math.sqrt(5)):
                """Siglip ì´ˆê¸°í™” ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ lecun_normal_"""
                # CRITICAL: Meta tensorëŠ” ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë¬´ì‹œ í›„ DeepSpeedê°€ ì²˜ë¦¬í•˜ê²Œ í•¨
                if tensor.is_meta:
                    return

                if tensor.dim() < 2:
                    # 1ì°¨ì› ì´í•˜ í…ì„œì—ëŠ” ì¼ë°˜ ì •ê·œë¶„í¬ ì‚¬ìš© (LeCun ì´ˆê¸°í™”ì˜ ê·¼ì‚¬ê°’)
                    with torch.no_grad():
                        # DeepSpeed íŒŒí‹°ì…”ë‹ìœ¼ë¡œ í¬ê¸°ê°€ 0ì¸ í…ì„œ ì²˜ë¦¬
                        if tensor.numel() == 0:
                            # ë¹ˆ í…ì„œëŠ” ì´ˆê¸°í™” ìƒëµ
                            return

                        # LeCun normal: std = sqrt(1/fan_in)
                        # 1ì°¨ì› í…ì„œì—ì„œëŠ” fan_inì„ tensor.size(0)ë¡œ ê·¼ì‚¬
                        if tensor.dim() == 1:
                            fan_in = tensor.size(0)
                        else:
                            # ìŠ¤ì¹¼ë¼ í…ì„œì˜ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                            fan_in = 1

                        std = 1.0 / math.sqrt(fan_in) if fan_in > 0 else 0.01
                        tensor.normal_(0, std)
                else:
                    # 2ì°¨ì› ì´ìƒ í…ì„œì—ëŠ” ê¸°ì¡´ lecun_normal_ ì‚¬ìš©
                    # ì›ë³¸ í•¨ìˆ˜ëŠ” tensorë§Œ ì¸ìë¡œ ë°›ìŒ (a íŒŒë¼ë¯¸í„° ë¬´ì‹œ)
                    original_lecun_normal(tensor)

            # íŒ¨ì¹˜ ì ìš©
            modeling_siglip.lecun_normal_ = _safe_lecun_normal

        # 2. variance_scaling_ íŒ¨ì¹˜ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„!)
        if hasattr(modeling_siglip, 'variance_scaling_'):
            original_variance_scaling = modeling_siglip.variance_scaling_

            def _safe_variance_scaling(tensor, scale=1.0, mode='fan_in', distribution='normal'):
                """Siglip ì´ˆê¸°í™” ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ variance_scaling_"""
                # CRITICAL: Meta tensorëŠ” ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë¬´ì‹œ í›„ DeepSpeedê°€ ì²˜ë¦¬í•˜ê²Œ í•¨
                if tensor.is_meta:
                    return

                if tensor.dim() < 2:
                    # 1ì°¨ì› ì´í•˜ í…ì„œì—ëŠ” ì•ˆì „í•œ ì´ˆê¸°í™” ì‚¬ìš©
                    with torch.no_grad():
                        # DeepSpeed íŒŒí‹°ì…”ë‹ìœ¼ë¡œ í¬ê¸°ê°€ 0ì¸ í…ì„œ ì²˜ë¦¬
                        if tensor.numel() == 0:
                            # ë¹ˆ í…ì„œëŠ” ì´ˆê¸°í™” ìƒëµ
                            return

                        # fan_in ê³„ì‚° (1ì°¨ì› í…ì„œì— ëŒ€í•œ ê·¼ì‚¬)
                        if tensor.dim() == 1:
                            fan = tensor.size(0) if mode == 'fan_in' else 1
                        else:
                            # ìŠ¤ì¹¼ë¼ í…ì„œ
                            fan = 1

                        # std ê³„ì‚°
                        std = math.sqrt(scale / fan) if fan > 0 else 0.01

                        # ë¶„í¬ì— ë”°ë¥¸ ì´ˆê¸°í™”
                        if distribution == 'normal':
                            tensor.normal_(0, std)
                        elif distribution == 'uniform':
                            bound = math.sqrt(3) * std
                            tensor.uniform_(-bound, bound)
                        elif distribution == 'truncated_normal':
                            tensor.normal_(0, std).clamp_(-2*std, 2*std)
                else:
                    # 2ì°¨ì› ì´ìƒ í…ì„œì—ëŠ” ê¸°ì¡´ variance_scaling_ ì‚¬ìš©
                    original_variance_scaling(tensor, scale, mode, distribution)

            # íŒ¨ì¹˜ ì ìš©
            modeling_siglip.variance_scaling_ = _safe_variance_scaling

        print("âœ… Siglip ì´ˆê¸°í™” í•¨ìˆ˜ë“¤ íŒ¨ì¹˜ ì ìš©ë¨")

    except ImportError:
        print("âš ï¸ Siglip ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íŒ¨ì¹˜ ìƒëµ")
    except Exception as e:
        print(f"âš ï¸ Siglip íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")

# ëª¨ë¸ import ì‹œì ì— íŒ¨ì¹˜ ì ìš©
_patch_siglip_initializers()

from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, HybridCache, StaticCache, DynamicCache
from transformers.generation.utils import GenerationMixin
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_layers import GradientCheckpointingLayer
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask
from transformers.processing_utils import Unpack
from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple
from transformers.utils.doc import (
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
    add_start_docstrings,
)
from transformers.utils.generic import (
    ModelOutput,
    can_return_tuple,
)
from transformers.utils import auto_docstring, logging
from transformers.utils.import_utils import (
    is_torchdynamo_compiling,
    is_torch_flex_attn_available,
    is_flash_attn_2_available
)
from transformers.modeling_utils import (
    restore_default_dtype,
    SpecificPreTrainedModelType,
)
from transformers.configuration_utils import PretrainedConfig
from transformers import logging
from transformers.utils.deprecation import deprecate_kwarg
from transformers import AutoModel, AutoConfig, AutoModelForCausalLM
from .spectra_config import SPECTRAConfig, SPECTRATextConfig

if is_torch_flex_attn_available():
    from torch.nn.attention.flex_attention import BlockMask
    from transformers.integrations.flex_attention import make_flex_block_causal_mask


logger = logging.get_logger(__name__)
_CONFIG_FOR_DOC = "SPECTRAConfig"



class DifferentiableAllReduce(torch.autograd.Function):
    """
    [ZeRO-3 Fix] Synchronize gradients correctly for shared collectives.
    Standard all_reduce is in-place and risky in complex autograd graphs.
    """
    @staticmethod
    def forward(ctx, tensor, op=dist.ReduceOp.SUM):
        if not dist.is_initialized(): return tensor
        output = tensor.detach().clone()
        dist.all_reduce(output, op=op)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        if not dist.is_initialized(): return grad_output, None
        # Gradients for SUM all_reduce are identity across ranks for this activation
        # Crucial for autograd: must return a gradient for EACH input to forward.
        # forward(tensor, op) -> 2 inputs. We return (grad_tensor, grad_op).
        # grad_op is None because 'op' is not a tensor.
        return grad_output, None

def safe_distributed_sync(tensor, op=dist.ReduceOp.SUM, average=True):
    if not dist.is_initialized(): return tensor
    synced = DifferentiableAllReduce.apply(tensor, op)
    if average:
        synced = synced / dist.get_world_size()
    return synced

def calculate_ortho_loss_for_experts(expert_weights: List[torch.Tensor]) -> torch.Tensor:
    """
    Calculates the orthogonalization loss for a set of expert weights from a single MoE layer.
    This loss encourages functional diversity among experts by penalizing similarity
    in their weight spaces. The loss is the squared Frobenius norm
    of (VV' - I) where V is the matrix of normalized expert weights.
    """
    if not expert_weights:
        return torch.tensor(0.0, device=expert_weights[0].device)

    flattened_weights = [w.view(-1) for w in expert_weights]
    V = torch.stack(flattened_weights)

    # Normalize rows to be unit vectors, preventing weights from collapsing to zero
    # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    V = V / (V.norm(p=2, dim=1, keepdim=True) + 1e-6)

    # Gram matrix: V @ V.T
    gram_matrix = torch.matmul(V, V.t())

    # Target: identity matrix
    identity = torch.eye(gram_matrix.size(0), device=gram_matrix.device, dtype=gram_matrix.dtype)

    # Loss: squared Frobenius norm of (VV' - I)
    ortho_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
    return ortho_loss


def _orthogonal_constraint_loss(
    num_experts: int,
    gate_logits: torch.Tensor
) -> torch.Tensor:
        """ë¼ìš°í„° ì¶œë ¥ê°’ë“¤ì˜ ì§êµì„± ì œì•½ ì†ì‹¤"""
        # router_outputs: [batch*seq, num_experts]

        # ê° í† í°ë³„ë¡œ expert ë°©í–¥ì´ ì§êµí•˜ë„ë¡
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        normalized_outputs = gate_logits / (gate_logits.norm(p=2, dim=-1, keepdim=True) + 1e-6)

        # Gram matrix: [num_experts, num_experts]
        gram_matrix = torch.matmul(normalized_outputs.T, normalized_outputs)

        # Target: identity matrix
        identity = torch.eye(num_experts, device=gate_logits.device, dtype=gate_logits.dtype)

        # Loss: squared Frobenius norm of (GG' - I)
        constraint_loss = torch.pow(torch.norm(gram_matrix - identity, p='fro'), 2)
        return constraint_loss


class ContrastiveRouterLoss(nn.Module):
    """
    Encourages experts to process tokens from distinct semantic spaces (Hidden States).
    Uses Soft Probabilities for differentiability.
    """
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.epsilon = 1e-6

    def forward(self, hidden_states, routing_weights):
        """
        hidden_states: [batch, seq, hidden_dim]
        routing_weights: [batch, seq, num_experts] (Softmax probabilities)
        """
        # Flatten: [batch, seq, hidden] -> [N, hidden], [batch*seq, experts] -> [N, experts]
        flattened_states = hidden_states.reshape(-1, hidden_states.size(-1))
        flattened_weights = routing_weights.reshape(-1, routing_weights.size(-1))

        # Ensure dtype consistency for matmul
        flattened_weights = flattened_weights.to(dtype=flattened_states.dtype)

        # Calculate weighted centroids for each expert
        # [experts, N] @ [N, hidden] -> [experts, hidden]
        expert_centroids = torch.matmul(flattened_weights.t(), flattened_states)

        # Normalize by total weight assigned to expert
        expert_weight_sums = flattened_weights.sum(dim=0) + self.epsilon
        expert_centroids = expert_centroids / expert_weight_sums.unsqueeze(-1)

        # Cosine similarity between centroids
        # [ìˆ˜ì •] ì—¡ì‹¤ë¡  ì¶”ê°€í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        normalized_centroids = expert_centroids / (expert_centroids.norm(p=2, dim=1, keepdim=True) + 1e-6)
        similarity = torch.matmul(normalized_centroids, normalized_centroids.t())

        # Minimize off-diagonal similarity (contrastive)
        num_experts = similarity.size(0)
        mask = torch.eye(num_experts, device=similarity.device).bool()
        contrastive_loss = similarity[~mask].mean()

        return contrastive_loss

def load_balancing_loss_func(
    gate_logits: torch.Tensor,
    num_experts: int,
    top_k: int = 2,
    attention_mask: Optional[torch.Tensor] = None,
    router_z_loss_coef: Optional[float] = None,
    router_entropy_coef: Optional[float] = None,
    usage_uniformity_coef: Optional[float] = None,
) -> torch.Tensor:
    r"""
    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.
    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss
    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between
    experts is too unbalanced.
    Args:
        gate_logits (Union[`torch.Tensor`, Tuple[torch.Tensor]):
            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of
            shape [batch_size X sequence_length, num_experts].
        attention_mask (`torch.Tensor`, None):
            The attention_mask used in forward function
            shape [batch_size X sequence_length] if not None.
        num_experts (`int`, *optional*):
            Number of experts
        router_z_loss_coef (`float`, *optional*):
            Coefficient for the z-loss term in the load balancing loss.
    Returns:
        The auxiliary loss.
    """
    if gate_logits is None:
        return torch.tensor(0.0)

    if isinstance(gate_logits, tuple):
        # Add a check for empty tuple
        if not gate_logits:
            return torch.tensor(0.0)
        compute_device = gate_logits[0].device
        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)
    else:
        # handle tensor input (single tensor from global router)
        concatenated_gate_logits = gate_logits
        compute_device = concatenated_gate_logits.device

    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)

    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)

    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)

    if attention_mask is None:
        # Compute the percentage of tokens routed to each experts
        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.mean(routing_weights, dim=0)
    else:
        batch_size, sequence_length = attention_mask.shape
        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)

        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask
        expert_attention_mask = (
            attention_mask[None, :, :, None, None]
            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))
            .reshape(-1, top_k, num_experts)
            .to(compute_device)
        )

        # Compute the percentage of tokens routed to each experts
        # Sum over batch*seq and top_k dimensions to get [num_experts]
        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=(0, 1)) / torch.sum(
            expert_attention_mask, dim=(0, 1)
        )

        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert
        router_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))
            .reshape(-1, num_experts)
            .to(compute_device)
        )

        # Compute the average probability of routing to these experts
        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(
            router_per_expert_attention_mask, dim=0
        )

    # Core Switch-style load balancing loss
    aux_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0)) * num_experts

    # Router z-loss (Switch Transformer) to prevent overconfident routers
    if router_z_loss_coef is not None and router_z_loss_coef > 0:
        log_z = torch.logsumexp(concatenated_gate_logits, dim=-1)
        z_loss = torch.square(log_z).mean()
        aux_loss = aux_loss + router_z_loss_coef * z_loss

    # Entropy regularization to avoid routing collapse (maximize entropy)
    if router_entropy_coef is not None and router_entropy_coef > 0:
        token_entropy = -(routing_weights * torch.log(routing_weights.clamp_min(1e-12))).sum(dim=-1)
        # Normalize by log(num_experts) for scale invariance across different expert counts
        normalized_entropy = token_entropy / math.log(max(num_experts, 2))
        entropy_reg = -normalized_entropy.mean()
        aux_loss = aux_loss + router_entropy_coef * entropy_reg

    # Usage uniformity (optional): encourage average routing probability per expert to be near-uniform
    if usage_uniformity_coef is not None and usage_uniformity_coef > 0:
        # router_prob_per_expert already accounts for attention mask when provided
        target = torch.full_like(router_prob_per_expert, 1.0 / float(num_experts))
        usage_uniformity_loss = torch.mean(torch.square(router_prob_per_expert - target))
        aux_loss = aux_loss + usage_uniformity_coef * usage_uniformity_loss

    return aux_loss


def spectra_lb_loss(
    gate_logits: Union[torch.Tensor, Tuple[torch.Tensor, ...]],
    num_experts: int,
    lb_l2_coef: float = 1.0,
    lb_cv_coef: float = 0.5,
    lb_entropy_floor_coef: float = 0.0,
    top_k: int = 2,
    lb_topk_l2_coef: float = 0.0,
    lb_topk_cv_coef: float = 0.0,
    attention_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    SPECTRA-aligned low-overhead load balancing loss.
    Uses per-expert statistics only (O(E)) to minimize compute overhead.
    No shape changes or fallback zeros/nan - skips invalid terms.

    Args:
        gate_logits: Routing logits, either tensor [N, E] or tuple of tensors
        num_experts: Number of experts E
        lb_l2_coef: Weight for L2 uniformity loss
        lb_cv_coef: Weight for CV minimization
        lb_entropy_floor_coef: Weight for entropy floor (optional)
        top_k: Number of experts selected per token
        lb_topk_l2_coef: Weight for top-k token count uniformity loss
        lb_topk_cv_coef: Weight for top-k token count CV minimization
        attention_mask: Attention mask for valid tokens

    Returns:
        Scalar loss tensor (sum of weighted terms)
    """
    # Handle tuple input (concatenate if needed)
    if isinstance(gate_logits, tuple):
        if not gate_logits:  # Empty tuple - skip entirely
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        tensors = [gl for gl in gate_logits if gl is not None and gl.numel() > 0]
        if not tensors:
            return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)
        gate_logits = torch.cat(tensors, dim=0)

    if gate_logits is None or gate_logits.numel() == 0:
        return torch.tensor(0.0, dtype=torch.float32, requires_grad=False)

    # Get routing weights using existing softmax path
    routing_weights = torch.nn.functional.softmax(gate_logits, dim=-1)
    device, dtype = routing_weights.device, routing_weights.dtype

    # Compute per-expert mean probabilities
    routing_per_expert_attention_mask = None
    if attention_mask is not None:
        # Use existing masking logic from load_balancing_loss_func
        batch_size, seq_length = attention_mask.shape
        tokens = routing_weights.shape[0]
        denom = batch_size * seq_length
        num_hidden_layers = max(tokens // denom, 1) if denom > 0 else 1

        routing_per_expert_attention_mask = (
            attention_mask[None, :, :, None]
            .expand((num_hidden_layers, batch_size, seq_length, num_experts))
            .reshape(-1, num_experts)
            .to(device)
        )

        # If shapes mismatch (e.g., tokens not divisible), fall back to full mask
        if routing_per_expert_attention_mask.shape[0] != routing_weights.shape[0]:
            routing_per_expert_attention_mask = torch.ones_like(routing_weights, device=device, dtype=dtype)

        # Mean per expert (masked)
        denom_mask = torch.sum(routing_per_expert_attention_mask, dim=0).clamp_min(1.0)
        p_bar = torch.sum(routing_weights * routing_per_expert_attention_mask, dim=0) / denom_mask
    else:
        # Simple mean across batch dimension
        p_bar = routing_weights.mean(dim=0)  # [E]

    # Uniform target distribution
    u = torch.full_like(p_bar, 1.0 / float(num_experts))

    # Numerical stability
    eps = torch.finfo(dtype).eps

    total_loss = torch.zeros((), dtype=dtype, device=device)

    # L2 uniformity loss
    if lb_l2_coef > 0:
        l2_loss = torch.sum((p_bar - u) ** 2)
        total_loss = total_loss + lb_l2_coef * l2_loss

    # CV minimization (approximate)
    if lb_cv_coef > 0:
        mean_p = p_bar.mean()
        var_p = p_bar.var(unbiased=False)  # Population variance
        cv_loss = var_p / (mean_p + eps)
        total_loss = total_loss + lb_cv_coef * cv_loss

    # Entropy floor (optional)
    if lb_entropy_floor_coef > 0:
        # Token-level entropy floor
        token_entropy = -torch.sum(routing_weights * torch.log(routing_weights + eps), dim=-1)
        entropy_floor_loss = -token_entropy.mean()  # Minimize negative entropy = maximize entropy
        total_loss = total_loss + lb_entropy_floor_coef * entropy_floor_loss

    # Top-k token count based losses (works on discrete expert selection)
    if (lb_topk_l2_coef > 0 or lb_topk_cv_coef > 0) and top_k > 0:
        k = min(top_k, num_experts)
        # top-k indices based on routing probabilities (monotonic with logits)
        topk_probs, topk_indices = torch.topk(routing_weights, k=k, dim=-1)

        if attention_mask is not None and routing_per_expert_attention_mask is not None:
            token_mask = routing_per_expert_attention_mask.any(dim=-1)
            if token_mask.any():
                topk_indices = topk_indices[token_mask]
                topk_probs = topk_probs[token_mask]
            else:
                topk_indices = topk_indices[:0]
                topk_probs = topk_probs[:0]

        flat_indices = topk_indices.reshape(-1)
        flat_probs = topk_probs.reshape(-1)

        counts = torch.zeros(num_experts, dtype=dtype, device=device)
        if flat_indices.numel() > 0:
            ones = torch.ones_like(flat_indices, dtype=dtype, device=device)
            counts.scatter_add_(0, flat_indices, ones)

            weighted_counts = torch.zeros_like(counts)
            weighted_counts.scatter_add_(0, flat_indices, flat_probs.to(dtype))

            total_counts = counts.sum()
            if total_counts > 0 and lb_topk_l2_coef > 0:
                usage_distribution = counts / total_counts
                l2_topk_loss = torch.sum((usage_distribution - u) ** 2)
                total_loss = total_loss + lb_topk_l2_coef * l2_topk_loss

            mean_weight = weighted_counts.mean()
            var_weight = weighted_counts.var(unbiased=False) if weighted_counts.numel() > 0 else torch.tensor(0.0, device=device, dtype=dtype)
            if lb_topk_cv_coef > 0 and mean_weight > 0:
                topk_cv_loss = var_weight / (mean_weight + eps)
                total_loss = total_loss + lb_topk_cv_coef * topk_cv_loss

    return total_loss


# Copied from Phi-3.5-MoE
def _get_unpad_data(attention_mask):
    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)
    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
    max_seqlen_in_batch = seqlens_in_batch.max().item()
    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))
    return (
        indices,
        cu_seqlens,
        max_seqlen_in_batch,
    )

@dataclass
class SPECTRAModelOutputWithPast(BaseModelOutputWithPast):
    """
    Base class for SPECTRA outputs, with hidden states and attentions.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.
    """

    image_hidden_states: Optional[torch.FloatTensor] = None
    aux_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None
    expression_reg_loss: Optional[torch.FloatTensor] = None
    routing_uncertainty: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    load_balancing_loss: Optional[torch.FloatTensor] = None
    sinkhorn_loss: Optional[torch.FloatTensor] = None
    balance_loss: Optional[torch.FloatTensor] = None


@dataclass
class SPECTRACausalLMOutputWithPast(ModelOutput):
    """
    Base class for SPECTRA causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):
            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape
            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
            `past_key_values` input) to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        image_hidden_states (`torch.FloatTensor`, *optional*):
            A `torch.FloatTensor` of size `(batch_size, sequence_length, hidden_size)`.
            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.
    """

    loss: Optional[torch.FloatTensor] = None
    logits: Optional[torch.FloatTensor] = None
    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
    image_hidden_states: Optional[torch.FloatTensor] = None
    # this is moe specific
    aux_loss: Optional[torch.FloatTensor] = None
    ortho_loss: Optional[torch.FloatTensor] = None
    router_logits: Optional[Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]] = None
    speciality_loss: Optional[torch.FloatTensor] = None
    cosine_similarities: Optional[torch.FloatTensor] = None
    contrastive_loss: Optional[torch.FloatTensor] = None
    expression_reg_loss: Optional[torch.FloatTensor] = None
    entropy_loss: Optional[torch.FloatTensor] = None
    load_balancing_loss: Optional[torch.FloatTensor] = None
    sinkhorn_loss: Optional[torch.FloatTensor] = None
    # hn_context ì œê±° - ì°¨ì› ë¬¸ì œë¡œ ì¸í•´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ


class SPECTRATextScaledWordEmbedding(nn.Embedding):
    """
    ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
    TODO: THIS CODE IS CRITICAL FOR THE EXOSKELETON ARCHITECTURE. THIS CODE MUST REMOVED. SPECTRA MUST BUILT AS AN EXOSKELETON ARCHITECTURE FORM BASE LARGE LANGUAGE MODEL WEIGHTS, NOT FROM SCRATCH.
    ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
    """
    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float = 1.0):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        # CRITICAL ZeRO-3 FIX: Use float attribute instead of register_buffer to avoid meta-tensor copy crash
        self.embed_scale = embed_scale

    def forward(self, input_ids: torch.Tensor):
        return super().forward(input_ids) * self.embed_scale


class SPECTRAMLP(nn.Module):
    """
    ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
    TODO: THIS CODE IS CRITICAL FOR THE EXOSKELETON ARCHITECTURE. THIS CODE MUST REMOVED. SPECTRA MUST BUILT AS AN EXOSKELETON ARCHITECTURE FORM BASE LARGE LANGUAGE MODEL WEIGHTS, NOT FROM SCRATCH.
    ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
    """
    def __init__(self, config: SPECTRATextConfig, intermediate_size: Optional[int]=None, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        # Mark for fast init path
        setattr(self.gate_proj, "_is_spectra_gate_layer", True)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_activation]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class mp(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx,
        scores: torch.Tensor,
        multiplier: torch.Tensor,
        selected_experts: torch.Tensor,
        masked_gates: torch.Tensor,
        mask_for_one: torch.Tensor,
    ):
        ctx.save_for_backward(multiplier, selected_experts, masked_gates)
        return multiplier * mask_for_one

    @staticmethod
    def backward(
        ctx,
        grad_at_output: torch.Tensor,
    ):
        multiplier, selected_experts, masked_gates = ctx.saved_tensors

        grad_at_output = grad_at_output * multiplier

        grad_at_scores_expaned = masked_gates * grad_at_output.mul(-1)
        grad_at_scores_expaned.scatter_add_(
            dim=-1,
            index=selected_experts,
            src=grad_at_output,
        )

        return (
            grad_at_scores_expaned,
            None,
            None,
            None,
            None,
            None,
        )


def enhanced_soft_orthogonality_loss(
    expert_embeddings: torch.Tensor,
    lambda_so: float = 1e-4,
    use_srip: bool = True,
) -> torch.Tensor:
    """
    Enhanced Soft Orthogonality Loss with SRIP variant.

    í•µì‹¬ ì°¨ë³„ì :
    1. Frobenius Norm + Spectral Norm ê²°í•©
    2. ì–‘ë°©í–¥ ì–µì œ: cos(e_i, e_j)^2ë¡œ +1ê³¼ -1 ëª¨ë‘ í˜ë„í‹°
    3. Warm-up ì§€ì›

    Args:
        expert_embeddings: [num_experts, dim] or [batch, num_experts, dim]
        lambda_so: loss coefficient
        use_srip: use spectral norm variant

    Returns:
        loss: scalar orthogonality loss
    """
    if expert_embeddings.dim() == 3:
        # Batch case: average over batch
        # This is important: we want the EXPERTS to be orthogonal on average,
        # but momentarily they can shift. Averaging the REPRESENTATION first
        # stabilizes the gradient.
        expert_embeddings = expert_embeddings.mean(dim=0)  # [E, dim]

    # Check for empty or invalid input
    if expert_embeddings.numel() == 0:
        return torch.tensor(0.0, device=expert_embeddings.device, requires_grad=False)

    # Normalize to unit vectors
    E_norm = F.normalize(expert_embeddings, p=2, dim=-1)  # [E, dim]

    # Gram matrix (pairwise cosine similarities)
    # G_ij = cos(e_i, e_j)
    G = torch.matmul(E_norm, E_norm.t())  # [E, E]

    # Target: Identity matrix
    I = torch.eye(G.size(0), device=G.device, dtype=G.dtype)

    # Frobenius norm loss: ||G - I||_F^2
    # This penalizes off-diagonals (both positive and negative correlations)
    # (cos)^2 will be minimized
    diff = (G - I).float()
    frob_loss = torch.pow(torch.norm(diff, p='fro'), 2)

    if use_srip:
        # SRIP: Spectral norm of (G - I), bounds Lipschitz constant
        # Approximate with power iteration for efficiency?
        # For typical E (e.g., 8-64), exact computation via svd or matrix_norm is feasible/fast enough on GPU.
        # If E is very large (e.g., 256+), might be slow, but usually done once per step.
        # Use spectral norm (2-norm)
        # torch.linalg.matrix_norm with ord=2 computes the spectral norm (largest singular value)
        # ord=2 (spectral norm) requires float32 or higher
        spectral_loss = torch.linalg.matrix_norm(diff, ord=2) ** 2
        loss = 0.7 * frob_loss + 0.3 * spectral_loss
    else:
        loss = frob_loss

    return lambda_so * loss


class ExpressionProjector(nn.Module):
    """
    [Verified] Newton-Schulz Orthogonal Projector
    í•™ìŠµ/ì¶”ë¡  ëª¨ë‘ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ê°•ì œë¡œ ì§êµí™”í•˜ì—¬ ë¶•ê´´ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """

    def __init__(self, input_dim, output_dim, num_experts, method="newton_schulz", iterations=3, **kwargs):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_experts = num_experts
        self.method = method
        self.iterations = iterations

        # ë‹¨ì¼ ì„ í˜•ì¸µ + ì •ì§êµ ì´ˆê¸°í™”
        self.projection = nn.Linear(input_dim, output_dim, bias=False)
        # ZeRO-3 compatibility: Skip init if weights are partitioned (1D) or on meta device
        if self.projection.weight.dim() > 1 and not self.projection.weight.is_meta:
            safe_orthogonal_(self.projection.weight)

    def newton_schulz(self, W: torch.Tensor, steps: int = 3) -> torch.Tensor:
        """SVD-free orthogonalization."""
        norms = W.norm(p="fro") + 1e-8
        X = W / norms  # Always normalize for stability

        transpose = X.shape[0] < X.shape[1]
        if transpose:
            X = X.t()

        for _ in range(steps):
            A = torch.matmul(X.t(), X)
            X = 1.5 * X - 0.5 * torch.matmul(X, A)

        if transpose:
            X = X.t()
        return X

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # [ZeRO-3 Fix] Always use self.projection (nn.Linear) which handles ZeRO-3 gathering correctly.
        # Direct access to self.projection.weight (partitioned) causes shape mismatch (0 vs 4608).
        if x.numel() == 0:
            # Handle empty input manually to avoid DeepSpeed/Linear issues with empty inputs
            return torch.zeros(x.shape[0], self.output_dim, device=x.device, dtype=x.dtype)
        return self.projection(x)

    def orthogonal_loss(self):
        # ê°•ì œ ì§êµí™” ê²½ë¡œì´ë¯€ë¡œ ë³„ë„ ì†ì‹¤ ë¶ˆí•„ìš”
        return torch.tensor(0.0, device=self.projection.weight.device)


class IntentGatedContextCell(nn.Module):
    """
    Agentic Intent Cell (Memory-Efficient).
    Extracts global intent via gated mean-pooling instead of sequential GRU loop.
    This avoids the OOM bottleneck of 64-step GRU Ã— 48 layers = 3072 GRUCell calls.
    """
    def __init__(self, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        
        # Gating Network (Lightweight: determines how much of global context to use)
        self.gate_proj1 = nn.Linear(hidden_size, hidden_size // 2)
        self.gate_norm = nn.LayerNorm(hidden_size // 2)
        self.gate_act = nn.ReLU()
        self.gate_proj2 = nn.Linear(hidden_size // 2, 1)
        self.gate_sigmoid = nn.Sigmoid()

        # Intent Projection (replaces GRU: single matmul instead of 64 sequential steps)
        self.intent_proj = nn.Linear(hidden_size, hidden_size)
        nn.init.orthogonal_(self.intent_proj.weight)

        # Expression Projector for compatibility with legacy code calling conventions
        self.expression_projector = nn.Linear(hidden_size, hidden_size)

    def forward(self, x, h=None):
        """
        Single-step intent extraction.
        x: [batch, hidden] or [batch, seq, hidden]
        Returns: [batch, hidden] intent vector
        """
        # Handle both 2D and 3D input
        if x.dim() == 3:
            # Mean pool across sequence dimension -> [batch, hidden]
            x_pooled = x.mean(dim=1)
        else:
            x_pooled = x
        
        # Gating: how much global context to inject
        global_ctx = x_pooled  # Already batch-level
        g = self.gate_proj1(global_ctx)
        g = self.gate_norm(g)
        g = self.gate_act(g)
        g = self.gate_proj2(g)
        gate_val = self.gate_sigmoid(g)  # [batch, 1]
        
        # Gated projection (replaces sequential GRU)
        intent = self.intent_proj(x_pooled * gate_val)
        
        return intent.detach()  # Detach to prevent BPTT

class SPECTRARouter(nn.Module):
    def __init__(self, config: SPECTRATextConfig, **kwargs):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.top_k = config.num_experts_per_tok
        self.router_dim = config.router_dim
        
        # [1] Intent Exoskeleton (Global Context)
        self.context_cell = IntentGatedContextCell(self.hidden_size)
        
        # Priority Projector (Semantic Routability)
        self.priority_proj = nn.Linear(self.hidden_size, self.num_experts, bias=False)
        nn.init.zeros_(self.priority_proj.weight)
        
        # [2] Orthogonalized Speciality Gate (Codebook)
        self.expert_codebook = nn.Parameter(torch.randn(self.num_experts, self.hidden_size, dtype=torch.float32))
        nn.init.orthogonal_(self.expert_codebook)
        
        # [3] Agentic Load Balancing (DeepSeek ALF + GLN)
        # Persistent Bias Buffer
        self.register_buffer("expert_bias", torch.zeros(self.num_experts))
        
        # GLN Parameters (Gate Logit Normalization)
        self.lambda_gln = nn.Parameter(torch.tensor(10.0)) # Learnable scale, init high for sharpness
        
        # Update Hypers
        self.bias_update_rate = 0.001 
        self.temperature = 0.1

        # [Gradient Checkpointing] Deferred bias updates
        self._bias_delta_acc = None

        # Curriculum / step tracking (align with trainable_spectra so step 1 is reachable)
        self.curriculum_steps = getattr(config, "curriculum_steps", 10000)
        self.register_buffer("training_step", torch.tensor(0, dtype=torch.long))

    def _increment_step(self):
        if self.training:
            self.training_step.add_(1)

    def flush_bias_updates(self):
        """Apply accumulated bias delta (Sign-based) once."""
        acc = getattr(self, "_bias_delta_acc", None)
        if acc is not None:
            with torch.no_grad():
                # Sign-based update: acc contains sum of signs or errors
                # Here we just add the accumulated delta directly
                self.expert_bias.add_(acc)
                # Safety Clamp
                self.expert_bias.clamp_(min=-20.0, max=20.0)
            acc.zero_()

    def compute_affinity_loss(self):
        # Orthogonality of Expert Codes
        codes = F.normalize(self.expert_codebook.float(), p=2, dim=1)
        gram = torch.matmul(codes, codes.t())
        identity = torch.eye(self.num_experts, device=gram.device)
        return ((gram - identity) ** 2).mean()

    def forward(self, x, hn=None, top_k=2, jitter_eps=0.01, step_frac=0.0, layer_idx=0):
        """
        Agentic SPECTRA Routing (ALF + GLN + Global Intent)
        """
        if self.training:
            self._increment_step()
        batch_size, seq_len, _ = x.shape
        x_flat = x.view(-1, self.hidden_size)
        N = x_flat.size(0)
        E = self.num_experts
        k = self.config.num_experts_per_tok # Use config k, ignore Arg if needed
        
        zero = x_flat.sum() * 0.0
        
        # [Deadlock Prevention]
        is_dummy_batch = (N == 0)
        if is_dummy_batch:
            # Create dummy to force connectivity
            x_flat = torch.zeros(1, self.hidden_size, device=x.device, dtype=x.dtype)
            N = 1
            batch_size = 1

        # ----------------------------------------------------------------
        # 1. Global Intent Injection (Memory-Efficient: single pooling, no GRU loop)
        # ----------------------------------------------------------------
        # Mean-pool the full sequence to get batch-level intent (1 matmul, not 64 GRU steps)
        intent_batch = self.context_cell(x)  # [B, H] - handles 3D input internally
        # Broadcast to all tokens: [B, H] -> [N, H]
        intent = intent_batch.unsqueeze(1).expand(-1, seq_len, -1).reshape(-1, self.hidden_size)
        if is_dummy_batch:
            intent = torch.zeros_like(x_flat)

        # Priority from Intent + Current State
        priority_scores = self.priority_proj(x_flat + intent)

        # ----------------------------------------------------------------
        # 2. Semantic Affinity (Sigmoid + Ortho)
        # ----------------------------------------------------------------
        x_norm = F.normalize(x_flat, p=2, dim=-1)
        codebook = self.expert_codebook.to(x.dtype)
        codes_norm = F.normalize(codebook, p=2, dim=1)
        
        # Cosine Similarity -> Sigmoid (DeepSeek ALF style standardizes range 0-1)
        semantic_logits = F.linear(x_norm, codes_norm) 
        # semantic_logits = torch.sigmoid(semantic_logits) # Sigmoid not typically used with Linear directly, 
        # DeepSeek uses Sigmoid on the final logits or as a gate. 
        # Here we stick to Linear for Logits but use GLN to normalize.
        
        # Base Logits
        logits = semantic_logits + priority_scores

        # ----------------------------------------------------------------
        # 3. Gate Logit Normalization (GLN - Skywork)
        # ----------------------------------------------------------------
        # Normalize logits across experts to recover Bias Authority
        mean_logits = logits.mean(dim=-1, keepdim=True)
        std_logits = logits.std(dim=-1, keepdim=True) + 1e-6
        logits_norm = self.lambda_gln * (logits - mean_logits) / std_logits
        
        # Deterministic Jitter
        if self.training and jitter_eps > 0:
            seed = (layer_idx * 16381 + N * 31 + E * 7) % (2**31)
            gen = torch.Generator(device=x.device).manual_seed(int(seed))
            logits_norm = logits_norm + torch.empty_like(logits_norm).uniform_(-jitter_eps, jitter_eps, generator=gen)

        # ----------------------------------------------------------------
        # 4. Agentic Load Balancing (DeepSeek V3 Sign-Update)
        # ----------------------------------------------------------------
        # Add Bias
        biased_logits = logits_norm + self.expert_bias.unsqueeze(0)
        
        # [Deadlock Prevention] Always compute local_load and run all_reduce when dist is up,
        # so all ranks participate in the same collective regardless of self.training.
        with torch.no_grad():
            probs = F.softmax(biased_logits.float(), dim=-1)
            if is_dummy_batch:
                local_load = torch.zeros(E, device=x.device, dtype=probs.dtype)
            else:
                local_load = probs.sum(0)
            if dist.is_initialized():
                dist.all_reduce(local_load, op=dist.ReduceOp.SUM)
        if self.training:
            with torch.no_grad():
                # Target (Uniform)
                target = local_load.sum() / E
                # Load Error
                load_error = local_load - target
                # Sign Update (Robust to scale)
                delta = -1.0 * self.bias_update_rate * load_error.sign()
                if self._bias_delta_acc is None:
                    self._bias_delta_acc = torch.zeros(E, device=x.device, dtype=self.expert_bias.dtype)
                self._bias_delta_acc.add_(delta.to(self.expert_bias.dtype))

        # ----------------------------------------------------------------
        # 5. Top-K Selection & Output
        # ----------------------------------------------------------------
        topk_weights, topk_indices = torch.topk(biased_logits, k=k, dim=-1)
        routing_weights = F.softmax(topk_weights, dim=-1).to(dtype=x.dtype)
        
        # Full Probs for Logging
        routing_probs_full = torch.zeros(N, E, device=x.device, dtype=x.dtype)
        routing_probs_full.scatter_(1, topk_indices, routing_weights)

        # Losses
        ortho_loss = self.compute_affinity_loss() * 0.05
        balance_loss = zero # Aux-free

        if is_dummy_batch:
             return (
                torch.empty(0, k, device=x.device, dtype=routing_weights.dtype), 
                torch.empty(0, k, device=x.device, dtype=topk_indices.dtype),
                None,  # expression_logits
                torch.empty(0, self.hidden_size, device=x.device, dtype=x.dtype), # hn
                zero, zero, zero,   
                torch.empty(0, E, device=x.device, dtype=routing_probs_full.dtype), 
                zero, zero, zero,
                balance_loss,
                zero,
                ortho_loss,   
                zero
            )

        return (
            routing_weights,    
            topk_indices,      
            None,               
            intent, # Return intent as 'hn' context            
            zero, zero, zero,   
            routing_probs_full, 
            zero, zero, zero,
            balance_loss,
            zero,
            ortho_loss,         
            zero
        )
iterations = 0
class SPECTRAMoE(nn.Module):
    """Hybrid Router: í•˜ë‚˜ì˜ linear layerì—ì„œ sigmoidë¡œ expert ì„ íƒ, sparsemixerë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""

    def __init__(
        self,
        config,
        global_router: IntentGatedContextCell,
        skip_expert_init: bool = False,  # [EXOSKELETON] Skip expert creation when injecting from base model
        **kwargs
    ):
        super().__init__()
        config = config
        self.hidden_dim = config.hidden_size
        self.ffn_dim = config.hidden_size
        self.num_experts = config.n_routed_experts
        self.top_k = config.num_experts_per_tok
        self.router_dim = config.router_dim
        global iterations
        iterations += 1
        self.iter = iterations
        self.router = global_router

        # [EXOSKELETON] Skip expert creation if we'll inject base model experts later
        if not skip_expert_init:
            expert_intermediate_size = getattr(config, "moe_intermediate_size", 768)
            self.experts = nn.ModuleList([SPECTRAMLP(config, intermediate_size=expert_intermediate_size) for _ in range(self.num_experts)])
        else:
            # Placeholder - will be replaced by base model experts during injection
            self.experts = None

        # Only create shared experts if enabled (affects 31B vs 37B param count)
        self.n_shared_experts = getattr(config, "n_shared_experts", 0)
        if self.n_shared_experts > 0:
            self.shared_experts = SPECTRAMLP(config=config, intermediate_size=config.intermediate_size * self.n_shared_experts)
        else:
            self.shared_experts = None

        self.router_jitter_noise = getattr(config, 'router_jitter_noise', 0.01)
        self.input_jitter_noise = getattr(config, 'input_jitter_noise', 0.0)

        setattr(self.router, "_is_spectra_router", True)
        if hasattr(self.router, "expression_projector"):
            setattr(self.router.expression_projector, "_is_spectra_expression_projector", True)

        # Adaptive filter parameters for load balancing

        # Enhanced Expert Utilization
        self.register_buffer("expert_specialization_ema", torch.zeros(self.num_experts, self.hidden_dim), persistent=True)
        self.register_buffer("expert_strength_ema", torch.zeros(self.num_experts), persistent=True) # New: Expert strength tracking
        self.routing_temperature = nn.Parameter(torch.ones(1))
        self.specialization_strength = getattr(config, "specialization_strength", 0.01)

        # Routing parameters
        self.enable_uncertainty_broadcast = getattr(config, "enable_uncertainty_broadcast", True)
        self.uncertainty_threshold = getattr(config, "uncertainty_threshold", 0.7)

        # shared_experts freeze ì—¬ë¶€ (ê¸°ë³¸ê°’ì€ Trueë¡œ ì„¤ì •)
        self.freeze_shared_experts = getattr(config, 'freeze_shared_experts', True)
        if self.freeze_shared_experts:
            self._freeze_shared_experts()

        # Orthogonal projectorëŠ” ì´ì œ global routerì—ì„œ ì²˜ë¦¬ë¨
        self.ortho_strength = getattr(config, 'ortho_strength', 1.0)

    def _freeze_shared_experts(self):
        """shared_expertsì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ freeze"""
        if self.shared_experts is not None:
            for param in self.shared_experts.parameters():
                param.requires_grad = False
            logger.debug(f"Shared experts frozen for layer {self.iter}")

    def _unfreeze_shared_experts(self):
        """shared_expertsì˜ íŒŒë¼ë¯¸í„°ë“¤ì„ unfreeze"""
        for param in self.shared_experts.parameters():
            param.requires_grad = True
        logger.debug(f"Shared experts unfrozen for layer {self.iter}")

    def freeze_shared_experts_manual(self):
        """ìˆ˜ë™ìœ¼ë¡œ shared_experts freeze"""
        self._freeze_shared_experts()
        self.freeze_shared_experts = True

    def unfreeze_shared_experts_manual(self):
        """ìˆ˜ë™ìœ¼ë¡œ shared_experts unfreeze"""
        self._unfreeze_shared_experts()
        self.freeze_shared_experts = False

    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def forward(
        self,
        hidden_states: torch.Tensor,
        global_routing_logits: Optional[torch.Tensor] = None,
        **kwargs
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        # [ZeRO-3 Fix] Removed Early Exit to ensure Graph Consistency via Router Dummy Path
        # if hidden_states.shape[0] == 0 or hidden_states.numel() == 0: ... (Deleted)

        # ============================================================
        # [COMPONENT TEST MODE] í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ì»´í¬ë„ŒíŠ¸ ë¹„í™œì„±í™”
        # ============================================================
        _TEST_MODE = os.environ.get("SPECTRA_TEST_MODE", "0") == "1"
        _DISABLE_SHARED_EXPERTS = os.environ.get("SPECTRA_DISABLE_SHARED_EXPERTS", "0") == "1"

        residual = hidden_states
        final_hidden_states, routing_info = self._sparse_routing(hidden_states, global_routing_logits)
        router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss = routing_info

        # shared_experts ë¹„í™œì„±í™” ì˜µì…˜
        if _DISABLE_SHARED_EXPERTS:
            if _TEST_MODE:
                print(f"[TEST] Layer {self.iter}: Shared experts disabled")
            # shared_experts ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°
            pass
        elif self.shared_experts is not None:
            with torch.no_grad():
                # [ZeRO-3 Fix] Manual Hydration for shared experts
                for p in self.shared_experts.parameters():
                    if hasattr(p, 'ds_numel') and p.numel() < p.ds_numel:
                        p.all_gather()
                pretriained_residual = self.shared_experts(residual)

            # final_hidden_statesë¥¼ pretrained_residualì˜ í¬ê¸°ì— ë§ì¶° normalize
            pretrained_norm = torch.norm(pretriained_residual, dim=-1, keepdim=True)
            final_norm = torch.norm(final_hidden_states, dim=-1, keepdim=True)
            # ì•ˆì „ì¥ì¹˜: 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
            scale_factor = torch.where(
                final_norm > 1e-8,
                pretrained_norm / (final_norm + 1e-8),
                torch.ones_like(pretrained_norm)
            )
            final_hidden_states_normalized = final_hidden_states * scale_factor
            final_hidden_states = final_hidden_states_normalized + pretriained_residual * 1.0
        else:
            # No shared experts, just use routed results
            pass
        if self.training:
            # Clean up: removed redundant requires_grad_ calls that can disconnect the graph
            # and cause issues with DeepSpeed ZeRO-3 partitioned gradients.
            final_hidden_states = final_hidden_states
        return final_hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss)

    def compute_pairwise_expert_similarity(self, expert_outputs: torch.Tensor, expert_mask: torch.Tensor) -> torch.Tensor:
        """
        Compute pairwise cosine similarity between expert outputs for the same inputs.
        Ideally, we want experts to be orthogonal (diverse).
        Returns scalar average similarity.
        """

        if self.expert_specialization_ema is not None and self.expert_specialization_ema.numel() > 0:
             # Normalize
            normalized_specs = F.normalize(self.expert_specialization_ema, dim=-1)
            # Similarity matrix
            sim_matrix = torch.matmul(normalized_specs, normalized_specs.t())
            # We want to minimize off-diagonal elements
            mask = torch.eye(self.num_experts, device=sim_matrix.device).bool()
            off_diagonal = sim_matrix[~mask]

            # ZeRO-3 safety: Ensure we return a persistent scalar
            if off_diagonal.numel() > 0:
                return off_diagonal.mean()
            return torch.tensor(0.0, device=self.expert_load_ema.device)

        return torch.tensor(0.0, device=self.expert_load_ema.device)

    @torch._dynamo.disable  # Disable torch.compile for this method due to data-dependent branching
    def _sparse_routing(
        self,
        hidden_states: torch.Tensor,
        global_routing_logits: torch.Tensor
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        
        batch_size, sequence_length, hidden_dim = hidden_states.shape
        device = hidden_states.device
        dtype = hidden_states.dtype

        # ============================================================
        # [COMPONENT TEST MODE] í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ì»´í¬ë„ŒíŠ¸ ë¹„í™œì„±í™”
        # ============================================================
        _TEST_MODE = os.environ.get("SPECTRA_TEST_MODE", "0") == "1"
        _DISABLE_EXPERT_DISPATCH = os.environ.get("SPECTRA_DISABLE_EXPERT_DISPATCH", "0") == "1"
        _DISABLE_AUX_LOSSES = os.environ.get("SPECTRA_DISABLE_AUX_LOSSES", "0") == "1"
        _DISABLE_ROUTER = os.environ.get("SPECTRA_DISABLE_ROUTER", "0") == "1"

        # ë”ë¯¸ routing_info ìƒì„± í•¨ìˆ˜
        def _create_dummy_routing_info(bs=None, seq=None, dev=None, dt=None):
            bs = bs if bs is not None else batch_size
            seq = seq if seq is not None else sequence_length
            dev = dev if dev is not None else device
            dt = dt if dt is not None else dtype
            zero_scalar = torch.tensor(0.0, device=dev, dtype=dt)
            dummy_logits = torch.zeros(bs * seq, self.num_experts, device=dev, dtype=dt)
            dummy_hn = torch.zeros(bs, self.router.num_experts * self.router.router_dim, device=dev, dtype=dt)
            return (dummy_logits, dummy_hn, zero_scalar, zero_scalar, zero_scalar, zero_scalar, zero_scalar, zero_scalar, zero_scalar, zero_scalar, zero_scalar, zero_scalar)

        # Expert dispatchë§Œ ë¹„í™œì„±í™”: hidden_states ê·¸ëŒ€ë¡œ ë°˜í™˜
        if _DISABLE_EXPERT_DISPATCH:
            final_hidden_states = hidden_states.view(-1, hidden_dim)
            routing_info = _create_dummy_routing_info()
            if _TEST_MODE:
                print(f"[TEST] Layer {self.iter}: Expert dispatch disabled, returning hidden_states directly")
            return final_hidden_states, routing_info

        if self.training and self.input_jitter_noise > 0:
            seed = (getattr(self, "iter", 0) * 16381 + batch_size * sequence_length * 31 + 7) % (2**31)
            gen = torch.Generator(device=hidden_states.device).manual_seed(int(seed))
            jitter = torch.empty_like(hidden_states).uniform_(1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise, generator=gen)
            hidden_states = hidden_states * jitter
        # DEBUG: Check input validity
        if torch.isnan(hidden_states).any():
             print(f"CRITICAL: Input to _sparse_routing layer {self.iter} is NaN! Mean={hidden_states.mean().item()}")
             # We can't easily recover, but logging this confirms the fault lies in the previous layer.

        # Global routerì—ì„œ ì „ì²´ ë¼ìš°íŒ… ì²˜ë¦¬ (GRU + expression projection + sparsemixer)
        router_output = self.router(
            hidden_states,
            global_routing_logits,
            top_k=self.top_k,
            jitter_eps=self.router_jitter_noise,
            layer_idx=getattr(self, "iter", 0),
        )
        # Unpack including full probabilities, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss
        routing_weights, selected_experts, expression_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, routing_probs_full, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss = router_output
        # multiplierì™€ selected_expertsëŠ” ì´ë¯¸ global routerì—ì„œ sparsemixerë¥¼ í†µí•´ ê³„ì‚°ë¨
        assert routing_weights.isnan().sum() == 0, f"{self.iter} layer routing_weights is nan Line: 826"
        # Flatten routing outputs for per-token processing
        routing_weights = routing_weights.view(batch_size * sequence_length, -1)  # [tokens, top_k]
        selected_experts = selected_experts.view(batch_size * sequence_length, -1)  # [tokens, top_k]
        final_hidden_states = torch.zeros(
            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device
        )
        # Expert mask from selected experts (always same ops for checkpoint recompute)
        expert_mask = torch.nn.functional.one_hot(
            selected_experts, num_classes=self.num_experts
        ).bool()
        # Uncertainty-based broadcasting for uncertain tokens (training only); no .any() branch
        if self.training and self.enable_uncertainty_broadcast and routing_uncertainty is not None:
            uncertainty_flat = routing_uncertainty.view(-1) if routing_uncertainty.dim() > 1 else routing_uncertainty
            uncertain_mask = uncertainty_flat > self.uncertainty_threshold
            broadcast = uncertain_mask.view(-1, 1, 1).expand_as(expert_mask)
            expert_mask = expert_mask | broadcast

        # =======================================================================
        # [UNIVERSAL EXOSKELETON CONTROLLER]
        # Dispatch to base model experts regardless of their internal structure (Fused or ModuleList)
        # =======================================================================
        hidden_states_flat = hidden_states.view(-1, hidden_dim)

        # Fused path only (user requirement).
        if not isinstance(self.experts, nn.ModuleList) and callable(self.experts):
            fwd_input = hidden_states_flat.view(batch_size, sequence_length, hidden_dim)
            num_tokens = batch_size * sequence_length

            # [Deadlock Fix] All ranks must call experts() with the SAME shape so backward
            # runs the same collectives. Pad to max_tokens across ranks when distributed.
            if dist.is_initialized():
                max_tokens_t = torch.tensor(num_tokens, device=device, dtype=torch.long)
                dist.all_reduce(max_tokens_t, op=dist.ReduceOp.MAX)
                max_tokens = max_tokens_t.item()
            else:
                max_tokens = num_tokens

            # Use actual top_k from router output (may differ from self.top_k after inject)
            top_k_actual = routing_weights.size(1)

            if max_tokens == 0:
                dummy_hidden = torch.zeros(1, 1, hidden_dim, device=device, dtype=dtype)
                dummy_weights = torch.zeros(1, self.num_experts, device=device, dtype=dtype)
                dummy_ind = torch.zeros(1, top_k_actual, device=device, dtype=torch.long)
                fwd_out = self.experts(dummy_hidden, dummy_weights, dummy_ind)
                final_hidden_states = fwd_out[:, :0, :].reshape(0, hidden_dim)
            else:
                fwd_input_padded = torch.zeros(1, max_tokens, hidden_dim, device=device, dtype=dtype)
                if num_tokens > 0:
                    fwd_input_padded[:, :num_tokens, :] = fwd_input.view(1, num_tokens, hidden_dim)
                full_weights = torch.zeros(max_tokens, self.num_experts, device=device, dtype=routing_weights.dtype)
                if num_tokens > 0:
                    full_weights[:num_tokens].scatter_(1, selected_experts, routing_weights)
                selected_padded = torch.zeros(max_tokens, top_k_actual, device=device, dtype=torch.long)
                if num_tokens > 0:
                    selected_padded[:num_tokens] = selected_experts
                fwd_out = self.experts(fwd_input_padded, full_weights, selected_padded)
                final_hidden_states = fwd_out[:, :num_tokens, :].reshape(num_tokens, hidden_dim)

            # --- Legacy EMA Update Removed ---
            # Using simple stateless routing for stability
            if self.training and hasattr(self, 'expert_specialization_ema'):
                 # Placeholder to keep buffer alive if needed, but no logic
                 pass
            # --- End EMA Update ---

        else:
            # ğŸš€ Standard MoE Path (ModuleList / Loop)
            # Unified path handling both empty and non-empty inputs
            # No fixed padding to max_capacity to avoid OOM
            
            expert_mask = expert_mask.permute(2, 0, 1)  # [E, tokens, top_k]
            routing_weights_flat = routing_weights.view(-1)
            
            # [ZeRO-3 Optimization] Global Sparse Activation
            local_expert_usage = (expert_mask.sum(dim=(1, 2)) > 0).long()  # [E]
            
            # Sync to find which experts are active GLOBALLY
            if dist.is_initialized():
                global_expert_usage = local_expert_usage.clone()
                dist.all_reduce(global_expert_usage, op=dist.ReduceOp.MAX)
            else:
                global_expert_usage = local_expert_usage
            
            # Accumulator for dummy loss to ensure graph connectivity for unused experts
            dummy_loss = torch.zeros(1, device=device, dtype=dtype)

            for expert_idx in range(self.num_experts):
                # [OOM FIX] Skip expert if NOBODY in the world is using it.
                if global_expert_usage[expert_idx] == 0:
                    continue

                expert_layer = self.experts[expert_idx]
                
                # Get indices for this expert
                token_idx, topk_idx = torch.where(expert_mask[expert_idx])
                
                if token_idx.numel() > 0:
                    # [Normal Execution] Expert has assigned tokens
                    current_idx = token_idx * self.top_k + topk_idx
                    states = hidden_states_flat[current_idx]
                    expert_out = expert_layer(states)
                    scale = routing_weights_flat[current_idx].unsqueeze(1)
                    final_hidden_states.index_add_(0, current_idx, expert_out * scale)
                else:
                    # [ZeRO-3 Safety] Expert is active GLOBALLY but NOT LOCALLY.
                    # Run a 1-token dummy forward to:
                    #   1) Trigger ZeRO-3 module hooks (matching normal forward exactly â†’ no deadlock)
                    #   2) Use minimal memory (1 token vs full batch)
                    #   3) Multiply output by 0.0 â†’ zero gradient, no effect on loss
                    dummy_token = torch.zeros(1, hidden_dim, device=device, dtype=dtype)
                    dummy_out = expert_layer(dummy_token)
                    dummy_loss = dummy_loss + dummy_out.sum() * 0.0

            # Connect dummy_loss to the output
            final_hidden_states = final_hidden_states + dummy_loss

        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)

        # ì½œë°±ì„ ìœ„í•´ ë¼ìš°íŒ… ì •ë³´ë¥¼ ëª¨ë“ˆì— ì €ì¥
        if self.training:
            with torch.no_grad():
                self.last_selected_experts = selected_experts.detach()
                self.last_routing_weights = routing_weights.detach()
                self.last_num_experts = self.num_experts

        return final_hidden_states, (routing_probs_full, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss)


class SPECTRARMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6, **kwargs):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.zeros(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        # Llama does x.to(float16) * w whilst SPECTRA is (x * w).to(float16)
        # See https://github.com/huggingface/transformers/pull/29402
        # [ZeRO-3 Fix] Handle un-gathered parameters (avoid Size([0]) mismatch)
        if self.weight.numel() > 0:
            output = output * (1.0 + self.weight.float())
        return output.type_as(x)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.eps}"


# =============================================================================
# EXOSKELETON ARCHITECTURE COMPONENTS
# =============================================================================
#
# The Exoskeleton architecture is designed to be a universal wrapper that can
# inject SPECTRA's advanced routing mechanisms into any pretrained model without
# reimplementing the model's core components (attention, layernorm, RoPE, etc.).
#
# Supported base models:
#   - Qwen3-VL-MoE (modeling_qwen3_vl_moe.py)
#   - Llama4 (modeling_llama4.py)
#   - DeepSeek-V3 (modeling_deepseek_v3.py)
#   - GLM4V-MoE (modeling_glm4v_moe.py)
#   - GPT-OSS (modeling_gpt_oss.py)
#   - Ernie4.5-MoE (modeling_ernie4_5_moe.py)
#   - Gemma3 and other transformer architectures
#
# The key principle is: DO NOT reimplement attention/RoPE/norm layers.
# Only replace the MoE routing mechanism while preserving all base model
# components for maximum compatibility and minimal code duplication.
# =============================================================================


class SPECTRAExoskeletonMoEInjector:
    """
    Universal Exoskeleton MoE Injector for pretrained models.

    This class wraps any transformer decoder layer and replaces its MoE/router
    component with SPECTRA's advanced routing mechanism while preserving:
    - Base model's attention implementation (FlashAttention, SDPA, eager, flex_attn)
    - Base model's positional encoding (RoPE variants, NoPE, ALiBi, etc.)
    - Base model's normalization layers (RMSNorm, LayerNorm, L2Norm)
    - Base model's shared experts (if any)

    This design ensures compatibility with:
    - DeepSpeed ZeRO-3 (no manual parameter gathering)
    - Tensor/Sequence/Context/Pipeline Parallelism
    - Gradient checkpointing
    - torch.compile and torch dynamo

    Usage:
        base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-VL-30B-A3B")
        injector = SPECTRAExoskeletonMoEInjector(spectra_config, spectra_router)
        spectra_model = injector.inject(base_model)
    """

    # Mapping of base model types to their MoE attribute names
    MOE_ATTR_NAMES = {
        # Model type -> (moe_attr_name, router_attr_name, experts_attr_name, shared_experts_attr_name)
        "qwen3_vl_moe": ("mlp", "gate", "experts", None),  # Qwen3-VL-MoE uses 'mlp' not 'feed_forward'
        "llama4": ("feed_forward", "router", "experts", "shared_expert"),
        "deepseek_v3": ("mlp", "gate", "experts", "shared_experts"),
        "glm4v_moe": ("mlp", "gate", "experts", "shared_expert"),
        "gpt_oss": ("mlp", "gate", "local_experts", None),
        "ernie4_5_moe": ("mlp", "gate", "experts", "shared_expert"),
        "gemma3": ("mlp", None, None, None),  # Dense model - will add MoE
        "default": ("mlp", "gate", "experts", None),
    }

    def __init__(
        self,
        spectra_config: SPECTRATextConfig,
        global_router: 'SPECTRARouter',
        preserve_shared_experts: bool = True,
        copy_expert_weights: bool = True,
    ):
        """
        Initialize the Exoskeleton MoE Injector.

        Args:
            spectra_config: SPECTRA configuration for the MoE layer
            global_router: Pre-initialized SPECTRA global router (shared across layers)
            preserve_shared_experts: If True, keep base model's shared experts frozen
            copy_expert_weights: If True, copy base model expert weights to SPECTRA experts (upcycling)
        """
        self.config = spectra_config
        self.global_router = global_router
        self.preserve_shared_experts = preserve_shared_experts
        self.copy_expert_weights = copy_expert_weights

    def detect_model_type(self, model) -> str:
        """
        Auto-detect the base model architecture type from its class name or config.

        Args:
            model: The pretrained model or module

        Returns:
            str: One of the keys in MOE_ATTR_NAMES
        """
        class_name = model.__class__.__name__.lower()
        config = getattr(model, 'config', None)
        model_type = getattr(config, 'model_type', '') if config else ''

        detection_patterns = {
            'qwen3_vl_moe': ['qwen3vlmoe', 'qwen3_vl_moe'],
            'llama4': ['llama4'],
            'deepseek_v3': ['deepseekv3', 'deepseek_v3'],
            'glm4v_moe': ['glm4vmoe', 'glm4v_moe'],
            'gpt_oss': ['gptoss', 'gpt_oss'],
            'ernie4_5_moe': ['ernie4', 'ernie45moe'],
            'gemma3': ['gemma3', 'gemma-3'],
        }

        for model_key, patterns in detection_patterns.items():
            for pattern in patterns:
                if pattern in class_name or pattern in model_type:
                    return model_key

        return 'default'

    def get_moe_components(self, layer, model_type: str) -> dict:
        """
        Extract MoE components from a decoder layer based on model type.

        Args:
            layer: A decoder layer module
            model_type: The detected model type

        Returns:
            dict: Contains 'moe', 'router', 'experts', 'shared_experts' (some may be None)
        """
        attr_names = self.MOE_ATTR_NAMES.get(model_type, self.MOE_ATTR_NAMES['default'])
        moe_attr, router_attr, experts_attr, shared_attr = attr_names

        moe = getattr(layer, moe_attr, None)
        if moe is None:
            return {'moe': None, 'router': None, 'experts': None, 'shared_experts': None}

        # Heuristic: Check if 'moe' is actually a MoE block by looking for router/gate and experts
        # This handles cases where a module exists but is a dense MLP, not a MoE block.
        has_router = hasattr(moe, router_attr) and (router_attr is None or getattr(moe, router_attr, None) is not None)
        has_experts = hasattr(moe, experts_attr) and (experts_attr is None or getattr(moe, experts_attr, None) is not None)

        # If neither router nor experts found, this is a dense layer, not MoE
        if router_attr and not has_router and not has_experts:
            logger.debug(f"Layer has '{moe_attr}' but it's a dense MLP (no {router_attr}/{experts_attr}), treating as None")
            return {'moe': None, 'router': None, 'experts': None, 'shared_experts': None}

        router = getattr(moe, router_attr, None) if router_attr else None
        experts = getattr(moe, experts_attr, None) if experts_attr else None
        shared_experts = getattr(moe, shared_attr, None) if shared_attr else None

        return {
            'moe': moe,
            'router': router,
            'experts': experts,
            'shared_experts': shared_experts,
            'moe_attr_name': moe_attr,
        }

    def create_spectra_moe_replacement(
        self,
        base_moe_components: dict,
        layer_idx: int,
    ) -> 'SPECTRAMoE':
        """
        Create a SPECTRAMoE module that replaces the base model's MoE.

        **CRITICAL**: For existing MoE models (e.g., Qwen3-VL-MoE), we REUSE the base model's
        expert MLPs directly - DO NOT create new experts. Only the router is replaced.

        For dense models being upcycled, new experts are created.

        Args:
            base_moe_components: Dict from get_moe_components()
            layer_idx: Layer index for router state tracking

        Returns:
            SPECTRAMoE: The replacement MoE module with SPECTRA router and base model experts
        """
        # Determine if we should skip expert creation (base model already has experts)
        has_base_experts = base_moe_components['experts'] is not None

        spectra_moe = SPECTRAMoE(
            config=self.config,
            global_router=self.global_router,
            skip_expert_init=has_base_experts,  # Skip if base model already has experts
        )
        spectra_moe.iter = layer_idx

        # =======================================================================
        # [CRITICAL FIX] REUSE base model experts - DO NOT copy weights to new experts!
        # This is NOT upcycling. Qwen3-VL-MoE already has experts, we just swap the router.
        # =======================================================================
        if has_base_experts:
            logger.debug(f"  Layer {layer_idx}: Reusing base model's {len(base_moe_components['experts']) if hasattr(base_moe_components['experts'], '__len__') else 'fused'} experts (router-only replacement)")
            # Direct assignment - use the SAME expert modules, no copying
            spectra_moe.experts = base_moe_components['experts']
        else:
            # No experts in base model - this is dense upcycling, keep new experts
            logger.debug(f"  Layer {layer_idx}: Base model has no experts, using newly initialized SPECTRA experts")

        # Preserve shared experts if requested
        if self.preserve_shared_experts and base_moe_components['shared_experts'] is not None:
            spectra_moe.shared_experts = base_moe_components['shared_experts']
            # Freeze shared experts to preserve pretrained behavior
            for param in spectra_moe.shared_experts.parameters():
                param.requires_grad = False

        return spectra_moe

    def inject_into_layer(self, layer, layer_idx: int, model_type: str) -> nn.Module:
        """
        Inject SPECTRA MoE into a single decoder layer using surgical hijacking.

        This method is ZeRO-3 SAFE because it does not replace the module object,
        preserving original DeepSpeed hooks and metadata.
        """
        # ============================================================
        # [COMPONENT TEST MODE] SPECTRA ì „ì²´ ë¹„í™œì„±í™” ì˜µì…˜
        # ============================================================
        _DISABLE_ALL = os.environ.get("SPECTRA_DISABLE_ALL", "0") == "1"
        _TEST_MODE = os.environ.get("SPECTRA_TEST_MODE", "0") == "1"

        if _DISABLE_ALL:
            if _TEST_MODE:
                print(f"[TEST] Layer {layer_idx}: SPECTRA disabled, keeping original Qwen3 MoE")
            return layer  # SPECTRA injection ê±´ë„ˆë›°ê¸°, ì›ë˜ Qwen3 MoE ìœ ì§€

        moe_components = self.get_moe_components(layer, model_type)

        if moe_components['moe'] is None:
            # Dense layer upcycling - still needs replacement for now as there's no base MoE structure
            # But we make it ZeRO-3 aware by ensuring it's done inside zero.Init
            if layer_idx >= self.config.first_k_dense_replace:
                logger.debug(f"Layer {layer_idx}: Adding SPECTRA MoE to dense layer")
                spectra_moe = SPECTRAMoE(config=self.config, global_router=self.global_router)
                spectra_moe.iter = layer_idx
                moe_attr = 'feed_forward' if hasattr(layer, 'feed_forward') else 'mlp'
                original_mlp = getattr(layer, moe_attr, None)
                if original_mlp and self.copy_expert_weights:
                    with torch.no_grad():
                        for expert in spectra_moe.experts:
                            if hasattr(original_mlp, 'gate_proj') and hasattr(expert, 'gate_proj'):
                                expert.gate_proj.weight.copy_(original_mlp.gate_proj.weight)
                            if hasattr(original_mlp, 'up_proj') and hasattr(expert, 'up_proj'):
                                expert.up_proj.weight.copy_(original_mlp.up_proj.weight)
                            if hasattr(original_mlp, 'down_proj') and hasattr(expert, 'down_proj'):
                                expert.down_proj.weight.copy_(original_mlp.down_proj.weight)
                setattr(layer, moe_attr, spectra_moe)
            return layer

        # MoE layer - SURGICAL HIJACKING
        logger.debug(f"Layer {layer_idx}: Surgically hijacking MoE router with SPECTRA")
        original_moe = moe_components['moe']

        # [ZeRO-3 Fix] DO NOT call add_module('router', global_router) here if it's already registered.
        # Registration happens once at the model level in self.inject().
        # We store it as a protected reference to avoid PyTorch/ZeRO-3 re-registration.
        object.__setattr__(original_moe, 'router', self.global_router)
        original_moe.iter = layer_idx

        # Copy config attributes needed by SPECTRAMoE.forward
        for attr in ['top_k', 'num_experts', 'input_jitter_noise', 'router_jitter_noise',
                    'enable_uncertainty_broadcast', 'uncertainty_threshold']:
            val = getattr(self.config, attr, None)
            # Fallback for different SPECTRATextConfig attribute names
            if val is None:
                if attr == 'num_experts': val = getattr(self.config, 'n_routed_experts', 1)
                elif attr == 'top_k': val = getattr(self.config, 'num_experts_per_tok', 1)
                else: val = 0.0 # Default for noise/uncertainty if missing
            setattr(original_moe, attr, val)

        # Initialize EMAs on the original module (using register_parameter for ZeRO-3)
        num_experts = getattr(self.config, 'num_experts', getattr(self.config, 'n_routed_experts', 1))
        if not hasattr(original_moe, 'expert_load_ema'):
            original_moe.register_parameter('expert_load_ema', nn.Parameter(
                torch.zeros(num_experts), requires_grad=False
            ))
        if not hasattr(original_moe, 'expert_specialization_ema'):
            original_moe.register_parameter('expert_specialization_ema', nn.Parameter(
                torch.zeros(num_experts, self.config.hidden_size), requires_grad=False
            ))

        # Ensure 'experts' and 'shared_experts' aliases exist for SPECTRAMoE.forward
        if not hasattr(original_moe, 'experts') or original_moe.experts is None:
            original_moe.experts = moe_components['experts']
        if not hasattr(original_moe, 'shared_experts') or original_moe.shared_experts is None:
            original_moe.shared_experts = moe_components['shared_experts']

        # 2. Hijack the forward and helper methods
        original_moe.forward = types.MethodType(SPECTRAMoE.forward, original_moe)
        original_moe._sparse_routing = types.MethodType(SPECTRAMoE._sparse_routing, original_moe)
        original_moe.compute_pairwise_expert_similarity = types.MethodType(SPECTRAMoE.compute_pairwise_expert_similarity, original_moe)

        return layer

    def inject(self, model) -> nn.Module:
        """
        Inject SPECTRA Exoskeleton into all decoder layers of a model using Surgical Hijacking.
        """
        model_type = self.detect_model_type(model)
        logger.info(f"ğŸ’‰ Injecting SPECTRA Exoskeleton (Type: {model_type}) into base model...")

        # 1. Register Global Router to the top-level model ONCE
        # This is CRITICAL for ZeRO-3: a shared module must have ONE clear parent.
        if not hasattr(model, 'spectra_global_router'):
            model.add_module('spectra_global_router', self.global_router)

        # Determine where the layers are
        language_model = model
        if hasattr(model, 'language_model'): language_model = model.language_model
        elif hasattr(model, 'model'): language_model = model.model

        layers = None
        if hasattr(language_model, 'layers'): layers = language_model.layers
        elif hasattr(language_model, 'h'): layers = language_model.h # GPT-style
        elif hasattr(language_model, 'transformer') and hasattr(language_model.transformer, 'layers'):
            layers = language_model.transformer.layers

        if layers is None:
            logger.warning("âš ï¸ Could not find layers to inject MoE! Check model structure.")
            return model

        # Recursive injection
        for i, layer in enumerate(layers):
            self.inject_into_layer(layer, i, model_type)

        # Add get_parameter_groups to the top-level model for LR separation support
        def get_parameter_groups(self_model):
            router_params = []
            backbone_params = []
            for name, param in self_model.named_parameters():
                if 'router' in name or 'expert_load_ema' in name or 'expert_specialization_ema' in name:
                    router_params.append(param)
                else:
                    backbone_params.append(param)
            return {'router': router_params, 'backbone': backbone_params}

        model.get_parameter_groups = types.MethodType(get_parameter_groups, model)
        logger.info(f"âœ… SPECTRA Exoskeleton injection complete ({len(layers)} layers hijacked)")

        return model


class SPECTRARotaryEmbedding(nn.Module):
    """
    [DEPRECATED - Use Exoskeleton Architecture]

    This is a Gemma3-specific RoPE implementation. For universal model support,
    use SPECTRAExoskeletonMoEInjector which preserves the base model's native
    positional encoding (RoPE, NoPE, ALiBi, etc.) without reimplementation.

    WARNING: This class will be removed in a future version. Migrate to Exoskeleton.

    The Exoskeleton architecture injects SPECTRA routing into pretrained models
    while preserving all base model components for maximum compatibility with:
    - Qwen3-VL-MoE, Llama4, DeepSeek-V3, GLM4V-MoE, GPT-OSS, Ernie4.5-MoE
    - DeepSpeed ZeRO-3 and tensor parallelism
    - FlashAttention, SDPA, flex_attn implementations
    """
    def __init__(self, config: SPECTRATextConfig, device=None, **kwargs):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        # self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.inv_freq = inv_freq
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    softcap: Optional[float] = None,
    **kwargs,
) -> Tuple[torch.Tensor, torch.Tensor]:
    if scaling is None:
        scaling = module.head_dim**-0.5

    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)
    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if softcap is not None:
        attn_weights = attn_weights / softcap
        attn_weights = torch.tanh(attn_weights)
        attn_weights = attn_weights * softcap
    if attention_mask is not None:  # no matter the length, we just slice it
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask
    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)
    combined_logits = torch.cat([attn_weights, sinks], dim=-1)
    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values
    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)
    scores = probs[..., :-1]  # we drop the sink here
    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output, attn_weights

def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights

class SPECTRAAttention(nn.Module):
    """
    [DEPRECATED - Use Exoskeleton Architecture]

    This is a Gemma3-specific attention implementation with Q/K normalization,
    sliding window attention, and logit softcapping. For universal model support,
    use SPECTRAExoskeletonMoEInjector which preserves the base model's native
    attention mechanism without reimplementation.

    WARNING: This class will be removed in a future version. Migrate to Exoskeleton.

    The Exoskeleton architecture:
    - Preserves base model attention (FlashAttention, SDPA, eager, flex_attn)
    - Preserves base model RoPE/positional encoding variants
    - Only replaces MoE routing while keeping attention intact
    - Ensures compatibility with all major transformer architectures:
      Qwen3-VL-MoE, Llama4, DeepSeek-V3, GLM4V-MoE, GPT-OSS, Ernie4.5-MoE
    """
    def __init__(self, config: SPECTRATextConfig, layer_idx: int, **kwargs):
        super().__init__()
        self.is_sliding = bool((layer_idx + 1) % config.sliding_window_pattern)
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = config.query_pre_attn_scalar**-0.5
        self.attention_dropout = self.config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.attn_logit_softcapping = self.config.attn_logit_softcapping
        self.sliding_window = config.sliding_window if self.is_sliding else None

        self.q_norm = SPECTRARMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)
        self.k_norm = SPECTRARMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:
        # [ZeRO-3 Fix] Explicitly gather all parameters (including biases)
        # This resolves the 'size mismatch ... vec (0)' error where bias was not gathered.
        import deepspeed
        with deepspeed.zero.GatheredParameters(list(self.parameters()), modifier_rank=None):
            return self._forward_impl(
                hidden_states,
                position_embeddings,
                attention_mask,
                past_key_value,
                output_attentions,
                use_cache,
                cache_position,
                **kwargs
            )

    def _forward_impl(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False, # Added default value
        use_cache: bool = False, # Added default value
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        key_states   = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        query_states = self.q_norm(query_states)
        key_states   = self.k_norm(key_states)

        cos, sin = None, None
        if position_embeddings is not None:
            cos, sin = position_embeddings
            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
        # else: NoPE, ê·¸ëŒ€ë¡œ ì‚¬ìš©

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {
                "sin": sin,
                "cos": cos,
                "cache_position": cache_position,
                "sliding_window": self.sliding_window,
            }
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

            # Here we need to slice as we use a static cache by default, but FA2 does not support it
            # if attention_mask is not None and self.config.attn_implementation == "flash_attention_2":
            #     if hasattr(past_key_value, "get_seq_length"):
            #         seq_len = past_key_value.get_seq_length()
            #     else:
            #         seq_len = key_states.shape[-1]
            #     key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=self.attention_dropout if self.training else 0.0,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            output_attentions=output_attentions,
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights, past_key_value


class SPECTRADecoderLayer(GradientCheckpointingLayer):
    def __init__(
        self,
        config: SPECTRATextConfig,
        layer_idx: int,
        global_router: SPECTRARouter, **kwargs
    ):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx
        self.attention_type = config.layer_types[layer_idx]
        self.self_attn = SPECTRAAttention(config=config, layer_idx=layer_idx, **kwargs)
        self.is_dense_replacement = layer_idx >= config.first_k_dense_replace
        expert_intermediate_size = getattr(config, "moe_intermediate_size", 768)
        if self.is_dense_replacement:
            self.moe = SPECTRAMoE(config=config, global_router=global_router)
        else:
            self.moe = SPECTRAMLP(config=config, intermediate_size=expert_intermediate_size)
        self.input_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.pre_feedforward_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_feedforward_layernorm = SPECTRARMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.is_sliding = self.self_attn.is_sliding
        self.sliding_window = config.sliding_window
        self.use_nope = (hasattr(config, 'no_rope_layers') and bool(config.no_rope_layers[self.layer_idx]))

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings_global: torch.Tensor,
        position_embeddings_local: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        global_routing_hn: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # í•˜ì´ë¸Œë¦¬ë“œ rope-nope positional embedding ì ìš©
        if self.use_nope:
            # NoPE: position embedding ì—†ì´ self-attn
            position_embeddings = None
        else:
            # ê¸°ì¡´ ë°©ì‹: RoPE
            position_embeddings = position_embeddings_local if self.self_attn.is_sliding else position_embeddings_global
        hidden_states, self_attn_weights, past_key_value = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = self.pre_feedforward_layernorm(hidden_states)
        if self.layer_idx >= self.config.first_k_dense_replace:
             # Case: MoE Layer
             moe_output = self.moe(hidden_states, global_routing_hn)
             if isinstance(moe_output, tuple) and len(moe_output) == 2:
                hidden_states, routing_info = moe_output
                (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss,
                 expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss,
                 sinkhorn_loss, ortho_loss, balance_loss) = routing_info
             else:
                # Fallback to direct unpacking if tuple structure is different
                hidden_states, (router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss,
                               expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss,
                               sinkhorn_loss, ortho_loss, balance_loss) = moe_output
        else:
            # Case: Dense MLP Layer (No Routing)
            hidden_states = self.moe(hidden_states)
            router_logits = hn = speciality_loss = cosine_similarities = contrastive_loss = \
            expression_reg_loss = routing_uncertainty = entropy_loss = load_balancing_loss = \
            sinkhorn_loss = ortho_loss = balance_loss = None

        hidden_states = self.post_feedforward_layernorm(hidden_states)
        hidden_states = residual + hidden_states
        outputs = (hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)
        outputs += ((router_logits, hn, speciality_loss, cosine_similarities, contrastive_loss, expression_reg_loss, routing_uncertainty, entropy_loss, load_balancing_loss, sinkhorn_loss, ortho_loss, balance_loss),)
        return outputs


spectra_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`SPECTRAConfig`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""

@auto_docstring
class SPECTRAPreTrainedModel(PreTrainedModel):
    config: SPECTRAConfig
    base_model_prefix = ""
    supports_gradient_checkpointing = True
    _no_split_modules = [
        "SPECTRADecoderLayer",
        "SiglipVisionEmbeddings",
        "SiglipEncoderLayer",
        "SiglipMultiheadAttentionPoolingHead",
    ]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": SPECTRADecoderLayer,
        "attentions": SPECTRAAttention
    }

    def _initialize_moe_router_and_temperature(self) -> None:
        """Initialize only MoE router weights and routing temperature if they were not loaded from a checkpoint.

        - Router linear weights: Xavier uniform for stable logits
        - Routing temperature: ones (softplus(1) ~ 1.313) keeps scale reasonable
        """
        with torch.no_grad():
            for module in self.modules():
                # Initialize router linears ONLY if not already initialized/loaded
                router = getattr(module, "router", None)
                if isinstance(router, nn.Linear):
                    already_init = getattr(router, "_is_hf_initialized", False)
                    # ZeRO-3 compatibility: Skip init if weights are partitioned (1D) or on meta
                    if not already_init and router.weight.dim() > 1 and not router.weight.is_meta:
                        nn.init.xavier_uniform_(router.weight)
                        if router.bias is not None and not router.bias.is_meta:
                            router.bias.zero_()
                # Initialize routing temperature ONLY if looks uninitialized/bad
                routing_temp = getattr(module, "routing_temperature", None)
                if isinstance(routing_temp, nn.Parameter):
                    if not routing_temp.is_meta and (not torch.isfinite(routing_temp).all() or routing_temp.abs().sum() == 0):
                        routing_temp.data.fill_(1.0)

    def _init_weights(self, module):
        # ZeRO-3 compatibility: Skip initialization for sharded (1D) or meta parameters
        # Failure to do so causes ValueError in xavier_uniform_ and other init functions
        if hasattr(module, "weight") and module.weight is not None:
             if module.weight.is_meta or module.weight.dim() < 2:
                 return

        # Only initialize router linears explicitly; skip expert MLPs and other dense layers during fine-tuning
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):
            if getattr(module, "_is_spectra_router", False):
                logging.get_logger('transformers').debug(f"Initializing router layer with Xavier uniform: {module}")
                # ZeRO-3 compatibility: Skip init if weights are partitioned (1D) or on meta
                if module.weight.dim() > 1 and not module.weight.is_meta:
                    nn.init.xavier_uniform_(module.weight)
                if module.bias is not None and not module.bias.is_meta:
                    module.bias.data.zero_()
        elif isinstance(module, ExpressionProjector):
            logging.get_logger('transformers').debug(f"Initializing expression projector layer with Xavier uniform: {module}")
            # ZeRO-3 compatibility: Skip init if weights are partitioned (1D) or on meta
            if module.projection.weight.dim() > 1 and not module.projection.weight.is_meta:
                safe_orthogonal_(module.projection.weight)
        elif isinstance(module, SPECTRAMultiModalProjector):
            # ZeRO-3 compatibility: Skip init if weights are partitioned (1D) or on meta
            if module.mm_input_projection_weight.dim() > 1 and not module.mm_input_projection_weight.is_meta:
                nn.init.xavier_uniform_(module.mm_input_projection_weight)
        elif isinstance(module, nn.GRU):
            for name, param in module.named_parameters():
                # ZeRO-3 compatibility: Skip init if weights are on meta
                if param.is_meta:
                    continue
                if 'weight_ih' in name and param.data.dim() > 1:
                    nn.init.xavier_uniform_(param.data)
                elif 'weight_hh' in name and param.data.dim() > 1:
                    nn.init.xavier_uniform_(param.data)
                elif 'weight_hr' in name and param.data.dim() > 1:
                    nn.init.xavier_uniform_(param.data)
                elif 'bias' in name:
                    param.data.fill_(0)
        else:
            super()._init_weights(module)

    @classmethod
    @restore_default_dtype
    def from_pretrained(
        cls: Type[SpecificPreTrainedModelType],
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,
        cache_dir: Optional[Union[str, os.PathLike]] = None,
        ignore_mismatched_sizes: bool = False,
        force_download: bool = False,
        local_files_only: bool = False,
        token: Optional[Union[str, bool]] = None,
        revision: str = "main",
        use_safetensors: Optional[bool] = None,
        weights_only: bool = True,
        ffn_checkpoint_for_moe_conversion: Optional[Union[str, os.PathLike, dict]] = None,
        **kwargs,
    ) -> SpecificPreTrainedModelType:
        """
        This method loads a pretrained base model using its native architecture
        and then injects SPECTRA's advanced routing system into its decoder layers.
        This ensures that we preserve the original model's high-performance
        Attention, RoPE, and Normalization implementations while gaining
        SPECTRA's routing stability and expert specialization.
        """
        # Resolve configuration
        if config is None:
            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs.get("config_kwargs", {}))

        # Ensure it's a SPECTRAConfig for MoE settings
        if not isinstance(config, (SPECTRAConfig, SPECTRATextConfig)):
            # Convert base config to SPECTRA config while preserving base model parameters
            config_dict = config.to_dict()
            if "text_config" in config_dict:
                # VLM case
                config = SPECTRAConfig(**config_dict)
            else:
                # Text-only case
                config = SPECTRATextConfig(**config_dict)

        # Determine strict text config
        text_config = config.text_config if hasattr(config, "text_config") else config

        # Determine model class (LLM vs VLM) using AutoModel
        from transformers import AutoModelForCausalLM, AutoModelForVision2Seq

        # Heuristic to detect VLM: config has vision_config OR model type contains 'vl'
        is_vlm = hasattr(config, "vision_config") or "vl" in str(pretrained_model_name_or_path).lower()
        model_class = AutoModelForVision2Seq if is_vlm else AutoModelForCausalLM

        if is_vlm:
            logger.debug(f"Detected VLM architecture for {pretrained_model_name_or_path}")
        else:
            logger.debug(f"Detected CausalLM architecture for {pretrained_model_name_or_path}")

        logger.debug(f"Loading Base Model Skeleton using {model_class.__name__}...")

        # 1. Load the base model skeleton and weights using native transformers classes
        # This preserves all optimized low-level CUDA kernels (FlashAttention, FlexAttn, etc.)
        # We explicitly assume 'trust_remote_code=True' for models like Qwen/DeepSeek
        if "trust_remote_code" in kwargs:
            kwargs["trust_remote_code"] = kwargs.get("trust_remote_code", True)
        base_model = model_class.from_pretrained(
            pretrained_model_name_or_path,
            *model_args,
            cache_dir=cache_dir,
            ignore_mismatched_sizes=True, # Essential for swapping components
            force_download=force_download,
            local_files_only=local_files_only,
            token=token,
            revision=revision,
            use_safetensors=use_safetensors,
            weights_only=weights_only,
            **kwargs
        )

        # 2. Universal Exoskeleton Injection
        # We replace the base model's router/gate with SPECTRA's global router mechanism
        force_upcycle = kwargs.get("force_upcycle", False)

        logger.debug("Initializing SPECTRA Global Router...")
        # Initialize SPECTRA Global Router
        # This shared router maintains states (GRU) across layers for hierarchical routing
        global_router = SPECTRARouter(text_config)

        # Create and apply the Exoskeleton Injector
        injector = SPECTRAExoskeletonMoEInjector(
            spectra_config=text_config,
            global_router=global_router,
            preserve_shared_experts=True,
            copy_expert_weights=force_upcycle
        )

        logger.debug("Injecting SPECTRA Routing Mechanism (Exoskeleton)...")
        # Swap existing MoE components or upcycle dense layers
        # This modifies base_model in-place
        base_model = injector.inject(base_model)

        # 3. Explicitly initialize newly injected SPECTRA parameters
        # This ensures Xavier uniform initialization for stable initial routing logits
        logger.debug("Initializing Router Weights...")
        if hasattr(base_model, "_initialize_moe_router_and_temperature"):
            # If it's a SPECTRA-compatible class with the helper method
            base_model._initialize_moe_router_and_temperature()
        else:
            # Manual initialization for injected parameters in arbitrary models
            for module in base_model.modules():
                if hasattr(module, "router") and isinstance(module.router, SPECTRARouter):
                    # SPECTRARouter handles its own internal init, but we check weights
                    pass
                elif getattr(module, "_is_spectra_router", False) and hasattr(module, "weight"):
                    if module.weight.dim() > 1 and not module.weight.is_meta:
                        nn.init.xavier_uniform_(module.weight)
                        if module.bias is not None: module.bias.data.zero_()

        logger.debug(f"SPECTRA Exoskeleton successfully injected into {pretrained_model_name_or_path}")
        return base_model

    @torch.no_grad()
    def _upcycle(self, model):
        processing = tqdm(
            enumerate(model.layers),
            total=len(model.layers),
            desc=f"Copying MLP weights to {self.__class__.__name__} MoE experts: start",
            leave=False)

        for layer_idx, decoder_layer in processing:
            if hasattr(decoder_layer.moe, 'experts') or hasattr(decoder_layer.moe, 'shared_experts'):
                if hasattr(decoder_layer.moe, 'shared_experts'):
                    processing.set_description(f"Copying mlp {layer_idx} â†’ shared experts")
                    decoder_layer.moe.shared_experts.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                    decoder_layer.moe.shared_experts.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                    decoder_layer.moe.shared_experts.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

                for expert_idx, expert in enumerate(decoder_layer.moe.experts):
                    if expert_idx % 2 == 0:
                        processing.set_description(f"Copying mlp {layer_idx} â†’ expert {expert_idx}")
                    expert.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                    expert.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                    expert.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)

            elif hasattr(decoder_layer.moe, 'gate_proj'):
                processing.set_description(f"Copying mlp {layer_idx} â†’ dense MoE")
                decoder_layer.moe.gate_proj.weight.copy_(decoder_layer.mlp.gate_proj.weight)
                decoder_layer.moe.up_proj.weight.copy_(decoder_layer.mlp.up_proj.weight)
                decoder_layer.moe.down_proj.weight.copy_(decoder_layer.mlp.down_proj.weight)
            else:
                raise Exception("MoE model has no MLP or shared MLP")
            del decoder_layer.mlp
        processing.set_description("Copy finished")
        return model

    def get_parameter_groups(self):
        """
        Returns a list of parameter groups for the optimizer, which allows to apply different
        learning rates to different parts of the model. This is particularly useful for MoE models
        where components like routers and experts can benefit from different learning schedules.
        """

        router_params = []
        expert_params = []
        shared_expert_params = []
        attention_params = []
        other_params = []

        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue

            if 'gate.weight' in name or 'router' in name:
                router_params.append(param)
            elif 'shared_experts' in name:
                shared_expert_params.append(param)
            elif 'experts' in name:
                expert_params.append(param)
            elif 'self_attn' in name:
                attention_params.append(param)
            else:
                other_params.append(param)

        # In a training script, you can assign different learning rates to these groups.
        # For example:
        # optimizer_grouped_parameters = [
        #     {'params': model.get_parameter_groups()['router'], 'lr': 1e-4},
        #     {'params': model.get_parameter_groups()['expert'], 'lr': 5e-5},
        #     ...
        # ]
        return {
            'router': router_params,
            'expert': expert_params,
            'shared_expert': shared_expert_params,
            'attention': attention_params,
            'other': other_params,
        }

spectra_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            Two formats are allowed:
            - a [`~cache_utils.Cache`] instance, see our
            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
            cache format.

            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
            legacy cache format will be returned.

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""

@add_start_docstrings(
    "The bare SPECTRAText Model outputting raw hidden-states without any specific head on top.",
    spectra_START_DOCSTRING,
)
class SPECTRATextModel(SPECTRAPreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`SPECTRATextDecoderLayer`]

    Args:
        config: SPECTRATextConfig
    """
    config: SPECTRATextConfig

    def __init__(self, config: SPECTRATextConfig, **kwargs):
        super().__init__(config)
        # Robustly resolve text config without relying on class identity across module boundaries
        if getattr(config, "model_type", None) == "spectra_text" or not hasattr(config, "text_config"):
            self.config = config
        else:
            self.config = config.text_config
        config = self.config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        # Expose a tensor-parallel plan for vLLM on the base text model
        if not hasattr(self, "_tp_plan") or self._tp_plan is None:
            default_tp_plan = {
                "layers.*.self_attn.q_proj": "colwise",
                "layers.*.self_attn.k_proj": "colwise",
                "layers.*.self_attn.v_proj": "colwise",
                "layers.*.self_attn.o_proj": "rowwise",
                "layers.*.moe.experts.*.gate_proj": "colwise",
                "layers.*.moe.experts.*.up_proj": "colwise",
                "layers.*.moe.experts.*.down_proj": "rowwise",
            }
            # allow overriding via config if provided
            self._tp_plan = getattr(config, "base_model_tp_plan", default_tp_plan)

        # Restore SPECTRATextScaledWordEmbedding for structural consistency
        # Fixed to avoid ZeRO-3 meta tensor conflicts by using float scale instead of buffer
        self.embed_tokens = SPECTRATextScaledWordEmbedding(
            config.vocab_size, config.hidden_size, self.padding_idx, embed_scale=self.config.hidden_size**0.5
        )
        self.global_router = SPECTRARouter(config)
        self.layers = nn.ModuleList(
            [SPECTRADecoderLayer(config, layer_idx, self.global_router, **kwargs) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = SPECTRARMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = SPECTRARotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        self.router_aux_loss_coef = config.router_aux_loss_coef
        # TODO: raushan fix this after RoPE refactor. For now we hack it by reassigning thetas
        # when we want to create a local RoPE layer. Config defaults should hold values for global RoPE
        config = copy.deepcopy(config)
        config.rope_theta = config.rope_local_base_freq
        config.rope_scaling = config.rope_scaling if config.rope_scaling is not None else {"rope_type":  "default"}
        self.rotary_emb_local = SPECTRARotaryEmbedding(config=config)
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    @classmethod
    def from_config(cls, **kwargs):
        return cls._from_config(**kwargs)

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> SPECTRACausalLMOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        # NEFTune implementation
        if self.training:
            neftune_noise_alpha = getattr(self.config, "neftune_noise_alpha", 0.0)
            if neftune_noise_alpha > 0.0:
                dims = torch.tensor(inputs_embeds.size(1) * inputs_embeds.size(2), device=inputs_embeds.device)
                mag_norm = neftune_noise_alpha / torch.sqrt(dims)
                inputs_embeds = inputs_embeds + torch.zeros_like(inputs_embeds).uniform_(-mag_norm, mag_norm)


        if use_cache and past_key_values is None and not self.training:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens,
                past_seen_tokens + inputs_embeds.shape[1],
                device=inputs_embeds.device,
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        # embed positions
        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings_global = self.rotary_emb(hidden_states, position_ids)
        position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None
        all_router_logits = []  # ëª¨ë“  MoE ë ˆì´ì–´ì˜ router_logitsë¥¼ ìˆ˜ì§‘
        global_routing_hn = None

        # ê° layerì˜ lossë¥¼ ëˆ„ì í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        all_speciality_losses = []
        all_cosine_similarities = []
        all_contrastive_losses = []

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            if output_hidden_states:
                all_hidden_states += (hidden_states,)

            layer_outputs = decoder_layer(
                hidden_states,
                position_embeddings_global=position_embeddings_global,
                position_embeddings_local=position_embeddings_local,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_value=past_key_values,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                global_routing_hn=global_routing_hn,
                **flash_attn_kwargs,
            )
            hidden_states = layer_outputs[0]
            routing_result = layer_outputs[-1]
            if routing_result is not None:
                router_logits = routing_result[0]
                if router_logits is not None:
                    all_router_logits.append(router_logits)
                global_routing_hn = routing_result[1]

                layer_speciality_loss = routing_result[2]
                layer_cosine_similarities = routing_result[3]
                layer_contrastive_loss = routing_result[4]
                layer_expression_reg_loss = routing_result[5] if len(routing_result) > 5 else None
                layer_routing_uncertainty = routing_result[6] if len(routing_result) > 6 else None
                layer_entropy_loss = routing_result[7] if len(routing_result) > 7 else None
                layer_load_balancing_loss = routing_result[8] if len(routing_result) > 8 else None
                layer_sinkhorn_loss = routing_result[9] if len(routing_result) > 9 else None
                layer_ortho_loss = routing_result[10] if len(routing_result) > 10 else None
                layer_balance_loss = routing_result[11] if len(routing_result) > 11 else None

                if layer_speciality_loss is not None:
                    all_speciality_losses.append(layer_speciality_loss)
                if layer_cosine_similarities is not None:
                    all_cosine_similarities.append(layer_cosine_similarities)
                if layer_contrastive_loss is not None:
                    all_contrastive_losses.append(layer_contrastive_loss)
                if layer_expression_reg_loss is not None:
                    if not hasattr(self, 'all_expression_reg_losses'):
                        self.all_expression_reg_losses = []
                    self.all_expression_reg_losses.append(layer_expression_reg_loss)
                if layer_routing_uncertainty is not None:
                    if not hasattr(self, 'all_routing_uncertainties'):
                        self.all_routing_uncertainties = []
                    self.all_routing_uncertainties.append(layer_routing_uncertainty)
                if layer_entropy_loss is not None:
                    if not hasattr(self, 'all_entropy_losses'):
                        self.all_entropy_losses = []
                    self.all_entropy_losses.append(layer_entropy_loss)
                if layer_load_balancing_loss is not None:
                    if not hasattr(self, 'all_load_balancing_losses'):
                        self.all_load_balancing_losses = []
                    self.all_load_balancing_losses.append(layer_load_balancing_loss)
                if layer_sinkhorn_loss is not None:
                    if not hasattr(self, 'all_sinkhorn_losses'):
                        self.all_sinkhorn_losses = []
                    self.all_sinkhorn_losses.append(layer_sinkhorn_loss)
                if layer_ortho_loss is not None:
                    if not hasattr(self, 'all_ortho_losses'):
                        self.all_ortho_losses = []
                    self.all_ortho_losses.append(layer_ortho_loss)
                if layer_balance_loss is not None:
                    if not hasattr(self, 'all_balance_losses'):
                        self.all_balance_losses = []
                    self.all_balance_losses.append(layer_balance_loss)

            if output_attentions:
                all_self_attns += (layer_outputs[1],)

        # [Gradient Checkpointing] Deferred bias updates â€” expert_bias was constant during forward/recompute.
        if self.training and hasattr(self, 'global_router'):
            self.global_router.flush_bias_updates()

        hidden_states = self.norm(hidden_states)

        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        # router_logitsë¥¼ íŠœí”Œë¡œ ë³€í™˜ (ë¹„ì–´ìˆìœ¼ë©´ None)
        router_logits_tuple = tuple(all_router_logits) if all_router_logits else None

        # ëª¨ë“  layerì˜ lossë¥¼ ì§‘ê³„ (gradient ìœ ì§€)
        speciality_loss = None
        cosine_similarities = None
        contrastive_loss = None
        expression_reg_loss = None

        # expression_reg_loss ì§‘ê³„
        if hasattr(self, 'all_expression_reg_losses') and self.all_expression_reg_losses:
            stacked = torch.stack(self.all_expression_reg_losses)
            expression_reg_loss = stacked.mean()
            # Removed redundant requires_grad_ call
            expression_reg_loss = expression_reg_loss
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_expression_reg_losses = []

        if all_speciality_losses:
            # speciality_lossëŠ” í‰ê·  (ìŠ¤ì¹¼ë¼ ê°’ë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            stacked = torch.stack(all_speciality_losses)
            speciality_loss = stacked.mean()
            # gradient ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
            # Removed redundant requires_grad_ call
            speciality_loss = speciality_loss

        if all_cosine_similarities:
            # cosine_similaritiesëŠ” í‰ê·  (í…ì„œë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            # ëª¨ë“  í…ì„œê°€ ë™ì¼í•œ shapeì¸ì§€ í™•ì¸
            try:
                stacked = torch.stack(all_cosine_similarities)
                cosine_similarities = stacked.mean(dim=0)
                # gradient ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
                # Removed redundant requires_grad_ call
                cosine_similarities = cosine_similarities
            except RuntimeError as e:
                # Shapeì´ ë‹¤ë¥¸ ê²½ìš° ê°ê° í‰ê· ì„ ë‚´ê³  ë‹¤ì‹œ í‰ê· 
                if "size" in str(e).lower() or "shape" in str(e).lower():
                    # ê° í…ì„œì˜ í‰ê· ì„ êµ¬í•œ í›„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
                    means = [cs.mean() if torch.is_tensor(cs) and cs.numel() > 0 else torch.tensor(0.0, device=all_cosine_similarities[0].device, requires_grad=False)
                            for cs in all_cosine_similarities if cs is not None]
                    if means:
                        cosine_similarities = torch.stack(means).mean()
                        # Removed redundant requires_grad_ call
                        cosine_similarities = cosine_similarities
                else:
                    raise

        if all_contrastive_losses:
            # contrastive_lossëŠ” í‰ê·  (ìŠ¤ì¹¼ë¼ ê°’ë“¤ì˜ í‰ê· ) - gradient ìœ ì§€
            stacked = torch.stack(all_contrastive_losses)
            contrastive_loss = stacked.mean()
            # Removed redundant requires_grad_ call
            contrastive_loss = contrastive_loss

        # routing_uncertainty ì§‘ê³„
        routing_uncertainty = None
        if hasattr(self, 'all_routing_uncertainties') and self.all_routing_uncertainties:
            # routing_uncertaintyëŠ” í‰ê·  (í…ì„œë“¤ì˜ í‰ê· )
            try:
                stacked = torch.stack(self.all_routing_uncertainties)
                routing_uncertainty = stacked.mean(dim=0)
                # Removed redundant requires_grad_ call
                routing_uncertainty = routing_uncertainty
            except RuntimeError:
                # Shapeì´ ë‹¤ë¥¸ ê²½ìš° ê°ê° í‰ê· ì„ ë‚´ê³  ë‹¤ì‹œ í‰ê· 
                means = [ru.mean() if torch.is_tensor(ru) and ru.numel() > 0 else torch.tensor(0.0, device=self.all_routing_uncertainties[0].device, requires_grad=False)
                        for ru in self.all_routing_uncertainties if ru is not None]
                if means:
                    routing_uncertainty = torch.stack(means).mean()
                # Removed redundant requires_grad_ call
                routing_uncertainty = routing_uncertainty
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_routing_uncertainties = []

        # entropy_loss ì§‘ê³„ (CV ê°ì†Œë¥¼ ìœ„í•œ gradient ìˆëŠ” loss)
        entropy_loss = None
        if hasattr(self, 'all_entropy_losses') and self.all_entropy_losses:
            stacked = torch.stack(self.all_entropy_losses)
            entropy_loss = stacked.mean()
            # Removed redundant requires_grad_ call
            entropy_loss = entropy_loss
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_entropy_losses = []

        # load_balancing_loss ì§‘ê³„ (CV ê°ì†Œë¥¼ ìœ„í•œ gradient ìˆëŠ” loss)
        load_balancing_loss = None
        if hasattr(self, 'all_load_balancing_losses') and self.all_load_balancing_losses:
            stacked = torch.stack(self.all_load_balancing_losses)
            load_balancing_loss = stacked.mean()
            # Removed redundant requires_grad_ call
            load_balancing_loss = load_balancing_loss
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_load_balancing_losses = []

        # sinkhorn_loss ì§‘ê³„ (SpecHorn-G: GRUê°€ Sinkhornì„ í•™ìŠµí•˜ë„ë¡ í•˜ëŠ” loss)
        sinkhorn_loss = None
        if hasattr(self, 'all_sinkhorn_losses') and self.all_sinkhorn_losses:
            stacked = torch.stack(self.all_sinkhorn_losses)
            sinkhorn_loss = stacked.mean()
            # Removed redundant requires_grad_ call
            sinkhorn_loss = sinkhorn_loss
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_sinkhorn_losses = []

        # ortho_loss ì§‘ê³„ (ì „ë¬¸ê°€ë“¤ì˜ ê°€ì¤‘ì¹˜ ì§êµì„± Loss)
        ortho_loss = None
        if hasattr(self, 'all_ortho_losses') and self.all_ortho_losses:
            stacked = torch.stack(self.all_ortho_losses)
            ortho_loss = stacked.mean()
            # Removed redundant requires_grad_ call
            ortho_loss = ortho_loss
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_ortho_losses = []

        # balance_loss ì§‘ê³„ (GRU Solverì˜ constraint violation loss)
        balance_loss = None
        if hasattr(self, 'all_balance_losses') and self.all_balance_losses:
            stacked = torch.stack(self.all_balance_losses)
            balance_loss = stacked.mean()
            # Removed redundant requires_grad_ call
            balance_loss = balance_loss
            # ë‹¤ìŒ forwardë¥¼ ìœ„í•´ ì´ˆê¸°í™”
            self.all_balance_losses = []

        return SPECTRAModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
            router_logits=router_logits_tuple,
            speciality_loss=speciality_loss,
            cosine_similarities=cosine_similarities,
            contrastive_loss=contrastive_loss,
            expression_reg_loss=expression_reg_loss,
            routing_uncertainty=routing_uncertainty,
            entropy_loss=entropy_loss,
            load_balancing_loss=load_balancing_loss,
            sinkhorn_loss=sinkhorn_loss,
            ortho_loss=ortho_loss,
            balance_loss=balance_loss,
        )


@auto_docstring
class SPECTRAForCausalLM(SPECTRAPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}
    config: SPECTRAConfig
    base_model_prefix = "language_model"

    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)

    def __init__(self, config: SPECTRATextConfig, **kwargs):
        super().__init__(config)
        self.model = SPECTRATextModel(config, **kwargs)
        # Ensure config refers to resolved text config from the submodule
        self.config = self.model.config
        self.vocab_size = self.config.vocab_size
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        pixel_values: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **loss_kwargs,
    ) -> CausalLMOutputWithPast:
        r"""
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

            logits_to_keep (`int` or `torch.Tensor`, *optional*):
                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
                This is useful when using packed tensor format (single dimension for batch and sequence length).

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, SPECTRAForCausalLM

        >>> model = SPECTRAForCausalLM.from_pretrained("google/gemma-2-9b")
        >>> tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b")

        >>> prompt = "What is your favorite condiment?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "What is your favorite condiment?"
        ```"""
        if self.config._attn_implementation is None:
            self.config._attn_implementation = "eager"
            logger.warning_once(
                "No attention implementation is specified, using `eager` as default."
            )
        if self.training and self.config.attn_implementation != "eager":
            logger.warning_once(
                "It is strongly recommended to train SPECTRA models with the `eager` attention implementation "
                f"instead of `{self.config.attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`."
            )
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            cache_position=cache_position,
            **loss_kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping

        loss = None
        aux_loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)

            # Speciality loss: Output orthogonality (encourages diverse expert outputs)
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš© (ì´ì¤‘ ìŠ¤ì¼€ì¼ë§ ë°©ì§€)
            speciality_loss_coef = getattr(self.model.config, "speciality_loss_coef", 1.0)  # 0.02 -> 1.0
            if outputs.speciality_loss is not None and speciality_loss_coef > 0:
                loss += outputs.speciality_loss * speciality_loss_coef

            # Contrastive loss: Input clustering (encourages experts to process distinct token types)
            # [ìˆ˜ì •] Routerê°€ ì´ë¯¸ Adaptive Weightë¥¼ ì ìš©í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 1.0ì„ ì‚¬ìš©
            contrastive_loss_coef = getattr(self.model.config, "contrastive_loss_coef", 1.0)  # 0.01 -> 1.0
            if outputs.contrastive_loss is not None and contrastive_loss_coef > 0:
                loss += outputs.contrastive_loss * contrastive_loss_coef

            # Expression projector regularization loss: Direct connection to expression_logits for gradient flow
            # This ensures expression_projector parameters receive gradients
            expression_reg_loss_coef = getattr(self.model.config, "expression_reg_loss_coef", 1.0)
            if outputs.expression_reg_loss is not None and expression_reg_loss_coef > 0:
                loss += outputs.expression_reg_loss * expression_reg_loss_coef

            # Expression projector loss: Ensure expression_logits contributes to loss for gradient flow
            # cosine_similarities (domain_orthogonality)ë¥¼ lossì— ì¶”ê°€í•˜ì—¬ expression_projectorê°€ í•™ìŠµë˜ë„ë¡ í•¨
            cosine_similarities_loss_coef = getattr(self.model.config, "cosine_similarities_loss_coef", 0.001)
            if outputs.cosine_similarities is not None and cosine_similarities_loss_coef > 0:
                # cosine_similaritiesëŠ” [batch, seq, num_experts] í˜•íƒœì˜ í…ì„œ ë˜ëŠ” ìŠ¤ì¹¼ë¼
                if torch.is_tensor(outputs.cosine_similarities) and outputs.cosine_similarities.numel() > 0:
                    # í…ì„œì¸ ê²½ìš° mean squared valueë¥¼ ìµœì†Œí™”í•˜ì—¬ expression diversityë¥¼ ìœ ì§€
                    expr_loss = torch.mean(outputs.cosine_similarities ** 2) * cosine_similarities_loss_coef
                    loss += expr_loss
                elif isinstance(outputs.cosine_similarities, (int, float)):
                    # ìŠ¤ì¹¼ë¼ì¸ ê²½ìš° ì§ì ‘ ì‚¬ìš©
                    expr_loss = outputs.cosine_similarities * cosine_similarities_loss_coef
                    loss += expr_loss

            # ======================================================================================
            # [Minimalist Loss: Sinkhorn + Sharpening]
            # Sinkhornì€ êµ¬ì¡°ì ìœ¼ë¡œ ì´ë¯¸ ë¶€í•˜ ë¶„ì‚°ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ë³„ë„ loss ë¶ˆí•„ìš”
            # Sharpeningë§Œ entropy minimizationìœ¼ë¡œ ì²˜ë¦¬
            # ======================================================================================

            # [Sharpening] Entropy Minimization: "í•œ ë†ˆë§Œ íŒ¨ë¼" (í™•ì‹¤í•œ ì „ë¬¸ê°€ ì„ íƒ)
            # router_entropy_coefëŠ” ì–‘ìˆ˜ë¡œ ì‚¬ìš© (entropyë¥¼ ë‚®ì¶”ëŠ” ë°©í–¥)
            router_entropy_coef = getattr(self.model.config, "router_entropy_coef", 0.1)
            if outputs.entropy_loss is not None and router_entropy_coef > 0:
                loss += outputs.entropy_loss * router_entropy_coef

            # [Optional] Ortho Loss: ë³´í—˜ìœ¼ë¡œ ì•½í•˜ê²Œ ìœ ì§€ (í•™ìŠµ ì´ˆë°˜ ê°€ì´ë“œ)
            # Sinkhorn + Sharpeningë§Œìœ¼ë¡œë„ ë¶„ë¦¬ê°€ ë˜ì§€ë§Œ, ì´ˆë°˜ í—¤ë§¤ì§€ ì•Šë„ë¡ ë„ì›€
            ortho_loss_coef = getattr(self.model.config, "ortho_loss_coef", 0.01)  # 0.05 -> 0.01 (ì•½í•˜ê²Œ)
            if outputs.ortho_loss is not None and ortho_loss_coef > 0:
                loss += outputs.ortho_loss * ortho_loss_coef

            # ======================================================================================
            # [ì œê±°ëœ Lossë“¤]
            # - gslb_coef: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - router_z_loss_coef: ë¶ˆí•„ìš”
            # - sinkhorn_distillation_coef: Sharpeningì´ ëŒ€ì‹ í•¨
            # - usage_uniformity_coef: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - load_balancing_loss: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # - balance_loss: Sinkhornì´ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬
            # ======================================================================================

        try:
            import torch.distributed as dist
            is_main_proc = (not dist.is_available()) or (not dist.is_initialized()) or dist.get_rank() == 0
        except Exception:
            is_main_proc = True


        return SPECTRACausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            aux_loss=aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            ortho_loss=outputs.ortho_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
            expression_reg_loss=outputs.expression_reg_loss,
            entropy_loss=outputs.entropy_loss,
            load_balancing_loss=outputs.load_balancing_loss,
            sinkhorn_loss=outputs.sinkhorn_loss
        )


class SPECTRAMultiModalProjector(nn.Module):
    def __init__(self, config: SPECTRAConfig, **kwargs):
        super().__init__()

        vision_hidden_size = getattr(config.vision_config, 'hidden_size', 0) or 0
        # [Fix] Force SigLIP (768) if config is missing or reports incorrect size (4352)
        if vision_hidden_size == 0 or vision_hidden_size == 4352:
            vision_hidden_size = 768

        self.mm_input_projection_weight = nn.Parameter(
            torch.zeros(vision_hidden_size, config.text_config.hidden_size)
        )

        self.mm_soft_emb_norm = SPECTRARMSNorm(
            vision_hidden_size, eps=config.vision_config.layer_norm_eps
        )

        self.patches_per_image = int(config.vision_config.image_size // config.vision_config.patch_size)
        self.tokens_per_side = int(config.mm_tokens_per_image**0.5)
        # [Fix] Use Adaptive Pooling to handle variable input resolutions from Vision Tower
        self.avg_pool = nn.AdaptiveAvgPool2d((self.tokens_per_side, self.tokens_per_side))

    def forward(self, vision_outputs: torch.Tensor):
        # [Fix] Dynamic shape unpacking: (Batch, SeqLen, Hidden)
        # Note: Previous code mixed up variable names, corrected here.
        batch_size, seq_len, hidden_size = vision_outputs.shape

        # [Fix] Calculate dynamic spatial dimension (H=W=sqrt(SeqLen))
        side_dim = int(seq_len ** 0.5)

        reshaped_vision_outputs = vision_outputs.transpose(1, 2)
        reshaped_vision_outputs = reshaped_vision_outputs.reshape(
            batch_size, hidden_size, side_dim, side_dim
        )
        reshaped_vision_outputs = reshaped_vision_outputs.contiguous()

        pooled_vision_outputs = self.avg_pool(reshaped_vision_outputs)
        pooled_vision_outputs = pooled_vision_outputs.flatten(2)
        pooled_vision_outputs = pooled_vision_outputs.transpose(1, 2)

        # [ZeRO-3 Fix] Explicitly gather parameters to ensure they are available
        import deepspeed
        params_to_gather = [self.mm_input_projection_weight, self.mm_soft_emb_norm.weight]
        with deepspeed.zero.GatheredParameters(params_to_gather, modifier_rank=None):
            normed_vision_outputs = self.mm_soft_emb_norm(pooled_vision_outputs)
            projected_vision_outputs = torch.matmul(normed_vision_outputs, self.mm_input_projection_weight)

        return projected_vision_outputs.type_as(vision_outputs)


def token_type_ids_mask_function(
    token_type_ids: Optional[torch.Tensor],
    image_group_ids: Optional[torch.Tensor],
    tokens_per_image: int,
) -> Optional[Callable]:
    """
    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,
    not start and end indices.
    """
    # Do not return an additional mask in this case
    if token_type_ids is None:
        return None

    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:
        # If it's 1 for both query and key/value, we are in an image block
        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length
        # Since vmap doesn't support `if statement` we workaround it with `torch.where`
        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)
        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]
        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)

        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]
        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)

        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)
        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx

        # This is bidirectional attention whenever we are dealing with image tokens
        return is_image_block & same_image_block

    return inner_mask


@auto_docstring(
    custom_intro="""
    The Base SPECTRA model which consists of a vision backbone and a language model withou language modeling head.,
    """
)
class SPECTRAModel(SPECTRAPreTrainedModel):
    config: SPECTRAConfig
    _checkpoint_conversion_mapping = {"language_model.model": "language_model"}
    # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch
    accepts_loss_kwargs = False

    def __init__(self, config: SPECTRAConfig, vision_tower=None, language_model=None):
        super().__init__(config)
        self.visual = vision_tower if vision_tower is not None else AutoModel.from_config(config=config.vision_config, trust_remote_code=True)
        self.language_model = language_model if language_model is not None else SPECTRATextModel.from_config(config=config.text_config, trust_remote_code=True)
        self.multi_modal_projector = SPECTRAMultiModalProjector(config=config)
        self.vocab_size = config.text_config.vocab_size
        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1
        self.post_init()

    def get_input_embeddings(self):
        return self.language_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.language_model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.language_model = decoder

    def get_decoder(self):
        return self.language_model

    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:
        """
        Projects the last hidden state from the vision model into language model space.

        Args:
            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)
               The tensors corresponding to the input images.
        Returns:
            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
        """
        # [ZeRO-3 Fix] Ensure vision tower runs in no_grad to prevent backward pass from
        # attempting to traverse into frozen/partitioned vision parameters.

        # [Shape Fix] Handle 5D inputs (Batch, Time, Channel, Height, Width) for SigLIP/CLIP compatibility
        # Qwen models often pass 5D tensors for video/multi-image support
        if pixel_values.ndim == 5:
            b, t, c, h, w = pixel_values.shape
            pixel_values = pixel_values.view(b * t, c, h, w)

        with torch.no_grad():
            vision_outputs = self.visual(pixel_values=pixel_values, interpolate_pos_encoding=True).last_hidden_state

        # Projector should likely be trainable, so we apply it OUTSIDE no_grad?
        # Actually in Qwen3-VL, projector is usually part of the training.
        # But here 'multi_modal_projector' is separate.
        # If we include it in no_grad, we freeze the projector too!
        # So we split the operation.

        image_features = self.multi_modal_projector(vision_outputs)
        return image_features

    def get_placeholder_mask(
        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor
    ):
        """
        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is
        equal to the length of multimodal features. If the lengths are different, an error is raised.
        """
        if input_ids is None:
            special_image_mask = inputs_embeds == self.get_input_embeddings()(
                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)
            )
            special_image_mask = special_image_mask.all(-1)
        else:
            special_image_mask = input_ids == self.config.image_token_id

        n_image_tokens = special_image_mask.sum()
        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)
        n_image_features = image_features.shape[0] * image_features.shape[1]
        if inputs_embeds[special_image_mask].numel() != image_features.numel():
            raise ValueError(
                f"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}"
            )
        return special_image_mask

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **lm_kwargs,
    ) -> Union[tuple, SPECTRAModelOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, SPECTRAForConditionalGeneration

        >>> model = SPECTRAForConditionalGeneration.from_pretrained("google/gemma32-3b-mix-224")
        >>> processor = AutoProcessor.from_pretrained("google/gemma32-3b-mix-224")

        >>> prompt = "Where is the cat standing?"
        >>> url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> inputs = processor(images=image, text=prompt,  return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(**inputs,)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Where is the cat standing?\nsnow"
        ```"""
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # Replace image id with PAD if the image token if OOV, to avoid index-errors
        if input_ids is not None and self.config.image_token_id >= self.vocab_size:
            special_image_mask = input_ids == self.config.image_token_id
            llm_input_ids = input_ids.clone()
            llm_input_ids[special_image_mask] = 0
        else:
            llm_input_ids = input_ids

        if inputs_embeds is None:
            inputs_embeds = self.get_input_embeddings()(llm_input_ids)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        # Merge text and images
        if pixel_values is not None:
            # [Deadlock Prevention] Always run get_image_features when pixel_values exists so
            # all ranks participate in GatheredParameters (ZeRO-3). Divergent code paths
            # (e.g. only some ranks calling get_image_features) cause collective mismatch â†’ deadlock.
            image_features = self.get_image_features(pixel_values)
            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)
            has_image_tokens = (input_ids == self.config.image_token_id).any()
            if has_image_tokens:
                special_image_mask = self.get_placeholder_mask(
                    input_ids, inputs_embeds=inputs_embeds, image_features=image_features
                )
                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)
        else:
            image_features = None

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config.get_text_config(),
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            if token_type_ids is not None and inputs_embeds.shape[1] != 1:
                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

                # First find where a new image block starts: 1 if image and previous not image
                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
                is_image = (token_type_ids == 1).to(cache_position.device)
                new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
                image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
                image_group_ids = torch.where(
                    is_image, image_group_ids, torch.full_like(token_type_ids, -1, device=is_image.device)
                )
                mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                    token_type_ids.to(cache_position.device), image_group_ids, self.config.mm_tokens_per_image
                )

            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
                "sliding_attention": create_sliding_window_causal_mask(**mask_kwargs),
            }

        outputs = self.language_model(
            attention_mask=causal_mask_mapping,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True,
            cache_position=cache_position,
            **lm_kwargs,
        )

        return SPECTRAModelOutputWithPast(
            last_hidden_state=outputs.last_hidden_state,
            past_key_values=outputs.past_key_values if use_cache else None,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=image_features,
            aux_loss=outputs.aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
        )


@add_start_docstrings(
    """The SPECTRA model which consists of a vision backbone and a language model.""",
    spectra_START_DOCSTRING,
)
class SPECTRAForConditionalGeneration(SPECTRAPreTrainedModel, GenerationMixin):
    _checkpoint_conversion_mapping = {
        "^language_model.model": "model.language_model",
        "^visual": "model.visual",
        "^multi_modal_projector": "model.multi_modal_projector",
        "^language_model.lm_head": "lm_head",
        # MoE key mapping: Qwen uses "mlp", SPECTRA uses "moe"
        r"\.mlp\.": ".moe.",
        # LayerNorm key mapping: Qwen uses "post_attention_layernorm", SPECTRA uses "post_feedforward_layernorm"
        r"\.post_attention_layernorm": ".post_feedforward_layernorm",
    }
    _tied_weights_keys = ["lm_head.weight"]

    def save_pretrained(self, save_directory, safe_serialization=None, **kwargs):
        """Override to handle shared router parameters"""
        # Default to False if not specified to avoid shared tensor issues
        if safe_serialization is None:
            safe_serialization = False
        return super().save_pretrained(save_directory, safe_serialization=safe_serialization, **kwargs)

    def __init__(self, config: SPECTRAConfig, vision_tower=None, **kwargs):
        super().__init__(config)
        self.model = SPECTRAModel(config, vision_tower=vision_tower)
        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    def get_image_features(self, pixel_values):
        return self.model.get_image_features(pixel_values)

    # Make modules available through conditional class for BC
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def vision_tower(self):
        return self.model.visual

    @property
    def visual(self):
        return self.model.visual

    @property
    def multi_modal_projector(self):
        return self.model.multi_modal_projector

    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        pixel_values: torch.FloatTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,
        token_type_ids: Optional[torch.LongTensor] = None,
        cache_position: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **lm_kwargs,
    ) -> Union[Tuple, SPECTRACausalLMOutputWithPast]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.

        Example:

        ```python
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, SPECTRAForConditionalGeneration

        >>> model = SPECTRAForConditionalGeneration.from_pretrained("google/gemma-3-4b-it")
        >>> processor = AutoProcessor.from_pretrained("google/gemma-3-4b-it")

        >>> messages = [
        ...     {
        ...         "role": "system",
        ...         "content": [
        ...             {"type": "text", "text": "You are a helpful assistant."}
        ...         ]
        ...     },
        ...     {
        ...         "role": "user", "content": [
        ...             {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"},
        ...             {"type": "text", "text": "Where is the cat standing?"},
        ...         ]
        ...     },
        ... ]

        >>> inputs = processor.apply_chat_template(
        ...     messages,
        ...     tokenizer=True,
        ...     return_dict=True,
        ...     return_tensors="pt",
        ...     add_generation_prompt=True
        ... )
        >>> # Generate
        >>> generate_ids = model.generate(**inputs)
        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "user\nYou are a helpful assistant.\n\n\n\n\n\nWhere is the cat standing?\nmodel\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to"
        ```
        """

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        # [ZeRO-3 Fix] Untie lm_head.weight from embed_tokens.weight.
        # Shared weights frequently cause sharding/gathering conflicts in ZeRO-3 + TP.
        # By untieing them, we allow DeepSpeed to manage them independently.
        # This increases VRAM usage by ~1.5GB but ensures training stability.
        if not hasattr(self, "_weights_untied"):
            # Gather is required to clone the current data correctly regardless of sharding state
            with deepspeed.zero.GatheredParameters([self.lm_head.weight], modifier_rank=None):
                new_weight = nn.Parameter(self.lm_head.weight.clone().detach())
                new_weight.requires_grad = True
            self.lm_head.weight = new_weight
            self._weights_untied = True

        outputs: SPECTRAModelOutputWithPast = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            token_type_ids=token_type_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
            **lm_kwargs,
        )
        hidden_states = outputs.last_hidden_state

        # [ZeRO-3 + TP Fix] Re-gather sharded hidden states to match lm_head.
        # If AutoTP sharded the model, outputs might be [Seq, 576] while lm_head expects full dimension.
        # This gather bridges the gap between TP-sharded internals and standard container layers.
        # This resolves the 'got input (1241), mat (1241x576)' mismatch.
        if hasattr(self.config, "text_config") and hasattr(self.config.text_config, "hidden_size"):
            if hidden_states.shape[-1] < self.config.text_config.hidden_size:
                world_size = dist.get_world_size() if dist.is_initialized() else 1
                if world_size > 1:
                    # All-gather along the hidden dimension to restore full size
                    # Use a specialized list for gathering to prevent memory fragmentation
                    gathered_list = [torch.empty_like(hidden_states) for _ in range(world_size)]
                    dist.all_gather(gathered_list, hidden_states)
                    hidden_states = torch.cat(gathered_list, dim=-1)

        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep

        # [ZeRO-3 + TP Fix] Manual Bridge for Logit Calculation
        # This bridges the gap between AutoTP-sharded activations and ZeRO-3-sharded weights.
        # By manually gathering and slicing according to TP rank, we prevent size mismatches.
        hs = hidden_states[:, slice_indices, :].contiguous()
        p = self.lm_head.weight
        if hasattr(p, 'ds_id'):
            with deepspeed.zero.GatheredParameters([p], modifier_rank=None):
                # Now p is the full weight [Vocab, Hidden]
                hidden_dim = p.shape[-1]
                shard_dim = hs.shape[-1]

                # Check if it was sharded by TP (shard dimension usually matches the input)
                if hidden_dim > shard_dim and shard_dim > 0:
                    tp_size = max(1, hidden_dim // shard_dim)
                    tp_rank = dist.get_rank() % tp_size
                    start = tp_rank * shard_dim
                    end = start + shard_dim
                    w = p[:, start:end]
                    logits = F.linear(hs, w)
                else:
                    logits = F.linear(hs, p)
        else:
            # Fallback for non-DeepSpeed or non-sharded environments
            logits = self.lm_head(hs)

        if self.config.text_config.final_logit_softcapping is not None:
            logits = logits / self.config.text_config.final_logit_softcapping
            logits = torch.tanh(logits)
            logits = logits * self.config.text_config.final_logit_softcapping

        loss = None
        aux_loss = None
        if labels is not None:
            # Upcast to float if we need to compute the loss to avoid potential precision issues
            logits = logits.float()
            shift_logits = logits[..., :-1, :]
            shift_labels = labels[..., 1:]
            if attention_mask is not None:
                # we use the input attention mask to shift the logits and labels, because it is 2D.
                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft
                shift_attention_mask = attention_mask[:, -shift_logits.shape[1] :].to(logits.device)
                shift_logits = shift_logits[shift_attention_mask.to(logits.device) != 0].contiguous()
                shift_labels = shift_labels[shift_attention_mask.to(shift_labels.device) != 0].contiguous()
            else:
                shift_logits = shift_logits.contiguous()
                shift_labels = shift_labels.contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()

            flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)
            flat_labels = shift_labels.view(-1).to(shift_logits.device)
            loss = loss_fct(flat_logits, flat_labels)

            # Speciality loss: Output orthogonality (encourages diverse expert outputs)
            if outputs.speciality_loss is not None:
                loss += outputs.speciality_loss * 0.02  # ê°€ì¤‘ì¹˜ ì ìš©

            # Contrastive loss: Input clustering (encourages experts to process distinct token types)
            if outputs.contrastive_loss is not None:
                loss += outputs.contrastive_loss * 0.01  # ê°€ì¤‘ì¹˜ ì ìš©

            # Expression projector regularization loss: Direct connection to expression_logits for gradient flow
            if outputs.expression_reg_loss is not None:
                loss += outputs.expression_reg_loss

            # Expression projector loss: Ensure expression_logits contributes to loss for gradient flow
            if outputs.cosine_similarities is not None:
                if torch.is_tensor(outputs.cosine_similarities) and outputs.cosine_similarities.numel() > 0:
                    expr_loss = torch.mean(outputs.cosine_similarities ** 2) * 0.001
                    loss += expr_loss
                elif isinstance(outputs.cosine_similarities, (int, float)):
                    expr_loss = outputs.cosine_similarities * 0.001
                    loss += expr_loss

            # [Sharpening] Entropy Minimization
            router_entropy_coef = getattr(self.config.text_config, "router_entropy_coef", 0.1)
            if outputs.entropy_loss is not None and router_entropy_coef > 0:
                loss += outputs.entropy_loss * router_entropy_coef

            # [Optional] Ortho Loss
            ortho_loss_coef = getattr(self.config.text_config, "ortho_loss_coef", 0.01)
            if outputs.ortho_loss is not None and ortho_loss_coef > 0:
                loss += outputs.ortho_loss * ortho_loss_coef

        if not return_dict:
            output = (logits,) + outputs[1:]
            return (loss,) + output if loss is not None else output

        return SPECTRACausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            image_hidden_states=outputs.image_hidden_states,
            aux_loss=aux_loss,
            router_logits=outputs.router_logits,
            speciality_loss=outputs.speciality_loss,
            cosine_similarities=outputs.cosine_similarities,
            contrastive_loss=outputs.contrastive_loss,
            expression_reg_loss=outputs.expression_reg_loss,
        )

    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=None,
        inputs_embeds=None,
        cache_position=None,
        position_ids=None,
        pixel_values=None,
        attention_mask=None,
        token_type_ids=None,
        use_cache=True,
        logits_to_keep=None,
        labels=None,
        **kwargs,
    ):
        # Overwritten -- custom `position_ids` and `pixel_values` handling
        model_inputs = super().prepare_inputs_for_generation(
            input_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            cache_position=cache_position,
            use_cache=use_cache,
            logits_to_keep=logits_to_keep,
            token_type_ids=token_type_ids,
            **kwargs,
        )

        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore
        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always
        if cache_position[0] == 0:
            model_inputs["pixel_values"] = pixel_values

        return model_inputs

    @staticmethod
    def create_masks_for_generate(
        config: PretrainedConfig,
        input_embeds: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        cache_position: torch.Tensor,
        past_key_values: Optional[Cache],
        position_ids: Optional[torch.Tensor],
        token_type_ids: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> dict:
        # Prepare mask arguments
        mask_kwargs = {
            "config": config.get_text_config(),
            "input_embeds": input_embeds,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Add the token type ids mask for generate as well
        if token_type_ids is not None and input_embeds.shape[1] != 1:
            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`

            # First find where a new image block starts: 1 if image and previous not image
            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally
            is_image = (token_type_ids == 1).to(cache_position.device)
            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]
            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1
            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))
            mask_kwargs["or_mask_function"] = token_type_ids_mask_function(
                token_type_ids.to(cache_position.device), image_group_ids, config.mm_tokens_per_image
            )

        return create_masks_for_generate(**mask_kwargs)


class SPECTRARouterTrainingMonitor:
    """
    PyTorch í•™ìŠµ ë£¨í”„ì—ì„œ ë¼ìš°í„°ê°€ 'ì‹¤ì œë¡œ' í•™ìŠµë˜ëŠ”ì§€ ì§€ì† ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±.
    - on_batch_start: step ìŠ¤ëƒ…ìƒ·(íŒŒë¼ë¯¸í„° ê°’) ì €ì¥
    - on_after_backward: grad norm/ë¶„í¬/ì—”íŠ¸ë¡œí”¼/EMA/ë³´ì¡° ë¡œìŠ¤ ë¡œê¹…
    - on_step_end: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ëŸ‰(delta) í™•ì¸
    ì‚¬ìš©ìëŠ” í•™ìŠµ ë£¨í”„ì—ì„œ ê° íƒ€ì´ë°ì— í•´ë‹¹ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
    """
    def __init__(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        log_every: int = 100,
        log_fn: Optional[Callable[[str], None]] = None,
    ):
        self.model = model
        self.optimizer = optimizer
        self.log_every = max(int(log_every), 1)
        self.log_fn = log_fn if log_fn is not None else (lambda msg: logger.debug(msg))
        self._step = 0
        self._pre_step_snapshots: dict[str, dict[str, torch.Tensor]] = {}

    def _iter_router_modules(self):
        for name, module in self.model.named_modules():
            if getattr(module, "_is_spectra_router", False) or isinstance(module, SPECTRARouter):
                yield name, module

    @staticmethod
    def _check_requires_grad(module: nn.Module) -> bool:
        params = list(module.parameters(recurse=True))
        return len(params) > 0 and all(p.requires_grad for p in params)

    def _check_in_optimizer(self, module: nn.Module) -> bool:
        if self.optimizer is None:
            return False
        target_ids = {id(p) for p in module.parameters(recurse=True)}
        if not target_ids:
            return False
        opt_ids = set()
        for group in self.optimizer.param_groups:
            for p in group.get("params", []):
                opt_ids.add(id(p))
        return target_ids.issubset(opt_ids)

    @staticmethod
    def _grad_norms(module: nn.Module) -> dict[str, float]:
        norms: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if p.grad is not None:
                # ì‘ì€ ìˆ˜ì¹˜ ë…¸ì´ì¦ˆëŠ” 0ìœ¼ë¡œ ì·¨ê¸‰í•˜ì§€ ì•ŠìŒ
                norms[n] = float(p.grad.detach().norm().item())
        return norms

    @staticmethod
    def _snapshot_params(module: nn.Module) -> dict[str, torch.Tensor]:
        return {n: p.detach().clone() for n, p in module.named_parameters(recurse=True)}

    @staticmethod
    def _param_deltas(before: dict[str, torch.Tensor], module: nn.Module) -> dict[str, float]:
        deltas: dict[str, float] = {}
        for n, p in module.named_parameters(recurse=True):
            if n in before:
                deltas[n] = float((before[n] - p.detach()).abs().sum().item())
        return deltas

    @staticmethod
    def _router_usage_and_entropy(router_logits: Optional[Union[torch.Tensor, Tuple[torch.Tensor, ...]]]) -> tuple[Optional[List[float]], Optional[float]]:
        if router_logits is None:
            return None, None
        if isinstance(router_logits, tuple):
            if not router_logits:
                return None, None
            probs = torch.cat([t for t in router_logits if t is not None and t.numel() > 0], dim=0)
        else:
            probs = router_logits
        if probs.numel() == 0:
            return None, None
        # ë³¸ ì½”ë“œ ê²½ë¡œì—ì„œëŠ” router_logitsê°€ 'í™•ë¥ 'ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
        p = probs.clamp_min(1e-12)
        num_experts = p.size(-1)
        top1 = p.argmax(dim=-1)
        usage = torch.bincount(top1, minlength=num_experts).float()
        usage = usage / usage.sum().clamp_min(1.0)
        entropy = float((-(p * p.log()).sum(dim=-1)).mean().item())
        return usage.tolist(), entropy

    def on_batch_start(self):
        self._step += 1
        self._pre_step_snapshots.clear()
        for name, module in self._iter_router_modules():
            self._pre_step_snapshots[name] = self._snapshot_params(module)

    def on_after_backward(
        self,
        outputs: Optional[Union[SPECTRACausalLMOutputWithPast, SPECTRAModelOutputWithPast]] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ):
        if self._step % self.log_every != 0:
            return

        # ë¼ìš°í„° ëª¨ë“ˆë³„ ìƒíƒœ/grad
        lines = []
        for name, module in self._iter_router_modules():
            req = self._check_requires_grad(module)
            inopt = self._check_in_optimizer(module)
            norms = self._grad_norms(module)
            any_grad = any(v > 0.0 for v in norms.values())
            gsum = sum(norms.values()) if norms else 0.0
            gmax = max(norms.values()) if norms else 0.0
            lines.append(f"[router:{name}] requires_grad={req} in_optimizer={inopt} any_grad={any_grad} grad_sum={gsum:.6f} grad_max={gmax:.6f}")

        # ì‚¬ìš© ë¶„í¬/ì—”íŠ¸ë¡œí”¼
        usage, entropy = (None, None)
        if outputs is not None and hasattr(outputs, "router_logits"):
            usage, entropy = self._router_usage_and_entropy(outputs.router_logits)
            if usage is not None:
                lines.append(f"[routing] usage(top1_ratio)={','.join(f'{u:.3f}' for u in usage)}")
            if entropy is not None:
                lines.append(f"[routing] entropy={entropy:.6f}")

        # EMA (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            # ForCausalLM -> .model.global_router, TextModel -> .global_router
            global_router = None
            if hasattr(self.model, "model") and hasattr(self.model.model, "global_router"):
                global_router = self.model.model.global_router
            elif hasattr(self.model, "global_router"):
                global_router = self.model.global_router
            if global_router is not None and hasattr(global_router, "expert_load_ema"):
                ema = global_router.expert_load_ema.detach().float()
                s = float(ema.sum().item())
                ema_norm = (ema / s) if s > 0 else ema
                lines.append(f"[ema] expert_load_ema_norm={','.join(f'{float(x):.3f}' for x in ema_norm.tolist())}")
        except Exception:
            pass

        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))

        # ì„ íƒì ìœ¼ë¡œ ë³´ì¡° ë¡œìŠ¤ë„ ë¡œê¹…(ê³„ì‚° ë¹„ìš© ë‚®ìŒ)
        try:
            if outputs is not None and hasattr(outputs, "router_logits") and outputs.router_logits is not None:
                if hasattr(self.model, "model") and hasattr(self.model.model, "config"):
                    cfg = self.model.model.config
                elif hasattr(self.model, "config"):
                    cfg = self.model.config
                else:
                    cfg = None
                if cfg is not None:
                    aux = load_balancing_loss_func(
                        outputs.router_logits,
                        cfg.n_routed_experts,
                        getattr(cfg, "num_experts_per_tok", 2),
                        attention_mask,
                        router_z_loss_coef=getattr(cfg, "router_z_loss_coef", None),
                        router_entropy_coef=getattr(cfg, "router_entropy_coef", None),
                        usage_uniformity_coef=getattr(cfg, "usage_uniformity_coef", None),
                    ).detach().float().item()
                    self.log_fn(f"[step {self._step}] aux_lb_loss={aux:.6f}")
        except Exception:
            pass

    def on_step_end(self):
        if self._step % self.log_every != 0:
            return
        lines = []
        for name, module in self._iter_router_modules():
            before = self._pre_step_snapshots.get(name, {})
            deltas = self._param_deltas(before, module)
            delta_sum = sum(deltas.values()) if deltas else 0.0
            lines.append(f"[router:{name}] param_delta_sum={delta_sum:.6f}")
        if lines:
            self.log_fn(f"[step {self._step}] " + " | ".join(lines))
        self._pre_step_snapshots.clear()


__all__ = [
    "SPECTRAPreTrainedModel",
    "SPECTRATextModel",
    "SPECTRAForCausalLM",
    "SPECTRAForConditionalGeneration",
    "SPECTRAModel",
    "SPECTRARouterTrainingMonitor",
]
