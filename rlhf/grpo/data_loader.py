"""
TRL í‘œì¤€ ë°ì´í„° ë¡œë” for GRPO training
"""

import logging
from typing import Dict, Any, List, Optional, Set
from datasets import load_dataset, Dataset, DatasetDict
from transformers import AutoTokenizer, AutoProcessor
import trl.trainer
import pandas as pd
import os
import re

logger = logging.getLogger(__name__)

class GRPODataLoader:
    """TRL í‘œì¤€ ë°ì´í„° ë¡œë” for GRPO training"""

    def __init__(
        self,
        model_name: str = "unsloth/Qwen3-0.6B-bnb-4bit",
        max_length: int = 2048,
        data_mode: str = "instruction",
        csv_file_path: Optional[str] = None  # ëª…ë ¹ì–´ ì •ì˜ CSV ê²½ë¡œ
    ):
        self.model_name = model_name
        self.max_length = max_length
        self.data_mode = data_mode
        
        # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
        if csv_file_path is None:
            possible_paths = [
                "cmd_bot.csv",
                os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "cmd_bot.csv"),
                os.path.join(os.getcwd(), "cmd_bot.csv")
            ]
            for path in possible_paths:
                if os.path.exists(path):
                    csv_file_path = path
                    break
        
        self.csv_file_path = csv_file_path
        self.command_patterns = {}
        self.available_commands = []
        self.all_commands_info = {}  # ì „ì²´ ëª…ë ¹ì–´ ì •ë³´ (ëª…ë ¹ì–´ ë§µ)
        
        # ëª…ë ¹ì–´ ì •ë³´ ë¡œë“œ
        if self.csv_file_path and os.path.exists(self.csv_file_path):
            self._load_commands_from_csv()
        else:
            logger.warning(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}. ëª…ë ¹ì–´ ì •ë³´ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")

        # Load tokenizer only (TRL handles the rest)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Set pad token if not exists
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        logger.info(f"âœ… TRL DataLoader initialized with model: {model_name}")
        if self.command_patterns:
            logger.info(f"ğŸ“‹ {len(self.available_commands)}ê°œ ëª…ë ¹ì–´ ë¡œë“œ ì™„ë£Œ")
    
    def _load_commands_from_csv(self):
        """CSV íŒŒì¼ì—ì„œ ëª…ë ¹ì–´ ì •ë³´ ë¡œë“œ (ì „ì²´ ëª…ë ¹ì–´ ë§µ)"""
        try:
            df = pd.read_csv(self.csv_file_path)
            
            self.available_commands = sorted(df['cmd'].unique().tolist())
            
            # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ëª…ë ¹ì–´ ê·¸ë£¹í™”
            self.command_patterns = {}
            for _, row in df.iterrows():
                cmd = row['cmd']
                category = row.get('category', 'other')
                need_num = row.get('need_num', False)
                need_surface = row.get('need_surface', False)
                need_bridge = row.get('need_bridge', False)
                desc = row.get('desc', '')
                is_status = row.get('is_status', False)
                is_control = row.get('is_control', False)
                
                if category not in self.command_patterns:
                    self.command_patterns[category] = []
                
                cmd_info = {
                    'command': cmd,
                    'need_num': need_num,
                    'need_surface': need_surface,
                    'need_bridge': need_bridge,
                    'description': desc,
                    'is_status': is_status,
                    'is_control': is_control,
                }
                
                self.command_patterns[category].append(cmd_info)
                # ì „ì²´ ëª…ë ¹ì–´ ë§µì— ì €ì¥
                self.all_commands_info[cmd] = cmd_info
            
            logger.info(f"âœ… {len(self.available_commands)}ê°œ ëª…ë ¹ì–´ë¥¼ CSVì—ì„œ ë¡œë“œ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ëª…ë ¹ì–´ ìˆ˜: {dict((k, len(v)) for k, v in self.command_patterns.items())}")
            
        except Exception as e:
            logger.error(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.command_patterns = {}
            self.available_commands = []
            self.all_commands_info = {}

    def load_dataset(
        self,
        dataset_name: str = "HuggingFaceH4/ultrafeedback_binarized",
        split: str = "train_prefs",
        max_samples: Optional[int] = None,
        streaming: bool = False
    ) -> Dataset:
        """
        Load dataset from HuggingFace Hub

        Args:
            dataset_name: Name of the dataset on HuggingFace Hub
            split: Dataset split to load
            max_samples: Maximum number of samples to load
            streaming: Whether to use streaming mode

        Returns:
            Dataset: Loaded dataset (not DatasetDict)
        """
        logger.info(f"ğŸ“¦ Loading dataset: {dataset_name} (split: {split})")

        try:
            if streaming:
                dataset = load_dataset(dataset_name, split=split, streaming=True)
                if max_samples:
                    dataset = dataset.take(max_samples)
                return dataset
            else:
                dataset = load_dataset(dataset_name, split=split)
                if max_samples:
                    dataset = dataset.select(range(min(max_samples, len(dataset))))
                return dataset

        except Exception as e:
            logger.error(f"âŒ Failed to load dataset {dataset_name}: {e}")
            raise
    
    def load_custom_dataset(
        self,
        data_path: str,
        split: str = "train"
    ) -> Dataset:
        """
        Load custom dataset from local files

        Args:
            data_path: Path to the dataset file (JSON, JSONL, CSV, etc.)
            split: Dataset split to load (default: "train")

        Returns:
            Dataset: Loaded dataset from specified split
        """
        logger.info(f"ğŸ“ Loading custom dataset from: {data_path} (split: {split})")

        try:
            if data_path.endswith('.jsonl'):
                dataset_dict = load_dataset('json', data_files=data_path)
            elif data_path.endswith('.json'):
                dataset_dict = load_dataset('json', data_files=data_path)
            elif data_path.endswith('.csv'):
                dataset_dict = load_dataset('csv', data_files=data_path)
            else:
                raise ValueError(f"Unsupported file format: {data_path}")

            # Get the specified split (default to first available split if specified split doesn't exist)
            if split in dataset_dict:
                dataset = dataset_dict[split]
            else:
                # Fallback to first available split
                available_splits = list(dataset_dict.keys())
                if available_splits:
                    dataset = dataset_dict[available_splits[0]]
                    logger.warning(f"âš ï¸ Split '{split}' not found, using '{available_splits[0]}' instead")
                else:
                    raise ValueError(f"No splits available in dataset: {data_path}")

            return dataset

        except Exception as e:
            logger.error(f"âŒ Failed to load custom dataset: {e}")
            raise
    
    def _extract_used_commands(self, ground_truth: str) -> Set[str]:
        """ground_truthì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ëª…ë ¹ì–´ í† í°ë§Œ ì¶”ì¶œ"""
        if not ground_truth:
            return set()
        
        used_commands = set()
        
        # ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ëª…ë ¹ì–´ ë¶„ë¦¬
        commands = ground_truth.split(';')
        
        for cmd in commands:
            # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ í† í° ì¶”ì¶œ
            parts = [p.strip().lower() for p in cmd.split(',')]
            
            # CSVì˜ ëª…ë ¹ì–´ ëª©ë¡ê³¼ ì •í™•íˆ ë§¤ì¹­
            for part in parts:
                # ì •í™•í•œ ëª…ë ¹ì–´ ë§¤ì¹­
                for cmd_name in self.available_commands:
                    if cmd_name.lower() == part:
                        used_commands.add(cmd_name)
                        break
                
                # "pocket depth" ê°™ì€ ë³µí•© ëª…ë ¹ì–´ ì²˜ë¦¬
                if 'pocket depth' in part:
                    if 'pocket depth' in self.available_commands:
                        used_commands.add('pocket depth')
                    if 'probing' in self.available_commands:
                        used_commands.add('probing')
                elif 'probing' in part and 'probing' in self.available_commands:
                    used_commands.add('probing')
        
        return used_commands

    def _format_available_commands_map(self) -> str:
        """ì „ì²´ ëª…ë ¹ì–´ ë§µ í¬ë§·íŒ… (í‰ê°€ìš©)"""
        if not self.command_patterns:
            return "ëª…ë ¹ì–´ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        formatted.append("AVAILABLE COMMANDS MAP:")
        formatted.append("(You MUST select commands ONLY from this list)")
        formatted.append("")
        
        for category, commands in sorted(self.command_patterns.items()):
            formatted.append(f"=== {category.upper()} ===")
            for cmd_info in sorted(commands, key=lambda x: x['command']):
                cmd = cmd_info['command']
                requirements = []
                if cmd_info.get('need_num'):
                    requirements.append("needs tooth number")
                if cmd_info.get('need_surface'):
                    requirements.append("needs surface")
                if cmd_info.get('need_bridge'):
                    requirements.append("needs bridge")
                
                req_text = f" [{', '.join(requirements)}]" if requirements else ""
                formatted.append(f"- {cmd}{req_text}")
            
            formatted.append("")
        
        return "\n".join(formatted)

    def _analyze_ground_truth_patterns(self, ground_truth: str) -> Dict[str, Any]:
        """ground_truthì˜ ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
        if not ground_truth:
            return {}
        
        patterns = {
            'commands': set(),
            'has_surface': False,
            'has_positions': False,
            'status_expansion': None,  # 'full', 'mesial_distal', 'single'
            'probing_format': None,  # 'with_surface', 'no_surface'
            'value_format': None,  # 'with_surface', 'no_surface'
        }
        
        # ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ëª…ë ¹ì–´ ë¶„ë¦¬
        commands = ground_truth.split(';')
        
        for cmd in commands:
            cmd_lower = cmd.lower()
            parts = [p.strip() for p in cmd.split(',')]
            
            # ëª…ë ¹ì–´ ì¶”ì¶œ
            for part in parts:
                part_lower = part.strip().lower()
                for cmd_name in self.available_commands:
                    if cmd_name.lower() == part_lower:
                        patterns['commands'].add(cmd_name)
                        break
            
            # Surface íŒ¨í„´ ë¶„ì„
            if any(s in cmd_lower for s in ['buccal', 'lingual', 'palatal']):
                patterns['has_surface'] = True
            
            # Position íŒ¨í„´ ë¶„ì„ (mesial, middle, distal)
            has_mesial = 'mesial' in cmd_lower
            has_middle = 'middle' in cmd_lower
            has_distal = 'distal' in cmd_lower
            
            if has_mesial or has_middle or has_distal:
                patterns['has_positions'] = True
                
                # Status ëª…ë ¹ì–´ expansion íŒ¨í„´ ë¶„ì„
                if any(status in cmd_lower for status in ['bleeding', 'suppuration', 'plaque', 'calculus']):
                    if has_mesial and has_middle and has_distal:
                        patterns['status_expansion'] = 'full'
                    elif has_mesial and has_distal and not has_middle:
                        patterns['status_expansion'] = 'mesial_distal'
                    else:
                        patterns['status_expansion'] = 'single'
            
            # Probing íŒ¨í„´ ë¶„ì„
            if 'probing' in cmd_lower or 'pocket depth' in cmd_lower:
                if patterns['has_surface']:
                    patterns['probing_format'] = 'with_surface'
                else:
                    patterns['probing_format'] = 'no_surface'
            
            # Value íŒ¨í„´ ë¶„ì„ (furcation, mobility, recession)
            if any(value in cmd_lower for value in ['furcation', 'mobility', 'recession', 'gingival margin']):
                if patterns['has_surface']:
                    patterns['value_format'] = 'with_surface'
                else:
                    patterns['value_format'] = 'no_surface'
        
        return patterns

    def _build_minimal_rules(self, patterns: Dict[str, Any]) -> str:
        """íŒ¨í„´ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìµœì†Œí•œì˜ ê·œì¹™ ìƒì„±"""
        if not patterns or not patterns.get('commands'):
            return ""
        
        rules = []
        
        # Probing ê·œì¹™ (ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜)
        probing_cmds = [c for c in patterns['commands'] if c.lower() in ['probing', 'pocket depth']]
        if probing_cmds:
            rule = f"PROBING ({', '.join(probing_cmds)}): number N, "
            if patterns.get('probing_format') == 'with_surface':
                rule += "[surface, ] probing, X Y Z"
            else:
                rule += "probing, X Y Z"
            rule += " (three numbers, no positions)"
            rules.append(rule)
        
        # Status ê·œì¹™ (ì‹¤ì œ expansion íŒ¨í„´ ê¸°ë°˜)
        status_cmds = [c for c in patterns['commands'] 
                    if any(s in c.lower() for s in ['bleeding', 'suppuration', 'plaque', 'calculus'])]
        if status_cmds:
            expansion = patterns.get('status_expansion')
            if expansion == 'full':
                rule = f"STATUS ({', '.join(status_cmds)}): number N, [surface, ] mesial, [cmd], 1, middle, [cmd], 1, distal, [cmd], 1"
            elif expansion == 'mesial_distal':
                rule = f"STATUS ({', '.join(status_cmds)}): number N, [surface, ] mesial, [cmd], 1, distal, [cmd], 1"
            else:
                rule = f"STATUS ({', '.join(status_cmds)}): number N, [surface, ] [cmd], 1"
            
            # Surface íŒ¨í„´
            if patterns.get('has_surface'):
                rule += " (surface specified)"
            else:
                rule += " (both buccal and lingual if no surface)"
            
            rules.append(rule)
        
        # Value ê·œì¹™ (ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜)
        value_cmds = [c for c in patterns['commands'] 
                    if any(v in c.lower() for v in ['furcation', 'mobility', 'recession', 'gingival margin'])]
        if value_cmds:
            rule = f"VALUE ({', '.join(value_cmds)}): number N, "
            if patterns.get('value_format') == 'with_surface':
                rule += "[surface, ] [cmd], value"
            else:
                rule += "[cmd], value"
            rule += " (no positions)"
            rules.append(rule)
        
        # Restoration ê·œì¹™
        restoration_cmds = [c for c in patterns['commands'] 
                        if any(r in c.lower() for r in ['crown', 'implant', 'fixture', 'bridge'])]
        if restoration_cmds:
            rules.append(f"RESTORATION ({', '.join(restoration_cmds)}): number N, [number M, ] [cmd]")
        
        # Control ê·œì¹™
        control_cmds = [c for c in patterns['commands'] 
                    if any(ctrl in c.lower() for ctrl in ['jump', 'back', 'clear', 'delete'])]
        if control_cmds:
            rules.append(f"CONTROL ({', '.join(control_cmds)}): [cmd] or [cmd] to number N")
        
        return "\n".join(rules) if rules else ""

    def _analyze_transformation_logic(self, utterance: str, ground_truth: str) -> List[str]:
        """ì…ë ¥ê³¼ ì •ë‹µì„ ë¹„êµí•˜ì—¬ í•„ìš”í•œ ë³€í™˜ ë¡œì§ ê·œì¹™ ì¶”ì¶œ"""
        rules = []
        utterance_lower = utterance.lower()
        gt_lower = ground_truth.lower()

        # 1. Default Surface Logic (ê°€ì¥ ì¤‘ìš”)
        # ì…ë ¥ì—ëŠ” surfaceê°€ ì—†ëŠ”ë° ì •ë‹µì—ëŠ” ìˆëŠ” ê²½ìš° -> ê¸°ë³¸ê°’ ê·œì¹™ í•„ìš”
        status_keywords = ['bleeding', 'suppuration', 'plaque', 'calculus']
        has_status_cmd = any(k in utterance_lower for k in status_keywords)
        
        if has_status_cmd:
            input_surface = any(s in utterance_lower for s in ['buccal', 'lingual', 'palatal', 'facial', 'labial'])
            output_has_buccal = 'buccal' in gt_lower
            output_has_lingual = 'lingual' in gt_lower
            
            if not input_surface:
                if output_has_buccal and output_has_lingual:
                    rules.append("LOGIC: No surface mentioned for STATUS â†’ Output BOTH buccal AND lingual commands.")
                elif output_has_buccal:
                    rules.append("LOGIC: No surface mentioned â†’ Default to 'buccal' only.")

        # 2. Probing Logic (ê°’ ì²˜ë¦¬)
        if 'probing' in gt_lower or 'pocket depth' in gt_lower:
            # ì…ë ¥ì´ '323' ì²˜ëŸ¼ ë¶™ì–´ìˆê±°ë‚˜ '3, 2, 3' ì²˜ëŸ¼ ë–¨ì–´ì ¸ ìˆì–´ë„ ì •ë‹µì€ '3 2 3'
            rules.append("LOGIC: Input numbers (e.g., '3 2 3') â†’ Format as 'probing, X Y Z' (three distinct numbers).")

        # 3. Range/Sequence Logic (ì…ë ¥ í‚¤ì›Œë“œ ê¸°ë°˜)
        if any(k in utterance_lower for k in ['to', 'through', '-', 'all']):
            # ì •ë‹µì— numberê°€ ì—¬ëŸ¬ ê°œ ë“±ì¥í•˜ë©´ Range í™•ì¥ì„
            if gt_lower.count('number') > 3:
                rules.append("LOGIC: Range/All detected ('to', 'through', 'all') â†’ Expand to explicit command for EACH tooth in range.")

        # 4. Repeat Logic
        if 'repeat' in utterance_lower:
            rules.append("LOGIC: 'repeat' detected â†’ Apply previous command values to subsequent teeth explicitly.")

        # 5. Exception Logic
        if 'except' in utterance_lower or 'but' in utterance_lower:
            rules.append("LOGIC: 'except/but' detected â†’ Apply general rule first, then override specific teeth.")
            
        # 6. Position Logic (Proximal)
        if 'proximal' in utterance_lower:
            rules.append("LOGIC: 'proximal' detected â†’ Expand to 'mesial' AND 'distal' (skip middle).")

        return rules
    
    def _build_adaptive_cmd_prompt(
        self,
        utterance: str,
        numbering_system: str,
        ground_truth: Optional[str] = None,
        numbering_method: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """ë‹µì§€ ê¸°ë°˜ ì ì‘í˜• í”„ë¡¬í”„íŠ¸ ìƒì„± (system prompt + user prompt ë¶„ë¦¬)"""
        
        # Numbering system ì •ë³´
        if numbering_system == "FDI":
            quadrant_mapping = "Q1 â†’ teeth 11â€“18, Q2 â†’ 21â€“28, Q3 â†’ 31â€“38, Q4 â†’ 41â€“48"
            numbering_info = "[FDI] Q1(11-18), Q2(21-28), Q3(31-38), Q4(41-48)"
        else:  # UNS
            quadrant_mapping = "Q1 â†’ teeth 1â€“8, Q2 â†’ 9â€“16, Q3 â†’ 17â€“24, Q4 â†’ 25â€“32"
            numbering_info = "[UNS] Q1(1-8), Q2(9-16), Q3(17-24), Q4(25-32)"
        
        if numbering_method:
            numbering_info = numbering_method
        
        # System prompt êµ¬ì„±
        system_prompt = f"""ğŸ¦· PERIODONTAL CHARTING ASSISTANT

TASK: Convert natural language into structured command sequences.

CRITICAL: Use ONLY commands from AVAILABLE COMMANDS MAP below.

TOOTH NUMBERING: {numbering_system}
{numbering_info}
Quadrant: {quadrant_mapping}

"""
        
        # ì „ì²´ ëª…ë ¹ì–´ ë§µ (í‰ê°€ìš©)
        if self.available_commands:
            system_prompt += self._format_available_commands_map() + "\n\n"
        
        # ground_truth íŒ¨í„´ ë¶„ì„ ë° ê·œì¹™ ìƒì„±
        patterns = self._analyze_ground_truth_patterns(ground_truth) if ground_truth else {}
        format_rules = self._build_minimal_rules(patterns)
        logic_rules = self._analyze_transformation_logic(utterance, ground_truth) if ground_truth else []
        
        # 1. FORMAT RULES (Output Structure)
        if format_rules:
            system_prompt += "FORMAT RULES (Output Structure):\n"
            system_prompt += format_rules + "\n\n"
        
        # 2. TRANSFORMATION LOGIC (How to process Input)
        if logic_rules:
            system_prompt += "TRANSFORMATION LOGIC (How to process Input):\n"
            system_prompt += "\n".join([f"- {r}" for r in logic_rules]) + "\n\n"
        
        # ê³µí†µ ê·œì¹™ (ìµœì†Œí™”)
        common_rules = """COMMON RULES:
- Single line output, semicolons (;) separate commands
- Always start with "number N"
- Three numbers = probing values (NOT tooth number)
- Never output meta-commands: expand "repeat", "others", "all" to explicit commands
- VALIDATION: Check that all commands in your output exist in AVAILABLE COMMANDS MAP above

"""
        
        system_prompt += common_rules
        
        # User prompt êµ¬ì„±
        user_prompt = f"Convert: {utterance}\n\nOutput (commands only):"
        
        # Messages í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (chat template ì‚¬ìš©)
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    def prepare_grpo_data(
        self,
        dataset
    ) -> Dataset:
        """
        TRL í‘œì¤€ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        TRL GRPOëŠ” ë‹¤ìŒ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤:
        - prompt/chosen/rejected í•„ë“œ
        ë˜ëŠ”
        - messages í•„ë“œ (ëŒ€í™” í˜•ì‹)

        Args:
            dataset: Dataset ë˜ëŠ” DatasetDict ê°ì²´

        Returns:
            Dataset: TRL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ë°ì´í„°ì…‹
        """
        logger.info("ğŸ”„ Converting to TRL standard format")

        # DatasetDictì¸ ê²½ìš° train split ì‚¬ìš©
        if isinstance(dataset, DatasetDict):
            if "train" in dataset:
                dataset = dataset["train"]
            else:
                # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ split ì‚¬ìš©
                available_splits = list(dataset.keys())
                if available_splits:
                    dataset = dataset[available_splits[0]]
                    logger.warning(f"âš ï¸ Using split '{available_splits[0]}' from DatasetDict")
                else:
                    raise ValueError("No splits available in DatasetDict")

        if not isinstance(dataset, Dataset):
            raise ValueError(f"Expected Dataset, got {type(dataset)}")

        def convert_to_trl_format(example):
            """Convert to TRL standard format"""
            # ì´ë¯¸ TRL í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if "messages" in example:
                # messages ëŠ” (prompt, chosen, rejected) ì¡°í•©ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
                del example["messages"]

            if "prompt" in example:
                if not all([prompt for prompt in example.get("prompt") if type(prompt) == str and type(prompt) == list]):
                    example["prompt"] = [{"role": "user", "content": prompt} for prompt in example.get("prompt")]

            if "prompt" in example and not ("chosen" in example and "rejected" in example):
                if self.data_mode == "cmd":
                    # ì ì‘í˜• í”„ë¡¬í”„íŠ¸ ìƒì„± (system prompt + user prompt ë¶„ë¦¬)
                    numbering_system = example.get('numbering_system', 'UNS')
                    numbering_method = example.get('numbering_method', None)
                    ground_truth = example.get('ground_truth') or example.get('label')
                    utterance = example['prompt']  # ì›ë³¸ utterance
                    
                    # Messages í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                    messages = self._build_adaptive_cmd_prompt(
                        utterance=utterance,
                        numbering_system=numbering_system,
                        ground_truth=ground_truth,
                        numbering_method=numbering_method
                    )
                    
                    # TRLì€ messages í˜•ì‹ ë˜ëŠ” ë¬¸ìì—´ í˜•ì‹ ëª¨ë‘ ì§€ì›
                    # messages í˜•ì‹ìœ¼ë¡œ ì €ì¥ (chat template ì‚¬ìš©)
                    example["prompt"] = messages
                return {"prompt": example["prompt"]}

            # UltraFeedback í˜•ì‹ ë³€í™˜
            if "chosen" in example and "rejected" in example:
                chosen = example["chosen"]
                rejected = example["rejected"]

                if isinstance(chosen, list) and isinstance(rejected, list):
                    # chosenê³¼ rejectedê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ë©”ì‹œì§€ í˜•ì‹)
                    chosen_text = chosen[-1]["content"] if chosen else ""
                    rejected_text = rejected[-1]["content"] if rejected else ""
                    prompt = chosen[0]["content"] if chosen else ""

                    return {
                        "prompt": prompt,
                        "chosen": chosen_text,
                        "rejected": rejected_text
                    }

            # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ë°˜í™˜ (TRLì´ ì²˜ë¦¬)
            return example

        # ë°ì´í„° ë³€í™˜
        processed_dataset = dataset.map(
            convert_to_trl_format,
            desc="Converting to TRL format"
        )

        logger.info(f"âœ… Converted {len(processed_dataset)} samples to TRL format")
        return processed_dataset
    
    def get_sample_data(
        self,
        dataset_name: str = "HuggingFaceH4/ultrafeedback_binarized"
    ) -> Dict[str, Any]:
        """
        ìƒ˜í”Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ TRL í˜•ì‹ í™•ì¸

        Args:
            dataset_name: í™•ì¸í•  ë°ì´í„°ì…‹ ì´ë¦„
        """
        logger.info(f"ğŸ” Getting sample data from {dataset_name}")

        try:
            # ì‘ì€ ìƒ˜í”Œ ë¡œë“œ
            dataset = self.load_dataset(dataset_name, max_samples=5)

            # ì²« ë²ˆì§¸ ìƒ˜í”Œ ë°˜í™˜
            if len(dataset) > 0:
                sample = dict(dataset[0])
                logger.info("âœ… Sample data retrieved successfully")
                logger.info(f"ğŸ“‹ Sample keys: {list(sample.keys())}")
                return sample
            else:
                logger.warning("âš ï¸ No samples found in dataset")
                return {}

        except Exception as e:
            logger.error(f"âŒ Failed to get sample data: {e}")
            return {}


def create_grpo_dataloader(
    model_name: str = "unsloth/Qwen3-0.6B-bnb-4bit",
    dataset_name: str = "HuggingFaceH4/ultrafeedback_binarized",
    max_samples: int = 1000,
    max_length: int = 2048,
    split: str = "train_prefs"
) -> tuple[GRPODataLoader, Dataset]:
    """
    TRL í‘œì¤€ ë°ì´í„° ë¡œë” ìƒì„± ë° ë°ì´í„°ì…‹ ë¡œë“œ

    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        split: ì‚¬ìš©í•  ë°ì´í„°ì…‹ ë¶„í• 

    Returns:
        (data_loader, dataset) íŠœí”Œ
    """
    # ë°ì´í„° ë¡œë” ìƒì„±
    data_loader = GRPODataLoader(
        model_name=model_name,
        max_length=max_length,
        data_mode="cmd"  # Default to cmd mode for create_grpo_dataloader if intended for cmd training
    )

    # ë°ì´í„°ì…‹ ë¡œë“œ ë° TRL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    dataset = data_loader.load_dataset(dataset_name, split=split, max_samples=max_samples)
    processed_dataset = data_loader.prepare_grpo_data(dataset)

    return data_loader, processed_dataset
