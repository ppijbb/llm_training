"""
TRL í‘œì¤€ ë°ì´í„° ë¡œë” for GRPO training
"""

import logging
from typing import Dict, Any, List, Optional
from datasets import load_dataset, Dataset, DatasetDict
from transformers import AutoTokenizer, AutoProcessor
import trl.trainer

logger = logging.getLogger(__name__)

class GRPODataLoader:
    """TRL í‘œì¤€ ë°ì´í„° ë¡œë” for GRPO training"""

    def __init__(
        self,
        model_name: str = "unsloth/Qwen3-0.6B-bnb-4bit",
        max_length: int = 2048,
        data_mode: str = "instruction"
    ):
        self.model_name = model_name
        self.max_length = max_length
        self.data_mode = data_mode

        # Load tokenizer only (TRL handles the rest)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Set pad token if not exists
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        logger.info(f"âœ… TRL DataLoader initialized with model: {model_name}")
    
    def load_dataset(
        self,
        dataset_name: str = "HuggingFaceH4/ultrafeedback_binarized",
        split: str = "train_prefs",
        max_samples: Optional[int] = None,
        streaming: bool = False
    ) -> Dataset:
        """
        Load dataset from HuggingFace Hub

        Args:
            dataset_name: Name of the dataset on HuggingFace Hub
            split: Dataset split to load
            max_samples: Maximum number of samples to load
            streaming: Whether to use streaming mode

        Returns:
            Dataset: Loaded dataset (not DatasetDict)
        """
        logger.info(f"ğŸ“¦ Loading dataset: {dataset_name} (split: {split})")

        try:
            if streaming:
                dataset = load_dataset(dataset_name, split=split, streaming=True)
                if max_samples:
                    dataset = dataset.take(max_samples)
                return dataset
            else:
                dataset = load_dataset(dataset_name, split=split)
                if max_samples:
                    dataset = dataset.select(range(min(max_samples, len(dataset))))
                return dataset

        except Exception as e:
            logger.error(f"âŒ Failed to load dataset {dataset_name}: {e}")
            raise
    
    def load_custom_dataset(
        self,
        data_path: str,
        split: str = "train"
    ) -> Dataset:
        """
        Load custom dataset from local files

        Args:
            data_path: Path to the dataset file (JSON, JSONL, CSV, etc.)
            split: Dataset split to load (default: "train")

        Returns:
            Dataset: Loaded dataset from specified split
        """
        logger.info(f"ğŸ“ Loading custom dataset from: {data_path} (split: {split})")

        try:
            if data_path.endswith('.jsonl'):
                dataset_dict = load_dataset('json', data_files=data_path)
            elif data_path.endswith('.json'):
                dataset_dict = load_dataset('json', data_files=data_path)
            elif data_path.endswith('.csv'):
                dataset_dict = load_dataset('csv', data_files=data_path)
            else:
                raise ValueError(f"Unsupported file format: {data_path}")

            # Get the specified split (default to first available split if specified split doesn't exist)
            if split in dataset_dict:
                dataset = dataset_dict[split]
            else:
                # Fallback to first available split
                available_splits = list(dataset_dict.keys())
                if available_splits:
                    dataset = dataset_dict[available_splits[0]]
                    logger.warning(f"âš ï¸ Split '{split}' not found, using '{available_splits[0]}' instead")
                else:
                    raise ValueError(f"No splits available in dataset: {data_path}")

            return dataset

        except Exception as e:
            logger.error(f"âŒ Failed to load custom dataset: {e}")
            raise
    
    def prepare_grpo_data(
        self,
        dataset
    ) -> Dataset:
        """
        TRL í‘œì¤€ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        TRL GRPOëŠ” ë‹¤ìŒ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤:
        - prompt/chosen/rejected í•„ë“œ
        ë˜ëŠ”
        - messages í•„ë“œ (ëŒ€í™” í˜•ì‹)

        Args:
            dataset: Dataset ë˜ëŠ” DatasetDict ê°ì²´

        Returns:
            Dataset: TRL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ë°ì´í„°ì…‹
        """
        logger.info("ğŸ”„ Converting to TRL standard format")

        # DatasetDictì¸ ê²½ìš° train split ì‚¬ìš©
        if isinstance(dataset, DatasetDict):
            if "train" in dataset:
                dataset = dataset["train"]
            else:
                # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ split ì‚¬ìš©
                available_splits = list(dataset.keys())
                if available_splits:
                    dataset = dataset[available_splits[0]]
                    logger.warning(f"âš ï¸ Using split '{available_splits[0]}' from DatasetDict")
                else:
                    raise ValueError("No splits available in DatasetDict")

        if not isinstance(dataset, Dataset):
            raise ValueError(f"Expected Dataset, got {type(dataset)}")

        def convert_to_trl_format(example):
            """Convert to TRL standard format"""
            # ì´ë¯¸ TRL í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if "messages" in example:
                # messages ëŠ” (prompt, chosen, rejected) ì¡°í•©ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
                del example["messages"]

            if "prompt" in example:
                if not all([prompt for prompt in example.get("prompt") if type(prompt) == str and type(prompt) == list]):
                    example["prompt"] = [{"role": "user", "content": prompt} for prompt in example.get("prompt")]

            if "prompt" in example and not ("chosen" in example and "rejected" in example):
                if self.data_mode == "cmd":
                    # Fixed system prompt ë¶™ì—¬ì„œ ë„˜ê¸°ê¸°
                    example["prompt"] = (
                        f"Process the flowing utterance into dental commands. Given tooth numbering system is {example['numbering_system']}"
                        f"\n\n"
                        f"utterance: {example['prompt']}"
                        )
                return {"prompt": example["prompt"]}

            # UltraFeedback í˜•ì‹ ë³€í™˜
            if "chosen" in example and "rejected" in example:
                chosen = example["chosen"]
                rejected = example["rejected"]

                if isinstance(chosen, list) and isinstance(rejected, list):
                    # chosenê³¼ rejectedê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ë©”ì‹œì§€ í˜•ì‹)
                    chosen_text = chosen[-1]["content"] if chosen else ""
                    rejected_text = rejected[-1]["content"] if rejected else ""
                    prompt = chosen[0]["content"] if chosen else ""

                    return {
                        "prompt": prompt,
                        "chosen": chosen_text,
                        "rejected": rejected_text
                    }

            # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ë°˜í™˜ (TRLì´ ì²˜ë¦¬)
            return example

        # ë°ì´í„° ë³€í™˜
        processed_dataset = dataset.map(
            convert_to_trl_format,
            desc="Converting to TRL format"
        )

        logger.info(f"âœ… Converted {len(processed_dataset)} samples to TRL format")
        return processed_dataset
    
    def get_sample_data(
        self,
        dataset_name: str = "HuggingFaceH4/ultrafeedback_binarized"
    ) -> Dict[str, Any]:
        """
        ìƒ˜í”Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ TRL í˜•ì‹ í™•ì¸

        Args:
            dataset_name: í™•ì¸í•  ë°ì´í„°ì…‹ ì´ë¦„
        """
        logger.info(f"ğŸ” Getting sample data from {dataset_name}")

        try:
            # ì‘ì€ ìƒ˜í”Œ ë¡œë“œ
            dataset = self.load_dataset(dataset_name, max_samples=5)

            # ì²« ë²ˆì§¸ ìƒ˜í”Œ ë°˜í™˜
            if len(dataset) > 0:
                sample = dict(dataset[0])
                logger.info("âœ… Sample data retrieved successfully")
                logger.info(f"ğŸ“‹ Sample keys: {list(sample.keys())}")
                return sample
            else:
                logger.warning("âš ï¸ No samples found in dataset")
                return {}

        except Exception as e:
            logger.error(f"âŒ Failed to get sample data: {e}")
            return {}


def create_grpo_dataloader(
    model_name: str = "unsloth/Qwen3-0.6B-bnb-4bit",
    dataset_name: str = "HuggingFaceH4/ultrafeedback_binarized",
    max_samples: int = 1000,
    max_length: int = 2048,
    split: str = "train_prefs"
) -> tuple[GRPODataLoader, Dataset]:
    """
    TRL í‘œì¤€ ë°ì´í„° ë¡œë” ìƒì„± ë° ë°ì´í„°ì…‹ ë¡œë“œ

    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        split: ì‚¬ìš©í•  ë°ì´í„°ì…‹ ë¶„í• 

    Returns:
        (data_loader, dataset) íŠœí”Œ
    """
    # ë°ì´í„° ë¡œë” ìƒì„±
    data_loader = GRPODataLoader(
        model_name=model_name,
        max_length=max_length
    )

    # ë°ì´í„°ì…‹ ë¡œë“œ ë° TRL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    dataset = data_loader.load_dataset(dataset_name, split=split, max_samples=max_samples)
    processed_dataset = data_loader.prepare_grpo_data(dataset)

    return data_loader, processed_dataset
