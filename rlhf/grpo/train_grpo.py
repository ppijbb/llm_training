#!/usr/bin/env python3
import unsloth
from unsloth.tokenizer_utils import wandb
import weave
from transformers.utils import is_flash_attn_2_available
import inspect

if is_flash_attn_2_available():
    from flash_attn import flash_attn_func, flash_attn_varlen_func
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa

    _flash_supports_window_size = "window_size" in list(inspect.signature(flash_attn_func).parameters)
"""
Main script for GRPO training with Unsloth

Usage:
    python train_grpo.py --model llama-3.1-8b --dataset ultrafeedback
    python train_grpo.py --config config.json
    python train_grpo.py --custom-data /path/to/data.jsonl
"""
import argparse
import logging
import os
import sys
import json
from typing import List, Dict, Any
from pathlib import Path

# Add current directory to path for imports
sys.path.append(str(Path(__file__).parent))

from grpo_trainer import create_grpo_trainer
from transformers import TrainingArguments
from trl import GRPOConfig
from data_loader import GRPODataLoader
from config import (
    create_grpo_config,
    create_quick_test_config,
    create_production_config,
)
from reward.reward_functions import (
    SingleCustomRewardFunction,
    MultiRewardFunction,
    AccuracyComponent,
    LengthComponent,
    QualityComponent,
)
from reward.cmd_reward_functions import CommandRewardFunction


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('grpo_training.log')
    ]
)
logger = logging.getLogger(__name__)


def validate_grpo_config(config) -> bool:
    """Validate GRPOConfig"""
    logger.info("ğŸ” Validating configuration")

    try:
        # Check required fields for TRL GRPOConfig
        required_fields = [
            "learning_rate", "per_device_train_batch_size", "num_train_epochs"
        ]

        for field in required_fields:
            if not hasattr(config, field) or getattr(config, field) is None:
                logger.error(f"âŒ Missing required field: {field}")
                return False

        # Check value ranges
        if config.learning_rate <= 0:
            logger.error("âŒ Learning rate must be positive")
            return False

        if config.per_device_train_batch_size <= 0:
            logger.error("âŒ Batch size must be positive")
            return False

        if config.num_train_epochs <= 0:
            logger.error("âŒ Number of epochs must be positive")
            return False

        # Check TRL GRPOConfig specific fields
        if hasattr(config, 'max_prompt_length') and config.max_prompt_length <= 0:
            logger.error("âŒ Max prompt length must be positive")
            return False

        if hasattr(config, 'num_generations') and config.num_generations <= 0:
            logger.error("âŒ Number of generations must be positive")
            return False

        if hasattr(config, 'beta') and config.beta < 0:
            logger.error("âŒ Beta (KL penalty) must be non-negative")
            return False

        logger.info("âœ… Configuration validation passed")
        return True

    except Exception as e:
        logger.error(f"âŒ Configuration validation failed: {e}")
        return False


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="GRPO Training with Unsloth",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Quick test with default settings
  python train_grpo.py --quick-test
  
  # Train with specific model and dataset
  python train_grpo.py --model llama-3.1-8b --dataset ultrafeedback
  
  # Train with custom configuration file
  python train_grpo.py --config my_config.json
  
  # Train with custom data
  python train_grpo.py --custom-data /path/to/data.jsonl --model llama-3.1-8b
  
  # Production training
  python train_grpo.py --production --model llama-3.1-70b
        """
    )
    
    # Model selection
    parser.add_argument(
        "--model", 
        type=str, 
        default="llama-3.1-8b",
        choices=["llama-3.1-8b", "llama-3.1-70b", "gemma-2-9b", "qwen2.5-7b"],
        help="Model to train (default: llama-3.1-8b)"
    )
    
    # Dataset selection
    parser.add_argument(
        "--dataset", 
        type=str, 
        default="ultrafeedback",
        choices=["ultrafeedback", "ultrafeedback_large", "hh_rlhf", "openai_summarize"],
        help="Dataset to use (default: ultrafeedback)"
    )
    
    # Custom data
    parser.add_argument(
        "--custom-data",
        type=str,
        help="Path to custom dataset file (JSON, JSONL, CSV)"
    )
    
    # Configuration
    parser.add_argument(
        "--config",
        type=str,
        default="config/trainer_config.json",
        help="Path to configuration file"
    )
    
    # Training modes
    parser.add_argument(
        "--quick-test",
        action="store_true",
        default=False,
        help="Run quick test with minimal data"
    )
    
    parser.add_argument(
        "--production",
        action="store_true",
        help="Run production training with full dataset"
    )
    
    # Training parameters
    parser.add_argument(
        "--max-samples",
        type=int,
        help="Maximum number of samples to use"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        help="Number of training epochs"
    )
    
    parser.add_argument(
        "--learning-rate",
        type=float,
        help="Learning rate"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        help="Batch size"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        help="Output directory for trained model"
    )
    
    # Reward function configuration
    parser.add_argument(
        "--reward-function",
        type=str,
        nargs="+",
        default=["single"],
        choices=["single", "multi", "cmd", "accuracy", "length", "quality"],
        help="Reward function types to use: 'single' for unified, 'multi' for multi-component (default: single)"
    )

    parser.add_argument(
        "--reward-config",
        type=str,
        help="Path to reward function configuration file (JSON)"
    )
    
    
    # Other options
    parser.add_argument(
        "--wandb-project",
        type=str,
        default="grpo-training",
        help="Weights & Biases project name"
    )

    parser.add_argument(
        "--no-wandb",
        action="store_true",
        help="Disable Weights & Biases logging"
    )

    parser.add_argument(
        "--eval-only",
        action="store_true",
        help="Only run evaluation (requires trained model)"
    )

    parser.add_argument(
        "--resume-from-checkpoint",
        type=str,
        help="Resume training from checkpoint"
    )

    # Generation logging options (í•­ìƒ í™œì„±í™”, on/off ì˜µì…˜ ì œê±°)
    parser.add_argument(
        "--generation-log-dir",
        type=str,
        default=None,
        help="Directory to save generation logs (default: {output_dir}/generation_logs)"
    )

    parser.add_argument(
        "--max-generation-samples",
        type=int,
        default=5,
        help="Maximum number of samples to generate for logging (default: 5)"
    )

    parser.add_argument(
        "--generation-log-every-n-steps",
        type=int,
        default=50,
        help="Log generations every N steps (default: 50, í•­ìƒ í™œì„±í™”)"
    )
    
    return parser.parse_args()


def create_config_from_args(args) -> GRPOConfig:
    """Create configuration from command line arguments"""
    logger.info("ğŸ”§ Creating configuration from arguments")
    
    # Start with base configuration
    if args.config:
        # Load config from file
        logger.info(f"ğŸ“ Loading config from {args.config}")
        with open(args.config, 'r') as f:
            config = json.load(f)
            model_name = config["model_init_kwargs"]["model_name"]
        config = create_grpo_config(model_name=model_name, **config)
        # TODO: ì‹¤ì œ íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ êµ¬í˜„ í•„ìš”
    elif args.quick_test:
        config = create_quick_test_config()
        logger.info("âš¡ Using quick test configuration")
    elif args.production:
        config = create_production_config(args.model)
        logger.info("ğŸ­ Using production configuration")
    else:
        # ê¸°ë³¸ ì„¤ì • ìƒì„±
        config = create_grpo_config(
            model_name=f"unsloth/{args.model}-bnb-4bit" if not args.model.startswith("unsloth/") else args.model,
            output_dir=args.output_dir if args.output_dir else "./grpo_outputs"
        )
        logger.info(f"ğŸ”§ Using default configuration for {args.model}")
    
    # Override with command line arguments
    custom_config = {}
    
    if args.max_samples:
        custom_config["max_samples"] = args.max_samples
        logger.info(f"ğŸ“Š Max samples: {args.max_samples}")
    
    if args.epochs:
        custom_config["num_train_epochs"] = args.epochs
        logger.info(f"ğŸ”„ Epochs: {args.epochs}")
    
    if args.learning_rate:
        custom_config["learning_rate"] = args.learning_rate
        logger.info(f"ğŸ“ˆ Learning rate: {args.learning_rate}")
    
    if args.batch_size:
        custom_config["per_device_train_batch_size"] = args.batch_size
        custom_config["per_device_eval_batch_size"] = args.batch_size
        logger.info(f"ğŸ“¦ Batch size: {args.batch_size}")
    
    if args.output_dir:
        custom_config["output_dir"] = args.output_dir
        logger.info(f"ğŸ“ Output directory: {args.output_dir}")
    
    if args.no_wandb:
        custom_config["report_to"] = "none"
        logger.info("ğŸš« Weights & Biases disabled")
    else:
        custom_config["report_to"] = "wandb"
        logger.info(f"ğŸ“Š Weights & Biases project: {args.wandb_project}")

    # Note: Reward function configuration is handled by the trainer
    # TRL GRPOTrainer uses built-in reward functions
    logger.info(f"ğŸ¯ Using default reward function (handled by TRL GRPOTrainer)")


    # Apply custom configuration
    if custom_config:
        for key, value in custom_config.items():
            setattr(config, key, value)

    return config


def get_generation_logging_settings(args) -> tuple[str, int, int]:
    """Generation logging ì„¤ì •ì„ ë°˜í™˜ (í•­ìƒ í™œì„±í™”)"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    log_dir = args.generation_log_dir
    if not log_dir:
        # output_dirì´ argsì— ì—†ìœ¼ë©´ configì—ì„œ ê°€ì ¸ì˜¬ ì˜ˆì •
        log_dir = None  # ë‚˜ì¤‘ì— config.output_dir ì‚¬ìš©

    # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
    max_samples = args.max_generation_samples

    # ë¡œê¹… ì£¼ê¸° (ê¸°ë³¸ê°’ 50 step)
    log_every = getattr(args, 'generation_log_every_n_steps', 5) if hasattr(args, 'generation_log_every_n_steps') else 5

    return log_dir, max_samples, log_every


def load_dataset(args, config: GRPOConfig, reward_type: str):
    """TRL í‘œì¤€ ë°ì´í„°ì…‹ ë¡œë”©"""
    logger.info("ğŸ“¦ Loading dataset with TRL standard")

    # ë°ì´í„° ë¡œë” ìƒì„± (ê°„ì†Œí™”ëœ ë²„ì „)
    model_name = config.model_init_kwargs.get("model_name", "unsloth/Qwen3-0.6B-bnb-4bit")
    max_length = getattr(config, 'max_prompt_length', 2048)

    # TRL í‘œì¤€ ë°ì´í„° ë¡œë”©
    if args.custom_data:
        logger.info(f"ğŸ“ Loading custom data from {args.custom_data}")
        data_loader = GRPODataLoader(model_name, max_length, data_mode=reward_type)
        custom_split = getattr(args, 'split', 'train')  # ê¸°ë³¸ê°’ ì„¤ì •
        dataset = data_loader.load_custom_dataset(args.custom_data, split=custom_split)
    else:
        dataset_name = "HuggingFaceH4/ultrafeedback_binarized"
        max_samples = getattr(args, 'max_samples', 1000)
        split = getattr(args, 'split', 'train_prefs')  # ê¸°ë³¸ê°’ ì„¤ì •
        logger.info(f"ğŸ“¦ Loading dataset: {dataset_name} (split: {split})")
        data_loader = GRPODataLoader(model_name, max_length, data_mode=reward_type)
        dataset = data_loader.load_dataset(dataset_name, split=split, max_samples=max_samples)

    # TRL í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    dataset = data_loader.prepare_grpo_data(dataset)
    splited = dataset.train_test_split(test_size=0.1)
    train_dataset = splited["train"]
    eval_dataset = splited["test"].shuffle(seed=42).select(range(min(100, len(splited["test"]))))

    logger.info(f"âœ… Dataset split: {len(train_dataset)} train, {len(eval_dataset)} eval")
    
    return train_dataset, eval_dataset


def select_reward_function(reward_type: str, config: Dict[str, Any] = None):
    """
    í†µí•© ë³´ìƒ í•¨ìˆ˜ íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        reward_type: ë³´ìƒ í•¨ìˆ˜ íƒ€ì… ("single", "multi", "accuracy", "length", "quality")
        config: ë³´ìƒ í•¨ìˆ˜ ì„¤ì •

    Returns:
        ìƒì„±ëœ ë³´ìƒ í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤
    """
    config = config or {}

    if reward_type == "single":
        return SingleCustomRewardFunction(config)
    elif reward_type == "multi":
        return MultiRewardFunction(config=config)
    elif reward_type == "cmd":
        return CommandRewardFunction(config)
    elif reward_type == "accuracy":
        return AccuracyComponent(config)
    elif reward_type == "length":
        return LengthComponent(config)
    elif reward_type == "quality":
        return QualityComponent(config)
    else:
        raise ValueError(f"Unknown reward type: {reward_type}")


def create_reward_functions(args) -> List:
    """í†µí•© ë³´ìƒ í•¨ìˆ˜ ìƒì„±"""
    logger.info("ğŸ¯ Creating unified reward function")

    # ì„¤ì • íŒŒì¼ì—ì„œ ë³´ìƒ í•¨ìˆ˜ ì„¤ì • ë¡œë“œ
    config = {}
    if args.reward_config and os.path.exists(args.reward_config):
        try:
            with open(args.reward_config, 'r') as f:
                config = json.load(f)
            logger.info(f"ğŸ“ Loaded reward config from {args.reward_config}")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load reward config: {e}")
    
    # data.csv ê²½ë¡œë¥¼ configì— ì¶”ê°€ (CommandRewardFunctionì—ì„œ ì‚¬ìš©)
    if hasattr(args, 'custom_data') and args.custom_data:
        config['data_csv_path'] = args.custom_data
        logger.info(f"ğŸ“ Using data.csv from: {args.custom_data}")
    
    # cmd_bot.csv ê²½ë¡œë„ configì— ì¶”ê°€ (ê¸°ë³¸ ê²½ë¡œ ì‹œë„)
    if 'csv_file_path' not in config:
        possible_paths = [
            "cmd_bot.csv",
            os.path.join(os.getcwd(), "cmd_bot.csv"),
            os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "cmd_bot.csv")
        ]
        for path in possible_paths:
            if os.path.exists(path):
                config['csv_file_path'] = path
                logger.info(f"ğŸ“ Using cmd_bot.csv from: {path}")
                break

    # ë³´ìƒ í•¨ìˆ˜ íƒ€ì…ì— ë”°ë¼ ìƒì„±
    reward_functions = []
    for reward_type in args.reward_function:
        reward_func = select_reward_function(reward_type, config)
        
        # CommandRewardFunctionì¸ ê²½ìš° ê°œë³„ componentë¡œ í™•ì¥
        if isinstance(reward_func, CommandRewardFunction):
            logger.info("ğŸ”€ Expanding CommandRewardFunction to individual components")
            individual_rewards = reward_func.expand_to_individual_rewards()
            reward_functions.extend(individual_rewards)
            logger.info(f"âœ… Expanded into {len(individual_rewards)} component rewards")
        else:
            logger.info("âœ… Created reward function")
            reward_functions.append(reward_func)

    logger.info(f"ğŸ¯ Total reward functions: {len(reward_functions)}")
    return reward_functions


def main():
    """Main training function"""
    logger.info("ğŸš€ Starting GRPO Training")
    
    try:
        # Parse arguments
        args = parse_arguments()
        logger.info(f"ğŸ“‹ Arguments: {vars(args)}")
        
        # Create configuration
        config = create_config_from_args(args)
        logger.info(f"âš™ï¸ Configuration: {config.model_init_kwargs.get('model_name')}")
        
        # Validate configuration
        if not validate_grpo_config(config):
            logger.error("âŒ Configuration validation failed")
            return 1
        
        # Create output directory
        os.makedirs(config.output_dir, exist_ok=True)
        logger.info(f"ğŸ“ Output directory: {config.output_dir}")

        # Create reward functions
        reward_functions = create_reward_functions(args)

        # Load dataset
        train_dataset, eval_dataset = load_dataset(args, config, args.reward_function)

        if len(train_dataset) == 0:
            logger.error("âŒ No training data found")
            return 1

        # Get generation logging settings (í•­ìƒ í™œì„±í™”)
        log_dir, max_samples, log_every = get_generation_logging_settings(args)
        
        # log_dirì´ ì—†ìœ¼ë©´ config.output_dir ì‚¬ìš©
        if not log_dir:
            log_dir = os.path.join(config.output_dir, "generation_logs")

        logger.info(f"ğŸ“Š Generation logging: í•­ìƒ í™œì„±í™”ë¨")
        logger.info(f"ğŸ“ Generation log directory: {log_dir}")
        logger.info(f"ğŸ”¢ Max generation samples: {max_samples}")
        logger.info(f"â±ï¸ Log every {log_every} steps")
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Generation Logging ì„¤ì •")
        print(f"   ìƒíƒœ: í•­ìƒ í™œì„±í™”ë¨ (on/off ì˜µì…˜ ì œê±°ë¨)")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {log_dir}")
        print(f"   ìµœëŒ€ ìƒ˜í”Œ ìˆ˜: {max_samples}")
        print(f"   ë¡œê¹… ì£¼ê¸°: ë§¤ {log_every} stepë§ˆë‹¤")
        print(f"{'='*80}\n")
        wandb.init(project="nlp-cmdbot")
        # Create trainer with model initialization kwargs, reward functions, and generation logging (í•­ìƒ í™œì„±í™”)
        trainer = create_grpo_trainer(
            config=config,
            model_init_kwargs=config.model_init_kwargs,
            reward_functions=reward_functions,
            generation_log_dir=log_dir,
            max_generation_samples=max_samples,
            generation_log_every_n_steps=log_every)
        logger.info("âœ… GRPO Trainer created")
        
        if args.eval_only:
            # Only run evaluation
            logger.info("ğŸ“Š Running evaluation only")
            if eval_dataset is None:
                logger.error("âŒ No evaluation dataset available")
                return 1

            eval_result = trainer.evaluate(eval_dataset=eval_dataset)
            logger.info(f"ğŸ“Š Evaluation results: {eval_result}")
            
        else:
            # Run training
            logger.info("ğŸš€ Starting training")

            # Resume from checkpoint if specified
            if args.resume_from_checkpoint:
                logger.info(f"ğŸ”„ Resuming from checkpoint: {args.resume_from_checkpoint}")
                # Note: Actual checkpoint resuming would need to be implemented

            # Start training
            training_result = trainer.train(train_dataset, eval_dataset)

            logger.info("âœ… Training completed")

            # Save model
            trainer.save_model()
            logger.info("ğŸ’¾ Model saved")
            
            # Run final evaluation
            if eval_dataset:
                logger.info("ğŸ“Š Running final evaluation")
                eval_result = trainer.evaluate(eval_dataset=eval_dataset)
                logger.info(f"ğŸ“Š Final evaluation results: {eval_result}")
        
        logger.info("ğŸ‰ GRPO training completed successfully!")
        return 0
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Training interrupted by user")
        return 1
        
    except Exception as e:
        logger.error(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
