# GRPO (Group Relative Policy Optimization) Training

TRLì˜ í‘œì¤€ GRPO íŠ¸ë ˆì´ë„ˆë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ê°•í™” í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤. TRLì˜ í‘œì¤€ ë°ì´í„° í˜•ì‹ê³¼ ì„¤ì •ì„ ë”°ë¦…ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# TRL í‘œì¤€ ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch transformers datasets accelerate
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install trl wandb

# ë˜ëŠ” requirements.txt ì‚¬ìš©
pip install -r requirements.txt
```

### 2. ê¸°ë³¸ í›ˆë ¨

```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python train_grpo.py --quick-test

# ì •í™•ì„± ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜ë¡œ í›ˆë ¨
python train_grpo.py --reward-function accuracy

# ì—¬ëŸ¬ ë³´ìƒ í•¨ìˆ˜ë¡œ í›ˆë ¨
python train_grpo.py --reward-function accuracy length

# ê¸°ë³¸ í›ˆë ¨ (TRL í‘œì¤€ ë°ì´í„°ì…‹ ì‚¬ìš©)
python train_grpo.py --max-samples 1000

# í”„ë¡œë•ì…˜ í›ˆë ¨
python train_grpo.py --production
```

### 3. ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ í›ˆë ¨

```bash
# JSONL íŒŒì¼ ì‚¬ìš© (TRL í‘œì¤€ í˜•ì‹)
python train_grpo.py --custom-data /path/to/your_data.jsonl

# ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ:
# {"prompt": "ì§ˆë¬¸", "chosen": "ì„ í˜¸ ë‹µë³€", "rejected": "ë¹„ì„ í˜¸ ë‹µë³€"}
```

# ì •í™•ì„± ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©
python train_grpo.py --reward-function accuracy

# ê¸¸ì´ ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©
python train_grpo.py --reward-function length

# ì‚¬ìš©ì ì •ì˜ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©
python train_grpo.py --reward-function custom

# ì—¬ëŸ¬ ë³´ìƒ í•¨ìˆ˜ ê²°í•© ì‚¬ìš©
python train_grpo.py --reward-function accuracy length custom

# ì„¤ì • íŒŒì¼ê³¼ í•¨ê»˜ ì‚¬ìš©
python train_grpo.py --reward-function accuracy --reward-config config/reward_config.json
```


## ğŸ“Š ì§€ì›í•˜ëŠ” ëª¨ë¸

- `llama-3.1-8b`: Llama 3.1 8B (ê¸°ë³¸)
- `llama-3.1-70b`: Llama 3.1 70B
- `gemma-2-9b`: Gemma 2 9B
- `qwen2.5-7b`: Qwen2.5 7B

## ğŸ“¦ ì§€ì›í•˜ëŠ” ë°ì´í„°ì…‹

- `ultrafeedback`: HuggingFaceH4/ultrafeedback_binarized (ê¸°ë³¸)
- `ultrafeedback_large`: ë” í° UltraFeedback ë°ì´í„°ì…‹
- `hh_rlhf`: Anthropic/hh-rlhf
- `openai_summarize`: OpenAI summarize from feedback

## âš™ï¸ ì„¤ì • ì˜µì…˜

### ëª…ë ¹ì¤„ ì˜µì…˜

```bash
python train_grpo.py [OPTIONS]

ì˜µì…˜:
  --model {llama-3.1-8b,llama-3.1-70b,gemma-2-9b,qwen2.5-7b}
                        ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: llama-3.1-8b)
  --dataset {ultrafeedback,ultrafeedback_large,hh_rlhf,openai_summarize}
                        ì‚¬ìš©í•  ë°ì´í„°ì…‹ (ê¸°ë³¸: ultrafeedback)
  --custom-data PATH    ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ
  --config PATH         ì„¤ì • íŒŒì¼ ê²½ë¡œ
  --quick-test          ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
  --production          í”„ë¡œë•ì…˜ í›ˆë ¨ ì‹¤í–‰
  --max-samples N       ì‚¬ìš©í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
  --epochs N            í›ˆë ¨ ì—í¬í¬ ìˆ˜
  --learning-rate LR    í•™ìŠµë¥ 
  --batch-size N        ë°°ì¹˜ í¬ê¸°
  --output-dir PATH     ì¶œë ¥ ë””ë ‰í† ë¦¬
  --reward-function {accuracy,length,custom}
                        ë³´ìƒ í•¨ìˆ˜ íƒ€ì… (ê¸°ë³¸: accuracy)
  --reward-config PATH  ë³´ìƒ í•¨ìˆ˜ ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON)
                        ì»¤ìŠ¤í…€ ë³´ìƒ í•¨ìˆ˜ ì„¤ì • íŒŒì¼
  --wandb-project NAME  Weights & Biases í”„ë¡œì íŠ¸ ì´ë¦„
  --no-wandb           Weights & Biases ë¹„í™œì„±í™”
  --eval-only          í‰ê°€ë§Œ ì‹¤í–‰
  --resume-from-checkpoint PATH
                        ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
```

### ì„¤ì • íŒŒì¼ ì˜ˆì œ

```json
{
  "model_name": "unsloth/llama-3.1-8b-bnb-4bit",
  "max_seq_length": 2048,
  "learning_rate": 5e-7,
  "num_train_epochs": 3,
  "per_device_train_batch_size": 2,
  "per_device_eval_batch_size": 2,
  "gradient_accumulation_steps": 4,
  "max_samples": 1000,
  "output_dir": "./grpo_outputs",
  "beta": 0.1,
  "gamma": 1.0,
  "group_size": 4,
  "reward_function_type": "systematic",
  "reward_config_name": "balanced"
}
```

### ë³´ìƒ í•¨ìˆ˜ ì„¤ì • ì˜ˆì œ

`config/reward_config.json`:
```json
{
  "accuracy": {
    "correct_keywords": ["correct", "right", "yes", "ì •í™•", "ë§ì•„"]
  },
  "length": {
    "optimal_length": 150,
    "length_weight": 0.1
  },
  "custom": {
    "reward_scale": 1.0,
    "penalty_scale": -0.5
  }
}
```

## ğŸ“ ë°ì´í„° í˜•ì‹

### UltraFeedback í˜•ì‹

```json
{
  "chosen": [
    {"role": "user", "content": "What is the capital of France?"},
    {"role": "assistant", "content": "The capital of France is Paris."}
  ],
  "rejected": [
    {"role": "user", "content": "What is the capital of France?"},
    {"role": "assistant", "content": "I don't know."}
  ]
}
```

### ì»¤ìŠ¤í…€ ë°ì´í„° í˜•ì‹

```json
{
  "prompt": "What is the capital of France?",
  "chosen": "The capital of France is Paris.",
  "rejected": "I don't know."
}
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ì»¤ìŠ¤í…€ ì„¤ì • ìƒì„±

```python
from config import create_default_config

# ê¸°ë³¸ ì„¤ì • ìƒì„±
config = create_default_config("llama-3.1-8b", "ultrafeedback")

# ì»¤ìŠ¤í…€ ì„¤ì • ì ìš©
config.learning_rate = 1e-6
config.num_train_epochs = 5
config.max_samples = 5000

# ì„¤ì • ì €ì¥
from config import save_config_to_file
save_config_to_file(config, "my_config.json")
```

### 2. í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í›ˆë ¨

```python
from grpo_trainer import GRPOTrainer, GRPOConfig
from data_loader import create_grpo_dataloader

# ì„¤ì • ìƒì„±
config = GRPOConfig(
    model_name="unsloth/llama-3.1-8b-bnb-4bit",
    max_samples=1000,
    output_dir="./my_grpo_outputs"
)

# ë°ì´í„° ë¡œë” ìƒì„±
data_loader, dataset = create_grpo_dataloader(
    model_name=config.model_name,
    dataset_name="HuggingFaceH4/ultrafeedback_binarized",
    max_samples=config.max_samples
)

# í›ˆë ¨ê¸° ìƒì„± ë° í›ˆë ¨
trainer = GRPOTrainer(config)
trainer.load_model()
trainer.train(dataset)
trainer.save_model()
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### Weights & Biases

```bash
# W&B í”„ë¡œì íŠ¸ ì„¤ì •
python train_grpo.py --wandb-project my-grpo-project

# W&B ë¹„í™œì„±í™”
python train_grpo.py --no-wandb
```

### ë¡œê·¸ íŒŒì¼

í›ˆë ¨ ë¡œê·¸ëŠ” `grpo_training.log` íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.

## ğŸ› ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```bash
   # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   python train_grpo.py --batch-size 1
   
   # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©
   # config.pyì—ì„œ gradient_accumulation_steps ì¦ê°€
   ```

2. **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨**
   ```bash
   # 4bit ì–‘ìí™” ë¹„í™œì„±í™”
   # config.pyì—ì„œ load_in_4bit=Falseë¡œ ì„¤ì •
   ```

3. **ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨**
   ```bash
   # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš©
   # data_loader.pyì—ì„œ streaming=Trueë¡œ ì„¤ì •
   ```

### ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f grpo_training.log

# ì—ëŸ¬ ë¡œê·¸ë§Œ í™•ì¸
grep "ERROR" grpo_training.log
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [Unsloth Documentation](https://github.com/unslothai/unsloth)
- [GRPO Paper](https://arxiv.org/abs/2406.05896)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [TRL Library](https://huggingface.co/docs/trl)

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. ì´ìŠˆ ë¦¬í¬íŠ¸
2. í¬í¬ ë° ë¸Œëœì¹˜ ìƒì„±
3. ë³€ê²½ì‚¬í•­ ì»¤ë°‹
4. í’€ ë¦¬í€˜ìŠ¤íŠ¸ ìƒì„±

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License
