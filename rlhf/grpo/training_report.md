# GRPO (Group Relative Policy Optimization) 학습 진행 리포트

## 📊 학습 개요

- **학습 단계 (Global Steps)**: 0 ~ 120 (약 120 steps)
- **하이라이트 단계**: Global Step 76
- **모델**: unsloth/gemma-3-27b-it-unsloth-bnb-4bit
- **학습 방법**: GRPO (Group Relative Policy Optimization)
- **보상 함수**: ComponentRewardWrapper 기반 다중 보상 시스템
- **핵심 GRPO 설정**:
  - **Num Generations**: 4 (각 프롬프트당 4개의 생성물 생성)
  - Learning Rate: 5e-7
  - Batch Size: 1 (per device)
  - Max Prompt Length: 2048
  - Max Completion Length: 762
  - Beta (KL penalty): 0.1

---

## 🎯 GRPO 핵심 메커니즘 분석

### GRPO 작동 원리

GRPO는 **그룹 상대 정책 최적화** 방식으로, 각 프롬프트마다 **4개의 서로 다른 생성물**을 생성하고, 이들을 **그룹으로 묶어 상대적 비교**를 통해 학습합니다.

1. **생성 단계**: 각 프롬프트당 `num_generations=4`개의 생성물 생성
2. **보상 계산**: 각 생성물에 대해 개별 보상 계산
3. **그룹 평균화**: `mean_grouped_rewards = rewards.view(-1, 4).mean(dim=1)` - 4개 생성물의 보상을 그룹화하여 평균
4. **상대적 학습**: 그룹 내 상대적 순위를 기반으로 정책 업데이트

---

## 📈 GRPO 특화 메트릭 분석

### 1. 그룹 평균 보상 (`train/reward`)

- **기본 수준**: 약 **2.0** (안정적 유지)
- **의미**: 각 프롬프트당 생성된 **4개 생성물의 평균 보상**
- **주기적 하락**: Global Steps 5, 45, 78, 115에서 약 **1.3**으로 급락
- **하이라이트 (Step 76)**: **2.0** (정상 수준)
- **GRPO 관점 분석**: 
  - 평균 보상 2.0은 4개 생성물 그룹의 평균 품질이 높음을 의미
  - 주기적 하락은 어려운 프롬프트에서 4개 생성물 모두 낮은 보상을 받았거나, 그룹 내 다양성이 부족했을 가능성
  - 전반적으로 그룹 평균 보상이 높게 유지되어 **GRPO 학습이 효과적으로 진행**되고 있음

### 2. 그룹 내 보상 분산 (`train/reward_std`)

- **기본 수준**: **0** (대부분의 시간)
- **의미**: **그룹 간 보상 분산** (각 프롬프트의 4개 생성물 그룹 평균 간의 표준편차)
- **스파이크**: Global Steps 5, 50, 78, 115에서 최대 **0.35**까지 상승
- **하이라이트 (Step 76)**: **0** (안정적)
- **GRPO 관점 분석**: 
  - 표준편차 0은 **모든 프롬프트 그룹의 평균 보상이 거의 동일**함을 의미
  - 이는 모델이 일관된 품질의 생성물 그룹을 생성하고 있음을 나타냄
  - 스파이크 시점은 **프롬프트 간 난이도 차이**가 커져서 일부 그룹은 높은 보상, 일부 그룹은 낮은 보상을 받았을 때 발생
  - GRPO의 그룹 평균화 메커니즘이 작동하여 대부분의 시간 동안 일관된 보상 분포 유지

### 3. ComponentRewardWrapper 그룹 평균 (`train/rewards/ComponentRewardWrapper/mean`)

- **기본 수준**: 약 **0.5** (안정적)
- **의미**: Component 기반 보상의 **그룹 평균값**
- **주기적 하락**: Global Steps 5, 45, 78, 115에서 약 **0.33**으로 하락
- **하이라이트 (Step 76)**: **0.5** (정상 수준)
- **GRPO 관점 분석**: 
  - Component 보상도 그룹 평균화되어 안정적으로 유지
  - 전체 보상과 유사한 패턴으로 **Component 보상이 GRPO 그룹 메커니즘과 잘 통합**되어 있음

### 4. ComponentRewardWrapper 그룹 분산 (`train/rewards/ComponentRewardWrapper/std`)

- **기본 수준**: **0** (대부분의 시간)
- **스파이크**: Global Steps 5, 50, 78, 115에서 최대 **0.09**까지 상승
- **하이라이트 (Step 76)**: **0** (안정적)
- **GRPO 관점 분석**: 
  - Component 보상의 그룹 간 분산이 매우 낮음 (0.09 vs 전체 보상 0.35)
  - **Component 보상이 그룹 내에서 더 일관적**으로 계산되고 있음을 의미
  - GRPO의 그룹 평균화가 Component 보상에도 효과적으로 적용됨

### 5. 학습 손실 (`train/loss`)

- **기본 수준**: **0**에 근접 (매우 낮음)
- **주요 스파이크 시점**: Steps 5, 15, 25, 35, 45, 55, 60, 70, 78, 90, 98, 105, 115
- **최대 스파이크**: Step 115에서 약 **0.0025**
- **하이라이트 (Step 76)**: **0** (안정적)
- **GRPO 관점 분석**: 
  - GRPO 손실은 그룹 내 상대적 순위 기반이므로 절대값이 낮을 수 있음
  - 손실이 거의 0인 것은 **그룹 내 생성물들이 이미 상대적으로 잘 정렬**되어 있음을 의미
  - 주기적 스파이크는 **새로운 프롬프트 패턴**에서 그룹 내 순위 재정렬이 필요할 때 발생
  - GRPO의 그룹 상대 학습이 효과적으로 작동하여 손실이 낮게 유지됨

### 6. 토큰 처리량 (`train/num_tokens`)

- **누적 토큰 수**: 약 **380,000** tokens (Step 120 기준)
- **하이라이트 (Step 76)**: **260,447** tokens
- **증가 패턴**: 선형적이고 일관된 증가
- **GRPO 관점 분석**: 
  - 각 step마다 **4배의 생성물**이 생성되므로 (num_generations=4), 실제 생성 토큰은 더 많음
  - 약 3,167 tokens/step의 평균 처리 속도는 **그룹당 평균 토큰 수**
  - GRPO의 다중 생성 오버헤드에도 불구하고 안정적인 토큰 처리량 유지

---

## ⏱️ GRPO 성능 프로파일링 분석

### 1. 그룹 생성 시간 (`profiling/Time taken: CustomGRPOTrainer.transformers.generate`)

- **기본 수준**: 500 ~ 1,200 단위
- **GRPO 특성**: 각 프롬프트당 **4개 생성물 생성**이므로 시간이 상대적으로 길 수 있음
- **주요 변화**:
  - 초기 20 steps: 500 단위로 안정적
  - Step 25: 급격한 하락 (거의 0) - 평가 단계일 가능성
  - Steps 25-70: 900-1,000 단위로 회복
  - **Step 80: 최대 피크 (2,200+ 단위)** - 긴 시퀀스 생성 또는 복잡한 프롬프트 처리
  - Steps 80-120: 1,000-1,200 단위로 안정화
- **GRPO 관점 분석**: 
  - 4개 생성물을 순차적으로 생성하므로 생성 시간이 중요
  - Step 80의 피크는 **긴 시퀀스나 복잡한 프롬프트**에서 4개 생성물 모두 생성하는 데 시간이 오래 걸렸을 가능성
  - 이후 안정화는 모델이 효율적인 생성 패턴을 학습했음을 의미

### 2. 입력 준비 시간 (`profiling/Time taken: CustomGRPOTrainer._prepare_inputs`)

- **패턴**: `transformers.generate`와 거의 동일한 패턴
- **GRPO 관점 분석**: 
  - `_prepare_inputs`에서 **4개 생성물을 위한 입력 준비 및 생성 실행**이 포함
  - 생성 과정이 입력 준비의 주요 병목 지점
  - GRPO의 그룹 생성 오버헤드가 여기에 반영됨

### 3. 그룹 보상 계산 시간 (`profiling/Time taken: CustomGRPOTrainer._calculate_rewards`)

- **기본 수준**: 약 **0.002** 단위 (매우 낮음)
- **스파이크**: Steps 20 이후 주기적으로 **0.015-0.025** 단위로 상승
- **GRPO 관점 분석**: 
  - 4개 생성물 각각에 대해 보상을 계산하지만, 계산 시간은 상대적으로 작음
  - 주기적 스파이크는 **복잡한 보상 계산이 필요한 생성물**이 그룹 내에 포함되었을 때 발생
  - GRPO의 보상 계산이 효율적으로 구현되어 있음

### 4. ComponentRewardWrapper 그룹 계산 시간 (`profiling/Time taken: CustomGRPOTrainer.ComponentRewardWrapper`)

- **기본 수준**: 거의 **0** (매우 낮음)
- **스파이크**: Steps 20 이후 주기적으로 **0.003-0.005** 단위로 상승
- **GRPO 관점 분석**: 
  - Component 보상 계산이 그룹 내 4개 생성물에 대해 효율적으로 수행됨
  - 전체 보상 계산 시간의 일부만 차지하여 **GRPO 오버헤드가 최소화**됨

---

## 🔍 GRPO 학습 패턴 심층 분석

### 그룹 내 다양성 vs 일관성

**관찰된 패턴**:
- **보상 표준편차 0**: 그룹 간 평균 보상이 일관적
- **주기적 스파이크**: 프롬프트 난이도 차이로 인한 그룹 간 분산 증가

**GRPO 해석**:
1. **그룹 내 다양성**: 각 프롬프트마다 4개의 서로 다른 생성물 생성
2. **그룹 간 일관성**: 평균 보상이 일관적 (표준편차 0) → 모델이 **일관된 품질의 그룹**을 생성
3. **상대적 학습**: 그룹 내 상대적 순위를 통해 학습하므로, 절대 보상보다 **그룹 내 순위**가 중요

### 주기적 변동성의 GRPO 해석

**주요 변동 시점**: Steps 5, 45, 78, 115

**가능한 원인**:
1. **어려운 프롬프트 배치**: 해당 step에서 어려운 프롬프트들이 배치되어, 4개 생성물 그룹 모두 낮은 보상
2. **그룹 내 다양성 부족**: 4개 생성물이 유사하여 그룹 내 상대적 학습 효과 감소
3. **보상 함수 민감도**: 특정 프롬프트 패턴에서 보상 함수가 더 엄격하게 평가

**GRPO 관점**:
- 주기적 하락은 **정상적인 학습 과정**의 일부
- 그룹 평균화로 인해 개별 생성물의 변동성이 완화됨
- 상대적 학습 메커니즘으로 인해 절대 보상 하락에도 불구하고 학습은 계속 진행

### Step 76 안정성 분석

**Step 76 하이라이트**: 모든 메트릭이 안정적
- 보상: 2.0 (그룹 평균 보상이 높음)
- 표준편차: 0 (그룹 간 일관성)
- 손실: 0 (그룹 내 상대적 정렬 완료)
- 토큰: 260,447 (정상 증가)

**GRPO 해석**:
- **이상적인 GRPO 학습 상태**
- 그룹 내 생성물들이 상대적으로 잘 정렬되어 있음
- 그룹 간 평균 보상이 일관적
- 모델이 **안정적인 그룹 생성 패턴**을 학습한 상태

---

## 🎯 GRPO 특화 성능 평가

### 강점

1. **효과적인 그룹 평균화**: 
   - 그룹 간 보상 분산이 낮음 (표준편차 0)
   - 4개 생성물 그룹의 평균 품질이 일관적으로 유지

2. **안정적인 그룹 생성**: 
   - 대부분의 시간 동안 높은 그룹 평균 보상 (2.0)
   - 모델이 일관된 품질의 생성물 그룹을 생성

3. **효율적인 상대적 학습**: 
   - 손실이 거의 0으로 그룹 내 상대적 정렬이 잘 되어 있음
   - GRPO의 그룹 상대 학습 메커니즘이 효과적으로 작동

4. **최소한의 오버헤드**: 
   - 4개 생성물 생성에도 불구하고 보상 계산 시간이 작음
   - 생성 시간이 안정적으로 유지 (대부분 500-1,200 단위)

### 개선 필요 사항

1. **주기적 그룹 보상 하락**: 
   - Steps 5, 45, 78, 115에서 그룹 평균 보상 하락
   - **권장 조치**: 
     - 프롬프트 난이도 분산 분석
     - 그룹 내 생성물 다양성 증가 (temperature, top_p 조정)
     - 어려운 프롬프트에 대한 그룹 생성 전략 개선

2. **생성 시간 피크**: 
   - Step 80에서 생성 시간 급증
   - **권장 조치**: 
     - 긴 시퀀스 생성 시 early stopping 고려
     - 생성 파라미터 최적화 (max_new_tokens 조정)
     - 배치 내 프롬프트 길이 균형화

3. **그룹 간 분산 스파이크**: 
   - 주기적으로 보상 표준편차 증가
   - **권장 조치**: 
     - 그룹 평균 보상 정규화
     - 프롬프트 난이도 기반 그룹 가중치 조정
     - 보상 클리핑을 통한 안정화

### GRPO 학습 방향성

**현재 상태**: 
- GRPO의 그룹 상대 학습이 효과적으로 작동
- 그룹 평균 보상이 높고 일관적
- 그룹 내 상대적 정렬이 잘 되어 있음

**예상 결과**: 
- 현재 추세가 유지되면 모델이 **더 나은 그룹 생성 패턴**을 학습
- 그룹 내 다양성과 품질의 균형이 개선될 것으로 예상

**권장 조치**: 
- 그룹 내 생성물 다양성 모니터링 (4개 생성물 간 차이 분석)
- 그룹 평균 보상의 장기적 추세 추적
- 주기적 변동성 원인 분석 (프롬프트 난이도, 생성 다양성 등)

---

## 🎨 GRPO 생성 품질 분석

### 그룹 생성 특성

GRPO는 각 프롬프트마다 **4개의 서로 다른 생성물**을 생성하므로:

1. **그룹 내 다양성**: 
   - 4개 생성물이 서로 다른 스타일/접근 방식으로 생성
   - 상대적 비교를 통한 학습 가능

2. **그룹 평균 품질**: 
   - 개별 생성물보다 그룹 평균 보상이 중요
   - 일부 생성물이 낮은 보상을 받아도 그룹 평균이 높으면 학습 진행

3. **상대적 순위**: 
   - 그룹 내 최고/최저 보상 생성물 간 상대적 차이가 학습에 중요
   - 절대 보상보다 그룹 내 순위가 GRPO 학습의 핵심

### 생성 샘플 예시 (Step 1)

초기 학습 단계에서 생성된 샘플들을 분석한 결과:

1. **질문-답변 형식**: 모델이 질문에 대해 적절한 답변을 생성
2. **정보 정확성**: 생성된 내용이 사실적으로 정확함
3. **응답 길이**: 적절한 길이의 상세한 답변 생성
4. **구조적 일관성**: 논리적이고 구조화된 응답 생성

**GRPO 관점**: 
- 초기 단계에서도 일관된 품질의 생성물을 생성
- 4개 생성물 그룹이 모두 높은 보상을 받았을 가능성이 높음
- 그룹 평균 보상이 높게 유지되는 이유를 설명

---

## 📝 GRPO 학습 결론

GRPO 학습이 **전반적으로 효과적으로 진행**되고 있습니다.

### 핵심 성과

- ✅ **그룹 평균 보상이 높고 일관적** (2.0, 표준편차 0)
- ✅ **그룹 내 상대적 정렬 완료** (손실 거의 0)
- ✅ **안정적인 그룹 생성 패턴** 학습
- ✅ **효율적인 GRPO 메커니즘** 작동 (최소 오버헤드)

### GRPO 특화 관찰

- **그룹 평균화 효과**: 4개 생성물의 평균 보상이 일관적으로 유지
- **상대적 학습 성공**: 그룹 내 상대적 순위 기반 학습이 효과적
- **안정적인 그룹 생성**: 모델이 일관된 품질의 생성물 그룹을 생성

**Step 76 시점**에서 모든 메트릭이 안정적인 상태를 보여, **GRPO의 그룹 상대 학습 메커니즘이 정상적으로 작동**하고 있음을 확인할 수 있습니다.

---

*리포트 생성 시간: 학습 진행 중 (Global Step 0-120)*
*데이터 소스: WandB 대시보드 및 학습 로그*
*GRPO 특화 분석: Group Relative Policy Optimization 메커니즘 기반*
