# G3MoE SFT Training

ì´ ë””ë ‰í† ë¦¬ëŠ” TRL (Transformers Reinforcement Learning)ì„ ì‚¬ìš©í•˜ì—¬ G3MoE ëª¨ë¸ì„ Supervised Fine-Tuning(SFT)í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

## íŒŒì¼ êµ¬ì¡°

```
sft/
â”œâ”€â”€ custom_model_sft.py          # ë©”ì¸ SFT í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_g3moe_sft.sh            # í›ˆë ¨ ì‹¤í–‰ ì…¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ config/
â”‚   â””â”€â”€ g3moe_sft_config.json   # í›ˆë ¨ ì„¤ì • íŒŒì¼
â””â”€â”€ README.md                   # ì´ íŒŒì¼
```

## í•„ìš”í•œ íŒ¨í‚¤ì§€

```bash
pip install torch transformers trl peft datasets wandb accelerate
```

ë˜ëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ:

```bash
pip install -r requirements.txt
```

## ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ ì‹¤í–‰

**ì¼ë°˜ í›ˆë ¨ (ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸):**
```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
./sft/run_g3moe_sft.sh

# ì»¤ìŠ¤í…€ ì„¤ì • íŒŒì¼ë¡œ ì‹¤í–‰
./sft/run_g3moe_sft.sh path/to/your/config.json

# í›ˆë ¨ í›„ í‰ê°€ê¹Œì§€ ìˆ˜í–‰
./sft/run_g3moe_sft.sh sft/config/g3moe_sft_config.json --eval
```

**ğŸš€ DeepSpeed + 120K ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨ (80GB GPU ìµœì í™”):**
```bash
# ë‹¨ì¼ GPU 120K ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨
./sft/run_g3moe_deepspeed.sh sft/config/g3moe_120k_deepspeed_config.json

# ë©€í‹° GPU 120K ì»¨í…ìŠ¤íŠ¸ í›ˆë ¨ (2 GPUs)
./sft/run_g3moe_deepspeed.sh sft/config/g3moe_120k_deepspeed_config.json 2

# ZeRO-2 ì‚¬ìš© (ì¼ë°˜ ë°ì´í„°ì…‹)
./sft/run_g3moe_deepspeed.sh sft/config/g3moe_large_dataset_config.json

# ZeRO-3 ì‚¬ìš© (ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½)
./sft/run_g3moe_deepspeed.sh sft/config/g3moe_120k_deepspeed_config.json
```

### 2. Python ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
python sft/custom_model_sft.py

# JSON ì„¤ì • íŒŒì¼ ì‚¬ìš©
python sft/custom_model_sft.py sft/config/g3moe_sft_config.json

# ëª…ë ¹í–‰ ì¸ì ì‚¬ìš©
python sft/custom_model_sft.py \
    --model_name_or_path google/gemma-3-4b-it \
    --output_dir ./outputs/g3moe_sft \
    --dataset_name Gunulhona/open_m_3 \
    --num_train_epochs 3 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --learning_rate 2e-5 \
    --use_lora True \
    --lora_r 32 \
    --bf16 True
```

## Google ê³µì‹ Gemini Fine-tuning ê°€ì´ë“œë¼ì¸ ì ìš©

Google Cloudì˜ [Gemini fine-tuning ê°€ì´ë“œ](https://medium.com/google-cloud/fine-tuning-gemini-best-practices-for-data-hyperparameters-and-evaluation-65f7c7b6b15f)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¶Œì¥ í•˜ì´í¼íŒŒë¼ë¯¸í„°:

### ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°ë³„ ê¶Œì¥ ì„¤ì •

**ì†Œê·œëª¨ ë°ì´í„°ì…‹ (< 1000ê°œ ì˜ˆì œ, í‰ê·  ì»¨í…ìŠ¤íŠ¸ < 500í† í°):**
```bash
./sft/run_g3moe_sft.sh sft/config/g3moe_small_dataset_config.json
```
- `epochs = 20`
- `learning_rate = 2e-4` (Google ê¶Œì¥ multiplier 10x ì ìš©)
- `lora_r = 4` (adapter size)
- `max_seq_length = 1024`

**ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (>= 1000ê°œ ì˜ˆì œ ë˜ëŠ” í‰ê·  ì»¨í…ìŠ¤íŠ¸ >= 500í† í°):**
```bash
./sft/run_g3moe_sft.sh sft/config/g3moe_large_dataset_config.json
```
- `epochs = 10`
- `learning_rate = 1e-4` (ê¸°ë³¸ ë˜ëŠ” 5x multiplier)
- `lora_r = 8` (ë” í° adapter size)
- `max_seq_length = 2048`

### ğŸ¯ í•µì‹¬ ì›ì¹™

1. **í’ˆì§ˆ > ì–‘**: Googleì€ ê³ í’ˆì§ˆì˜ ì‘ì€ ë°ì´í„°ì…‹ì´ í° ë…¸ì´ì¦ˆ ë°ì´í„°ì…‹ë³´ë‹¤ íš¨ê³¼ì ì´ë¼ê³  ê°•ì¡°
2. **LoRA ì‚¬ìš©**: Vertex AIëŠ” LoRAë¥¼ í™œìš©í•˜ì—¬ ì ì€ íŒŒë¼ë¯¸í„°ë¡œ íš¨ê³¼ì ì¸ fine-tuning ìˆ˜í–‰
3. **ë³µì¡í•œ ì˜ˆì œ ì¤‘ì‹¬**: ê¸°ë³¸ ëª¨ë¸ì´ ì–´ë ¤ì›Œí•˜ëŠ” ì˜ˆì œì— ì§‘ì¤‘
4. **ê²€ì¦ ë°ì´í„°ì…‹ í•„ìˆ˜**: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë³„ë„ ê²€ì¦ ì„¸íŠ¸ ìœ ì§€

## ì„¤ì • ì˜µì…˜

### ëª¨ë¸ ì„¤ì • (ModelArguments)

- `model_name_or_path`: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HF Hub ID
- `tokenizer_name_or_path`: í† í¬ë‚˜ì´ì € ê²½ë¡œ (Noneì´ë©´ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©)
- `use_lora`: LoRA ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
- `lora_r`: LoRA rank (ê¸°ë³¸ê°’: 16)
- `lora_alpha`: LoRA alpha íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’: 32)
- `lora_dropout`: LoRA dropout (ê¸°ë³¸ê°’: 0.1)
- `trust_remote_code`: ì›ê²© ì½”ë“œ ì‹ ë¢° ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

### ë°ì´í„° ì„¤ì • (DataArguments)

- `dataset_name`: ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„ (ê¸°ë³¸ê°’: "Gunulhona/open_m_3")
- `max_seq_length`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 2048)
- `test_size`: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1)
- `text_only`: í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
- `streaming`: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)

### í›ˆë ¨ ì„¤ì • (SFTConfig)

ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°:
- `num_train_epochs`: í›ˆë ¨ ì—í¬í¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
- `per_device_train_batch_size`: ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 1)
- `gradient_accumulation_steps`: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: 16)
- `learning_rate`: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 2e-5)
- `weight_decay`: ê°€ì¤‘ì¹˜ ê°ì‡  (ê¸°ë³¸ê°’: 0.01)
- `lr_scheduler_type`: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ê¸°ë³¸ê°’: "cosine")
- `warmup_ratio`: ì›œì—… ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.03)

## ğŸ”¥ 80GB GPU + 120K ì»¨í…ìŠ¤íŠ¸ ìµœì í™”

### DeepSpeed ZeRO ì‚¬ìš© (í•„ìˆ˜)

**ZeRO-2 ì„¤ì • (ê¸°ë³¸ ê¶Œì¥):**
```json
{
    "deepspeed": "sft/config/deepspeed_zero2.json",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 32
}
```

**ZeRO-3 ì„¤ì • (ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½):**
```json
{
    "deepspeed": "sft/config/deepspeed_zero3.json",
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 64
}
```

### 120K ì»¨í…ìŠ¤íŠ¸ íŠ¹í™” ì„¤ì •

```json
{
    "max_seq_length": 120000,
    "gradient_checkpointing": true,
    "dataloader_num_workers": 0,
    "dataloader_pin_memory": false,
    "learning_rate": 5e-5,
    "num_train_epochs": 5
}
```

### ë©”ëª¨ë¦¬ ìµœì í™” íŒ

1. **LoRA ì‚¬ìš©**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ
2. **CPU Offload**: DeepSpeed ZeRO-3ì˜ optimizer/parameter offload
3. **í™˜ê²½ ë³€ìˆ˜**: `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024`
4. **ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­**: ìµœì†Œ 64GB RAM ê¶Œì¥

## ëª¨ë‹ˆí„°ë§

### Weights & Biases

WandB ë¡œê·¸ì¸ í›„ ì„¤ì •ì—ì„œ í™œì„±í™”:

```bash
wandb login
```

```json
{
    "report_to": ["wandb"],
    "run_name": "g3moe-sft-experiment"
}
```

### ë¡œì»¬ ë¡œê·¸

```json
{
    "logging_dir": "./outputs/g3moe_sft/logs",
    "logging_steps": 5,
    "eval_steps": 100,
    "save_steps": 500
}
```

## ë©€í‹°GPU í›ˆë ¨

### DeepSpeed ë©€í‹°GPU í›ˆë ¨

**ìë™ GPU ê°ì§€ë¡œ í›ˆë ¨:**
```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPU ì‚¬ìš©
./sft/run_g3moe_deepspeed.sh sft/config/g3moe_120k_deepspeed_config.json

# íŠ¹ì • GPU ê°œìˆ˜ ì§€ì •
./sft/run_g3moe_deepspeed.sh sft/config/g3moe_120k_deepspeed_config.json 4
```

**ìˆ˜ë™ torchrun ì‹¤í–‰:**
```bash
# 4 GPU DeepSpeed í›ˆë ¨
torchrun --nproc_per_node=4 sft/custom_model_sft.py sft/config/g3moe_120k_deepspeed_config.json

# íŠ¹ì • GPUë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 sft/custom_model_sft.py sft/config/g3moe_120k_deepspeed_config.json
```

### DeepSpeed ì„¤ì • íŒŒì¼ë“¤

- `sft/config/deepspeed_zero2.json`: ZeRO-2 (ê· í˜•ì¡íŒ ì„±ëŠ¥)
- `sft/config/deepspeed_zero3.json`: ZeRO-3 (ìµœëŒ€ ë©”ëª¨ë¦¬ ì ˆì•½ + CPU offload)

## í›ˆë ¨ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (Google ê°€ì´ë“œë¼ì¸)

### ğŸ“ˆ í•µì‹¬ ë©”íŠ¸ë¦­

**Training Loss & Validation Loss:**
- Training lossëŠ” ê°ì†Œí•´ì•¼ í•¨
- Validation lossê°€ training lossë³´ë‹¤ í˜„ì €íˆ ë†’ìœ¼ë©´ ê³¼ì í•© ì‹ í˜¸
- ì´ìƒì : ë‘ loss ëª¨ë‘ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œ

**Next Token Prediction Accuracy:**
- ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ì •í™•ë„ê°€ ì‹œê°„ì— ë”°ë¼ ì¦ê°€í•´ì•¼ í•¨

### ğŸš¨ ë¬¸ì œ ìœ í˜•ë³„ ì§„ë‹¨

**1. ì„±ëŠ¥ ë¶€ì¡± (Suboptimal Performance)**
- **ì¦ìƒ**: Training/validation lossê°€ ê°ì†Œí•˜ì§€ë§Œ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ
- **ì›ì¸**: ë°ì´í„°ì…‹ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ ë‹¤ì–‘ì„± ë¶€ì¡±
- **í•´ê²°ì±…**: 
  - epochs ìˆ˜ ì¦ê°€ ë˜ëŠ” learning rate multiplier ì¦ê°€
  - ë” ë§ì€ ê³ í’ˆì§ˆ ë°ì´í„° ìˆ˜ì§‘

**2. ê³¼ì í•© (Overfitting)**
- **ì¦ìƒ**: Training lossëŠ” ê°ì†Œí•˜ì§€ë§Œ validation lossê°€ ì¦ê°€
- **ì›ì¸**: ëª¨ë¸ ìš©ëŸ‰ì´ ë°ì´í„° ëŒ€ë¹„ ê³¼ë„í•¨
- **í•´ê²°ì±…**:
  - epochs ìˆ˜ë¥¼ validation loss ìµœì†Œì ê¹Œì§€ ê°ì†Œ
  - í›ˆë ¨ ë°ì´í„° í¬ê¸°ì™€ ë‹¤ì–‘ì„± ì¦ê°€

**3. ë°ì´í„° ë¬¸ì œ (Data Issues)**
- **ì¦ìƒ**: ì´ˆê¸° lossê°€ ë§¤ìš° ë†’ìŒ (>10)
- **ì›ì¸**: ì…ë ¥ ê¸¸ì´ê°€ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼ë¡œ ì¸í•œ truncation
- **í•´ê²°ì±…**: ë°ì´í„°ì…‹ í˜•ì‹ê³¼ ê¸¸ì´ ì¬ê²€í† 

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. CUDA Out of Memory

- ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°: `per_device_train_batch_size`ë¥¼ 1ë¡œ ì„¤ì •
- ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì¦ê°€: `gradient_accumulation_steps` ì¦ê°€
- ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
- ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°: `max_seq_length` ê°ì†Œ

### 2. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨

G3MoE ëª¨ë¸ì´ ì•„ì§ ê³µê°œë˜ì§€ ì•Šì€ ê²½ìš°, ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ ê¸°ë³¸ Gemma ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.

### 3. ë°ì´í„°ì…‹ ë¡œë”© ì˜¤ë¥˜

- ì¸í„°ë„· ì—°ê²° í™•ì¸
- HuggingFace Hub ì ‘ê·¼ ê¶Œí•œ í™•ì¸
- ë¡œì»¬ ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œ ê²½ë¡œ í™•ì¸

## ì˜ˆì œ ì‹¤í–‰ ê²°ê³¼

```
========================================
      G3MoE SFT Training Script        
========================================
Project Root: /path/to/llm_training
Config File: /path/to/config.json
Output Directory: ./outputs/g3moe_sft
CUDA Devices: 0
All dependencies found!
GPU detected: 1 device(s) available
Starting G3MoE SFT training...

Setting up model and tokenizer...
Loaded G3MoE config
Loaded G3MoE model successfully

Setting up dataset...
Loading dataset: Gunulhona/open_m_3
Dataset loaded:
  train: 8000 examples
  test: 2000 examples

Setting up trainer...
Starting training...
Training completed successfully!
Model saved to: ./outputs/g3moe_sft
```

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [TRL ë¬¸ì„œ](https://huggingface.co/docs/trl/)
- [PEFT ë¬¸ì„œ](https://huggingface.co/docs/peft/)
- [G3MoE ë…¼ë¬¸](https://arxiv.org/abs/2501.xxxxx) (ì˜ˆì •) 