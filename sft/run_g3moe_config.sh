#!/bin/bash

# G3MoE SFT Training Script with Config File
# Usage: ./run_g3moe_config.sh [config_file] [options]
# Options:
#   --resume: Resume from checkpoint
#   --dry-run: Show config and exit without training
#   --multi-gpu: Use multiple GPUs with torchrun
#   --help: Show this help message

set -e

# Function to show help
show_help() {
    echo "G3MoE SFT Training Script"
    echo "========================="
    echo ""
    echo "Usage: $0 [config_file] [options]"
    echo ""
    echo "Arguments:"
    echo "  config_file    Path to JSON config file (default: sft/config/g3moe_training_config.json)"
    echo ""
    echo "Options:"
    echo "  --resume       Resume training from checkpoint"
    echo "  --dry-run      Show configuration and exit without training"
    echo "  --multi-gpu    Use multiple GPUs with torchrun"
    echo "  --help         Show this help message"
    echo ""
    echo "Examples:"
    echo "  $0                                                    # Use default config"
    echo "  $0 sft/config/g3moe_deepspeed_config.json           # Use specific config"
    echo "  $0 --multi-gpu                                       # Multi-GPU training"
    echo "  $0 my_config.json --resume                           # Resume training"
    echo "  $0 --dry-run                                         # Check config only"
    echo ""
    echo "Available config files:"
    find sft/config -name "*.json" -type f 2>/dev/null | sort || echo "  No config files found in sft/config/"
}

# Parse arguments
CONFIG_FILE=""
RESUME=false
DRY_RUN=false
MULTI_GPU=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --help|-h)
            show_help
            exit 0
            ;;
        --resume)
            RESUME=true
            shift
            ;;
        --dry-run)
            DRY_RUN=true
            shift
            ;;
        --multi-gpu)
            MULTI_GPU=true
            shift
            ;;
        --*)
            echo "âŒ Unknown option: $1"
            echo "Use --help for usage information"
            exit 1
            ;;
        *)
            if [ -z "$CONFIG_FILE" ]; then
                CONFIG_FILE="$1"
            else
                echo "âŒ Multiple config files specified"
                exit 1
            fi
            shift
            ;;
    esac
done

# Set default config file if not specified
CONFIG_FILE=${CONFIG_FILE:-"sft/config/g3moe_training_config.json"}

echo "ğŸš€ G3MoE SFT Training Script"
echo "============================"
echo "Config file: $CONFIG_FILE"
echo "Resume: $RESUME"
echo "Dry run: $DRY_RUN"
echo "Multi-GPU: $MULTI_GPU"
echo ""

# Check if config file exists
if [ ! -f "$CONFIG_FILE" ]; then
    echo "âŒ Config file not found: $CONFIG_FILE"
    echo ""
    echo "Available config files:"
    find sft/config -name "*.json" -type f 2>/dev/null | sort || echo "  No config files found in sft/config/"
    exit 1
fi

# Validate JSON config
echo "ğŸ” Validating config file..."
if ! python -c "import json; json.load(open('$CONFIG_FILE'))" 2>/dev/null; then
    echo "âŒ Invalid JSON in config file: $CONFIG_FILE"
    exit 1
fi
echo "âœ… Config file is valid JSON"

# Show config summary
echo ""
echo "ğŸ“‹ Configuration Summary:"
echo "------------------------"
python -c "
import json
config = json.load(open('$CONFIG_FILE'))
print(f\"Model: {config['model_config']['model_name_or_path']}\")
print(f\"Dataset: {config['data_config']['dataset_name']}\")
print(f\"Max seq length: {config['data_config']['max_seq_length']:,}\")
print(f\"Epochs: {config['training_config']['num_train_epochs']}\")
print(f\"Batch size: {config['training_config']['per_device_train_batch_size']}\")
print(f\"Learning rate: {config['training_config']['learning_rate']}\")
print(f\"Output dir: {config['training_config']['output_dir']}\")
if config['model_config'].get('deepspeed_config'):
    print(f\"DeepSpeed: {config['model_config']['deepspeed_config']}\")
print(f\"LoRA: {config['model_config']['use_lora']} (r={config['model_config']['lora_r']})\")
g3moe = config['model_config']['g3moe_params']
print(f\"G3MoE: {g3moe['n_routed_experts']} experts, {g3moe['n_group']} groups\")
"

# Exit if dry run
if [ "$DRY_RUN" = true ]; then
    echo ""
    echo "ğŸ Dry run completed. Exiting without training."
    exit 0
fi

# Check Python environment
echo ""
echo "ğŸ” Checking Python environment..."
python --version

# Check required packages
REQUIRED_PACKAGES=("transformers" "trl" "peft" "torch" "datasets")
MISSING_PACKAGES=()

for package in "${REQUIRED_PACKAGES[@]}"; do
    if ! pip show "$package" > /dev/null 2>&1; then
        MISSING_PACKAGES+=("$package")
    fi
done

if [ ${#MISSING_PACKAGES[@]} -ne 0 ]; then
    echo "âŒ Missing required packages: ${MISSING_PACKAGES[*]}"
    echo "Please install requirements:"
    echo "pip install -r requirements.txt"
    exit 1
fi
echo "âœ… All required packages are installed"

# Check GPU availability and memory
echo ""
if command -v nvidia-smi &> /dev/null; then
    GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
    echo "ğŸ® GPU Status ($GPU_COUNT GPUs detected):"
    nvidia-smi --query-gpu=index,name,memory.total,memory.free,utilization.gpu --format=csv,noheader,nounits | while read line; do
        echo "  GPU $line"
    done
    
    # Check if multi-GPU is requested but only one GPU available
    if [ "$MULTI_GPU" = true ] && [ "$GPU_COUNT" -eq 1 ]; then
        echo "âš ï¸  Multi-GPU requested but only 1 GPU detected. Using single GPU."
        MULTI_GPU=false
    fi
    
    # Memory check
    MIN_MEMORY=$(nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits | sort -n | head -1)
    if [ "$MIN_MEMORY" -lt 10000 ]; then
        echo "âš ï¸  Low GPU memory detected (${MIN_MEMORY}MB free). Consider using DeepSpeed or reducing batch size."
    fi
else
    echo "âš ï¸  No GPU detected. Training will be very slow on CPU."
    MULTI_GPU=false
fi

# Check disk space
echo ""
echo "ğŸ’¾ Checking disk space..."
OUTPUT_DIR=$(python -c "import json; print(json.load(open('$CONFIG_FILE'))['training_config']['output_dir'])")
AVAILABLE_SPACE=$(df -BG "$(dirname "$OUTPUT_DIR")" | awk 'NR==2 {print $4}' | sed 's/G//')
if [ "$AVAILABLE_SPACE" -lt 10 ]; then
    echo "âš ï¸  Low disk space: ${AVAILABLE_SPACE}GB available. Model checkpoints may require significant space."
else
    echo "âœ… Sufficient disk space: ${AVAILABLE_SPACE}GB available"
fi

# Set environment variables for optimization
echo ""
echo "âš™ï¸  Setting environment variables..."
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
export TOKENIZERS_PARALLELISM=false
export WANDB_PROJECT=g3moe-sft
export HF_HOME=~/.cache/huggingface

# Additional optimizations
if [ "$MULTI_GPU" = true ]; then
    export NCCL_DEBUG=INFO
    export CUDA_LAUNCH_BLOCKING=0
fi

echo "âœ… Environment configured"

# Prepare training command
TRAIN_CMD="python sft/custom_model_sft.py --config $CONFIG_FILE"

# Add resume flag if requested
if [ "$RESUME" = true ]; then
    if [ -d "$OUTPUT_DIR" ] && [ "$(ls -A "$OUTPUT_DIR"/checkpoint-* 2>/dev/null | wc -l)" -gt 0 ]; then
        LATEST_CHECKPOINT=$(ls -td "$OUTPUT_DIR"/checkpoint-* | head -1)
        echo "ğŸ“‚ Found checkpoint: $LATEST_CHECKPOINT"
        # Note: TRL handles resume automatically if output_dir contains checkpoints
    else
        echo "âš ï¸  No checkpoints found in $OUTPUT_DIR. Starting fresh training."
    fi
fi

# Setup multi-GPU training
if [ "$MULTI_GPU" = true ]; then
    echo ""
    echo "ğŸ”„ Setting up multi-GPU training with torchrun..."
    TRAIN_CMD="torchrun --nproc_per_node=$GPU_COUNT --master_port=29500 sft/custom_model_sft.py --config $CONFIG_FILE"
fi

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Save training info
echo ""
echo "ğŸ“ Saving training info..."
cat > "$OUTPUT_DIR/training_info.txt" << EOF
G3MoE SFT Training Information
=============================
Start time: $(date)
Config file: $CONFIG_FILE
Resume: $RESUME
Multi-GPU: $MULTI_GPU ($GPU_COUNT GPUs)
Command: $TRAIN_CMD

System Info:
- Python: $(python --version)
- PyTorch: $(python -c "import torch; print(torch.__version__)")
- Transformers: $(python -c "import transformers; print(transformers.__version__)")
- TRL: $(python -c "import trl; print(trl.__version__)")
- CUDA: $(python -c "import torch; print(torch.version.cuda if torch.cuda.is_available() else 'Not available')")

GPU Info:
$(nvidia-smi --query-gpu=name,memory.total --format=csv 2>/dev/null || echo "No GPU info available")
EOF

# Start training with error handling
echo ""
echo "ğŸƒ Starting training..."
echo "Command: $TRAIN_CMD"
echo ""

# Trap to handle interruption
trap 'echo ""; echo "âš ï¸  Training interrupted. Checkpoints saved in: $OUTPUT_DIR"; exit 130' INT

# Run training and capture exit code
set +e
$TRAIN_CMD
TRAIN_EXIT_CODE=$?
set -e

# Training completion
echo ""
if [ $TRAIN_EXIT_CODE -eq 0 ]; then
    echo "âœ… Training completed successfully!"
    echo "ğŸ“ Results saved in: $OUTPUT_DIR"
    echo "ğŸ“Š Check logs and model files in the output directory"
    
    # Show final model info
    if [ -f "$OUTPUT_DIR/pytorch_model.bin" ] || [ -f "$OUTPUT_DIR/model.safetensors" ]; then
        echo "ğŸ¯ Final model saved successfully"
    fi
    
    # Show training summary
    if [ -f "$OUTPUT_DIR/trainer_state.json" ]; then
        echo "ğŸ“ˆ Training summary available in trainer_state.json"
    fi
else
    echo "âŒ Training failed with exit code: $TRAIN_EXIT_CODE"
    echo "ğŸ“ Check logs in: $OUTPUT_DIR"
    echo "ğŸ’¡ Common issues:"
    echo "   - Out of memory: Reduce batch size or use DeepSpeed"
    echo "   - CUDA errors: Check GPU compatibility"
    echo "   - Dataset errors: Verify dataset name and access"
    exit $TRAIN_EXIT_CODE
fi

# Final cleanup and summary
echo ""
echo "ğŸ Training session completed at $(date)"
echo "ğŸ“‹ Session summary:"
echo "   - Config: $CONFIG_FILE"
echo "   - Output: $OUTPUT_DIR"
echo "   - Duration: Training completed"
echo "   - Status: $([ $TRAIN_EXIT_CODE -eq 0 ] && echo "SUCCESS" || echo "FAILED")" 