#!/usr/bin/env python3
"""
G3MoE SFT Training Script using Config File
"""

import os
import sys
import json
import torch
import traceback
import argparse
import logging
import time
from datetime import datetime
from typing import Dict, Any
from torchinfo import summary
from PIL import Image
from transformers.utils.import_utils import is_flash_attn_2_available
from transformers import (
    AutoTokenizer,
    AutoProcessor,
    AutoConfig,
    AutoModel,
    AutoModelForCausalLM
)
from transformers import logging as transformers_logging

from transformers.trainer_utils import set_seed
from trl import SFTTrainer, SFTConfig
from peft import get_peft_model
from peft.tuners.lora.config import LoraConfig
from peft.utils.peft_types import TaskType
import wandb

# Add parent directory to path to import custom modules
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import custom modules  
from models import G3MoEForCausalLM, G3MoEConfig, G3MoEForConditionalGeneration, G3MoETextConfig, G3MoETextModel, G3MoEModel
from data.base_model_sft_dataset import get_dataset, create_multimodal_collate_fn
from data.simple_sft_dataset import get_simple_sft_dataset, create_simple_collate_fn, smoltalk_dataset, orca_mini_dataset, validate_image_data
from data.multi_domain_sft_dataset import get_multi_domain_sft_dataset, create_simple_collate_fn as create_multi_domain_collate_fn, all_domains_dataset

from training_utils.utils import format_parameters, load_config, setup_deepspeed_environment
from optimizers.custom_optimizers import get_custom_optimizer
from optimizers.deepspeed_optimizer_registry import register_custom_optimizers
from eval.callbacks import ModelEvalCallback
from eval.ifeval_callback import IFEvalCallback
from eval.moe_monitoring_callback import create_moe_callback_for_transformers

# Register custom optimizers with DeepSpeed
register_custom_optimizers()
try:
    # AutoConfig.register("g3moe", G3MoEConfig)
    AutoConfig.register("g3moe", G3MoEConfig)
    AutoConfig.register("g3moe_text", G3MoETextConfig)
    AutoModel.register(G3MoEConfig, G3MoEModel)
    AutoModel.register(G3MoETextConfig, G3MoETextModel)
    AutoModelForCausalLM.register(G3MoEConfig, G3MoEForConditionalGeneration)

    from transformers.modeling_utils import VLMS
    VLMS.append("g3moe")
except Exception as e:
    import traceback
    traceback.format_exc()
    print(f"Failed to register G3MoE model: {e}")
    print("G3MoE cannot train without registering model... exiting...")
    raise e

transformers_logging.enable_progress_bar()
transformers_logging.set_verbosity_warning()

# Setup comprehensive logging system
def setup_logging(log_dir: str = "logs", log_level: str = "INFO"):
    """Setup comprehensive logging system for training monitoring"""
    os.makedirs(log_dir, exist_ok=True)
    
    # Create timestamp for log files
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Configure root logger
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # Clear existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Create formatters
    detailed_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(name)-20s | %(funcName)-15s:%(lineno)-4d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    simple_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    # File handler for detailed logs
    file_handler = logging.FileHandler(
        os.path.join(log_dir, f"training_detailed_{timestamp}.log"),
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(detailed_formatter)
    logger.addHandler(file_handler)
    
    # File handler for error logs
    error_handler = logging.FileHandler(
        os.path.join(log_dir, f"training_errors_{timestamp}.log"),
        encoding='utf-8'
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(detailed_formatter)
    logger.addHandler(error_handler)
    
    # Console handler for important messages
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(simple_formatter)
    logger.addHandler(console_handler)
    
    return logger

# Global logger instance
logger = setup_logging()

def log_gpu_memory(logger, stage: str, device: int = 0):
    """Log detailed GPU memory information"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB
        reserved = torch.cuda.memory_reserved(device) / 1024**3    # GB
        max_allocated = torch.cuda.max_memory_allocated(device) / 1024**3  # GB
        max_reserved = torch.cuda.max_memory_reserved(device) / 1024**3    # GB
        
        logger.info(f"ðŸ”§ GPU Memory [{stage}] - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
        logger.debug(f"ðŸ”§ GPU Memory [{stage}] - Max Allocated: {max_allocated:.2f}GB, Max Reserved: {max_reserved:.2f}GB")
        
        return {
            'allocated': allocated,
            'reserved': reserved,
            'max_allocated': max_allocated,
            'max_reserved': max_reserved
        }
    return None

def log_training_progress(logger, trainer, step: int = None, epoch: float = None, loss: float = None):
    """Log detailed training progress information"""
    if hasattr(trainer, 'state') and trainer.state is not None:
        state = trainer.state
        current_step = step or state.global_step
        current_epoch = epoch or state.epoch
        current_loss = loss or getattr(state, 'log_history', [{}])[-1].get('train_loss', 'N/A')
        
        logger.info(f"ðŸ“Š Training Progress - Step: {current_step}, Epoch: {current_epoch:.3f}, Loss: {current_loss}")
        
        # Log learning rate if available
        if hasattr(trainer, 'lr_scheduler') and trainer.lr_scheduler is not None:
            lr = trainer.lr_scheduler.get_last_lr()[0] if hasattr(trainer.lr_scheduler, 'get_last_lr') else 'N/A'
            logger.debug(f"ðŸ“Š Learning Rate: {lr}")
        
        # Log gradient norm if available
        if hasattr(trainer, 'accelerator') and trainer.accelerator is not None:
            if hasattr(trainer.accelerator, 'unwrap_model'):
                model = trainer.accelerator.unwrap_model(trainer.model)
                total_norm = 0
                param_count = 0
                for p in model.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.data.norm(2)
                        total_norm += param_norm.item() ** 2
                        param_count += 1
                if param_count > 0:
                    total_norm = total_norm ** (1. / 2)
                    logger.debug(f"ðŸ“Š Gradient Norm: {total_norm:.6f}")

def log_error_context(logger, error: Exception, context: str = ""):
    """Log detailed error context with system state"""
    logger.error(f"âŒ Error in {context}: {str(error)}")
    logger.error(f"âŒ Error type: {type(error).__name__}")
    
    # Log traceback
    logger.error(f"âŒ Traceback:\n{traceback.format_exc()}")
    
    # Log GPU memory state
    if torch.cuda.is_available():
        memory_info = log_gpu_memory(logger, "ERROR")
        if memory_info:
            logger.error(f"âŒ GPU Memory at error - Allocated: {memory_info['allocated']:.2f}GB, Reserved: {memory_info['reserved']:.2f}GB")
    
    # Log system state
    logger.error(f"âŒ System state - CUDA available: {torch.cuda.is_available()}, Device count: {torch.cuda.device_count()}")
    if torch.cuda.is_available():
        logger.error(f"âŒ Current device: {torch.cuda.current_device()}, Device name: {torch.cuda.get_device_name()}")

def load_config(config_path: str):
    """ê°„ë‹¨í•œ config ë¡œë”"""
    with open(config_path, 'r') as f:
        return json.load(f)

def setup_deepspeed_environment():
    """Setup environment variables for DeepSpeed optimization"""
    if "PYTORCH_CUDA_ALLOC_CONF" not in os.environ:
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512,expandable_segments:True"

    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["TORCH_NCCL_ASYNC_ERROR_HANDLING"] = "1"
    if "DEEPSPEED_ZERO_INIT" not in os.environ:
        os.environ["DEEPSPEED_ZERO_INIT"] = "1"
    # Ensure global AMP default uses BF16 under CUDA
    try:
        if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
            torch.set_autocast_gpu_dtype(torch.bfloat16)
    except Exception as _:
        pass
    print("DeepSpeed environment variables set")


def clear_gpu_memory():
    """Clear GPU memory and run garbage collection with detailed logging"""
    import gc
    logger.info("ðŸ§¹ Starting GPU memory cleanup...")
    
    # Log memory before cleanup
    memory_before = log_gpu_memory(logger, "BEFORE_CLEANUP")
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        logger.debug("ðŸ§¹ CUDA cache cleared and synchronized")
    
    # Force garbage collection
    collected = gc.collect()
    logger.debug(f"ðŸ§¹ Garbage collection freed {collected} objects")
    
    # Log memory after cleanup
    memory_after = log_gpu_memory(logger, "AFTER_CLEANUP")
    
    if memory_before and memory_after:
        freed_allocated = memory_before['allocated'] - memory_after['allocated']
        freed_reserved = memory_before['reserved'] - memory_after['reserved']
        logger.info(f"ðŸ§¹ Memory cleanup completed - Freed: {freed_allocated:.2f}GB allocated, {freed_reserved:.2f}GB reserved")
    else:
        logger.info("ðŸ§¹ Memory cleanup completed")


def eval_with_memory_optimization(trainer, original_eval_fn, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
    """Memory-optimized evaluation function with detailed logging"""
    logger.info("ðŸ”§ Starting memory-optimized evaluation...")
    
    # Log evaluation context
    if hasattr(trainer, 'state') and trainer.state is not None:
        logger.info(f"ðŸ”§ Evaluation context - Step: {trainer.state.global_step}, Epoch: {trainer.state.epoch:.3f}")
    
    # Log memory before evaluation
    memory_before = log_gpu_memory(logger, "BEFORE_EVAL")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_gpu_memory()
    
    # ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  ë©”ëª¨ë¦¬ ìµœì í™”
    logger.debug("ðŸ”§ Setting model to eval mode...")
    trainer.model.eval()
    
    # eval ì‹œì—ëŠ” gradient checkpointing ë¹„í™œì„±í™”
    original_gc = trainer.args.gradient_checkpointing
    trainer.args.gradient_checkpointing = False
    logger.debug(f"ðŸ”§ Disabled gradient checkpointing for evaluation (was: {original_gc})")
    
    try:
        logger.info("ðŸ”§ Starting evaluation with torch.no_grad()...")
        start_time = time.time()
        
        with torch.no_grad():
            # ì›ëž˜ evaluate í•¨ìˆ˜ í˜¸ì¶œ (ë¬´í•œ ìž¬ê·€ ë°©ì§€)
            eval_results = original_eval_fn(
                eval_dataset=eval_dataset, 
                ignore_keys=ignore_keys, 
                metric_key_prefix=metric_key_prefix
            )
        
        eval_time = time.time() - start_time
        logger.info(f"ðŸ”§ Evaluation completed in {eval_time:.2f} seconds")
        
        # Log evaluation results
        if eval_results:
            logger.info(f"ðŸ”§ Evaluation results: {eval_results}")
        
        # Log memory after evaluation
        memory_after = log_gpu_memory(logger, "AFTER_EVAL")
        
        # ê²°ê³¼ ë°˜í™˜
        return eval_results
        
    except Exception as e:
        logger.error(f"âŒ Error during evaluation: {str(e)}")
        log_error_context(logger, e, "memory_optimized_evaluation")
        raise e
        
    finally:
        # ì›ëž˜ ì„¤ì • ë³µì›
        logger.debug(f"ðŸ”§ Restoring gradient checkpointing to: {original_gc}")
        trainer.args.gradient_checkpointing = original_gc
        clear_gpu_memory()


def setup_model_and_tokenizer(model_config: Dict[str, Any]):
    """Setup G3MoE model and tokenizer with detailed logging"""
    logger.info("ðŸš€ Starting model and tokenizer setup...")
    
    # NOTE: Delay DeepSpeed env setup until AFTER model load to avoid HF ZeRO-3 init slow path
    logger.info("ðŸ”§ Setting up DeepSpeed environment...")
    setup_deepspeed_environment()
    
    # Load tokenizer - ì•ˆì •ì ì¸ ë¡œë”© ë¡œì§
    tokenizer_path = model_config.get("tokenizer_name_or_path") or model_config["model_name_or_path"]
    logger.info(f"ðŸ”¤ Loading tokenizer from: {tokenizer_path}")
    
    tokenizer = None
    try:
        logger.debug("  - Attempting AutoProcessor...")
        tokenizer = AutoProcessor.from_pretrained(
            tokenizer_path,
            trust_remote_code=model_config["trust_remote_code"]
        )
        logger.info("  âœ… AutoProcessor loaded successfully")
    except Exception as e:
        logger.warning(f"  âŒ AutoProcessor failed: {e}")
        try:
            logger.debug("  - Attempting AutoTokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(
                tokenizer_path,
                trust_remote_code=model_config["trust_remote_code"]
            )
            logger.info("  âœ… AutoTokenizer loaded successfully")
        except Exception as e2:
            logger.error(f"  âŒ AutoTokenizer also failed: {e2}")
            log_error_context(logger, e2, "tokenizer_loading")
            raise RuntimeError(f"í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e2}")
    
    # Set chat template with error handling
    try:
        with open("/home/conan/workspace/llm_training/sft/config/chat_template.txt", "r") as f:
            chat_template = f.read()
        
        # AutoProcessorì¸ ê²½ìš° tokenizer ì†ì„±ì— ì„¤ì •
        if hasattr(tokenizer, 'tokenizer'):
            tokenizer.tokenizer.chat_template = chat_template
            print("  âœ… ì±„íŒ… í…œí”Œë¦¿ì„ tokenizer.tokenizerì— ì„¤ì •")
        else:
            tokenizer.chat_template = chat_template
            print("  âœ… ì±„íŒ… í…œí”Œë¦¿ì„ tokenizerì— ì„¤ì •")
        
        print(f"  - í…œí”Œë¦¿ ê¸¸ì´: {len(chat_template)}")
    except Exception as e:
        print(f"  âš ï¸ ì±„íŒ… í…œí”Œë¦¿ ì„¤ì • ì‹¤íŒ¨: {e}")
        print("  - ê¸°ë³¸ í…œí”Œë¦¿ìœ¼ë¡œ ê³„ì† ì§„í–‰")
    
    # Set padding side for multimodal models
    if hasattr(tokenizer, 'tokenizer'):
        tokenizer.tokenizer.padding_side = "right"
        print("  âœ… tokenizer.tokenizer.padding_side = 'right' ì„¤ì •")
    else:
        tokenizer.padding_side = "right"
        print("  âœ… tokenizer.padding_side = 'right' ì„¤ì •")

    # Ensure tokenizer has pad token
    if not hasattr(tokenizer, 'pad_token'):
        if hasattr(tokenizer, 'tokenizer'):
            tokenizer.pad_token = tokenizer.tokenizer.pad_token if tokenizer.tokenizer.pad_token is not None else tokenizer.eos_token
        else:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
    
    if not hasattr(tokenizer, 'convert_tokens_to_ids'):
        tokenizer.convert_tokens_to_ids = tokenizer.tokenizer.convert_tokens_to_ids

    # Prefer config value; default to eager
    attn_from_cfg = (model_config.get("g3moe_params") or {}).get("attn_implementation")
    if attn_from_cfg in {"eager", "sdpa", "flash_attention_2"}:
        attn_implementation = attn_from_cfg
    else:
        attn_implementation = "eager"

    # Load and configure G3MoE model configuration
    print("Loading base model configuration...")
    base_config = AutoConfig.from_pretrained(
        model_config["model_name_or_path"],
        trust_remote_code=model_config["trust_remote_code"]
    )
    
    # Convert to dict and update with G3MoE parameters
    base_model_config = base_config.to_dict()
    
    # G3MoE configuration parameters from config file
    g3moe_params = model_config["g3moe_params"]
    
    # Handle different model config structures (Gemma vs others)
    if 'text_config' in base_model_config:
        # Multi-modal model with text_config
        text_config = base_model_config['text_config']
        num_attention_heads = text_config['num_attention_heads']
    else:
        # Direct text model config
        text_config = base_model_config
        num_attention_heads = base_model_config['num_attention_heads']
    
    g3moe_config = {
        "n_shared_experts": g3moe_params["n_shared_experts"],
        "n_routed_experts": g3moe_params["n_routed_experts"],
        "n_group": g3moe_params["n_group"],
        "topk_group": g3moe_params["topk_group"],
        "num_experts_per_tok": g3moe_params["num_experts_per_tok"],
        "first_k_dense_replace": g3moe_params["first_k_dense_replace"],
        "router_aux_loss_coef": g3moe_params["router_aux_loss_coef"],
        "router_jitter_noise": g3moe_params["router_jitter_noise"],
        "input_jitter_noise": g3moe_params["input_jitter_noise"],
        "model_type": "g3moe_text",
        "rope_scaling": {
            "rope_type": g3moe_params["rope_scaling"]["rope_type"],
            "factor": g3moe_params["rope_scaling"]["factor"]
        },
        "use_bfloat16": True,
        "attn_implementation": attn_implementation
    }
    base_model_config["text_config"].update(g3moe_config)
    # Create G3MoE configuration
    config = G3MoEConfig(
        text_config=base_model_config["text_config"],
        vision_config=base_model_config["vision_config"],
        boi_token_index=base_model_config["boi_token_index"],
        eoi_token_index=base_model_config["eoi_token_index"],
        image_token_index=base_model_config["image_token_index"],
        initializer_range=base_model_config["initializer_range"],
        attn_implementation=attn_implementation,
        **{
            k:v for k,v in base_model_config.items() 
            if k not in [
                "text_config", "vision_config", "boi_token_index",
                "eoi_token_index", "image_token_index", "initializer_range",
                "attn_implementation"
            ]
        }
    )
    print("G3MoE configuration created successfully")
    print(f"  - Shared experts: {g3moe_config['n_shared_experts']}")
    print(f"  - Routed experts: {g3moe_config['n_routed_experts']}")
    print(f"  - Expert groups: {g3moe_config['n_group']}")
    print(f"  - Top-k per group: {g3moe_config['topk_group']}")
    print(f"  - Experts per token: {g3moe_config['num_experts_per_tok']}")
    
    # Load model - use different device_map strategy based on DeepSpeed usage
    device_map = None
    if model_config.get("deepspeed_config"):
        # With DeepSpeed, let DeepSpeed handle device placement
        device_map = None
        print("Using DeepSpeed - letting DeepSpeed handle device placement")
    elif torch.cuda.device_count() > 1:
        # Without DeepSpeed, use auto device mapping for multi-GPU
        device_map = "auto"
        print(f"Using auto device mapping for {torch.cuda.device_count()} GPUs")
    
    # Load G3MoE model with the configured parameters
    logger.info("ðŸ¤– Loading G3MoE model...")
    logger.info(f"ðŸ¤– Model path: {model_config['model_name_or_path']}")
    logger.info(f"ðŸ¤– Device map: {device_map}")
    logger.info(f"ðŸ¤– Attention implementation: {attn_implementation}")
    
    # Log memory before model loading
    memory_before = log_gpu_memory(logger, "BEFORE_MODEL_LOAD")
    
    try:
        start_time = time.time()
        model = G3MoEForConditionalGeneration.from_pretrained(
            model_config["model_name_or_path"],
            config=config,
            torch_dtype=torch.bfloat16, # Using bfloat16
            trust_remote_code=model_config["trust_remote_code"],
            device_map=device_map,
            low_cpu_mem_usage=True,
            offload_state_dict=True,
            use_cache=False,
            gradient_checkpointing=False,
            # load_in_4bit=True,
            attn_implementation=attn_implementation
        )
        load_time = time.time() - start_time
        logger.info(f"âœ… G3MoE model loaded successfully in {load_time:.2f} seconds")
        logger.info(f"  - Attn implementation: {attn_implementation}")
        
        # Log memory after model loading
        memory_after = log_gpu_memory(logger, "AFTER_MODEL_LOAD")
        
        total_params = model.num_parameters()
        logger.info(f"  - Total parameters: {format_parameters(total_params)}")
        logger.info(f"  - Model Memory consumption: {memory_before['allocated']:.2f}GB â†’ {memory_after['allocated']:.2f}GB")
        # Log model device placement
        if hasattr(model, 'device'):
            logger.info(f"  - Model device: {model.device}")
        elif hasattr(model, 'hf_device_map'):
            logger.info(f"  - Model device map: {model.hf_device_map}")
            
    except Exception as e:
        logger.error(f"âŒ Failed to load G3MoE model: {str(e)}")
        log_error_context(logger, e, "model_loading")
        raise e

    # Setup LoRA if requested
    if model_config["use_lora"]:
        # G3MoERouterëŠ” PEFTì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ target_modulesì—ì„œ ì œì™¸
        # RouterëŠ” PEFT ì ìš© í›„ ìˆ˜ë™ìœ¼ë¡œ trainableë¡œ ì„¤ì •
        lora_config = LoraConfig(
            r=model_config["lora_r"],
            lora_alpha=model_config["lora_alpha"],
            lora_dropout=model_config["lora_dropout"],
            target_modules=[
                # "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj",
                # "router", "routing_temperature", "global_router" ì œì™¸ - PEFT ë¯¸ì§€ì›
                "rnn.weight_ih_l0", "rnn.weight_hh_l0"
            ],
            # modules_to_saveì—ì„œë„ router ì œì™¸ (PEFTê°€ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŒ)
            bias="none",
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,  # í›ˆë ¨ ëª¨ë“œ ëª…ì‹œ
            fan_in_fan_out=False,  # LoRA í˜¸í™˜ì„± í–¥ìƒ
        )
        model = get_peft_model(model, lora_config)
        model.enable_input_require_grads()
        model.print_trainable_parameters()
        
        # LoRA ì–´ëŒ‘í„° ì„¤ì •
        for name, module in model.named_modules():
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                module.lora_A.requires_grad_(True)
                module.lora_B.requires_grad_(True)
        
        # G3MoERouterë¥¼ ì°¾ì•„ì„œ trainableë¡œ ì„¤ì • (PEFT ì ìš© í›„)
        from models.g3moe_model import G3MoERouter
        router_count = 0
        for name, module in model.named_modules():
            if isinstance(module, G3MoERouter):
                for p in module.parameters(recurse=True):
                    p.requires_grad_(True)
                router_count += 1
                logger.info(f"âœ“ Router module '{name}' set to trainable (not LoRA-wrapped)")
        
        if router_count > 0:
            logger.info(f"âœ“ {router_count} router module(s) set to fully trainable")
        else:
            logger.warning("âš ï¸ No G3MoERouter modules found - router may not be trainable")
        # DDP ì •ì  ê·¸ëž˜í”„ ë¹„í™œì„±í™”: MoE ë¼ìš°íŒ…/LoRAë¡œ ìŠ¤í…ë§ˆë‹¤ í™œì„± íŒŒë¼ë¯¸í„°ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ë™ì  ê·¸ëž˜í”„ í—ˆìš©
        if hasattr(model, '_set_static_graph'):
            model._set_static_graph(True)
        # Ensure all parameters incl. LoRA adapters are bfloat16 for consistency
        try:
            model.to(torch.bfloat16)
            for name, param in model.named_parameters():
                if param.requires_grad and param.dtype != torch.bfloat16:
                    param.data = param.data.to(torch.bfloat16)
            print("âœ“ Parameters cast to bfloat16")
        except Exception as cast_e:
            print(f"âš ï¸ BF16 cast warning: {cast_e}")
        print("âœ“ LoRA ì ìš©")
        
    return model, tokenizer


def setup_dataset(data_config: Dict[str, Any], tokenizer):
    """Setup training dataset"""    
    use_multi_domain = data_config.get("use_multi_domain", False)
    dataset_name = data_config.get("dataset_name", "HuggingFaceTB/smoltalk")
    max_samples = data_config.get("max_samples", 100000)
    max_samples_per_domain = data_config.get("max_samples_per_domain", None)  # multi-domainìš©
    max_seq_length = data_config.get("max_seq_length", 131072) or 131072
    test_size = data_config.get("test_size", 0.1)
    use_streaming = data_config.get("streaming", False)
    max_workers = data_config.get("max_workers", 4)  # multi-domain ë³‘ë ¬ ì²˜ë¦¬ìš©
    
    print(f"Loading dataset: {dataset_name}")
    print(f"  - Use multi-domain: {use_multi_domain}")
    print(f"  - Max samples: {max_samples}")
    if max_samples_per_domain:
        print(f"  - Max samples per domain: {max_samples_per_domain}")
    print(f"  - Max sequence length: {max_seq_length}")
    print(f"  - Test size: {test_size}")
    print(f"  - Streaming: {use_streaming}")
    print(f"  - í† í¬ë‚˜ì´ì € íƒ€ìž…: {type(tokenizer)}")
    print(f"  - í† í¬ë‚˜ì´ì €ì— chat_template ìžˆìŒ: {hasattr(tokenizer, 'chat_template')}")
    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:
        print(f"  - chat_template ê¸¸ì´: {len(str(tokenizer.chat_template))}")
    else:
        print(f"  - âš ï¸ chat_templateì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ!")
    with open("/home/conan/workspace/llm_training/sft/config/chat_template.txt", "r") as f:
        chat_template = f.read()
        
        # AutoProcessorì¸ ê²½ìš° tokenizer ì†ì„±ì— ì„¤ì •
        if hasattr(tokenizer, 'tokenizer'):
            tokenizer.tokenizer.chat_template = chat_template
            print("  âœ… ì±„íŒ… í…œí”Œë¦¿ì„ tokenizer.tokenizerì— ì„¤ì •")
        
        tokenizer.chat_template = chat_template
        print("  âœ… ì±„íŒ… í…œí”Œë¦¿ì„ tokenizerì— ì„¤ì •")
    
    try:
        # Multi-domain ë°ì´í„°ì…‹ ì‚¬ìš©
        if use_multi_domain:
            print(f"ðŸ”„ Multi-domain ë°ì´í„°ì…‹ ë¡œë” ì‚¬ìš©")
            # domain_configsê°€ ì§€ì •ë˜ì–´ ìžˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ëª¨ë“  ë„ë©”ì¸ ì‚¬ìš©
            domain_configs = data_config.get("domain_configs", None)
            
            if max_samples_per_domain is None:
                # max_samples_per_domainì´ ì—†ìœ¼ë©´ max_samplesë¥¼ ë„ë©”ì¸ ìˆ˜ë¡œ ë‚˜ëˆ”
                if domain_configs:
                    num_domains = len(domain_configs)
                else:
                    from data.multi_domain_sft_dataset import DOMAIN_DATASETS
                    num_domains = len(DOMAIN_DATASETS)
                max_samples_per_domain = max(1, max_samples // num_domains)
                print(f"  - ìžë™ ê³„ì‚°ëœ ë„ë©”ì¸ë‹¹ ìƒ˜í”Œ ìˆ˜: {max_samples_per_domain}")
            
            dataset = get_multi_domain_sft_dataset(
                domain_configs=domain_configs,
                tokenizer=tokenizer,
                max_length=max_seq_length,
                max_samples_per_domain=max_samples_per_domain,
                test_size=test_size,
                use_streaming=use_streaming,
                max_workers=max_workers
            )
            # Multi-domainìš© collate í•¨ìˆ˜ ì‚¬ìš© (allow_text_only=True)
            # processor ìƒì„± (AutoProcessor ë˜ëŠ” tokenizer)
            # tokenizerê°€ ì´ë¯¸ AutoProcessorì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if hasattr(tokenizer, 'tokenizer'):
                # AutoProcessorì¸ ê²½ìš°
                processor = tokenizer
            else:
                # AutoTokenizerì¸ ê²½ìš°, tokenizerë¥¼ processorë¡œ ì‚¬ìš©
                # (multi_domain_collate_fnì´ tokenizerë„ ì²˜ë¦¬í•  ìˆ˜ ìžˆìŒ)
                processor = tokenizer
            
            collate_fn = create_multi_domain_collate_fn(processor, max_length=max_seq_length, allow_text_only=True)
        
        # ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ë¡œë” ì‚¬ìš©
        elif "smoltalk" in dataset_name.lower() or "orca" in dataset_name.lower() or "llava" in dataset_name.lower():
            print(f"ì¼ë°˜ ë°ì´í„°ì…‹ ë¡œë” ì‹œë„: {dataset_name}")
            dataset = get_simple_sft_dataset(
                dataset_name=dataset_name,
                tokenizer=tokenizer,
                max_length=max_seq_length,
                max_samples=max_samples,
                test_size=test_size,
                use_streaming=use_streaming
            )
            # ì´ë¯¸ì§€ ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì»¤ìŠ¤í…€ data collator ì‚¬ìš©
            collate_fn = create_simple_collate_fn(tokenizer, max_length=max_seq_length)
        else:
            # open_m_3 ë°ì´í„°ì…‹ ë¡œë” ì‹œë„
            dataset = get_dataset(
                tokenizer=tokenizer,
                dataset_name=data_config["dataset_name"],
                max_length=data_config["max_seq_length"],
                test_size=data_config["test_size"],
                text_only=data_config.get("text_only", False),
                streaming=data_config["streaming"]
            )
            collate_fn = create_multimodal_collate_fn(tokenizer)
        
        print(f"Dataset loaded:")
        for split, data in dataset.items():
            try:
                if use_streaming and hasattr(data, 'info') and hasattr(data.info, 'dataset_size'):
                    size = data.info.dataset_size
                else:
                    size = len(data) if hasattr(data, '__len__') else "unknown"
                print(f"  {split}: {size} examples")
            except Exception as e:
                print(f"  {split}: size unknown ({e})")
        
        # ë¹ˆ ë°ì´í„°ì…‹ ì²´í¬
        train_dataset = dataset.get("train", None)
        if train_dataset is None:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤!")
        
        if use_streaming:
            if hasattr(train_dataset, 'info') and hasattr(train_dataset.info, 'dataset_size'):
                if train_dataset.info.dataset_size == 0:
                    raise ValueError("í›ˆë ¨ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤!")
        else:
            if hasattr(train_dataset, '__len__') and len(train_dataset) == 0:
                raise ValueError("í›ˆë ¨ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤!")

        return dataset, collate_fn
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
        assert False, "ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨"
        print("ðŸ”„ ëŒ€ì•ˆ ë°ì´í„°ì…‹ìœ¼ë¡œ ìž¬ì‹œë„ (SmolTalk)")
        try:
            dataset = smoltalk_dataset(tokenizer, max_samples=max_samples)
            print(f"ëŒ€ì•ˆ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ:")
            for split, data in dataset.items():
                print(f"  {split}: {len(data)} examples")
            return dataset
        except Exception as e2:
            print(f"âŒ ëŒ€ì•ˆ ë°ì´í„°ì…‹ë„ ì‹¤íŒ¨: {e2}")
            raise RuntimeError(f"ëª¨ë“  ë°ì´í„°ì…‹ ë¡œë”© ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e2}")


def create_training_args(
    training_config: Dict[str, Any], 
    deepspeed_config: str | None = None 
) -> SFTConfig:
    """Create SFTConfig from training configuration"""
    
    # Create SFTConfig with all parameters
    training_args = SFTConfig(
        **training_config,
        dataset_kwargs={"skip_prepare_dataset": True}
    )
    
    # Add DeepSpeed config if provided
    if deepspeed_config:
        import os, json
        ds_cfg_path_abs = os.path.abspath(deepspeed_config)
        training_args.deepspeed = ds_cfg_path_abs
        print(f"DeepSpeed config set: {ds_cfg_path_abs}")
        # Validate that CPU offload is disabled as required
        try:
            with open(ds_cfg_path_abs, "r") as f:
                ds_cfg = json.load(f)
            zero = ds_cfg.get("zero_optimization", {})
            off_opt = (zero.get("offload_optimizer") or {}).get("device", "none").lower()
            off_param = (zero.get("offload_param") or {}).get("device", "none").lower()
            print(f"DeepSpeed zero stage: {zero.get('stage')}")
            print(f"DeepSpeed offload_optimizer.device: {off_opt}")
            print(f"DeepSpeed offload_param.device: {off_param}")
            # assert off_opt in {"none", None, ""} and off_param in {"none", None, ""}, (
            #     "DeepSpeed CPU offload detected in config but must be disabled (device='none')."
            # )
            # Workaround: ZeRO-3 + gradient checkpointing can trigger duplicate ds_id assertion
            try:
                zero_stage = int(zero.get("stage", 0) or 0)
            except Exception:
                zero_stage = 0
            # if zero_stage == 3 and getattr(training_args, "gradient_checkpointing", False):
            #     print("âš ï¸ Detected ZeRO-3 with gradient checkpointing enabled. Disabling to avoid ds_id assertion.")
            #     training_args.gradient_checkpointing = False
        except Exception as e:
            print(f"âš ï¸ DeepSpeed config validation warning: {e}")
    
    return training_args


def main(
    model_config: Dict[str, Any], 
    data_config: Dict[str, Any], 
    training_config: Dict[str, Any]
):
    register_custom_optimizers()
    # Setup model and tokenizer
    print("Setting up model and tokenizer...")
    model, tokenizer = setup_model_and_tokenizer(model_config)
    
    # Setup dataset
    print("Setting up dataset...")
    dataset, collate_fn = setup_dataset(data_config, tokenizer)
    
    # ëª¨ë¸ ë° ë°ì´í„°ì…‹ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
    logger.info("ðŸ§¹ ëª¨ë¸ ë° ë°ì´í„°ì…‹ ë¡œë“œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬...")
    clear_gpu_memory()
    
    # Create training arguments
    training_args = create_training_args(
        training_config, 
        model_config.get("deepspeed_config")
    )
    
    # Optionally build a custom optimizer (e.g., Muon) prior to DeepSpeed init
    custom_optimizer = None
    try:
        ds_cfg_path = model_config.get("deepspeed_config")
        if ds_cfg_path:
            with open(ds_cfg_path, "r") as f:
                ds_cfg = json.load(f)
            # Prefer explicit custom optimizer block
            custom_opt_section = ds_cfg.get("custom_optimizer")
            from optimizers.deepspeed_optimizer_registry import create_optimizer_from_config
            if custom_opt_section:
                trainable_params = (p for p in model.parameters() if p.requires_grad)
                custom_optimizer = create_optimizer_from_config(custom_opt_section, trainable_params)
                print(f"âœ“ Using custom optimizer: {custom_opt_section.get('type')}")
            else:
                # Fallback: if optimizer.type is a custom one, build it here
                opt_section = ds_cfg.get("optimizer")
                if opt_section:
                    opt_type = str(opt_section.get("type", "")).lower()
                    if opt_type in {"muon", "muonoptimizer", "lion", "adafactor", "sophia"}:
                        trainable_params = (p for p in model.parameters() if p.requires_grad)
                        custom_optimizer = create_optimizer_from_config(opt_section, trainable_params)
                        print(f"âœ“ Using custom optimizer from optimizer block: {opt_section.get('type')}")
    except Exception as opt_e:
        print(f"âš ï¸ Custom optimizer setup skipped: {opt_e}")

    # Setup trainer
    print("Setting up trainer...")
    
    # ë°ì´í„°ì…‹ ê²€ì¦
    train_dataset = dataset.get("train", None)
    eval_dataset = dataset.get("test", None)
    if eval_dataset is None:
        splited = train_dataset.train_test_split(test_size=0.1)
        train_dataset = splited["train"]
        eval_dataset = splited["test"]
    
    if train_dataset is None or len(train_dataset) == 0:
        raise ValueError(f"í›ˆë ¨ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤! ë°ì´í„°ì…‹ ë¡œë”©ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    print(f"âœ… ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ:")
    print(f"  - í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ")
    if eval_dataset is not None:
        print(f"  - í‰ê°€ ë°ì´í„°: {len(eval_dataset)} ìƒ˜í”Œ")
    else:
        print(f"  - í‰ê°€ ë°ì´í„°: ì—†ìŒ")
    
    # SFTTrainerì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ ë°ì´í„°ì…‹ í˜•íƒœë¥¼ í•œë²ˆ ë” í™•ì¸
    print("ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸:")
    print(f"  - ì²« ë²ˆì§¸ í›ˆë ¨ ìƒ˜í”Œ í‚¤: {list(train_dataset[0].keys())}")
    print(f"  - ì²« ë²ˆì§¸ ìƒ˜í”Œ messages: {train_dataset[0]['messages'][:100]}")
    
    # ì´ë¯¸ì§€ê°€ ìžˆëŠ” ê²½ìš°ì—ë§Œ ì¶œë ¥ (multi-domainì—ì„œëŠ” í…ìŠ¤íŠ¸ ì „ìš© ìƒ˜í”Œì´ ìžˆì„ ìˆ˜ ìžˆìŒ)
    first_sample_images = train_dataset[0].get('images', [])
    if first_sample_images and len(first_sample_images) > 0:
        if hasattr(first_sample_images[0], 'size'):
            print(f"  - ì²« ë²ˆì§¸ ìƒ˜í”Œ images: {first_sample_images[0].size}")
        else:
            print(f"  - ì²« ë²ˆì§¸ ìƒ˜í”Œ images: {type(first_sample_images[0])} (ì´ë¯¸ì§€ ê°ì²´)")
    else:
        print(f"  - ì²« ë²ˆì§¸ ìƒ˜í”Œ images: ì—†ìŒ (í…ìŠ¤íŠ¸ ì „ìš© ìƒ˜í”Œ)")
    
    trainer = SFTTrainer( 
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        processing_class=tokenizer,
        data_collator=collate_fn,
        optimizers=(custom_optimizer, None) if custom_optimizer is not None else (None, None)
    )
    
    # Trainer ìƒì„± í›„ wandbê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ì´ˆê¸°í™”
    # DeepSpeedê°€ Trainerë¥¼ ì´ˆê¸°í™”í•  ë•Œ wandbë¥¼ ìžë™ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì§€ë§Œ,
    # callbackì´ wandbë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— í™•ì‹¤ížˆ ì´ˆê¸°í™”ë˜ì–´ ìžˆëŠ”ì§€ ë³´ìž¥
    if training_config.get("report_to", None) and "wandb" in training_config["report_to"]:
        import wandb
        rank = int(os.getenv("RANK", "0"))
        if rank == 0 and (wandb.run is None or not wandb.run):
            # Trainerê°€ ì•„ì§ wandbë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì´ˆê¸°í™”
            run = wandb.init(
                project="g3moe-sft",
                name=training_config["run_name"],
                config=config,
                mode="online"  # í•­ìƒ onlineìœ¼ë¡œ wandbì— ê¸°ë¡
            )
            run.define_metric("train/*", step_metric="train/step")
            run.define_metric("validation/*", step_metric="validation/step")
            run.define_metric("eval/*", step_metric="eval/step")
            run.define_metric("moe/*", step_metric="train/step")
            run.define_metric("multi_modality/*", step_metric="train/step")
            run.define_metric("router/*", step_metric="train/step")
            run.define_metric("other/*", step_metric="train/step")

            logger.info("âœ… wandb initialized after Trainer creation")
        elif wandb.run is not None:
            logger.info("âœ… wandb already initialized by Trainer")
    # ZeRO-3ì—ì„œë„ gradient checkpointing ì‚¬ìš© ê°€ëŠ¥ (DeepSpeed activation checkpointingê³¼ í•¨ê»˜ ì‚¬ìš©)
    # ë‹¨, DeepSpeed configì— activation_checkpointingì´ í™œì„±í™”ë˜ì–´ ìžˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©
    try:
        ds_cfg_path = getattr(trainer.args, "deepspeed", None)
        if ds_cfg_path:
            import json
            with open(ds_cfg_path, "r") as f:
                ds_cfg = json.load(f)
            _zero_stage = int((ds_cfg.get("zero_optimization", {}) or {}).get("stage", 0) or 0)
            # DeepSpeed activation checkpointingì´ í™œì„±í™”ë˜ì–´ ìžˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            ds_activation_checkpointing = ds_cfg.get("activation_checkpointing", {})
            if ds_activation_checkpointing and ds_activation_checkpointing.get("partition_activations", False):
                print("âœ“ DeepSpeed activation checkpointing í™œì„±í™”ë¨ - PyTorch gradient checkpointingê³¼ í•¨ê»˜ ì‚¬ìš©")
                # PyTorch gradient checkpointingë„ í™œì„±í™” (DeepSpeedì™€ í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥)
                trainer.args.gradient_checkpointing = True
            elif _zero_stage == 3:
                # ZeRO-3ì´ê³  DeepSpeed activation checkpointingì´ ì—†ìœ¼ë©´ PyTorch gradient checkpointing ì‚¬ìš©
                print("âœ“ ZeRO-3ì—ì„œ PyTorch gradient checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)")
                trainer.args.gradient_checkpointing = True
    except Exception as e:
        print(f"âš ï¸ Gradient checkpointing ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        pass
    # Add MoE monitoring callback
    trainer.add_callback(
        create_moe_callback_for_transformers(
            num_experts=model_config["g3moe_params"]["n_routed_experts"],
            log_every_n_steps=1,             # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ê¸°ë¡
            logger=wandb,                    # ì‚¬ìš©í•  ë¡œê±° ì§€ì • (wandb)
            log_to_console=True,             # ì½˜ì†”ì—ë„ ì£¼ìš” ë©”íŠ¸ë¦­ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            debug_logging=True,              # ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™”
                       #  === (ì„ íƒì‚¬í•­) ===  #
            log_heatmap_every=5,             # 500 ìŠ¤í…ë§ˆë‹¤ Expert ì‚¬ìš©ë¥  ížˆíŠ¸ë§µ ë¡œê¹…
            alert_threshold_imbalance=4.0,   # íŠ¹ì • Expert ì‚¬ìš©ë¥ ì´ í‰ê· ì˜ 4ë°°ë¥¼ ì´ˆê³¼í•˜ë©´ ê²½ê³ 
            unused_expert_threshold=0.25,    # 25% ì´ìƒì˜ Expertê°€ ë¯¸ì‚¬ìš©ë˜ë©´ ê²½ê³ 
            entropy_threshold=0.1,           # ë¼ìš°íŒ… ì—”íŠ¸ë¡œí”¼ê°€ 0.1 ë¯¸ë§Œì´ë©´ ê²½ê³ 
            save_detailed_logs=False,        # ìƒì„¸ JSON ë¡œê·¸ ì €ìž¥ ì—¬ë¶€
            enable_generation_logging=True,  # ìƒì„± ë¡œê¹… í™œì„±í™”
        ))
    
    # Add custom training progress callback
    from transformers import TrainerCallback
    
    # ë°°ì¹˜ ì •ë³´ë¥¼ ì €ìž¥í•˜ëŠ” callback (OOM ë””ë²„ê¹…ìš©)
    class BatchTrackingCallback(TrainerCallback):
        """ë°°ì¹˜ ì •ë³´ë¥¼ ì¶”ì í•˜ì—¬ OOM ë°œìƒ ì‹œ ë””ë²„ê¹… ì •ë³´ ì œê³µ"""
        def __init__(self, trainer_ref):
            self.last_batch_info = None
            self.last_batch_step = -1
            self.trainer_ref = trainer_ref  # Trainer ì°¸ì¡°
        
        def on_step_begin(self, args, state, control, **kwargs):
            """Step ì‹œìž‘ ì‹œ ë°°ì¹˜ ì •ë³´ ì €ìž¥ ì‹œë„"""
            try:
                # Trainerì˜ ë‚´ë¶€ ìƒíƒœì—ì„œ ë°°ì¹˜ í™•ì¸
                trainer = kwargs.get('trainer') or self.trainer_ref
                if trainer is not None:
                    # Trainerì˜ _current_batch ë˜ëŠ” ìµœê·¼ ë°°ì¹˜ í™•ì¸
                    if hasattr(trainer, '_current_batch') and trainer._current_batch is not None:
                        self._save_batch_info(trainer._current_batch, state.global_step, trainer)
            except Exception:
                pass  # ë°°ì¹˜ ì •ë³´ ì €ìž¥ ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ê³„ì†
        
        def on_step_end(self, args, state, control, **kwargs):
            """Step ì¢…ë£Œ ì‹œ ë°°ì¹˜ ì •ë³´ ì €ìž¥ ì‹œë„"""
            try:
                trainer = kwargs.get('trainer') or self.trainer_ref
                if trainer is not None:
                    # Trainerì˜ ë‚´ë¶€ ìƒíƒœì—ì„œ ë°°ì¹˜ í™•ì¸
                    if hasattr(trainer, '_current_batch') and trainer._current_batch is not None:
                        self._save_batch_info(trainer._current_batch, state.global_step, trainer)
            except Exception:
                pass
        
        def _save_batch_info(self, batch, step, trainer):
            """ë°°ì¹˜ ì •ë³´ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì €ìž¥"""
            try:
                batch_info = {}
                
                # Input IDs ì •ë³´
                if 'input_ids' in batch and torch.is_tensor(batch['input_ids']):
                    input_ids = batch['input_ids']
                    batch_info['input_ids_shape'] = list(input_ids.shape)
                    if len(input_ids.shape) > 1:
                        # ê° ìƒ˜í”Œì˜ ì‹¤ì œ ê¸¸ì´ (pad ì œì™¸)
                        pad_token_id = 0
                        # processing_classì—ì„œ tokenizer ê°€ì ¸ì˜¤ê¸° (deprecatedëœ tokenizer ëŒ€ì‹ )
                        processing_class = getattr(trainer, 'processing_class', None)
                        if processing_class is not None:
                            # AutoProcessorì¸ ê²½ìš° tokenizer ì†ì„±ì— ì ‘ê·¼
                            tokenizer = getattr(processing_class, 'tokenizer', processing_class)
                            pad_token_id = getattr(tokenizer, 'pad_token_id', 0) or getattr(tokenizer, 'eos_token_id', 0)
                        
                        sample_lengths = (input_ids != pad_token_id).sum(dim=1).cpu().tolist()
                        batch_info['sample_lengths'] = sample_lengths[:10]  # ìµœëŒ€ 10ê°œë§Œ
                        batch_info['max_length'] = max(sample_lengths) if sample_lengths else 0
                        batch_info['min_length'] = min(sample_lengths) if sample_lengths else 0
                        batch_info['avg_length'] = sum(sample_lengths) / len(sample_lengths) if sample_lengths else 0
                        batch_info['total_tokens'] = input_ids.numel()
                
                # Attention mask ì •ë³´
                if 'attention_mask' in batch and torch.is_tensor(batch['attention_mask']):
                    attn_mask = batch['attention_mask']
                    batch_info['attention_mask_shape'] = list(attn_mask.shape)
                    batch_info['attention_mask_total'] = attn_mask.numel()
                
                # Pixel values (ì´ë¯¸ì§€) ì •ë³´
                if 'pixel_values' in batch and torch.is_tensor(batch['pixel_values']):
                    pixel_values = batch['pixel_values']
                    batch_info['pixel_values_shape'] = list(pixel_values.shape)
                    batch_info['pixel_values_dtype'] = str(pixel_values.dtype)
                    batch_info['pixel_values_memory_mb'] = pixel_values.numel() * pixel_values.element_size() / 1024 / 1024
                    batch_info['num_images'] = pixel_values.shape[0] if len(pixel_values.shape) > 0 else 0
                
                # Image grid ì •ë³´
                if 'image_grid_thw' in batch:
                    batch_info['image_grid_thw'] = batch['image_grid_thw']
                
                # Labels ì •ë³´
                if 'labels' in batch and torch.is_tensor(batch['labels']):
                    labels = batch['labels']
                    batch_info['labels_shape'] = list(labels.shape)
                    if labels.numel() > 0:
                        non_ignore = (labels != -100).sum().item()
                        batch_info['non_ignore_tokens'] = non_ignore
                        batch_info['ignore_tokens'] = (labels == -100).sum().item()
                
                # ë°°ì¹˜ í¬ê¸°
                if 'input_ids' in batch and torch.is_tensor(batch['input_ids']):
                    batch_info['batch_size'] = batch['input_ids'].shape[0] if len(batch['input_ids'].shape) > 0 else 1
                
                self.last_batch_info = batch_info
                self.last_batch_step = step
            except Exception as e:
                pass  # ë°°ì¹˜ ì •ë³´ ì €ìž¥ ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ê³„ì†
    
    # ë°°ì¹˜ ì¶”ì  callback ì¶”ê°€
    batch_tracker = BatchTrackingCallback(trainer)
    trainer.add_callback(batch_tracker)
    
    # Trainerì˜ training_stepì„ overrideí•˜ì—¬ ë°°ì¹˜ ì •ë³´ ì €ìž¥
    original_training_step = trainer.training_step
    
    def training_step_with_batch_tracking(self, model, inputs, num_items_in_batch=None):
        """ë°°ì¹˜ ì •ë³´ë¥¼ ì €ìž¥í•˜ëŠ” training_step wrapper"""
        try:
            # ë°°ì¹˜ ì •ë³´ë¥¼ trainerì— ì €ìž¥
            self._current_batch = inputs
            # ë°°ì¹˜ ì •ë³´ë¥¼ callbackì—ë„ ì €ìž¥
            if hasattr(self, 'state') and self.state:
                batch_tracker._save_batch_info(inputs, self.state.global_step, self)
        except Exception:
            pass  # ë°°ì¹˜ ì •ë³´ ì €ìž¥ ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ê³„ì†
        
        # ì›ëž˜ training_step í˜¸ì¶œ (ì¸ìž ê°œìˆ˜ì— ë§žê²Œ)
        if num_items_in_batch is not None:
            return original_training_step(model, inputs, num_items_in_batch)
        else:
            return original_training_step(model, inputs)
    
    import types
    trainer.training_step = types.MethodType(training_step_with_batch_tracking, trainer)
    
    class DetailedTrainingCallback(TrainerCallback):
        def __init__(self, logger):
            self.logger = logger
            self.last_log_time = time.time()
            self.log_interval = 10  # Log every 10 seconds during training
            
        def on_step_begin(self, args, state, control, **kwargs):
            current_time = time.time()
            if current_time - self.last_log_time >= self.log_interval:
                log_training_progress(
                    self.logger, 
                    kwargs.get('trainer'), 
                    step=state.global_step, 
                    epoch=state.epoch)
                log_gpu_memory(self.logger, f"STEP_{state.global_step}")
                self.last_log_time = current_time
                
        def on_step_end(self, args, state, control, **kwargs):
            # Log every 10 steps for detailed monitoring
            if state.global_step % 10 == 0:
                self.logger.debug(f"ðŸ“Š Step {state.global_step} completed")
                
        def on_epoch_begin(self, args, state, control, **kwargs):
            self.logger.info(f"ðŸ“… Starting epoch {int(state.epoch)}")
            log_gpu_memory(self.logger, f"EPOCH_{int(state.epoch)}_START")
            
        def on_epoch_end(self, args, state, control, **kwargs):
            self.logger.info(f"ðŸ“… Completed epoch {int(state.epoch)}")
            log_gpu_memory(self.logger, f"EPOCH_{int(state.epoch)}_END")
            
        def on_train_begin(self, args, state, control, **kwargs):
            self.logger.info("ðŸš€ Training started")
            log_gpu_memory(self.logger, "TRAINING_BEGIN")
            
        def on_train_end(self, args, state, control, **kwargs):
            self.logger.info("âœ… Training ended")
            log_gpu_memory(self.logger, "TRAINING_END")
            
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs:
                # Log important metrics
                if 'train_loss' in logs:
                    self.logger.info(f"ðŸ“Š Train Loss: {logs['train_loss']:.6f}")
                if 'learning_rate' in logs:
                    self.logger.debug(f"ðŸ“Š Learning Rate: {logs['learning_rate']:.2e}")
                if 'grad_norm' in logs:
                    self.logger.debug(f"ðŸ“Š Gradient Norm: {logs['grad_norm']:.6f}")
    
    # trainer.add_callback(DetailedTrainingCallback(logger))
    # trainer.add_callback(
    #     ModelEvalCallback(
    #         trainer=trainer,  # Will be set by Trainer
    #         enable_benchmarks=True,  # Enable benchmark evaluation
    #         benchmarks_to_run=['mmlu', 'hellaswag', 'gsm8k', 'truthfulqa', 'arc', 'piqa'],  # Run multiple benchmarks
    #         benchmark_eval_frequency=training_config["eval_steps"],  # Run benchmarks every 2 epochs
    #         mme_max_samples=10,  # Limit MME samples for faster evaluation
    #     ))
    # trainer.add_callback(
    #     IFEvalCallback(
    #         eval_dataset_name="google/IFEval",
    #         max_samples=100
    #     ))

    # Print training info
    print("\n" + "="*50)
    print("TRAINING CONFIGURATION")
    print("="*50)
    print(f"Model: {model_config['model_name_or_path']}")
    print(f"Dataset: {data_config['dataset_name']}")
    print(f"Max sequence length: {data_config['max_seq_length']}")
    print(f"Use LoRA: {model_config['use_lora']}")
    if model_config['use_lora']:
        print(f"LoRA rank: {model_config['lora_r']}")
    print(f"DeepSpeed config: {model_config.get('deepspeed_config', 'None')}")
    print(f"Training epochs: {training_config['num_train_epochs']}")
    print(f"Batch size per device: {training_config['per_device_train_batch_size']}")
    print(f"Gradient accumulation steps: {training_config['gradient_accumulation_steps']}")
    print(f"Learning rate: {training_config['learning_rate']}")
    print(f"FP16: {training_config['fp16']}")
    print(f"BF16: {training_config['bf16']}")
    print("="*50)
    # summary(
    #     trainer.model,
    #     input_data={
    #         'input_ids': torch.randint(0, tokenizer.tokenizer.vocab_size, (1, 1024), device=trainer.model.device)
    #     }, depth=3)
    # Start training
    print("Starting training...")
    # Guard heavy profiler behind an env flag to avoid OOM from profiler buffers during full training
    try:
        # Log training start
        logger.info(f"ðŸš€ Starting training...")
        logger.info(f"ðŸ”§ Training configuration:")
        logger.info(f"  - Epochs: {training_config['num_train_epochs']}")
        logger.info(f"  - Batch size per device: {training_config['per_device_train_batch_size']}")
        logger.info(f"  - Gradient accumulation steps: {training_config['gradient_accumulation_steps']}")
        logger.info(f"  - Learning rate: {training_config['learning_rate']}")
        logger.info(f"  - Max sequence length: {data_config['max_seq_length']}")
        
        enable_profiler = bool(int(os.getenv("PROFILE_TRAINING", "0")))
        if enable_profiler:
            from torch.profiler import profile, record_function, ProfilerActivity
            with profile(
                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                record_shapes=True,
                profile_memory=True,
                with_stack=True,
            ) as prof:
                try:
                    trainer.train()
                    profiler_table = prof.key_averages().table(sort_by="self_cuda_memory_usage", row_limit=10)
                    wandb.log({"profiler_table": wandb.Table(data=[profiler_table])})
                except Exception as e:
                    traceback.print_exc()
                    print(f"âš ï¸ Profiler error: {e}")
        else:
            # eval ìµœì í™”ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ eval í•¨ìˆ˜ ì„¤ì •
            logger.info("ðŸ”§ Setting up memory-optimized evaluation...")
            original_eval_fn = getattr(trainer, 'evaluate', None)
            trainer.evaluate = lambda eval_dataset=None, ignore_keys=None, metric_key_prefix="eval": eval_with_memory_optimization(trainer, original_eval_fn, eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
            
            # í•™ìŠµ ì‹œìž‘ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
            logger.info("ðŸ§¹ í•™ìŠµ ì‹œìž‘ ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬...")
            clear_gpu_memory()
            
            # DataLoader ìµœì í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            if hasattr(trainer.args, 'dataloader_num_workers'):
                if trainer.args.dataloader_num_workers is None or trainer.args.dataloader_num_workers > 0:
                    logger.info(f"ðŸ”§ DataLoader num_workersë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)")
                    trainer.args.dataloader_num_workers = 1
            
            # Log initial memory state
            log_gpu_memory(logger, "TRAINING_START")
            
            # Start training with progress monitoring
            start_time = time.time()
            trainer.train()
            training_time = time.time() - start_time
            
            logger.info(f"âœ… Training completed successfully in {training_time:.2f} seconds")
        
    except KeyboardInterrupt as e:
        logger.error(f"âŒ KeyboardInterrupt during training: {str(e)}")
        log_error_context(logger, e, "training_keyboard_interrupt")
        raise e

    except RuntimeError as e:
        error_msg = str(e)
        logger.error(f"âŒ RuntimeError during training: {error_msg}")
        
        # CUBLAS ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨ë„ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì²˜ë¦¬
        is_memory_error = (
            "CUDA out of memory" in error_msg or
            "CUBLAS_STATUS_ALLOC_FAILED" in error_msg or
            "cublasCreate" in error_msg
        )
        
        if is_memory_error:
            logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ ë°œìƒ! (CUDA OOM ë˜ëŠ” CUBLAS í• ë‹¹ ì‹¤íŒ¨)")
            logger.error("   ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            
            # Log detailed memory state at OOM
            log_gpu_memory(logger, "OOM_ERROR")
            
            # Log training state at OOM
            if hasattr(trainer, 'state') and trainer.state is not None:
                state = trainer.state
                logger.error(f"âŒ Training state at OOM:")
                logger.error(f"  - Global step: {state.global_step}")
                logger.error(f"  - Epoch: {state.epoch:.3f}")
                logger.error(f"  - Current loss: {getattr(state, 'log_history', [{}])[-1].get('train_loss', 'N/A')}")
            
            # Log model state
            logger.error(f"âŒ Model state at OOM:")
            logger.error(f"  - Model device: {next(trainer.model.parameters()).device}")
            logger.error(f"  - Model dtype: {next(trainer.model.parameters()).dtype}")
            logger.error(f"  - Model requires_grad: {next(trainer.model.parameters()).requires_grad}")
            
            # Log batch information
            if hasattr(trainer, 'train_dataloader'):
                try:
                    batch_size = trainer.per_device_train_batch_size
                    grad_accum = trainer.gradient_accumulation_steps
                    effective_batch = batch_size * grad_accum
                    logger.error(f"âŒ Batch configuration at OOM:")
                    logger.error(f"  - Per device batch size: {batch_size}")
                    logger.error(f"  - Gradient accumulation: {grad_accum}")
                    logger.error(f"  - Effective batch size: {effective_batch}")
                except Exception as batch_e:
                    logger.error(f"âŒ Could not get batch info: {batch_e}")
            
            # í˜„ìž¬ ë°°ì¹˜ì˜ ë°ì´í„° ìƒ˜í”Œ ì •ë³´ ìˆ˜ì§‘
            logger.error("ðŸ“Š Collecting data sample information at OOM...")
            try:
                # ë°°ì¹˜ ì¶”ì  callbackì—ì„œ ì €ìž¥ëœ ì •ë³´ ì‚¬ìš©
                batch_info = None
                if hasattr(trainer, 'callback_handler') and trainer.callback_handler is not None:
                    for callback in trainer.callback_handler.callbacks:
                        if hasattr(callback, 'last_batch_info') and callback.last_batch_info is not None:
                            batch_info = callback.last_batch_info
                            logger.error(f"âŒ Last processed batch information (step {getattr(callback, 'last_batch_step', 'unknown')}):")
                            break
                
                if batch_info:
                    # Input IDs ì •ë³´
                    if 'input_ids_shape' in batch_info:
                        logger.error(f"  - Input IDs shape: {batch_info['input_ids_shape']}")
                        logger.error(f"  - Input IDs total tokens: {batch_info.get('total_tokens', 'N/A')}")
                        if 'sample_lengths' in batch_info:
                            logger.error(f"  - Sample lengths: {batch_info['sample_lengths']}")
                            logger.error(f"  - Max sample length: {batch_info.get('max_length', 'N/A')}")
                    
                    # Attention mask ì •ë³´
                    if 'attention_mask_shape' in batch_info:
                        logger.error(f"  - Attention mask shape: {batch_info['attention_mask_shape']}")
                        logger.error(f"  - Attention mask total elements: {batch_info.get('attention_mask_total', 'N/A')}")
                    
                    # Pixel values (ì´ë¯¸ì§€) ì •ë³´
                    if 'pixel_values_shape' in batch_info:
                        logger.error(f"  - Pixel values shape: {batch_info['pixel_values_shape']}")
                        logger.error(f"  - Pixel values memory (MB): {batch_info.get('pixel_values_memory_mb', 'N/A'):.2f}")
                        logger.error(f"  - Number of images in batch: {batch_info.get('num_images', 'N/A')}")
                    
                    # Image grid ì •ë³´
                    if 'image_grid_thw' in batch_info:
                        logger.error(f"  - Image grid info: {batch_info['image_grid_thw']}")
                    
                    # Labels ì •ë³´
                    if 'labels_shape' in batch_info:
                        logger.error(f"  - Labels shape: {batch_info['labels_shape']}")
                        logger.error(f"  - Non-ignore tokens: {batch_info.get('non_ignore_tokens', 'N/A')}")
                
                # Trainerì˜ ë‚´ë¶€ ìƒíƒœì—ì„œ í˜„ìž¬ ë°°ì¹˜ ì •ë³´ í™•ì¸ (fallback)
                if not batch_info:
                    if hasattr(trainer, '_current_batch') and trainer._current_batch is not None:
                        batch = trainer._current_batch
                        logger.error(f"âŒ Current batch information (from trainer._current_batch):")
                        logger.error(f"  - Batch keys: {list(batch.keys()) if isinstance(batch, dict) else 'N/A'}")
                        
                        # Input IDs ì •ë³´
                        if 'input_ids' in batch and torch.is_tensor(batch['input_ids']):
                            input_ids = batch['input_ids']
                            logger.error(f"  - Input IDs shape: {input_ids.shape}")
                            logger.error(f"  - Input IDs total tokens: {input_ids.numel()}")
                            
                            # ê° ìƒ˜í”Œì˜ ê¸¸ì´
                            if len(input_ids.shape) > 1:
                                # processing_classì—ì„œ tokenizer ê°€ì ¸ì˜¤ê¸° (deprecatedëœ tokenizer ëŒ€ì‹ )
                                processing_class = getattr(trainer, 'processing_class', None)
                                pad_token_id = 0
                                if processing_class is not None:
                                    # AutoProcessorì¸ ê²½ìš° tokenizer ì†ì„±ì— ì ‘ê·¼
                                    tokenizer = getattr(processing_class, 'tokenizer', processing_class)
                                    pad_token_id = getattr(tokenizer, 'pad_token_id', 0) or getattr(tokenizer, 'eos_token_id', 0)
                                sample_lengths = (input_ids != pad_token_id).sum(dim=1).cpu().tolist()
                                logger.error(f"  - Sample lengths: {sample_lengths}")
                                logger.error(f"  - Max sample length: {max(sample_lengths) if sample_lengths else 'N/A'}")
                                logger.error(f"  - Min sample length: {min(sample_lengths) if sample_lengths else 'N/A'}")
                                logger.error(f"  - Avg sample length: {sum(sample_lengths) / len(sample_lengths) if sample_lengths else 'N/A':.2f}")
                        
                        # Pixel values (ì´ë¯¸ì§€) ì •ë³´
                        if 'pixel_values' in batch and torch.is_tensor(batch['pixel_values']):
                            pixel_values = batch['pixel_values']
                            logger.error(f"  - Pixel values shape: {pixel_values.shape}")
                            logger.error(f"  - Pixel values memory (MB): {pixel_values.numel() * pixel_values.element_size() / 1024 / 1024:.2f}")
                            logger.error(f"  - Number of images in batch: {pixel_values.shape[0] if len(pixel_values.shape) > 0 else 'N/A'}")
                
                # ìµœê·¼ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
                if hasattr(trainer, 'train_dataset') and trainer.train_dataset is not None:
                    try:
                        state = trainer.state
                        if state and hasattr(state, 'global_step'):
                            # í˜„ìž¬ stepì—ì„œ ì²˜ë¦¬ ì¤‘ì¸ ìƒ˜í”Œ ì¸ë±ìŠ¤ ì¶”ì •
                            dataset_size = len(trainer.train_dataset) if hasattr(trainer.train_dataset, '__len__') else 'unknown'
                            logger.error(f"  - Dataset size: {dataset_size}")
                            
                            # ìƒ˜í”Œ ëª‡ ê°œ í™•ì¸ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìµœì†Œí•œë§Œ)
                            if dataset_size != 'unknown' and dataset_size > 0:
                                sample_indices = []
                                if hasattr(trainer, 'per_device_train_batch_size'):
                                    batch_size = trainer.per_device_train_batch_size
                                    if hasattr(trainer, 'gradient_accumulation_steps'):
                                        batch_size *= trainer.gradient_accumulation_steps
                                    
                                    # í˜„ìž¬ stepì—ì„œ ì²˜ë¦¬ ì¤‘ì¸ ìƒ˜í”Œ ë²”ìœ„ ì¶”ì •
                                    start_idx = (state.global_step * batch_size) % dataset_size
                                    end_idx = min(start_idx + batch_size, dataset_size)
                                    sample_indices = list(range(start_idx, end_idx))[:5]  # ìµœëŒ€ 5ê°œë§Œ
                                
                                if sample_indices:
                                    logger.error(f"  - Estimated sample indices at OOM: {sample_indices}")
                                    for idx in sample_indices[:3]:  # ìµœëŒ€ 3ê°œë§Œ ìƒì„¸ í™•ì¸
                                        try:
                                            sample = trainer.train_dataset[idx]
                                            sample_info = {}
                                            
                                            # Messages ì •ë³´
                                            if 'messages' in sample:
                                                messages = sample['messages']
                                                if isinstance(messages, list):
                                                    total_text_len = 0
                                                    for msg in messages:
                                                        if isinstance(msg, dict) and 'content' in msg:
                                                            content = msg['content']
                                                            if isinstance(content, list):
                                                                for item in content:
                                                                    if isinstance(item, dict) and 'text' in item:
                                                                        total_text_len += len(str(item['text']))
                                                            elif isinstance(content, str):
                                                                total_text_len += len(content)
                                                    sample_info['messages_text_length'] = total_text_len
                                                    sample_info['num_messages'] = len(messages)
                                            
                                            # Images ì •ë³´
                                            if 'images' in sample:
                                                images = sample['images']
                                                if isinstance(images, list):
                                                    sample_info['num_images'] = len(images)
                                                    if images:
                                                        try:
                                                            from PIL import Image
                                                            if isinstance(images[0], Image.Image):
                                                                sample_info['image_sizes'] = [img.size for img in images[:3]]
                                                        except:
                                                            pass
                                                elif images is not None:
                                                    sample_info['has_image'] = True
                                            
                                            logger.error(f"    Sample {idx}: {sample_info}")
                                        except Exception as sample_e:
                                            logger.error(f"    Sample {idx}: Could not inspect ({sample_e})")
                    except Exception as dataset_e:
                        logger.error(f"  - Could not inspect dataset: {dataset_e}")
                
            except Exception as data_collect_e:
                logger.error(f"âŒ Failed to collect data sample information: {data_collect_e}")
                import traceback
                logger.error(f"  Traceback: {traceback.format_exc()}")
            
            logger.error("âŒ ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ìž¬ì‹œë„...")
            clear_gpu_memory()
            logger.error("âŒ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ.")
            logger.error("ðŸ’¡ í•´ê²° ë°©ë²• ì œì•ˆ:")
            logger.error("   1. per_device_train_batch_sizeë¥¼ ë” ì¤„ì´ê¸° (í˜„ìž¬: {})".format(
                trainer.per_device_train_batch_size if hasattr(trainer, 'per_device_train_batch_size') else 'N/A'
            ))
            logger.error("   2. gradient_accumulation_stepsë¥¼ ë” ëŠ˜ë¦¬ê¸° (í˜„ìž¬: {})".format(
                trainer.gradient_accumulation_steps if hasattr(trainer, 'gradient_accumulation_steps') else 'N/A'
            ))
            logger.error("   3. max_lengthë¥¼ ì¤„ì´ê¸° (í˜„ìž¬: {})".format(
                trainer.args.max_length if hasattr(trainer.args, 'max_length') else 'N/A'
            ))
            logger.error("   4. ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ GPUë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸ (nvidia-smi)")
            logger.error("   5. DeepSpeed ZeRO-3 CPU offloadê°€ ì œëŒ€ë¡œ ìž‘ë™í•˜ëŠ”ì§€ í™•ì¸")
            logger.error("   6. ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ìƒ˜í”Œì´ ë§Žìœ¼ë©´ ì´ë¯¸ì§€ ì „ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬ ê³ ë ¤")
            logger.error("   7. ìœ„ì˜ ë°ì´í„° ìƒ˜í”Œ ì •ë³´ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œê°€ ë˜ëŠ” ìƒ˜í”Œì„ í•„í„°ë§í•˜ê±°ë‚˜ ì²˜ë¦¬ ë°©ì‹ ë³€ê²½ ê³ ë ¤")
            
        else:
            logger.error(f"âŒ Other RuntimeError: {error_msg}")
            log_error_context(logger, e, "training_runtime_error")
        
        raise e
        
    except Exception as e:
        logger.error(f"âŒ Unexpected error during training: {str(e)}")
        log_error_context(logger, e, "training_unexpected_error")
        raise e
        
    finally:
        # ì›ëž˜ eval í•¨ìˆ˜ ë³µì›
        # Save final model
        print("Saving final model...")
        if config.get("deepspeed_config") is not None:
            trainer.deepspeed.save_checkpoint(training_args.output_dir)
        trainer.save_model()
        
        # Save tokenizer``
        tokenizer.save_pretrained(training_args.output_dir)
        print("Training End")
        if original_eval_fn:
            logger.debug("ðŸ”§ Restoring original evaluation function...")
            trainer.evaluate = original_eval_fn


if __name__ == "__main__":
    register_custom_optimizers()
    try:
        # Parse command line arguments
        parser = argparse.ArgumentParser(description="G3MoE SFT Training with Config File")
        parser.add_argument(
            "--config", 
            type=str, 
            default="sft/config/g3moe_training_config.json",
            help="Path to training configuration JSON file"
        )
        args = parser.parse_args()
        
        # Load configuration
        config = load_config(args.config)
        
        model_config = config["model_config"]
        data_config = config["data_config"]
        training_config = config["training_config"]
        
        # Set seed
        set_seed(training_config["seed"])
        # wandb.init()ì€ Trainerê°€ ìžë™ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë„ë¡ í•¨
        # DeepSpeedê°€ Trainerë¥¼ ì´ˆê¸°í™”í•  ë•Œ wandbë¥¼ ìž¬ì´ˆê¸°í™”í•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ
        # ì—¬ê¸°ì„œ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì§€ ì•Šê³  Trainerì˜ ìžë™ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©
        
        main(model_config, data_config, training_config)

    except Exception as e:
        logger.error(f"âŒ Fatal error in main: {str(e)}")
        log_error_context(logger, e, "main_function")
        
        # Log final memory state
        if torch.cuda.is_available():
            logger.error("âŒ Final GPU memory state:")
            logger.error(f"âŒ Memory summary:\n{torch.cuda.memory_summary()}")
            logger.error(f"âŒ Max memory allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f}GB")
            logger.error(f"âŒ Max memory reserved: {torch.cuda.max_memory_reserved() / 1024**3:.2f}GB")
        
        # Re-raise the exception
        raise e
