{
    "model_config": {
        "model_name_or_path": "google/gemma-2b-it",
        "tokenizer_name_or_path": null,
        "trust_remote_code": true,
        "use_lora": true,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "deepspeed_config": null,
        "g3moe_params": {
            "n_shared_experts": 2,
            "n_routed_experts": 8, 
            "n_group": 2,
            "topk_group": 1,
            "num_experts_per_tok": 2,
            "rope_scaling_factor": 1.0
        }
    },
    "data_config": {
        "dataset_name": "HuggingFaceTB/smoltalk",
        "max_samples": 500,
        "max_seq_length": 1024,
        "test_size": 0.1
    },
    "training_config": {
        "output_dir": "./test_sft_output",
        "num_train_epochs": 1,
        "per_device_train_batch_size": 2,
        "per_device_eval_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "learning_rate": 5e-5,
        "weight_decay": 0.01,
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.1,
        "logging_steps": 10,
        "eval_steps": 50,
        "save_steps": 100,
        "save_total_limit": 2,
        "eval_strategy": "steps",
        "load_best_model_at_end": true,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": false,
        "fp16": false,
        "bf16": true,
        "dataloader_pin_memory": false,
        "remove_unused_columns": false,
        "gradient_checkpointing": true,
        "report_to": [],
        "run_name": "simple_sft_test",
        "seed": 42
    }
} 