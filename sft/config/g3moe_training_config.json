{
  "model_config": {
    "model_name_or_path": "google/gemma-3-4b-it",
    "tokenizer_name_or_path": null,
    "trust_remote_code": true,
    "use_lora": true,
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "deepspeed_config": null,
    "g3moe_params": {
      "n_shared_experts": 1,
      "n_routed_experts": 2,
      "n_group": 4,
      "topk_group": 8,
      "num_experts_per_tok": 2,
      "rope_scaling_factor": 8.0
    }
  },
  "data_config": {
    "dataset_name": "Gunulhona/open_m_3",
    "max_seq_length": 131072,
    "test_size": 0.1,
    "text_only": false,
    "streaming": false
  },
  "training_config": {
    "output_dir": "./g3moe_sft_output",
    "num_train_epochs": 3,
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 2,
    "gradient_accumulation_steps": 8,
    "learning_rate": 2e-5,
    "weight_decay": 0.01,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.1,
    "logging_steps": 10,
    "eval_steps": 500,
    "save_steps": 500,
    "save_total_limit": 3,
    "evaluation_strategy": "steps",
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "fp16": false,
    "bf16": true,
    "dataloader_pin_memory": false,
    "remove_unused_columns": false,
    "gradient_checkpointing": true,
    "report_to": ["wandb"],
    "run_name": "g3moe-sft-run",
    "seed": 42
  }
} 