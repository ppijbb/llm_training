"""
Error handling utilities for training
"""
import os
import json
import traceback
import logging
from datetime import datetime
from typing import Optional
import torch


def log_error_context(logger: logging.Logger, error: Exception, context: str = ""):
    """Log detailed error context with system state"""
    logger.error(f"âŒ Error in {context}: {str(error)}")
    logger.error(f"âŒ Error type: {type(error).__name__}")
    
    # Log traceback
    logger.error(f"âŒ Traceback:\n{traceback.format_exc()}")
    
    # Log GPU memory state
    if torch.cuda.is_available():
        from training_utils.logging_utils import log_gpu_memory
        memory_info = log_gpu_memory(logger, "ERROR")
        if memory_info:
            logger.error(f"âŒ GPU Memory at error - Allocated: {memory_info['allocated']:.2f}GB, Reserved: {memory_info['reserved']:.2f}GB")
    
    # Log system state
    logger.error(f"âŒ System state - CUDA available: {torch.cuda.is_available()}, Device count: {torch.cuda.device_count()}")
    if torch.cuda.is_available():
        logger.error(f"âŒ Current device: {torch.cuda.current_device()}, Device name: {torch.cuda.get_device_name()}")


def save_oom_error_info(
    logger: logging.Logger, 
    trainer, 
    error: Exception, 
    batch_info: Optional[dict] = None, 
    output_dir: Optional[str] = None
) -> Optional[str]:
    """OOM ì—ëŸ¬ ì •ë³´ ì €ìž¥"""
    try:
        if output_dir is None:
            output_dir = getattr(trainer.args, 'output_dir', None) if trainer and hasattr(trainer, 'args') else "logs"
        os.makedirs(output_dir, exist_ok=True)
        error_file = os.path.join(output_dir, f"oom_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        from training_utils.logging_utils import log_gpu_memory
        oom_info = {
            'timestamp': datetime.now().isoformat(),
            'error': {'type': type(error).__name__, 'message': str(error)},
            'gpu_memory': log_gpu_memory(logger, "OOM") if torch.cuda.is_available() else {}
        }
        
        if batch_info:
            oom_info['batch_info'] = batch_info
        
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(oom_info, f, indent=2, ensure_ascii=False, default=str)
        logger.error(f"ðŸ’¾ OOM ì—ëŸ¬ ì •ë³´ ì €ìž¥: {error_file}")
        return error_file
    except Exception as e:
        logger.error(f"âŒ OOM ì—ëŸ¬ ì •ë³´ ì €ìž¥ ì‹¤íŒ¨: {e}")
        return None


def handle_cuda_oom(e: torch.OutOfMemoryError, trainer, logger: logging.Logger):
    """CUDA OOM ì²˜ë¦¬"""
    logger.error(f"âŒ CUDA OOM: {str(e)}")
    from training_utils.logging_utils import log_gpu_memory
    log_gpu_memory(logger, "OOM")
    if hasattr(trainer, 'state') and trainer.state:
        epoch_str = f"{trainer.state.epoch:.3f}" if trainer.state.epoch is not None else "N/A"
        logger.error(f"Step: {trainer.state.global_step}, Epoch: {epoch_str}")
    save_oom_error_info(logger, trainer, e)
    from training_utils.memory_utils import clear_gpu_memory
    clear_gpu_memory(logger)
    logger.error("ðŸ’¡ í•´ê²° ë°©ë²•: batch_size ê°ì†Œ, gradient_accumulation_steps ì¦ê°€, max_length ê°ì†Œ")


def handle_ram_oom(e: MemoryError, trainer, logger: logging.Logger):
    """RAM OOM ì²˜ë¦¬"""
    logger.error(f"âŒ RAM OOM: {str(e)}")
    save_oom_error_info(logger, trainer, e)
    logger.error("ðŸ’¡ í•´ê²° ë°©ë²•: streaming=True, ë°°ì¹˜ í¬ê¸° ê°ì†Œ, CPU offload í™œì„±í™”")


def handle_training_exception(
    e: Exception, 
    trainer, 
    logger: logging.Logger, 
    context: str = "training"
):
    """
    í•™ìŠµ ì¤‘ ë°œìƒí•˜ëŠ” ì¼ë°˜ exceptionì„ í†µí•© ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        e: Exception ê°ì²´
        trainer: Trainer ê°ì²´
        logger: Logger ê°ì²´
        context: ì—ëŸ¬ ë°œìƒ ì»¨í…ìŠ¤íŠ¸ (ì˜ˆ: "training", "training_keyboard_interrupt", "training_runtime_error")
    """
    error_msg = str(e)
    error_type = type(e).__name__
    
    logger.error(f"âŒ {error_type} during {context}: {error_msg}")
    log_error_context(logger, e, context)
    
    # íŠ¹ì • ì—ëŸ¬ íƒ€ìž…ë³„ ì¶”ê°€ ì²˜ë¦¬
    if isinstance(e, KeyboardInterrupt):
        logger.error("âŒ í•™ìŠµì´ ì‚¬ìš©ìžì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif isinstance(e, RuntimeError):
        # CUBLAS ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨ ë“± RuntimeError ì²˜ë¦¬
        if "CUBLAS_STATUS_ALLOC_FAILED" in error_msg or "cublasCreate" in error_msg:
            logger.error("âŒ CUBLAS ë©”ëª¨ë¦¬ í• ë‹¹ ì‹¤íŒ¨ - GPU ë©”ëª¨ë¦¬ ë¬¸ì œì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
            from training_utils.logging_utils import log_gpu_memory
            log_gpu_memory(logger, "CUBLAS_ERROR")
        # NCCL ì˜¤ë¥˜ ì²˜ë¦¬
        elif "NCCL" in error_msg or "nccl" in error_msg.lower() or "DistBackendError" in error_type:
            logger.error("âŒ NCCL ë¶„ì‚° í†µì‹  ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            logger.error("   ê°€ëŠ¥í•œ ì›ì¸:")
            logger.error("   1. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ")
            logger.error("   2. ì›ê²© í”„ë¡œì„¸ìŠ¤ê°€ ì˜ˆê¸°ì¹˜ ì•Šê²Œ ì¢…ë£Œë¨")
            logger.error("   3. GPU ê°„ í†µì‹  ë¬¸ì œ")
            logger.error("   4. DeepSpeed ì´ˆê¸°í™” ì‹¤íŒ¨")
            logger.error("   ðŸ’¡ í•´ê²° ë°©ë²•:")
            logger.error("   - ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸")
            logger.error("   - ëª¨ë“  ë…¸ë“œê°€ ì •ìƒ ìž‘ë™í•˜ëŠ”ì§€ í™•ì¸")
            logger.error("   - NCCL í™˜ê²½ ë³€ìˆ˜ ì¡°ì • (NCCL_DEBUG=INFO)")
            logger.error("   - ë‹¨ì¼ GPUë¡œ í…ŒìŠ¤íŠ¸")
        else:
            logger.error(f"âŒ RuntimeError: {error_msg}")
    else:
        logger.error(f"âŒ Unexpected {error_type}: {error_msg}")


def collect_environment_info() -> dict:
    """ê°„ë‹¨í•œ í™˜ê²½ ì •ë³´ ìˆ˜ì§‘"""
    env_info = {'timestamp': datetime.now().isoformat()}
    try:
        env_info['pytorch'] = {'version': torch.__version__, 'cuda_available': torch.cuda.is_available()}
        if torch.cuda.is_available():
            env_info['cuda'] = {
                'device_count': torch.cuda.device_count(),
                'device_name': torch.cuda.get_device_name(),
                'memory_allocated_gb': torch.cuda.memory_allocated() / 1024**3
            }
    except Exception as e:
        env_info['error'] = str(e)
    return env_info

