
import torch
import logging
from typing import Dict, Any, List, Optional, Tuple, Callable
from PIL import Image

def setup_dataset(data_config: Dict[str, Any], tokenizer, logger: logging.Logger, training_config: Dict[str, Any] = None, allow_text_only: bool = False) -> Tuple[Dict, Callable]:
    """
    Qwen3-VL-MoE ì „ìš©: Native Processorë¥¼ 100% í™œìš©í•˜ëŠ” ë°ì´í„°ì…‹ ì„¤ì •.
    Universal Exoskeleton êµ¬ì¡°ë¥¼ ìœ„í•´ ë¶ˆí•„ìš”í•œ ë˜í¼ë¥¼ ì œê±°í•¨.
    """
    from data.multi_domain_sft_dataset import get_multi_domain_sft_dataset
    
    dataset_name = data_config.get("dataset_name", "HuggingFaceTB/smoltalk")
    max_samples = data_config.get("max_samples", 100000)
    max_seq_length = data_config.get("max_seq_length", 131072)
    test_size = data_config.get("test_size", 0.005)
    use_streaming = data_config.get("streaming", False)
    
    logger.info(f"ğŸ’¾ Loading Multi-Modal Dataset: {dataset_name} (Max: {max_samples})")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ (í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ í¬í•¨)
    dataset = get_multi_domain_sft_dataset(
        tokenizer=tokenizer,
        max_length=max_seq_length,
        max_samples_per_domain=max_samples,
        test_size=test_size,
        use_streaming=use_streaming,
        allow_text_only=False # ë°˜ë“œì‹œ ë©€í‹°ëª¨ë‹¬ ì‚¬ìš©
    )

    # -------------------------------------------------------------
    # [FIX] Native Processor ê¸°ë°˜ì˜ ê°€ì¥ ë‹¨ìˆœí•œ Collator
    # Qwen3-VLì€ ë°˜ë“œì‹œ processorë¥¼ í†µí•´ textì™€ imageë¥¼ í•œêº¼ë²ˆì— tokenizeí•´ì•¼ í•¨.
    #
    # [ZeRO-3 Deadlock ë°©ì§€] ëª¨ë“  ìƒ˜í”Œì´ ë™ì¼í•œ computation pathë¥¼ ë”°ë¼ì•¼ í•¨.
    # - í…ìŠ¤íŠ¸ ì „ìš©(ì´ë¯¸ì§€ ì—†ìŒ)ì´ë¼ë„: ì´ë¯¸ì§€ placeholder + dummy ì´ë¯¸ì§€ ì£¼ì…
    # - ëª¨ë“  rankê°€ í•­ìƒ (pixel_values í¬í•¨) ë™ì¼í•œ VLM forward/backwardë¥¼ ìˆ˜í–‰
    # -------------------------------------------------------------

    def pure_qwen_collate_fn(examples):
        # examples: List[Dict["messages": ..., "images": ...]]
        texts = []
        images = []
        for example in examples:
            msgs = example["messages"]
            has_image_placeholder = any(
                any(c.get("type") == "image" for c in m.get("content", []))
                for m in msgs if isinstance(m.get("content"), list)
            )
            has_actual_images = bool(example.get("images"))

            # [ZeRO-3] ì´ë¯¸ì§€ placeholderê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ ì¶”ê°€ (í…ìŠ¤íŠ¸ ì „ìš© â†’ ë™ì¼ path ê°•ì œ)
            if not has_image_placeholder:
                for m in msgs:
                    if m["role"] == "user":
                        if isinstance(m["content"], str):
                            m["content"] = [{"type": "image"}, {"type": "text", "text": m["content"]}]
                        elif isinstance(m["content"], list):
                            m["content"].insert(0, {"type": "image"})
                        break
                has_image_placeholder = True

            text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)
            texts.append(text)

            # [ZeRO-3] í•­ìƒ ì´ë¯¸ì§€ 1ê°œ per sample. placeholderì™€ ê°œìˆ˜ ì¼ì¹˜ í•„ìˆ˜.
            if has_actual_images:
                img = example["images"][0] if isinstance(example["images"], list) else example["images"]
                images.append(img)
            else:
                images.append(Image.new('RGB', (224, 224), (0, 0, 0)))

        # [0vs2048] rankë³„ vision patch ìˆ˜ ë¶ˆì¼ì¹˜(256 vs 1064) ë°©ì§€: ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë™ì¼ í•´ìƒë„ë¡œ ë§ì¶¤ â†’ processor ì¶œë ¥ patch ìˆ˜ ë™ì¼.
        _fix_size = (224, 224)
        def _resize_to_fixed(img):
            if not hasattr(img, "resize"):
                return img
            if hasattr(img, "size") and img.size != _fix_size:
                if getattr(img, "mode", "RGB") != "RGB":
                    img = img.convert("RGB")
                return img.resize(_fix_size, Image.BILINEAR)
            return img
        images = [_resize_to_fixed(im) for im in images]

        # [ZeRO-3] ëª¨ë“  ìƒ˜í”Œì— ì´ë¯¸ì§€ ìˆìŒ â†’ í•­ìƒ ë™ì¼ path. None/í˜¼í•© ë¶„ê¸° ì œê±°.
        batch = tokenizer(
            text=texts,
            images=images,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_seq_length
        )
        
        # Labels ìƒì„±
        batch["labels"] = batch["input_ids"].clone()
        # Padding (-100) ë° Vision Special Tokens ë§ˆìŠ¤í‚¹
        # Qwen3-VL-MoEì˜ pad_token_id í™•ì¸
        pad_id = tokenizer.tokenizer.pad_token_id if hasattr(tokenizer, 'tokenizer') else tokenizer.pad_token_id
        if pad_id is not None:
            batch["labels"][batch["labels"] == pad_id] = -100
            
        # [CRITICAL] Vision Token IDë“¤(-100 ë§ˆìŠ¤í‚¹)ì„ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³  ë™ì ìœ¼ë¡œ ì²˜ë¦¬
        # Qwen3-VL-MoE: <|vision_start|>, <|image_pad|>, <|vision_end|> ë“±
        # ì´ë“¤ì€ í•™ìŠµ ëŒ€ìƒì´ ì•„ë‹˜
        for token_name in ["<|vision_start|>", "<|vision_end|>", "<|image_pad|>", "<|video_pad|>"]:
            try:
                tid = tokenizer.tokenizer.convert_tokens_to_ids(token_name)
                batch["labels"][batch["labels"] == tid] = -100
            except Exception:
                pass

        # [0vs2048] ì²´í¬: ì²« ë°°ì¹˜ 1íšŒë§Œ ë¡œê·¸ (collator ì¶œë ¥ shape ê²€ì¦)
        if not getattr(pure_qwen_collate_fn, "_logged_once", False):
            pure_qwen_collate_fn._logged_once = True
            _pv = batch.get("pixel_values")
            _grid = batch.get("image_grid_thw")
            _num_patches = int(_pv.shape[0]) if _pv is not None and hasattr(_pv, "shape") else None
            _grid_str = tuple(_grid.shape) if _grid is not None and hasattr(_grid, "shape") else str(_grid)
            import sys
            print(f"[0vs2048] COLLATOR CHECK: first batch | pixel_values.shape={getattr(_pv, 'shape', None)}, image_grid_thw.shape={_grid_str} | num_patches(seq)={_num_patches} (must be same on all ranks)", flush=True)
            sys.stdout.flush()

        return batch

    logger.info("[0vs2048] Collator: pure_qwen_collate_fn | image resize 224x224 â†’ uniform vision patch count across ranks | CHECK: COLLATOR CHECK line = first batch shape, STEP0 BATCH CHECK = per-rank step0 input")
    return dataset, pure_qwen_collate_fn
