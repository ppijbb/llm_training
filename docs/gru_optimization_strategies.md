# Global GRU μ—°μ‚°λ‰ μµμ ν™” μ „λµ

## ν„μ¬ μƒν™© λ¶„μ„

### ν„μ¬ μ„¤μ •
```json
{
  "hidden_size": 512,
  "num_experts": 128,
  "router_dim": 128
}
```

**GRU μ—°μ‚°λ‰:**
- Input size: `512`
- Hidden size: `128 * 128 = 16,384` β οΈ **λ§¤μ° νΌ!**
- GRU νλΌλ―Έν„° μ: `3 * (512 * 16384 + 16384 * 16384) β‰ 850M` (λ§¤μ° νΌ!)
- μ—°μ‚° λ³µμ΅λ„: `O(batch * seq_len * input_size * hidden_size)`

### λ¬Έμ μ 
- Hidden dimensionμ΄ κ³Όλ„ν•κ² νΌ (16K)
- GRU μ—°μ‚°μ΄ μ „μ²΄ λ¨λΈμ λ³‘λ©
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰ κ³Όλ‹¤

## μµμ ν™” μ „λµ (μ°μ„ μμ„ μ)

### π¥‡ **1. router_dim μ¶•μ† (κ°€μ¥ ν¨κ³Όμ , μ¦‰μ‹ μ μ© κ°€λ¥)**

**ν„μ¬:** `router_dim = 128`
**κ¶μ¥:** `router_dim = 32` λλ” `64`

**ν¨κ³Ό:**
- Hidden size: `128 * 128 = 16,384` β†’ `128 * 32 = 4,096` (75% κ°μ†)
- νλΌλ―Έν„° μ: `850M` β†’ `~200M` (75% κ°μ†)
- μ—°μ‚°λ‰: **4λ°° κ°μ†**

**κµ¬ν„:**
```python
# configμ—μ„ router_dimλ§ λ³€κ²½
"router_dim": 32  # λλ” 64
```

**μ£Όμμ‚¬ν•­:**
- router_dimμ΄ λ„λ¬΄ μ‘μΌλ©΄ routing ν‘ν„λ ¥ μ €ν• κ°€λ¥
- 32-64 λ²”μ„κ°€ μΌλ°μ μΌλ΅ μ¶©λ¶„ν•¨
- μ‹¤ν—μ μΌλ΅ μµμ κ°’ μ°ΎκΈ°

---

### π¥ **2. Low-Rank Factorization (GRU Weight λ¶„ν•΄)**

**μ•„μ΄λ””μ–΄:** GRUμ ν° weight matrixλ¥Ό λ‘ κ°μ μ‘μ€ matrixλ΅ λ¶„ν•΄

```python
# κΈ°μ΅΄: W: [input_size, hidden_size] = [512, 16384]
# λ¶„ν•΄: W = U @ V^T
#       U: [512, rank], V: [16384, rank]
#       rank << min(input_size, hidden_size)

# μ: rank = 256
# νλΌλ―Έν„°: 512*16384 = 8.4M β†’ 512*256 + 16384*256 = 4.3M (50% κ°μ†)
```

**κµ¬ν„ μμ‹:**
```python
class LowRankGRU(nn.Module):
    def __init__(self, input_size, hidden_size, rank=256):
        super().__init__()
        self.rank = rank
        # Input projection: [input_size, rank]
        self.U_ih = nn.Linear(input_size, rank, bias=False)
        self.U_hh = nn.Linear(hidden_size, rank, bias=False)
        # Output projection: [rank, hidden_size]
        self.V_ih = nn.Linear(rank, hidden_size, bias=False)
        self.V_hh = nn.Linear(rank, hidden_size, bias=False)
        
    def forward(self, x, h):
        # x: [batch, seq, input_size]
        # h: [batch, hidden_size]
        
        # Low-rank projection
        x_proj = self.U_ih(x)  # [batch, seq, rank]
        h_proj = self.U_hh(h)   # [batch, rank]
        
        # Expand and compute gates
        x_gate = self.V_ih(x_proj)  # [batch, seq, hidden_size]
        h_gate = self.V_hh(h_proj)  # [batch, hidden_size]
        
        # GRU gates (simplified)
        # ... (μ‹¤μ  GRU λ΅μ§)
```

**ν¨κ³Ό:**
- νλΌλ―Έν„°: 50-75% κ°μ†
- μ—°μ‚°λ‰: 30-50% κ°μ†
- rank μ„ νƒμ΄ μ¤‘μ” (256-512 κ¶μ¥)

---

### π¥‰ **3. Lightweight GRU (Linear + Gating)**

**μ•„μ΄λ””μ–΄:** Full GRU λ€μ‹  λ‹¨μν• Linear + Gating κµ¬μ΅°

```python
class LightweightRouter(nn.Module):
    def __init__(self, input_size, hidden_size, num_experts, router_dim):
        super().__init__()
        # λ‹¨μ Linear projection
        self.proj = nn.Linear(input_size, num_experts * router_dim, bias=False)
        # Optional: Lightweight gating
        self.gate = nn.Linear(input_size, num_experts * router_dim, bias=False)
        
    def forward(self, x, h_prev=None):
        # x: [batch, seq, input_size]
        proj_out = self.proj(x)  # [batch, seq, num_experts * router_dim]
        
        if h_prev is not None:
            # Simple gating with previous hidden state
            gate_signal = torch.sigmoid(self.gate(x))
            # Residual connection with gating
            output = gate_signal * proj_out + (1 - gate_signal) * h_prev
        else:
            output = proj_out
            
        return output, output  # (output, hidden_state)
```

**ν¨κ³Ό:**
- νλΌλ―Έν„°: 80-90% κ°μ†
- μ—°μ‚°λ‰: 70-85% κ°μ†
- λ‹¨μν•μ§€λ§ μ„±λ¥ μ €ν• κ°€λ¥μ„±

---

### 4. **Sparse GRU (Structured Sparsity)**

**μ•„μ΄λ””μ–΄:** GRU weightμ— structured sparsity μ μ©

```python
# Block-sparse λλ” Group-sparse GRU
# μ: 4κ° κ·Έλ£ΉμΌλ΅ λ‚λ„μ–΄ κ° κ·Έλ£Ήλ§ ν™μ„±ν™”
```

**ν¨κ³Ό:**
- νλΌλ―Έν„°: 50-75% κ°μ†
- μ—°μ‚°λ‰: 50-75% κ°μ†
- κµ¬ν„ λ³µμ΅λ„ λ†’μ

---

### 5. **Grouped/Block-wise GRU**

**μ•„μ΄λ””μ–΄:** Hidden stateλ¥Ό μ—¬λ¬ κ·Έλ£ΉμΌλ΅ λ‚λ„μ–΄ κ°κ° μ‘μ€ GRU μ‚¬μ©

```python
class GroupedGRU(nn.Module):
    def __init__(self, input_size, hidden_size, num_groups=4):
        super().__init__()
        self.num_groups = num_groups
        group_size = hidden_size // num_groups
        self.grus = nn.ModuleList([
            nn.GRU(input_size, group_size, batch_first=True)
            for _ in range(num_groups)
        ])
        
    def forward(self, x, h):
        # κ° κ·Έλ£Ήλ³„λ΅ λ…λ¦½μ μΌλ΅ μ²λ¦¬
        outputs = []
        h_outs = []
        for i, gru in enumerate(self.grus):
            h_i = h[:, i*group_size:(i+1)*group_size] if h is not None else None
            out_i, h_i = gru(x, h_i)
            outputs.append(out_i)
            h_outs.append(h_i)
        return torch.cat(outputs, dim=-1), torch.cat(h_outs, dim=0)
```

**ν¨κ³Ό:**
- νλΌλ―Έν„°: 30-50% κ°μ† (κ·Έλ£Ή μμ— λ”°λΌ)
- μ—°μ‚°λ‰: λ³‘λ ¬ν™” κ°€λ¥
- κµ¬ν„ λ³µμ΅λ„ μ¤‘κ°„

---

## μ¦‰μ‹ μ μ© κ°€λ¥ν• μµμ ν™” (μ°μ„ μμ„)

### β… **1λ‹¨κ³„: router_dim μ¶•μ† (μ¦‰μ‹ μ μ©)**

```json
// spectra_small_config.json
{
  "router_dim": 32  // 128 β†’ 32 (75% κ°μ†)
}
```

**μμƒ ν¨κ³Ό:**
- GRU hidden size: 16,384 β†’ 4,096
- νλΌλ―Έν„°: 850M β†’ ~200M
- μ—°μ‚°λ‰: **4λ°° κ°μ†**
- λ©”λ¨λ¦¬: **4λ°° κ°μ†**

**κ²€μ¦ λ°©λ²•:**
- router_dimμ„ 128 β†’ 64 β†’ 32λ΅ μ μ§„μ  μ¶•μ†
- Routing μ„±λ¥ λ¨λ‹ν„°λ§
- μµμ  trade-off μ°ΎκΈ°

---

### β… **2λ‹¨κ³„: Low-Rank Factorization (κµ¬ν„ ν•„μ”)**

ν„μ¬ μ½”λ“μ— Low-Rank GRU κµ¬ν„ μ¶”κ°€

**κµ¬ν„ μ„μΉ:**
- `models/spectra_model.py`μ `SPECTRARouter` ν΄λμ¤
- `self.load_balancer`λ¥Ό Low-Rank GRUλ΅ κµμ²΄

**κ¶μ¥ rank:**
- router_dim=32μΌ λ•: rank=128-256
- router_dim=64μΌ λ•: rank=256-512

---

### β… **3λ‹¨κ³„: Lightweight Router (μ„±λ¥ κ²€μ¦ ν›„)**

Full GRU λ€μ‹  Linear + Gating κµ¬μ΅°λ΅ κµμ²΄

**μ μ© μ΅°κ±΄:**
- router_dim μ¶•μ† + Low-Rankλ΅λ„ λ¶€μ΅±ν•  λ•
- μ„±λ¥ μ €ν•κ°€ ν—μ© κ°€λ¥ν• λ²”μ„μΌ λ•

---

## μ„±λ¥ λΉ„κµ μμƒ

| λ°©λ²• | νλΌλ―Έν„° κ°μ† | μ—°μ‚°λ‰ κ°μ† | κµ¬ν„ λ‚μ΄λ„ | μ„±λ¥ μν–¥ |
|------|-------------|-----------|-----------|----------|
| router_dim μ¶•μ† | 75% | 75% | β­ λ§¤μ° μ‰¬μ›€ | λ‚®μ |
| Low-Rank | 50-75% | 30-50% | β­β­ μ‰¬μ›€ | λ‚®μ-μ¤‘κ°„ |
| Lightweight | 80-90% | 70-85% | β­β­ μ‰¬μ›€ | μ¤‘κ°„-λ†’μ |
| Sparse | 50-75% | 50-75% | β­β­β­ μ–΄λ ¤μ›€ | μ¤‘κ°„ |
| Grouped | 30-50% | 30-50% | β­β­ μ‰¬μ›€ | λ‚®μ |

---

## κ¶μ¥ μ‹¤ν–‰ κ³„ν

### Phase 1: μ¦‰μ‹ μ μ© (μ¤λ)
1. β… `router_dim: 128 β†’ 64` λ³€κ²½
2. β… ν•™μµ μ‹μ‘, μ„±λ¥ λ¨λ‹ν„°λ§
3. β… μ„±λ¥ μ μ§€ ν™•μΈ ν›„ `router_dim: 64 β†’ 32` μ‹λ„

### Phase 2: μ¶”κ°€ μµμ ν™” (1-2μΌ)
1. Low-Rank GRU κµ¬ν„
2. router_dim=32 + Low-Rank μ μ©
3. μ„±λ¥ λΉ„κµ

### Phase 3: κ³ κΈ‰ μµμ ν™” (ν•„μ”μ‹)
1. Lightweight Router κµ¬ν„
2. μ„±λ¥-ν¨μ¨μ„± trade-off μµμ ν™”

---

## κµ¬ν„ μμ‹: Low-Rank GRU

```python
class LowRankGRU(nn.Module):
    """
    Low-rank factorization of GRU for efficient routing.
    W = U @ V^T where U: [input_size, rank], V: [hidden_size, rank]
    """
    def __init__(self, input_size, hidden_size, rank=None, num_layers=1, batch_first=True):
        super().__init__()
        if rank is None:
            rank = min(input_size, hidden_size) // 4  # Default: 1/4 of smaller dimension
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.rank = rank
        self.num_layers = num_layers
        self.batch_first = batch_first
        
        # Low-rank projections for input-to-hidden
        self.U_ih = nn.Linear(input_size, 3 * rank, bias=False)  # 3 gates
        self.V_ih = nn.Linear(rank, hidden_size, bias=False)
        
        # Low-rank projections for hidden-to-hidden
        self.U_hh = nn.Linear(hidden_size, 3 * rank, bias=False)
        self.V_hh = nn.Linear(rank, hidden_size, bias=False)
        
    def forward(self, x, h=None):
        # x: [batch, seq, input_size] if batch_first else [seq, batch, input_size]
        if not self.batch_first:
            x = x.transpose(0, 1)
        
        batch_size, seq_len, _ = x.shape
        
        if h is None:
            h = torch.zeros(self.num_layers, batch_size, self.hidden_size, 
                          device=x.device, dtype=x.dtype)
        
        outputs = []
        for t in range(seq_len):
            x_t = x[:, t, :]  # [batch, input_size]
            h_t = h[-1]  # [batch, hidden_size]
            
            # Low-rank input projection
            x_proj = self.U_ih(x_t)  # [batch, 3*rank]
            x_proj = x_proj.view(batch_size, 3, self.rank)  # [batch, 3, rank]
            x_gates = self.V_ih(x_proj)  # [batch, 3, hidden_size]
            
            # Low-rank hidden projection
            h_proj = self.U_hh(h_t)  # [batch, 3*rank]
            h_proj = h_proj.view(batch_size, 3, self.rank)  # [batch, 3, rank]
            h_gates = self.V_hh(h_proj)  # [batch, 3, hidden_size]
            
            # GRU gates
            r_gate = torch.sigmoid(x_gates[:, 0] + h_gates[:, 0])  # reset
            z_gate = torch.sigmoid(x_gates[:, 1] + h_gates[:, 1])  # update
            n_gate = torch.tanh(x_gates[:, 2] + r_gate * h_gates[:, 2])  # new
            
            # Update hidden state
            h_t = (1 - z_gate) * n_gate + z_gate * h_t
            h[-1] = h_t
            outputs.append(h_t)
        
        output = torch.stack(outputs, dim=1)  # [batch, seq, hidden_size]
        
        if not self.batch_first:
            output = output.transpose(0, 1)
        
        return output, h
```

**μ‚¬μ©λ²•:**
```python
# κΈ°μ΅΄ μ½”λ“ λ€μ²΄
# self.load_balancer = nn.GRU(...)
self.load_balancer = LowRankGRU(
    input_size=self.hidden_size,
    hidden_size=self.num_experts * self.router_dim,
    rank=256,  # λλ” router_dim * 2
    num_layers=1,
    batch_first=True
)
```

---

## κ²°λ΅ 

**κ°€μ¥ ν¨κ³Όμ μ΄κ³  μ¦‰μ‹ μ μ© κ°€λ¥ν• λ°©λ²•:**
1. β… **router_dim μ¶•μ†** (128 β†’ 32): 75% μ—°μ‚°λ‰ κ°μ†, κµ¬ν„ 5λ¶„
2. β… **Low-Rank Factorization**: μ¶”κ°€ 50% κ°μ†, κµ¬ν„ 1-2μ‹κ°„

**λ‘ λ°©λ²•μ„ μ΅°ν•©ν•λ©΄:**
- μ΄ μ—°μ‚°λ‰: **87.5% κ°μ†** (4λ°° Γ— 2λ°° = 8λ°°)
- νλΌλ―Έν„°: **87.5% κ°μ†**
- λ©”λ¨λ¦¬: **87.5% κ°μ†**

μ΄ μ •λ„λ©΄ Global GRUμ μ—°μ‚° λΉ„μ¤‘μ΄ ν¬κ² μ¤„μ–΄λ“¤ κ²ƒμ…λ‹λ‹¤!
