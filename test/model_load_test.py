import sys
import os
import copy
import torch
from tqdm.auto import tqdm
import logging
from transformers import AutoProcessor, AutoModelForImageTextToText
from transformers.generation.configuration_utils import GenerationConfig
from transformers.utils.quantization_config import BitsAndBytesConfig
from transformers import Gemma3ForCausalLM, Gemma3ForConditionalGeneration, Gemma3Config, Gemma3Model
from transformers.utils.import_utils import is_flash_attn_2_available
from transformers.image_utils import load_image

from peft.peft_model import PeftModel
import tensorrt
import random
from torch.nn.attention import SDPBackend, sdpa_kernel

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import (#G2MoEConfig, G2MoEForCausalLM, G2MoETextConfig,
                    G3MoEModel, G3MoETextModel,
                    G3MoEConfig, G3MoEForCausalLM, G3MoEForConditionalGeneration, G3MoETextConfig)
from transformers import AutoConfig, AutoModel, AutoModelForCausalLM
from transformers.modeling_utils import VLMS
AutoConfig.register("g3moe", G3MoEConfig)
AutoConfig.register("g3moe", G3MoEConfig)
AutoConfig.register("g3moe_text", G3MoETextConfig)
AutoModel.register(G3MoEConfig, G3MoEModel)
AutoModel.register(G3MoETextConfig, G3MoETextModel)
AutoModelForCausalLM.register(G3MoETextConfig, G3MoEForCausalLM)
VLMS.append("g3moe")
os.environ["CUDA_VISIBLE_DEVICES"] = "3"
os.environ["TORCH_COMPILE_DISABLE"] = "1"
os.environ["TORCHDYNAMO_DISABLE"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "warning"
torch._dynamo.config.suppress_errors = True
torch._dynamo.config.capture_dynamic_output_shape_ops = False
torch.compiler.disable()

# Additional safety: reset any existing dynamo state
torch._dynamo.reset()
print("version of tensorrt: " ,tensorrt.__version__)

def format_parameters(number):
    if number >= 1_000_000_000:
        return f"{number / 1_000_000_000:.4f} B"
    elif number >= 1_000_000:
        return f"{number / 1_000_000:.4f} M"
    else:
        return str(number)


base_model_name = "Gunulhona/Gemma-3-4B"
model_architecture = G3MoEForConditionalGeneration
base_config = AutoConfig.from_pretrained(base_model_name, trust_remote_code=True)
base_config = base_config.to_dict()
moe_config = {
        "n_shared_experts": 1,
        "n_routed_experts": 8, # 256, 15, 6
        "n_group": 4,
        "topk_group": 8,
        "num_experts_per_tok": 1,
        "first_k_dense_replace": 18,
        "router_aux_loss_coef": 0.001,
        "router_jitter_noise": 0.01,
        "input_jitter_noise": 0.01,
        "model_type": "g3moe_text",
        "no_rope_layer_interval": 0,
        "rope_scaling":{
            "rope_type": "yarn",
            "factor": 8.0
        },
        "use_bfloat16": True
    }
if "text_config" not in base_config:
    base_config['text_config'] = copy.deepcopy(base_config)

base_config['text_config'].update(moe_config)
base_config.update(base_config['text_config'])
model_config = G3MoEConfig(**base_config)
model_config.model_type = "gemma3"
model_config.text_config.model_type = "gemma3_text"
# BitsAndBytesConfig int-4 config
model_config.architectures = [
    "G3MoEForConditionalGeneration", 
    # "G3MoEModel", 
    # "G3MoEForCausalLM"
    ]

def count_active_parameters(model, sample_inputs=None, top_k=None, verbose=True):
    """
    Inference Ïãú Ïã§Ï†ú ÌôúÏÑ±ÌôîÎêòÎäî ÌååÎùºÎØ∏ÌÑ∞ ÏàòÎ•º Í≥ÑÏÇ∞Ìï©ÎãàÎã§.
    
    Ïã§Ï†ú forward passÎ•º Ïã§ÌñâÌïòÏó¨ ÌôúÏÑ±ÌôîÎêú expertÎ•º Ï∂îÏ†ÅÌï©ÎãàÎã§.
    
    Args:
        model: G3MoE Î™®Îç∏
        sample_inputs: Ïã§Ï†ú forward passÎ•º ÏúÑÌïú ÏÉòÌîå ÏûÖÎ†• (dict, optional)
                      Ï†úÍ≥µÎêòÏßÄ ÏïäÏúºÎ©¥ Ïù¥Î°†Ï†Å Í≥ÑÏÇ∞ ÏÇ¨Ïö©
        top_k: ÌôúÏÑ±ÌôîÎêòÎäî expert Ïàò (NoneÏù¥Î©¥ configÏóêÏÑú Í∞ÄÏ†∏Ïò¥)
        verbose: ÏÉÅÏÑ∏ Ï∂úÎ†• Ïó¨Î∂Ä
    
    Returns:
        dict: ÌôúÏÑ±Ìôî ÌååÎùºÎØ∏ÌÑ∞ Ï†ïÎ≥¥
    """
    # ConfigÏóêÏÑú MoE ÏÑ§Ï†ï Í∞ÄÏ†∏Ïò§Í∏∞
    if hasattr(model, 'config'):
        config = model.config
        if hasattr(config, 'text_config'):
            config = config.text_config
    else:
        raise ValueError("Model must have config attribute")
    
    n_routed_experts = getattr(config, 'n_routed_experts', 0)
    n_shared_experts = getattr(config, 'n_shared_experts', 1)
    num_experts_per_tok = top_k if top_k is not None else getattr(config, 'num_experts_per_tok', 2)
    first_k_dense_replace = getattr(config, 'first_k_dense_replace', 0)
    num_hidden_layers = getattr(config, 'num_hidden_layers', 0)
    
    # MoE Î†àÏù¥Ïñ¥ Ïàò Í≥ÑÏÇ∞ (first_k_dense_replace Ïù¥ÌõÑ Î†àÏù¥Ïñ¥Îßå MoE)
    num_moe_layers = max(0, num_hidden_layers - first_k_dense_replace)
    
    # Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞ Ïπ¥Ïö¥Ìä∏
    total_params = sum(p.numel() for p in model.parameters())
    
    # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÌååÎùºÎØ∏ÌÑ∞ Ïπ¥Ïö¥Ìä∏
    embedding_params = 0
    attention_params = 0
    shared_expert_params = 0
    routed_expert_params = 0
    dense_mlp_params = 0  # Dense Î†àÏù¥Ïñ¥Ïùò MLP (MoEÍ∞Ä ÏïÑÎãå Î†àÏù¥Ïñ¥)
    router_params = 0
    global_router_params = 0  # Global router (Í≥µÏú†Îê®, Ìïú Î≤àÎßå Ïπ¥Ïö¥Ìä∏)
    norm_params = 0
    lm_head_params = 0
    vision_params = 0
    other_params = 0
    
    # Global routerÎäî Ìïú Î≤àÎßå Ïπ¥Ïö¥Ìä∏Ìï¥Ïïº Ìï®
    global_router_seen = set()
    
    # Î†àÏù¥Ïñ¥ Ïù∏Îç±Ïä§ Ï∂îÏ∂ú Ìó¨Ìçº Ìï®Ïàò
    def get_layer_idx(name):
        """Î†àÏù¥Ïñ¥ Ïù∏Îç±Ïä§ Ï∂îÏ∂ú (Ïòà: 'model.layers.5.moe' -> 5)"""
        import re
        match = re.search(r'\.layers\.(\d+)\.', name)
        if match:
            return int(match.group(1))
        return -1
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        layer_idx = get_layer_idx(name)
        is_moe_layer = layer_idx >= first_k_dense_replace if layer_idx >= 0 else False
        
        # Vision tower ÌååÎùºÎØ∏ÌÑ∞
        if 'vision_tower' in name or 'vision_model' in name:
            vision_params += param_count
        # Embedding ÌååÎùºÎØ∏ÌÑ∞
        elif 'embed' in name.lower():
            embedding_params += param_count
        # Attention ÌååÎùºÎØ∏ÌÑ∞
        elif 'self_attn' in name or 'attn' in name:
            attention_params += param_count
        # Global Router ÌååÎùºÎØ∏ÌÑ∞ (Í≥µÏú†ÎêòÎØÄÎ°ú Ìïú Î≤àÎßå Ïπ¥Ïö¥Ìä∏)
        elif 'global_router' in name or ('router' in name and 'global' in name and 'language_model' in name):
            # Global routerÎäî Ìïú Î≤àÎßå Ïπ¥Ïö¥Ìä∏
            router_key = 'global_router'
            if router_key not in global_router_seen:
                global_router_params += param_count
                global_router_seen.add(router_key)
        # Shared Expert ÌååÎùºÎØ∏ÌÑ∞ (Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî)
        elif 'shared_experts' in name or 'shared_expert' in name:
            shared_expert_params += param_count
        # Routed Expert ÌååÎùºÎØ∏ÌÑ∞ (MoE Î†àÏù¥Ïñ¥Ïùò expertsÎßå)
        elif 'experts' in name and 'shared' not in name:
            # MoE Î†àÏù¥Ïñ¥Ïùò expertsÎßå Ïπ¥Ïö¥Ìä∏
            if is_moe_layer or 'moe' in name.lower():
                routed_expert_params += param_count
            else:
                # Dense Î†àÏù¥Ïñ¥Ïùò MLPÎäî Îã§Î•∏ Í≥≥ÏóêÏÑú Ï≤òÎ¶¨
                other_params += param_count
        # Dense MLP ÌååÎùºÎØ∏ÌÑ∞ (MoEÍ∞Ä ÏïÑÎãå Î†àÏù¥Ïñ¥Ïùò MLP, Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî)
        elif ('mlp' in name.lower() or 'gate_proj' in name or 'up_proj' in name or 'down_proj' in name) and \
             'moe' not in name.lower() and 'expert' not in name.lower():
            # MoEÍ∞Ä ÏïÑÎãå Î†àÏù¥Ïñ¥Ïùò MLPÎßå Ïπ¥Ïö¥Ìä∏
            if not is_moe_layer:
                dense_mlp_params += param_count
            else:
                other_params += param_count
        # Router ÌååÎùºÎØ∏ÌÑ∞ (Î°úÏª¨ router, Í∞Å Î†àÏù¥Ïñ¥ÎßàÎã§ ÏûàÏßÄÎßå Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî)
        elif 'router' in name or 'gate' in name:
            # Global routerÍ∞Ä ÏïÑÎãå Í≤ΩÏö∞Îßå Ïπ¥Ïö¥Ìä∏
            if 'global' not in name:
                router_params += param_count
        # LayerNorm ÌååÎùºÎØ∏ÌÑ∞
        elif 'norm' in name.lower() or 'layernorm' in name.lower():
            norm_params += param_count
        # LM Head ÌååÎùºÎØ∏ÌÑ∞
        elif 'lm_head' in name or 'score' in name:
            lm_head_params += param_count
        else:
            other_params += param_count
    
    # Forward passÏóêÏÑú Î™®Îì† Î†àÏù¥Ïñ¥Í∞Ä ÌôúÏÑ±ÌôîÎê©ÎãàÎã§!
    # Îã®, MoE Î†àÏù¥Ïñ¥ ÎÇ¥Ïùò Routed ExpertsÎßå sparse activation (ÌÜ†ÌÅ∞Îãπ top_kÍ∞úÎßå)
    
    # Ìï≠ÏÉÅ ÌôúÏÑ±ÌôîÎêòÎäî ÌååÎùºÎØ∏ÌÑ∞ (Î™®Îì† Î†àÏù¥Ïñ¥)
    always_active_params = (
        embedding_params +
        attention_params +  # Î™®Îì† Î†àÏù¥Ïñ¥Ïùò attention
        shared_expert_params +  # Î™®Îì† MoE Î†àÏù¥Ïñ¥Ïùò shared expert
        dense_mlp_params +  # Dense Î†àÏù¥Ïñ¥Ïùò MLP (Î™®Îëê ÌôúÏÑ±Ìôî)
        global_router_params +  # Global router (Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî)
        router_params +  # Î™®Îì† router (Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî)
        norm_params +  # Î™®Îì† LayerNorm
        lm_head_params +
        vision_params +
        other_params
    )
    
    # Routed ExpertsÎßå sparse activation
    # Ïã§Ï†ú forward passÎ•º ÌÜµÌï¥ ÌôúÏÑ±ÌôîÎêú expert Ï∂îÏ†Å
    if sample_inputs is not None and n_routed_experts > 0:
        # Ïã§Ï†ú forward passÎ°ú ÌôúÏÑ±ÌôîÎêú expert Ï∂îÏ†Å
        model.eval()
        activated_experts_per_layer = {}
        
        def hook_fn(module, input, output):
            """Forward hookÏúºÎ°ú Ïã§Ï†ú ÌôúÏÑ±ÌôîÎêú expert Ï∂îÏ†Å"""
            if hasattr(module, 'last_selected_experts'):
                selected_experts = module.last_selected_experts
                if selected_experts is not None:
                    # Ïã§Ï†ú ÌôúÏÑ±ÌôîÎêú expert ID Ï∂îÏ∂ú
                    unique_experts = torch.unique(selected_experts)
                    layer_name = f"layer_{getattr(module, 'iter', 'unknown')}"
                    activated_experts_per_layer[layer_name] = unique_experts.cpu().tolist()
        
        # Hook Îì±Î°ù
        hooks = []
        for name, module in model.named_modules():
            if hasattr(module, 'experts') and hasattr(module, 'num_experts'):
                # MoE Î†àÏù¥Ïñ¥Ïóê hook Îì±Î°ù
                hook = module.register_forward_hook(hook_fn)
                hooks.append(hook)
        
        # Forward pass Ïã§Ìñâ
        with torch.no_grad():
            try:
                _ = model(**sample_inputs)
            except Exception as e:
                if verbose:
                    print(f"Warning: Forward pass failed: {e}. Using theoretical calculation.")
                sample_inputs = None  # Fallback to theoretical
        
        # Hook Ï†úÍ±∞
        for hook in hooks:
            hook.remove()
        
        # Ïã§Ï†ú ÌôúÏÑ±ÌôîÎêú expert Ïàò Í≥ÑÏÇ∞
        if activated_experts_per_layer:
            # Í∞Å Î†àÏù¥Ïñ¥ÏóêÏÑú ÌôúÏÑ±ÌôîÎêú expert ÏàòÏùò ÌèâÍ∑†
            total_activated = sum(len(experts) for experts in activated_experts_per_layer.values())
            avg_activated_per_layer = total_activated / len(activated_experts_per_layer) if activated_experts_per_layer else num_experts_per_tok
            activation_ratio = avg_activated_per_layer / n_routed_experts
            active_routed_expert_params = int(routed_expert_params * activation_ratio)
            actual_measurement = True
        else:
            # Fallback to theoretical
            activation_ratio = num_experts_per_tok / n_routed_experts
            active_routed_expert_params = int(routed_expert_params * activation_ratio)
            actual_measurement = False
    else:
        # Ïù¥Î°†Ï†Å Í≥ÑÏÇ∞ (forward pass ÏóÜÏù¥)
        if n_routed_experts > 0:
            activation_ratio = num_experts_per_tok / n_routed_experts
            active_routed_expert_params = int(routed_expert_params * activation_ratio)
        else:
            activation_ratio = 0.0
            active_routed_expert_params = 0
        actual_measurement = False
    
    # Ï†ÑÏ≤¥ ÌôúÏÑ±Ìôî ÌååÎùºÎØ∏ÌÑ∞ = Ìï≠ÏÉÅ ÌôúÏÑ±Ìôî + Routed Experts Ï§ë ÌôúÏÑ±Ìôî Î∂ÄÎ∂Ñ
    active_params = always_active_params + active_routed_expert_params
    activation_rate = active_params / total_params if total_params > 0 else 0.0
    
    result = {
        'total_params': total_params,
        'active_params': active_params,
        'activation_rate': activation_rate,
        'breakdown': {
            'embedding': embedding_params,
            'attention': attention_params,
            'shared_experts': shared_expert_params,
            'dense_mlp': dense_mlp_params,
            'routed_experts_total': routed_expert_params,
            'routed_experts_active': active_routed_expert_params,
            'global_router': global_router_params,
            'router': router_params,
            'norm': norm_params,
            'lm_head': lm_head_params,
            'vision': vision_params,
            'other': other_params,
        },
        'config': {
            'n_routed_experts': n_routed_experts,
            'n_shared_experts': n_shared_experts,
            'num_experts_per_tok': num_experts_per_tok,
            'num_moe_layers': num_moe_layers,
            'first_k_dense_replace': first_k_dense_replace,
            'num_hidden_layers': num_hidden_layers,
        },
        'actual_measurement': actual_measurement if 'actual_measurement' in locals() else False
    }
    
    if verbose:
        print("\n" + "="*80)
        print("MoE Model Active Parameter Analysis")
        print("="*80)
        print(f"\nüìä Configuration:")
        print(f"  - Total Hidden Layers: {num_hidden_layers}")
        print(f"  - MoE Layers: {num_moe_layers} (starting from layer {first_k_dense_replace})")
        print(f"  - Routed Experts: {n_routed_experts}")
        print(f"  - Shared Experts per Layer: {n_shared_experts}")
        print(f"  - Active Experts per Token (top_k): {num_experts_per_tok}")
        if actual_measurement:
            print(f"  - Expert Activation Ratio: {activation_ratio:.4f} (Ïã§Ï†ú Ï∏°Ï†ï: ÌèâÍ∑† {activation_ratio*n_routed_experts:.1f}Í∞ú expert ÌôúÏÑ±Ìôî)")
        else:
            print(f"  - Expert Activation Ratio: {activation_ratio:.4f} ({num_experts_per_tok}/{n_routed_experts}) [Ïù¥Î°†Ï†Å Í≥ÑÏÇ∞]")
        
        print(f"\nüìà Parameter Breakdown:")
        print(f"  Total Parameters:           {format_parameters(total_params):>15} (100.00%)")
        print(f"  Active Parameters:         {format_parameters(active_params):>15} ({activation_rate*100:.2f}%)")
        print(f"\n  Always Active Components:")
        print(f"    - Embedding:              {format_parameters(embedding_params):>15} ({embedding_params/total_params*100:.2f}%)")
        print(f"    - Attention:              {format_parameters(attention_params):>15} ({attention_params/total_params*100:.2f}%)")
        print(f"    - Dense MLP:              {format_parameters(dense_mlp_params):>15} ({dense_mlp_params/total_params*100:.2f}%)")
        print(f"    - Shared Experts:         {format_parameters(shared_expert_params):>15} ({shared_expert_params/total_params*100:.2f}%)")
        print(f"    - Global Router:          {format_parameters(global_router_params):>15} ({global_router_params/total_params*100:.2f}%)")
        print(f"    - Local Router:           {format_parameters(router_params):>15} ({router_params/total_params*100:.2f}%)")
        print(f"    - LayerNorm:               {format_parameters(norm_params):>15} ({norm_params/total_params*100:.2f}%)")
        print(f"    - LM Head:                 {format_parameters(lm_head_params):>15} ({lm_head_params/total_params*100:.2f}%)")
        print(f"    - Vision Tower:            {format_parameters(vision_params):>15} ({vision_params/total_params*100:.2f}%)")
        print(f"    - Other:                   {format_parameters(other_params):>15} ({other_params/total_params*100:.2f}%)")
        print(f"\n  Routed Experts (Sparse Activation):")
        print(f"    - Total Routed Experts:    {format_parameters(routed_expert_params):>15} ({routed_expert_params/total_params*100:.2f}%)")
        print(f"    - Active Routed Experts:  {format_parameters(active_routed_expert_params):>15} ({active_routed_expert_params/total_params*100:.2f}%)")
        if actual_measurement:
            print(f"      (Ïã§Ï†ú forward passÏóêÏÑú {activation_ratio*100:.2f}% ÌôúÏÑ±ÌôîÎê®)")
        else:
            print(f"      (Ïù¥Î°†Ï†Å: {activation_ratio*100:.2f}% ÌôúÏÑ±Ìôî, Ïã§Ï†ú Ï∏°Ï†ïÌïòÎ†§Î©¥ sample_inputs Ï†úÍ≥µ ÌïÑÏöî)")
        
        print(f"\nüí° Key Insight:")
        print(f"  During inference:")
        print(f"    - All layers are activated (attention, MLP, shared experts, router, etc.)")
        print(f"    - Only Routed Experts use sparse activation: {activation_ratio*100:.2f}% per token")
        print(f"    - Overall active parameters: {activation_rate*100:.2f}% of total")
        print(f"    - Inactive parameters: {format_parameters(total_params - active_params)}")
        print(f"    - Efficiency: {format_parameters(active_params)} active / {format_parameters(total_params)} total")
        print("="*80 + "\n")
    
    return result

def test_train_forward():
    """Tests the model's forward pass in training mode."""
    print("\n--- Starting Training Mode Forward Pass Test ---")

    # 1. Load model for training test
    print("Loading model for training test...")
    train_test_model = model_architecture.from_pretrained(
        pretrained_model_name_or_path=base_model_name,
        config=model_config,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_3",
    ).to("cuda")
    train_test_model.train()
    print("Model loaded and set to training mode.")

    # 2. Create dummy inputs
    print("Creating dummy inputs...")
    tokenizer = AutoProcessor.from_pretrained(base_model_name, use_fast=True)
    with open("/home/conan/workspace/llm_training/sft/config/chat_template.txt", "r") as f:
        tokenizer.chat_template = f.read()
    try:
        image_size = train_test_model.config.vision_config.image_size
    except AttributeError:
        print("Could not find vision_config.image_size, defaulting to a standard size.")
        image_size = 224 # A common default

    test_input = tokenizer.apply_chat_template(
        [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "You are a helpful assistant."}
                ]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Describe this image in Korean."},
                    # {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG"},
                    # {"type": "image", "url": "https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg"},
                    {"type": "image"}
                ]
            }
        ],
        # tokenize=True,
        add_generation_prompt=True,
        # return_tensors="pt",
        # return_dict=True,
    )

    # Load images
    # image = load_image("https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg")
    image = load_image("https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg")
    
    inputs = tokenizer(
        text=test_input.replace("<bos>", "")[:-1],
        images=image,
        return_tensors="pt").to(train_test_model.device)

    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]
        
    # Create labels for loss calculation
    inputs["labels"] = inputs.input_ids.clone()
    print(f"Inputs created with input_ids shape: {inputs.input_ids.shape}")

    # 3. Forward and backward pass
    try:
        print("Performing forward pass...")
        outputs = train_test_model(**inputs)
        loss = outputs.loss
        print(f"Forward pass successful. Loss: {loss.item()}")

        print("Performing backward pass...")
        loss.backward()
        print("Backward pass successful.")

        # 4. Check for gradients
        grad_found = False
        for name, param in train_test_model.named_parameters():
            if param.requires_grad and param.grad is not None:
                print(f"Gradient found for parameter: {name}. Mean grad: {param.grad.abs().mean().item()}")
                grad_found = True
                break
        if not grad_found:
            print("Warning: No gradients were found after backward pass. Check model setup.")

    except Exception as e:
        print(f"An error occurred during forward/backward pass: {e}")
        import traceback
        traceback.print_exc()

    print("--- Training Mode Forward Pass Test Finished ---\n")


def main():
    logging.getLogger("transformers.processing_utils").setLevel(logging.WARN)
    test_model = model_architecture.from_pretrained(
        pretrained_model_name_or_path=base_model_name,
        config=model_config,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_3",
        # trust_remote_code=True,
        # quantization_config=BitsAndBytesConfig(
        #     load_in_4bit=True,
        #     bnb_4bit_use_double_quant=True,
        #     bnb_4bit_quant_type="nf4",
        #     bnb_4bit_compute_dtype=torch.bfloat16,
        #     bnb_4bit_quant_storage=torch.bfloat16)
        ).to("cuda").eval()
    # test_model = PeftModel.from_pretrained(test_model, "/mnt/disks/local-ssd/training_logs/outputs/")
    tokenizer = AutoProcessor.from_pretrained("google/gemma-3-4b-it", use_fast=True)
    with open("/home/conan/workspace/llm_training/sft/config/chat_template.txt", "r") as f:
        tokenizer.chat_template = f.read()
    # logging.set_verbosity_warning()
    test_text = f"""
ÏïàÎÖïÌïòÏÑ∏Ïöî.<end_of_turn>
<start_of_turn>system
You are a helpful assistant named Sparkle.
Always answer in shortest possible sentence.
But you should remember... Try to answer with Korean.üòâ<end_of_turn>
<start_of_turn>user
this is the test text message. now you must instruct the model to generate a response to this message.<end_of_turn>
""" * 1

    test_input = tokenizer.apply_chat_template(
        [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": test_text.strip()}
                ]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Describe this image in Korean."},
                    # {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG"},
                    # {"type": "image", "url": "https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg"},
                    {"type": "image"}
                ]
            }
        ],
        # tokenize=True,
        add_generation_prompt=True,
        # return_tensors="pt",
        # return_dict=True,
    )
    sample_image_urls = [
        "https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg",
        "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg",
        "https://ocr.space/Content/Images/table-ocr-original.webp",
    ]
    ran_image = random.choice(sample_image_urls)
    image = load_image(ran_image)
    
    inputs = tokenizer(
        text=test_input.replace("<bos>", "")[:-1],
        images=image,
        return_tensors="pt").to(test_model.device)
    
    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]
    logging.getLogger("transformers.processing_utils").setLevel(logging.INFO)
    print(test_model)
    # print(test_model.config)
    print(format_parameters(test_model.num_parameters()))
    print("Test Sequence Length:", inputs.input_ids.shape[1])
    
    # ÌôúÏÑ±Ìôî ÌååÎùºÎØ∏ÌÑ∞ Ï∏°Ï†ï (Ïã§Ï†ú forward passÎ°ú Ï∏°Ï†ï)
    print("\n" + "="*80)
    print("Measuring Active Parameters During Inference")
    print("="*80)
    # Ïã§Ï†ú forward passÎ•º ÏúÑÌï¥ sample inputs ÏÇ¨Ïö©
    active_param_info = count_active_parameters(test_model, sample_inputs=inputs, verbose=True)

    with torch.inference_mode():
        # torch._dynamo.config.capture_dynamic_output_shape_ops = True
        fast_inputs =tokenizer(text="What's poppin?", return_tensors="pt")
        del fast_inputs["token_type_ids"]
        response =tokenizer.batch_decode(
            test_model.generate(
                **fast_inputs.to(test_model.device),
                generation_config=GenerationConfig(
                    device=test_model.device,
                ),
                tokenizer=tokenizer
            )
        )[0]
        print(response)
        response = tokenizer.batch_decode(
            test_model.generate(
                **inputs,
                generation_config=GenerationConfig(
                    device=test_model.device,
                    # max_new_tokens=10,
                    # do_sample=True,
                    # top_p=0.9,
                    # top_k=1,
                    # temperature=0.7,
                    # repetition_penalty=1.2,
                    # length_penalty=1.0,
                    # num_beams=1,
                    # num_beam_groups=1,
                    # num_beam_hyps=1
                    ),
                tokenizer=tokenizer
                )
            )[0]
        print(test_text)
        print("--- Model Response ---")
        print(response[len(test_text):].split("<start_of_turn>model\n")[-1])


def check_params_diff():
    
    model_1 = G3MoEForCausalLM.from_pretrained(base_model_name, config=model_config, dtype=torch.bfloat16) # G3MoEForCausalLM.from_pretrained(base_model_name, config=model_config, torch_dtype=torch.bfloat16)
    model_2 = G3MoEForConditionalGeneration.from_pretrained(base_model_name, config=model_config, dtype=torch.bfloat16)
    vision_model = Gemma3Model.from_pretrained(base_model_name, dtype=torch.bfloat16)

    def architecture_diff_check(item):
        return "moe" not in item and "mlp" not in item and "router" not in item

    def compare_model_weights(model_a, model_b, prefix_a="model_1", prefix_b="model_2"):
        """
        Compare the weights of two torch.nn.Module models and print the names of layers with different weights.
        """
        print("Start comparing weights of the two models...")
        state_dict_a = model_a.state_dict()
        state_dict_b = model_b.state_dict()

        # Find common keys
        common_keys = set(state_dict_a.keys()) & set(state_dict_b.keys())
        different_layers = []
        progress_bar =tqdm(sorted(common_keys), total=len(common_keys), desc="Comparing weights")
        for key in progress_bar:
            progress_bar.set_description(f"Comparing weights: {key}")
            tensor_a = state_dict_a[key]
            tensor_b = state_dict_b[key]
            if not torch.allclose(tensor_a, tensor_b, atol=1e-6, rtol=1e-5):

                different_layers.append(key)

        if different_layers:
            print("Layers with different weights:")
            for layer in different_layers:
                print(f" - {layer}")
        else:
            print("All weights match between the two models.")

        # Optionally, print keys only in one model
        only_in_a = set(item for item in state_dict_a.keys() - state_dict_b.keys() if architecture_diff_check(item))
        only_in_b = set(item for item in state_dict_b.keys() - state_dict_a.keys() if architecture_diff_check(item))
        if only_in_a:
            print(f"\nKeys only in {prefix_a}({len(only_in_a)}):")
            # for k in sorted(only_in_a):
            #     print(f" - {k}")
        if only_in_b:
            print(f"\nKeys only in {prefix_b}({len(only_in_b)}):")
            # for k in sorted(only_in_b):
            #     print(f" - {k}")

    # Compare the weights of the two models
    compare_model_weights(
        model_a=vision_model, 
        model_b=model_2.model, 
        prefix_a="BaseModel", 
        prefix_b="ConditionalGeneration")
    compare_model_weights(
        model_a=vision_model.vision_tower, 
        model_b=model_2.vision_tower, 
        prefix_a="VisionModel", 
        prefix_b="VisionModel_loaded")
    compare_model_weights(
        model_a=vision_model.language_model, 
        model_b=model_1.model, 
        prefix_a="VisionModel", 
        prefix_b="VisionModel_loaded")


if __name__ == "__main__":
    # check_params_diff()
    test_train_forward()
    main()
    # model_1 = G3MoEForConditionalGeneration.from_pretrained(base_model_name, config=model_config, dtype=torch.bfloat16)
    # model_2 = Gemma3ForConditionalGeneration.from_pretrained(base_model_name, dtype=torch.bfloat16)
