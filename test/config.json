{
    "boi_token_index": 255999,
    "eoi_token_index": 256000,
    "image_token_index": 262144,
    "initializer_range": 0.02,
    "mm_tokens_per_image": 256,
    "model_type": "g3moe_text",
    "text_config": {
      "architectures": [
        "G3MoEForCausalLM"
      ],
      "attention_bias": false,
      "attention_dropout": 0.0,
      "attn_logit_softcapping": null,
      "cache_implementation": "hybrid",
      "final_logit_softcapping": null,
      "first_k_dense_replace": 18,
      "freeze_shared_experts": true,
      "head_dim": 256,
      "hidden_activation": "gelu_pytorch_tanh",
      "hidden_size": 2304,
      "initializer_range": 0.02,
      "input_jitter_noise": 0.01,
      "intermediate_size": 9216,
      "kv_lora_rank": 512,
      "max_position_embeddings": 131072,
      "model_type": "g3moe_text",
      "n_group": 4,
      "n_routed_experts": 5,
      "n_shared_experts": 1,
      "norm_topk_prob": true,
      "num_attention_heads": 8,
      "num_experts_per_tok": 2,
      "num_hidden_layers": 26,
      "num_key_value_heads": 4,
      "q_lora_rank": 1536,
      "qk_rope_head_dim": 64,
      "query_pre_attn_scalar": 256,
      "rms_norm_eps": 1e-06,
      "rope_local_base_freq": 10000.0,
      "rope_scaling": {
        "factor": 8.0,
        "rope_type": "yarn"
      },
      "ema_alpha": 0.99,
      "balancing_strength": 0.01,
      "no_rope_layers_interval": 4,
      "use_sliding_window": true,
      "rope_theta": 1000000.0,
      "routed_scaling_factor": 2.5,
      "router_aux_loss_coef": 0.001,
      "router_jitter_noise": 0.01,
      "sliding_window": 4096,
      "sliding_window_pattern": 6,
      "topk_group": 8,
      "torch_dtype": "float32",
      "use_bfloat16": true,
      "use_cache": true,
      "vocab_size": 262208
    },
    "transformers_version": "4.52.4",
    "vision_config": {
      "attention_dropout": 0.0,
      "hidden_act": "gelu_pytorch_tanh",
      "hidden_size": 768,
      "image_size": 224,
      "intermediate_size": 3072,
      "layer_norm_eps": 1e-06,
      "model_type": "siglip_vision_model",
      "num_attention_heads": 12,
      "num_channels": 3,
      "num_hidden_layers": 12,
      "patch_size": 16
    },
    "architectures": [
      "G3MoEForCausalLM"
    ],
    "auto_map": {
      "AutoConfig": "g3moe_config.G3MoETextConfig",
      "AutoModelForCausalLM": "g3moe_model.G3MoEForCausalLM"
    }
  }