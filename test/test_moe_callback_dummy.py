#!/usr/bin/env python3
"""
ë”ë¯¸ MoE ëª¨ë¸ë¡œ callback í…ŒìŠ¤íŠ¸
- íŒŒë¼ë¯¸í„° ìˆ˜: 0.1M ë¯¸ë§Œ
- DeepSpeed/Accelerate í˜¸í™˜
- Accumulated gradientì—ì„œë„ callback ì‘ë™ í™•ì¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple
import os
import sys
import time

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from eval.moe_monitoring_callback import create_moe_callback_for_transformers
from transformers import Trainer, TrainingArguments, TrainerCallback
from transformers import AutoTokenizer
from datasets import Dataset
import wandb


class DummyMoERouter(nn.Module):
    """ê°„ë‹¨í•œ MoE Router - routing ì •ë³´ ì €ì¥"""
    def __init__(self, hidden_size=64, num_experts=4, router_dim=16):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.router_dim = router_dim
        
        # ê°„ë‹¨í•œ router
        self.gate = nn.Linear(hidden_size, num_experts, bias=False)
        
    def forward(self, hidden_states):
        # router_logits: [batch*seq, num_experts]
        router_logits = self.gate(hidden_states)
        routing_probs = F.softmax(router_logits, dim=-1)
        selected_experts = routing_probs.argmax(dim=-1)  # [batch*seq]
        
        # Callbackì„ ìœ„í•´ ì •ë³´ ì €ì¥
        self.last_selected_experts = selected_experts.detach()
        self.last_routing_weights = routing_probs.detach()
        self.last_num_experts = self.num_experts
        
        return routing_probs, selected_experts


class DummyExpert(nn.Module):
    """ê°„ë‹¨í•œ Expert"""
    def __init__(self, hidden_size=64, intermediate_size=128):
        super().__init__()
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        
    def forward(self, x):
        gate = F.silu(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up)


class DummyMoELayer(nn.Module):
    """ê°„ë‹¨í•œ MoE Layer - G3MoEì™€ ìœ ì‚¬í•œ êµ¬ì¡° - ëª¨ë“  callback ë©”íŠ¸ë¦­ ìƒì„±"""
    def __init__(self, hidden_size=64, num_experts=4, router_dim=16):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        
        self.router = DummyMoERouter(hidden_size, num_experts, router_dim)
        self.experts = nn.ModuleList([
            DummyExpert(hidden_size, intermediate_size=128) 
            for _ in range(num_experts)
        ])
        
        # Callback ì¸ì‹ì„ ìœ„í•œ ì†ì„±
        # self.gate = self.router.gate  # ë©”ëª¨ë¦¬ ê³µìœ ë¡œ ì¸í•œ ì €ì¥ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
        
        # ëœë¤ ì‹œë“œ (ì¼ê´€ì„± ìˆëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)
        self._step_count = 0
        
    def forward(self, hidden_states, global_routing_hn=None):
        """
        G3MoEì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤: (hidden_states, (routing_probs_full, hn, speciality_loss, cosine_similarities, expression_loss))
        """
        batch_size, seq_len, hidden_dim = hidden_states.shape
        hidden_states_flat = hidden_states.view(-1, hidden_dim)
        
        # Routing
        routing_probs, selected_experts = self.router(hidden_states_flat)
        
        # Expert ì‹¤í–‰
        final_hidden = torch.zeros_like(hidden_states_flat)
        
        # top_k = 1ë¡œ ê°€ì • (DummyMoERouterëŠ” argmaxë§Œ ì‚¬ìš©)
        top_k = 1
        selected_experts_2d = selected_experts.unsqueeze(-1) if selected_experts.dim() == 1 else selected_experts  # [batch*seq, top_k]
        
        for expert_idx in range(self.num_experts):
            # top_kë¥¼ ê³ ë ¤í•œ ë§ˆìŠ¤í¬ ìƒì„±
            mask = (selected_experts_2d == expert_idx).any(dim=-1) if selected_experts_2d.dim() == 2 else (selected_experts == expert_idx)
            if mask.any():
                expert_input = hidden_states_flat[mask]
                expert_output = self.experts[expert_idx](expert_input)  # [num_tokens, hidden_dim]
                
                # routing_probsì—ì„œ í•´ë‹¹ expertì˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
                # routing_probs: [batch*seq, num_experts]
                weights = routing_probs[mask, expert_idx]  # [num_tokens]
                
                # weightsë¥¼ [num_tokens, 1]ë¡œ ë³€í™˜í•˜ì—¬ broadcasting
                weights = weights.unsqueeze(-1)  # [num_tokens, 1]
                
                # expert_output: [num_tokens, hidden_dim], weights: [num_tokens, 1]
                # broadcastingìœ¼ë¡œ [num_tokens, hidden_dim] ê²°ê³¼
                final_hidden[mask] = expert_output * weights
        
        # ===== Callbackì„ ìœ„í•œ ëª¨ë“  ë©”íŠ¸ë¦­ ìƒì„± =====
        # Routing entropy ê³„ì‚° (callbackì´ ì‚¬ìš©)
        safe_probs = routing_probs.clamp_min(1e-12)
        token_entropy = -torch.sum(safe_probs * torch.log(safe_probs), dim=-1)
        avg_routing_entropy = token_entropy.mean()
        
        # Aux loss (ëœë¤ ìƒì„± - callbackì´ ë¡œê¹…)
        # stepì— ë”°ë¼ ì•½ê°„ ë³€ë™í•˜ë„ë¡
        self._step_count += 1
        torch.manual_seed(self._step_count % 1000)
        aux_loss = torch.tensor(0.01 + 0.005 * torch.rand(1).item(), device=hidden_states.device)
        
        # Ortho loss (ëœë¤ ìƒì„± - callbackì´ ë¡œê¹…)
        ortho_loss = torch.tensor(0.02 + 0.01 * torch.rand(1).item(), device=hidden_states.device)
        
        # Expression loss (ëœë¤ ìƒì„± - callbackì´ ë¡œê¹…)
        expression_loss = torch.tensor(0.015 + 0.008 * torch.rand(1).item(), device=hidden_states.device)
        
        # Speciality loss (ëœë¤ ìƒì„± - callbackì´ ë¡œê¹…)
        speciality_loss = torch.tensor(0.03 + 0.01 * torch.rand(1).item(), device=hidden_states.device)
        
        # Cosine similarities (ëœë¤ ìƒì„± - callbackì´ ë¡œê¹…)
        # [num_experts] í˜•íƒœ
        cosine_similarities = torch.rand(self.num_experts, device=hidden_states.device) * 0.5 - 0.25  # -0.25 ~ 0.25
        
        # Router logits (callbackì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
        router_logits = self.router.gate(hidden_states_flat)
        
        # routing_probs_full ìƒì„± (G3MoEì™€ ë™ì¼í•œ í˜•íƒœ: [batch*seq, num_experts])
        routing_probs_full = routing_probs.detach()
        
        # hn ìƒì„± (dummy: None ë˜ëŠ” ë”ë¯¸ í…ì„œ)
        hn = global_routing_hn
        if hn is None:
            # Dummy hn: [num_layers=1, batch_size, hidden_dim]
            hn = torch.zeros(1, batch_size, self.num_experts * 4, device=hidden_states.device, dtype=hidden_states.dtype)
        
        # ===== Callbackì´ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì €ì¥ =====
        # G3MoEì™€ ë™ì¼: last_selected_expertsëŠ” [batch*seq, top_k] í˜•íƒœ
        selected_experts_for_callback = selected_experts_2d.detach()
        if selected_experts_for_callback.dim() == 1:
            selected_experts_for_callback = selected_experts_for_callback.unsqueeze(-1)
        # ìŒìˆ˜ ì œê±° ë° long íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        selected_experts_for_callback = selected_experts_for_callback.clamp(min=0).long()
        
        # ê¸°ë³¸ routing ì •ë³´ ì €ì¥ (G3MoEì™€ ë™ì¼)
        self.last_selected_experts = selected_experts_for_callback  # [batch*seq, top_k]
        self.last_routing_weights = routing_probs.detach()  # routing_probsë¡œë„ ì‚¬ìš©ë¨
        self.last_routing_probs = routing_probs.detach()  # alias
        self.last_num_experts = self.num_experts
        
        # Router logits
        self.last_router_logits = router_logits.detach()
        self.last_gate_logits = router_logits.detach()  # alias
        
        # Entropy (callbackì´ avg_routing_entropyë¡œ ì°¾ìŒ)
        self.last_avg_routing_entropy = avg_routing_entropy.detach()
        
        # Losses (callbackì´ ì°¾ìŒ)
        self.last_aux_loss = aux_loss.detach()
        self.last_ortho_loss = ortho_loss.detach()
        self.last_expression_loss = expression_loss.detach()
        self.last_speciality_loss = speciality_loss.detach()
        self.last_cosine_similarities = cosine_similarities.detach()
        
        # G3MoEì™€ ë™ì¼í•œ output êµ¬ì¡° ë°˜í™˜
        # (hidden_states, (routing_probs_full, hn, speciality_loss, cosine_similarities, expression_loss))
        final_hidden_reshaped = final_hidden.view(batch_size, seq_len, hidden_dim)
        routing_info = (routing_probs_full, hn, speciality_loss, cosine_similarities, expression_loss)
        return final_hidden_reshaped, routing_info


class DummyMoEModel(nn.Module):
    """ë”ë¯¸ MoE ëª¨ë¸ - íŒŒë¼ë¯¸í„° ìˆ˜ 0.1M ë¯¸ë§Œ"""
    def __init__(self, vocab_size=1000, hidden_size=64, num_layers=2, num_experts=4):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Embedding
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        
        # MoE Layers
        self.moe_layers = nn.ModuleList([
            DummyMoELayer(hidden_size, num_experts, router_dim=4)
            for _ in range(num_layers)
        ])
        
        # Output
        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)
        
    def forward(self, input_ids, labels=None, **kwargs):
        # Embedding
        hidden_states = self.embedding(input_ids)
        
        # MoE Layers (G3MoEì™€ ë™ì¼í•œ êµ¬ì¡°: íŠœí”Œ ë°˜í™˜)
        global_routing_hn = None
        for moe_layer in self.moe_layers:
            moe_output = moe_layer(hidden_states, global_routing_hn)
            if isinstance(moe_output, tuple) and len(moe_output) == 2:
                hidden_states_new, routing_info = moe_output
                # routing_info: (routing_probs_full, hn, speciality_loss, cosine_similarities, expression_loss)
                if len(routing_info) >= 2:
                    global_routing_hn = routing_info[1]  # hn ì—…ë°ì´íŠ¸
                hidden_states = hidden_states + hidden_states_new
            else:
                # Fallback: íŠœí”Œì´ ì•„ë‹Œ ê²½ìš°
                hidden_states = hidden_states + moe_output
        
        # LM Head
        logits = self.lm_head(hidden_states)
        
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
        
        return {
            'loss': loss,
            'logits': logits
        }
    
    def num_parameters(self):
        return sum(p.numel() for p in self.parameters())


def create_dummy_dataset(num_samples=100, seq_length=32):
    """ë”ë¯¸ ë°ì´í„°ì…‹ ìƒì„±"""
    data = {
        'input_ids': [
            torch.randint(0, 1000, (seq_length,)).tolist() 
            for _ in range(num_samples)
        ],
        'labels': [
            torch.randint(0, 1000, (seq_length,)).tolist() 
            for _ in range(num_samples)
        ]
    }
    return Dataset.from_dict(data)


class TestCallback(TrainerCallback):
    """í…ŒìŠ¤íŠ¸ìš© callback - stepë§ˆë‹¤ í˜¸ì¶œë˜ëŠ”ì§€ í™•ì¸"""
    def __init__(self):
        self.step_count = 0
        self.gradient_accumulation_steps = None
        
    def on_step_begin(self, args, state, control, **kwargs):
        self.step_count += 1
        print(f"[TestCallback] Step {state.global_step} BEGIN (accumulation step: {state.global_step % args.gradient_accumulation_steps})")
        
    def on_step_end(self, args, state, control, **kwargs):
        print(f"[TestCallback] Step {state.global_step} END")
        if self.gradient_accumulation_steps is None:
            self.gradient_accumulation_steps = args.gradient_accumulation_steps


def test_callback_with_dummy_model(use_deepspeed=True, use_accelerate=True):
    """ë”ë¯¸ ëª¨ë¸ë¡œ callback í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("ë”ë¯¸ MoE ëª¨ë¸ë¡œ Callback í…ŒìŠ¤íŠ¸")
    if use_deepspeed:
        print("DeepSpeed ëª¨ë“œ")
    elif use_accelerate:
        print("Accelerate ëª¨ë“œ")
    else:
        print("ê¸°ë³¸ ëª¨ë“œ")
    print("="*60)
    
    # ëª¨ë¸ ìƒì„±
    model = DummyMoEModel(vocab_size=1000, hidden_size=64, num_layers=2, num_experts=4)
    param_count = model.num_parameters()
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {param_count:,} ({param_count/1e6:.3f}M)")
    assert param_count < 1000000, f"íŒŒë¼ë¯¸í„° ìˆ˜ê°€ 1M ì´ìƒì…ë‹ˆë‹¤: {param_count}"
    
    # Tokenizer (ë”ë¯¸)
    try:
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
    except:
        # gpt2ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ tokenizer ìƒì„±
        from transformers import PreTrainedTokenizer
        class DummyTokenizer:
            def __init__(self):
                self.pad_token = "<pad>"
                self.eos_token = "<eos>"
                self.vocab_size = 1000
        tokenizer = DummyTokenizer()
    tokenizer.pad_token = tokenizer.eos_token if hasattr(tokenizer, 'eos_token') else "<pad>"
    
    # ë°ì´í„°ì…‹ (t-SNE í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¶©ë¶„í•œ stepì´ ìƒì„±ë˜ë„ë¡ ì¡°ì •)
    train_dataset = create_dummy_dataset(num_samples=500, seq_length=32)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="./test_output",
        num_train_epochs=2,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,  # t-SNE í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë” ì‘ì€ ê°’ìœ¼ë¡œ ì¡°ì • (ë” ë§ì€ step ìƒì„±)
        learning_rate=1e-4,
        logging_steps=1,
        save_steps=999999999,  # ëª¨ë¸ ì €ì¥ ë¹„í™œì„±í™” (ë§¤ìš° í° ê°’)
        save_total_limit=0,  # ì €ì¥ ì•ˆ í•¨
        save_strategy="no",  # ì €ì¥ ì „ëµ ë¹„í™œì„±í™”
        bf16=False,
        fp16=False,
        dataloader_num_workers=0,
        remove_unused_columns=False,
        report_to=["wandb"],  # wandb í™œì„±í™”
    )
    
    deepspeed_config = {
        "train_batch_size": "auto",
        "train_micro_batch_size_per_gpu": "auto",
        "gradient_accumulation_steps": "auto",
        "zero_optimization": {
            "stage": 3,
        },
        "fp16": {
            "enabled": False
        },
        "bf16": {
            "enabled": False
        }
    }
    import json
    os.makedirs("./test_output", exist_ok=True)
    with open("./test_output/deepspeed_config.json", "w") as f:
        json.dump(deepspeed_config, f)
    training_args.deepspeed = "./test_output/deepspeed_config.json"
    
    # Wandb ì´ˆê¸°í™” - rank 0ì—ì„œë§Œ ì‹¤í–‰, ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    rank = int(os.getenv("RANK", "0"))
    if rank == 0 and (wandb.run is None or not wandb.run):
        wandb.init(
            project="moe-callback-test",
            name=f"dummy-test-{int(time.time())}",
            config={
                "model": "dummy_moe",
                "num_experts": 4,
                "hidden_size": 16,
                "num_layers": 3,
                "use_deepspeed": use_deepspeed,
                "use_accelerate": use_accelerate,
            },
            mode="online"  # í•­ìƒ onlineìœ¼ë¡œ wandbì— ê¸°ë¡
        )
    
    # MoE Callback ìƒì„± - wandb ë¡œê±° ì‚¬ìš©
    # t-SNE ì‹œê°í™” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ log_tsne_everyë¥¼ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •
    moe_callback = create_moe_callback_for_transformers(
        num_experts=4,
        log_every_n_steps=1,
        logger=wandb,  # wandb í™œì„±í™”
        log_to_console=False,
        debug_logging=True,
        enable_generation_logging=False,  # ìƒì„± ë¡œê¹… ë¹„í™œì„±í™”
        log_heatmap_every=10,  # Heatmap í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì‘ì€ ê°’ ì„¤ì •
        log_tsne_every=20,  # t-SNE ì‹œê°í™” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì‘ì€ ê°’ ì„¤ì •
        tsne_sample_size=500,  # í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ìƒ˜í”Œ í¬ê¸°
    )
    
    # í…ŒìŠ¤íŠ¸ callback
    test_callback = TestCallback()
    
    # Trainer
    class DummyTrainer(Trainer):
        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            outputs = model(**inputs)
            loss = outputs['loss']
            return (loss, outputs) if return_outputs else loss
    
    trainer = DummyTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        callbacks=[test_callback, moe_callback],
    )
    
    print("\n" + "="*60)
    print("í•™ìŠµ ì‹œì‘ - Stepë§ˆë‹¤ callback í˜¸ì¶œ í™•ì¸")
    print("="*60)
    print(f"Gradient accumulation steps: {training_args.gradient_accumulation_steps}")
    print(f"Batch size: {training_args.per_device_train_batch_size}")
    print(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"\nğŸ“Š ì‹œê°í™” ì„¤ì •:")
    print(f"  - Heatmap ìƒì„± ì£¼ê¸°: {moe_callback.torch_callback.log_heatmap_every} steps")
    print(f"  - t-SNE ì‹œê°í™” ì£¼ê¸°: {moe_callback.torch_callback.log_tsne_every} steps")
    print(f"  - t-SNE ìƒ˜í”Œ í¬ê¸°: {moe_callback.torch_callback.tsne_sample_size}")
    print()
    
    # í•™ìŠµ ì‹¤í–‰
    try:
        trainer.train()
        print("\n" + "="*60)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print("="*60)
        print(f"TestCallbackì´ í˜¸ì¶œëœ step ìˆ˜: {test_callback.step_count}")
        print(f"Global step: {trainer.state.global_step}")
        
        # Callbackì´ ëª¨ë“  stepì—ì„œ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        expected_steps = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)
        if len(train_dataset) % (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) != 0:
            expected_steps += 1
        
        print(f"ì˜ˆìƒ step ìˆ˜: {expected_steps}")
        print(f"ì‹¤ì œ global step: {trainer.state.global_step}")
        
        # MoE callbackì´ routing ì •ë³´ë¥¼ ì œëŒ€ë¡œ ìº¡ì²˜í–ˆëŠ”ì§€ í™•ì¸
        if hasattr(moe_callback.torch_callback, 'layer_outputs'):
            layer_outputs_count = len(moe_callback.torch_callback.layer_outputs)
            print(f"MoE callbackì´ ìº¡ì²˜í•œ layer ìˆ˜: {layer_outputs_count}")
            if layer_outputs_count > 0:
                print("âœ… MoE callbackì´ routing ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìº¡ì²˜í–ˆìŠµë‹ˆë‹¤!")
            else:
                print("âš ï¸ Warning: MoE callbackì´ routing ì •ë³´ë¥¼ ìº¡ì²˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # t-SNE ë°ì´í„° ë²„í¼ í™•ì¸
        if hasattr(moe_callback.torch_callback, 'tsne_data_buffer'):
            tsne_buffer = moe_callback.torch_callback.tsne_data_buffer
            print(f"\nğŸ“Š t-SNE ë°ì´í„° ë²„í¼ ìƒíƒœ:")
            for layer_name, buffer in tsne_buffer.items():
                hidden_states_count = len(buffer.get('hidden_states', []))
                expert_assignments_count = len(buffer.get('expert_assignments', []))
                print(f"  - {layer_name}: hidden_states={hidden_states_count}, expert_assignments={expert_assignments_count}")
                if hidden_states_count > 0 and expert_assignments_count > 0:
                    print(f"    âœ… t-SNE ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
                else:
                    print(f"    âš ï¸ t-SNE ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        # Heatmap ë° t-SNE ì‹œê°í™” ìƒì„± í™•ì¸
        if hasattr(moe_callback.torch_callback, 'pending_heatmaps'):
            pending_heatmaps = moe_callback.torch_callback.pending_heatmaps
            print(f"\nğŸ“ˆ ìƒì„±ëœ ì‹œê°í™”:")
            print(f"  - Pending heatmaps/t-SNE: {len(pending_heatmaps)} step(s)")
            for step, visualizations in pending_heatmaps.items():
                print(f"    Step {step}: {len(visualizations)} visualization(s)")
                for viz_name in visualizations.keys():
                    if '_tsne' in viz_name:
                        print(f"      âœ… t-SNE: {viz_name}")
                    else:
                        print(f"      âœ… Heatmap: {viz_name}")
        
        if test_callback.step_count >= trainer.state.global_step:
            print("âœ… ëª¨ë“  stepì—ì„œ callbackì´ í˜¸ì¶œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print(f"âš ï¸ Warning: ì¼ë¶€ stepì—ì„œ callbackì´ í˜¸ì¶œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # Wandbì— ìµœì¢… ìš”ì•½ ë¡œê¹…
        layer_outputs_count = len(moe_callback.torch_callback.layer_outputs) if hasattr(moe_callback.torch_callback, 'layer_outputs') else 0
        tsne_layers_count = len(moe_callback.torch_callback.tsne_data_buffer) if hasattr(moe_callback.torch_callback, 'tsne_data_buffer') else 0
        heatmap_count = sum(len(v) for v in moe_callback.torch_callback.pending_heatmaps.values()) if hasattr(moe_callback.torch_callback, 'pending_heatmaps') else 0
        
        if os.environ.get("RANK") == "0":
            wandb.log({
                "test/summary/total_steps": trainer.state.global_step,
                "test/summary/callback_calls": test_callback.step_count,
                "test/summary/layers_captured": layer_outputs_count,
                "test/summary/tsne_layers": tsne_layers_count,
                "test/summary/visualizations_created": heatmap_count,
            })
            print("\nâœ… Wandbì— ëª¨ë“  ë©”íŠ¸ë¦­ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
        if wandb.run:
            print(f"   Wandb URL: {wandb.run.url}")
            
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # Wandb ì¢…ë£Œ
        if wandb.run:
            wandb.finish()


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--deepspeed", action="store_true", help="DeepSpeed ì‚¬ìš©")
    parser.add_argument("--accelerate", action="store_true", help="Accelerate ì‚¬ìš©")
    args = parser.parse_args()
    
    test_callback_with_dummy_model(
        use_deepspeed=args.deepspeed,
        use_accelerate=args.accelerate
    )

