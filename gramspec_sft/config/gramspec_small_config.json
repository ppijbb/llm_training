{
  "model_config": {
    "model_name_or_path": null,
    "tokenizer_name_or_path": "HuggingFaceTB/SmolVLM-256M-Instruct",
    "trust_remote_code": true,
    "use_lora": false,
    "lora_r": 32,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "deepspeed_config": "/home/conan/workspace/llm_training/sft/config/deepspeed_muon_optimizer.json",
    "initialize_from_scratch": true,
    "gramspec_moe_params": {
      "hidden_size": 512,
      "num_hidden_layers": 4,
      "num_attention_heads": 4,
      "num_key_value_heads": 2,
      "intermediate_size": 2048,
      "vocab_size": 256000,
      "max_position_embeddings": 2048,
      "n_shared_experts": 1,
      "n_routed_experts": 32,
      "n_group": 2,
      "topk_group": 2,
      "num_experts_per_tok": 2,
      "first_k_dense_replace": 0,
      "router_aux_loss_coef": 1e-3,
      "router_entropy_coef": 3e-3,
      "usage_uniformity_coef": 2e-3,
      "router_jitter_noise": 1e-05,
      "input_jitter_noise": 1e-05,
      "router_z_loss_coef": 1e-4,
      "router_dim": 128,
      "ema_alpha": 0.95,
      "balancing_strength": 1e-2,
      "neftune_noise_alpha": 5.0,
      "lb_bias_to_hn": true,
      "lb_bias_scale": 0.1,
      "gslb_coef": 0.02,
      "lb_l2_coef": 0.5,
      "lb_cv_coef": 0.2,
      "lb_entropy_floor_coef": 0.0,
      "lb_topk_l2_coef": 1.0,
      "lb_topk_cv_coef": 0.5,
      "no_rope_layer_interval": 0,
      "use_sliding_window": false,
      "rope_scaling": {
        "rope_type": "default",
        "factor": 1.0
      },
      "attn_implementation": "eager"
    }
  },
  "data_config": {
    "use_multi_domain": true,
    "dataset_name": "HuggingFaceTB/smoltalk",
    "max_samples": 100000,
    "max_samples_per_domain": null,
    "max_seq_length": 131072,
    "test_size": 0.1,
    "text_only": true,
    "streaming": false
  },
  "training_config": {
    "output_dir": "/mls/conan/training_logs/gramspec_small_outputs",
    "deepspeed": "sft/config/deepspeed_muon_optimizer.json",
    "num_train_epochs": 1,
    "per_device_train_batch_size": 1,
    "per_device_eval_batch_size": 1,
    "eval_accumulation_steps": 1,
    "gradient_accumulation_steps": 4,
    "learning_rate": 1e-4,
    "weight_decay": 0.01,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.1,
    "logging_steps": 1,
    "eval_steps": 10,
    "save_steps": 10,
    "save_total_limit": 2,
    "eval_strategy": "steps",
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "dataset_num_proc": 2,
    "fp16": false,
    "bf16": true,
    "bf16_full_eval": true,
    "max_length": 2048,
    "assistant_only_loss": false,
    "completion_only_loss": false,
    "activation_offloading": false,
    "packing": false,
    "padding_free": false,
    "dataloader_pin_memory": false,
    "remove_unused_columns": false,
    "dataloader_drop_last": true,
    "gradient_checkpointing": true,
    "gradient_checkpointing_kwargs": {
      "use_reentrant": false,
      "preserve_rng_state": false
    },
    "report_to": ["wandb"],
    "run_name": "gramspec-small-deepspeed-sft",
    "project": "gramspec-moe-sft",
    "seed": 4900,
    "log_on_each_node": false,
    "max_grad_norm": 1.0,
    "ddp_find_unused_parameters": true,
    "ddp_broadcast_buffers": true,
    "use_liger_kernel": false
  }
}

