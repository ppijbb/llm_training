#!/usr/bin/env python3
"""
ë©€í‹° ë„ë©”ì¸ SFT ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import logging
import traceback
from typing import Dict, Any

# Add project root directory to path for relative imports
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from transformers import AutoProcessor
from data.multi_domain_sft_dataset import (
    get_multi_domain_sft_dataset,
    DOMAIN_DATASETS,
    math_domain_dataset,
    science_domain_dataset,
    code_domain_dataset,
    puzzle_domain_dataset,
    vision_domain_dataset,
    ocr_domain_dataset,
    all_domains_dataset,
    log_memory_usage
)
from data.multi_domain_sft_dataset import create_simple_collate_fn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_tokenizer_loading():
    """í† í¬ë‚˜ì´ì € ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    logger.info("=" * 60)
    logger.info("ğŸ”§ í† í¬ë‚˜ì´ì € ë¡œë“œ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    try:
        tokenizer = AutoProcessor.from_pretrained("google/gemma-3-4b-it")
        with open("/home/conan/workspace/llm_training/sft/config/chat_template.txt", "r") as f:
            chat_template = f.read()
        if tokenizer.tokenizer.pad_token is None:
            tokenizer.tokenizer.pad_token = tokenizer.tokenizer.eos_token
        
        logger.info(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ: {tokenizer.__class__.__name__}")
        logger.info(f"   - vocab_size: {tokenizer.tokenizer.vocab_size}")
        logger.info(f"   - pad_token: {tokenizer.tokenizer.pad_token}")
        logger.info(f"   - eos_token: {tokenizer.tokenizer.eos_token}")
        
        log_memory_usage("í† í¬ë‚˜ì´ì € ë¡œë“œ í›„")
        return tokenizer
        
    except Exception as e:
        logger.error(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def test_single_domain_dataset(domain_name: str, tokenizer, max_samples: int = 50):
    """ë‹¨ì¼ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸"""
    logger.info("=" * 60)
    logger.info(f"ğŸ“¦ {domain_name.upper()} ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    domain_functions = {
        "math": math_domain_dataset,
        "science": science_domain_dataset,
        "code": code_domain_dataset,
        "puzzle": puzzle_domain_dataset,
        "vision": vision_domain_dataset,
        "ocr": ocr_domain_dataset,
        "chat": lambda tokenizer, max_samples, use_streaming: get_multi_domain_sft_dataset(
            domain_configs={"chat": DOMAIN_DATASETS["chat"]},
            tokenizer=tokenizer,
            max_samples_per_domain=max_samples,
            use_streaming=use_streaming
        ),
    }
    
    if domain_name not in domain_functions:
        logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë„ë©”ì¸: {domain_name}")
        return None
    
    try:
        log_memory_usage(f"{domain_name} ë„ë©”ì¸ ì‹œì‘")
        dataset = domain_functions[domain_name](
            tokenizer=tokenizer,
            max_samples=max_samples,
            use_streaming=True
        )
        log_memory_usage(f"{domain_name} ë„ë©”ì¸ ì™„ë£Œ")
        
        logger.info(f"âœ… {domain_name} ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ")
        
        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        if isinstance(dataset, dict):
            for split_name, split_data in dataset.items():
                if hasattr(split_data, '__len__'):
                    try:
                        length = len(split_data)
                        logger.info(f"   - {split_name}: {length} ìƒ˜í”Œ")
                    except:
                        logger.info(f"   - {split_name}: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹")
                else:
                    logger.info(f"   - {split_name}: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹")
        
        # ìƒ˜í”Œ í™•ì¸
        if 'train' in dataset:
            try:
                sample = dataset['train'][0]
                logger.info(f"   - ìƒ˜í”Œ í‚¤: {list(sample.keys())}")
                
                # messages êµ¬ì¡° í™•ì¸
                if 'messages' in sample:
                    messages = sample['messages']
                    logger.info(f"   - messages ê°œìˆ˜: {len(messages)}")
                    if len(messages) > 0:
                        logger.info(f"   - ì²« ë²ˆì§¸ message: {messages[0]}")
                
                # images í™•ì¸
                if 'images' in sample:
                    images = sample['images']
                    logger.info(f"   - images íƒ€ì…: {type(images)}")
                    if isinstance(images, list):
                        logger.info(f"   - images ê°œìˆ˜: {len(images)}")
                
                # domain í™•ì¸
                if 'domain' in sample:
                    logger.info(f"   - domain: {sample['domain']}")
                    
            except Exception as e:
                logger.warning(f"   âš ï¸ ìƒ˜í”Œ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
        
        return dataset
        
    except Exception as e:
        logger.error(f"âŒ {domain_name} ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def test_all_domains_dataset(tokenizer, max_samples_per_domain: int = 50):
    """ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸"""
    logger.info("=" * 60)
    logger.info("ğŸ“¦ ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    try:
        log_memory_usage("ì „ì²´ ë„ë©”ì¸ ì‹œì‘")
        dataset = all_domains_dataset(
            tokenizer=tokenizer,
            max_samples_per_domain=max_samples_per_domain,
            use_streaming=True
        )
        log_memory_usage("ì „ì²´ ë„ë©”ì¸ ì™„ë£Œ")
        
        logger.info(f"âœ… ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ")
        
        # ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
        if isinstance(dataset, dict):
            for split_name, split_data in dataset.items():
                if hasattr(split_data, '__len__'):
                    try:
                        length = len(split_data)
                        logger.info(f"   - {split_name}: {length} ìƒ˜í”Œ")
                    except:
                        logger.info(f"   - {split_name}: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹")
                else:
                    logger.info(f"   - {split_name}: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹")
        
        # ë„ë©”ì¸ ë¶„í¬ í™•ì¸
        if 'train' in dataset:
            try:
                domain_counts = {}
                sample_count = min(200, len(dataset['train']) if hasattr(dataset['train'], '__len__') else 200)
                
                for i in range(sample_count):
                    try:
                        sample = dataset['train'][i]
                        domain = sample.get('domain', 'unknown')
                        domain_counts[domain] = domain_counts.get(domain, 0) + 1
                    except:
                        break
                
                logger.info(f"   - ë„ë©”ì¸ ë¶„í¬ (ì²˜ìŒ {sample_count}ê°œ ìƒ˜í”Œ):")
                for domain, count in sorted(domain_counts.items()):
                    logger.info(f"     * {domain}: {count}ê°œ")
                    
            except Exception as e:
                logger.warning(f"   âš ï¸ ë„ë©”ì¸ ë¶„í¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return dataset
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def test_dataset_structure(dataset, split_name: str = 'train'):
    """ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦"""
    logger.info("=" * 60)
    logger.info(f"ğŸ” ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ ({split_name})")
    logger.info("=" * 60)
    
    if dataset is None or split_name not in dataset:
        logger.error(f"âŒ ë°ì´í„°ì…‹ ë˜ëŠ” splitì´ ì—†ìŠµë‹ˆë‹¤: {split_name}")
        return False
    
    try:
        split_data = dataset[split_name]
        
        # ìƒ˜í”Œ ê°œìˆ˜ í™•ì¸
        sample_count = 0
        try:
            if hasattr(split_data, '__len__'):
                sample_count = len(split_data)
                logger.info(f"   - ìƒ˜í”Œ ê°œìˆ˜: {sample_count}")
            else:
                logger.info(f"   - ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ (ê¸¸ì´ í™•ì¸ ë¶ˆê°€)")
        except:
            logger.info(f"   - ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ (ê¸¸ì´ í™•ì¸ ë¶ˆê°€)")
        
        # ì²« ëª‡ ê°œ ìƒ˜í”Œ ê²€ì¦
        check_count = min(5, sample_count if sample_count > 0 else 5)
        logger.info(f"   - ê²€ì¦í•  ìƒ˜í”Œ ìˆ˜: {check_count}ê°œ")
        
        valid_samples = 0
        invalid_samples = []
        
        for i in range(check_count):
            try:
                sample = split_data[i]
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                required_fields = ['messages']
                missing_fields = [field for field in required_fields if field not in sample]
                
                if missing_fields:
                    invalid_samples.append(f"ìƒ˜í”Œ {i}: í•„ìˆ˜ í•„ë“œ ëˆ„ë½ - {missing_fields}")
                    continue
                
                # messages êµ¬ì¡° í™•ì¸
                messages = sample['messages']
                if not isinstance(messages, list):
                    invalid_samples.append(f"ìƒ˜í”Œ {i}: messagesê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")
                    continue
                
                if len(messages) == 0:
                    invalid_samples.append(f"ìƒ˜í”Œ {i}: messagesê°€ ë¹„ì–´ìˆìŒ")
                    continue
                
                # ê° message êµ¬ì¡° í™•ì¸
                for j, msg in enumerate(messages):
                    if not isinstance(msg, dict):
                        invalid_samples.append(f"ìƒ˜í”Œ {i}, ë©”ì‹œì§€ {j}: dictê°€ ì•„ë‹˜")
                        continue
                    
                    if 'role' not in msg:
                        invalid_samples.append(f"ìƒ˜í”Œ {i}, ë©”ì‹œì§€ {j}: role í•„ë“œ ì—†ìŒ")
                        continue
                    
                    if 'content' not in msg:
                        invalid_samples.append(f"ìƒ˜í”Œ {i}, ë©”ì‹œì§€ {j}: content í•„ë“œ ì—†ìŒ")
                        continue
                    
                    # contentê°€ ë°°ì—´ì¸ì§€ í™•ì¸
                    content = msg['content']
                    if not isinstance(content, list):
                        invalid_samples.append(f"ìƒ˜í”Œ {i}, ë©”ì‹œì§€ {j}: contentê°€ ë°°ì—´ì´ ì•„ë‹˜ (íƒ€ì…: {type(content)})")
                        continue
                
                valid_samples += 1
                
            except Exception as e:
                invalid_samples.append(f"ìƒ˜í”Œ {i}: {str(e)}")
        
        logger.info(f"   - ìœ íš¨í•œ ìƒ˜í”Œ: {valid_samples}/{check_count}ê°œ")
        
        if invalid_samples:
            logger.warning(f"   - ë¬¸ì œê°€ ìˆëŠ” ìƒ˜í”Œ:")
            for issue in invalid_samples[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
                logger.warning(f"     * {issue}")
            if len(invalid_samples) > 10:
                logger.warning(f"     ... ë° {len(invalid_samples) - 10}ê°œ ë”")
        
        return valid_samples == check_count
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False


def test_collate_function(tokenizer, dataset, model_name: str = "google/gemma-2b-it"):
    """Collate í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    logger.info("=" * 60)
    logger.info("ğŸ”§ Collate í•¨ìˆ˜ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    try:
        # Processor ìƒì„± (multi-domainìš©, allow_text_only=True)
        try:
            processor = AutoProcessor.from_pretrained(model_name)
        except Exception as e:
            logger.warning(f"   âš ï¸ AutoProcessor ë¡œë“œ ì‹¤íŒ¨, tokenizerë¥¼ processorë¡œ ì‚¬ìš©: {e}")
            # Processorê°€ ì—†ìœ¼ë©´ tokenizerë¥¼ processorë¡œ ì‚¬ìš© (ì¼ë¶€ ëª¨ë¸ì€ tokenizerë§Œ ìˆìŒ)
            processor = tokenizer
        
        collate_fn = create_simple_collate_fn(processor, max_length=2048, allow_text_only=True)
        logger.info(f"âœ… Collate í•¨ìˆ˜ ìƒì„± ì„±ê³µ (allow_text_only=True)")
        
        if dataset is None or 'train' not in dataset:
            logger.warning("   âš ï¸ ë°ì´í„°ì…‹ì´ ì—†ì–´ collate í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            return False
        
        # ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
        try:
            samples = []
            for i in range(min(2, len(dataset['train']) if hasattr(dataset['train'], '__len__') else 2)):
                try:
                    sample = dataset['train'][i]
                    samples.append(sample)
                except:
                    break
            
            if not samples:
                logger.warning("   âš ï¸ ìƒ˜í”Œì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ collate í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                return False
            
            logger.info(f"   - í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ìˆ˜: {len(samples)}ê°œ")
            
            # Collate ì‹¤í–‰
            batch = collate_fn(samples)
            logger.info(f"âœ… Collate ì‹¤í–‰ ì„±ê³µ")
            logger.info(f"   - ë°°ì¹˜ í‚¤: {list(batch.keys())}")
            
            for key, value in batch.items():
                if isinstance(value, (list, tuple)):
                    logger.info(f"   - {key}: {type(value).__name__} (ê¸¸ì´: {len(value)})")
                elif hasattr(value, 'shape'):
                    logger.info(f"   - {key}: {type(value).__name__} (shape: {value.shape})")
                else:
                    logger.info(f"   - {key}: {type(value).__name__}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Collate ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
        
    except Exception as e:
        logger.error(f"âŒ Collate í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    log_memory_usage("í”„ë¡œê·¸ë¨ ì‹œì‘")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = test_tokenizer_loading()
    if tokenizer is None:
        logger.error("âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
        return
    
    # ê° ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸
    test_results = {}
    for domain_name in [k for k in DOMAIN_DATASETS.keys() if k == "ocr"]:
        dataset = test_single_domain_dataset(domain_name, tokenizer, max_samples=20)
        if dataset is not None:
            test_results[domain_name] = test_dataset_structure(dataset, 'train')
        else:
            test_results[domain_name] = False
    
    # ì „ì²´ ë„ë©”ì¸ í…ŒìŠ¤íŠ¸
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š ì „ì²´ ë„ë©”ì¸ í†µí•© í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)
    
    all_dataset = test_all_domains_dataset(tokenizer, max_samples_per_domain=20)
    if all_dataset is not None:
        structure_valid = test_dataset_structure(all_dataset, 'train')
        collate_valid = test_collate_function(tokenizer, all_dataset)
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        
        logger.info("ë„ë©”ì¸ë³„ í…ŒìŠ¤íŠ¸:")
        for domain, result in test_results.items():
            status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
            logger.info(f"   - {domain}: {status}")
        
        logger.info("\nì „ì²´ ë„ë©”ì¸ í…ŒìŠ¤íŠ¸:")
        logger.info(f"   - êµ¬ì¡° ê²€ì¦: {'âœ… ì„±ê³µ' if structure_valid else 'âŒ ì‹¤íŒ¨'}")
        logger.info(f"   - Collate í•¨ìˆ˜: {'âœ… ì„±ê³µ' if collate_valid else 'âŒ ì‹¤íŒ¨'}")
    else:
        logger.error("âŒ ì „ì²´ ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    log_memory_usage("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    logger.info("âœ… ë©€í‹° ë„ë©”ì¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        traceback.print_exc()
        sys.exit(1)

